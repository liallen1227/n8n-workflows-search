title,link,description
"Generate AI Videos with Google Veo3, Save to Google Drive and Upload to YouTube",https://n8n.io/workflows/4846-generate-ai-videos-with-google-veo3-save-to-google-drive-and-upload-to-youtube/,"This workflow allows users to generate AI videos using Google Veo3, save them to Google Drive, generate optimized YouTube titles with GPT-4o, and automatically upload them to YouTube . The entire process is triggered from a Google Sheet that acts as the central interface for input and output.
IT automates video creation, uploading, and tracking, ensuring seamless integration between Google Sheets, Google Drive, Google Veo3, and YouTube.
Benefits of this Workflow
üí° No Code Interface: Trigger and control the video production pipeline from a simple Google Sheet.
‚öôÔ∏è Full Automation: Once set up, the entire video generation and publishing process runs hands-free.
üß† AI-Powered Creativity:
Generates engaging YouTube titles using GPT-4o.
Leverages advanced generative video AI from Google Veo3.
üìÅ Cloud Storage & Backup: Stores all generated videos on Google Drive for safekeeping.
üìà YouTube Ready: Automatically uploads to YouTube with correct metadata, saving time and boosting visibility.
üß™ Scalable: Designed to process multiple video prompts by looping through new entries in Google Sheets.
üîí API-First: Utilizes secure API-based communication for all services.
How It Works
Trigger: The workflow can be started manually (""When clicking ‚ÄòTest workflow‚Äô"") or scheduled (""Schedule Trigger"") to run at regular intervals (e.g., every 5 minutes).
Fetch Data: The ""Get new video"" node retrieves unfilled video requests from a Google Sheet (rows where the ""VIDEO"" column is empty).
Video Creation:
The ""Set data"" node formats the prompt and duration from the Google Sheet.
The ""Create Video"" node sends a request to the Fal.run API (Google Veo3) to generate a video based on the prompt.
Status Check:
The ""Wait 60 sec."" node pauses execution for 60 seconds.
The ""Get status"" node checks the video generation status. If the status is ""COMPLETED,"" the workflow proceeds; otherwise, it waits again.
Video Processing:
The ""Get Url Video"" node fetches the video URL.
The ""Generate title"" node uses OpenAI (GPT-4.1) to create an SEO-optimized YouTube title.
The ""Get File Video"" node downloads the video file.
Upload & Update:
The ""Upload Video"" node saves the video to Google Drive.
The ""HTTP Request"" node uploads the video to YouTube via the Upload-Post API.
The ""Update Youtube URL"" and ""Update result"" nodes update the Google Sheet with the video URL and YouTube link.
Set Up Steps
Google Sheet Setup:
Create a Google Sheet with columns: PROMPT, DURATION, VIDEO, and YOUTUBE_URL.
Share the Sheet link in the ""Get new video"" node.
API Keys:
Obtain a Fal.run API key (for Veo3) and set it in the ""Create Video"" node (Header: Authorization: Key YOURAPIKEY).
Get an Upload-Post API key (for YouTube uploads) and configure the ""HTTP Request"" node (Header: Authorization: Apikey YOUR_API_KEY).
YouTube Upload Configuration:
Replace YOUR_USERNAME in the ""HTTP Request"" node with your Upload-Post profile name.
Schedule Trigger:
Configure the ""Schedule Trigger"" node to run periodically (e.g., every 5 minutes).
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"AI-Powered WhatsApp Chatbot for Text, Voice, Images, and PDF with RAG",https://n8n.io/workflows/4827-ai-powered-whatsapp-chatbot-for-text-voice-images-and-pdf-with-rag/,"Who is this for?
This template is designed for internal support teams, product specialists, and knowledge managers in technology companies who want to automate ingestion of product documentation and enable AI-driven, retrieval-augmented question answering via WhatsApp.
What problem is this workflow solving?
Support agents often spend too much time manually searching through lengthy documentation, leading to inconsistent or delayed answers. This solution automates importing, chunking, and indexing product manuals, then uses retrieval-augmented generation (RAG) to answer user queries accurately and quickly with AI via WhatsApp messaging.
What these workflows do
Workflow 1: Document Ingestion & Indexing
Manually triggered to import product documentation from Google Docs.
Automatically splits large documents into chunks for efficient searching.
Generates vector embeddings for each chunk using OpenAI embeddings.
Inserts the embedded chunks and metadata into a MongoDB Atlas vector store, enabling fast semantic search.
Workflow 2: AI-Powered Query & Response via WhatsApp
Listens for incoming WhatsApp user messages, supporting various types:
Text messages: Plain text queries from users.
Audio messages: Voice notes transcribed into text for processing.
Image messages: Photos or screenshots analyzed to provide contextual answers.
Document messages: PDFs, spreadsheets, or other files parsed for relevant content.
Converts incoming queries to vector embeddings and performs similarity search on the MongoDB vector store.
Uses OpenAI‚Äôs GPT-4o-mini model with retrieval-augmented generation to produce concise, context-aware answers.
Maintains conversation context across multiple turns using a memory buffer node.
Routes different message types to appropriate processing nodes to maximize answer quality.
Setup
Setting up vector embeddings
Authenticate Google Docs and connect your Google Docs URL containing the product documentation you want to index.
Authenticate MongoDB Atlas and connect the collection where you want to store the vector embeddings. Create a search index on this collection to support vector similarity queries.
Ensure the index name matches the one configured in n8n (data_index).
See the example MongoDB search index template below for reference.
Setting up chat
Authenticate the WhatsApp node with your Meta account credentials to enable message receiving and sending.
Connect the MongoDB collection containing embedded product documentation to the MongoDB Vector Search node used for similarity queries.
Set up the system prompt in the Knowledge Base Agent node to reflect your company‚Äôs tone, answering style, and any business rules, ensuring it references the connected MongoDB collection for context retrieval.
Make sure
Both MongoDB nodes (in ingestion and chat workflows) are connected to the same collection with:
An embedding field storing vector data,
Relevant metadata fields (e.g., document ID, source), and
The same vector index name configured (e.g., data_index).
Search Index Example:
{
""mappings"": {
""dynamic"": false,
""fields"": {
""_id"": { ""type"": ""string"" },
""text"": { ""type"": ""string"" },
""embedding"": {
""type"": ""knnVector"",
""dimensions"": 1536,
""similarity"": ""cosine""
},
""source"": { ""type"": ""string"" },
""doc_id"": { ""type"": ""string"" }
}
}
}"
Gmail AI Email Manager,https://n8n.io/workflows/4722-gmail-ai-email-manager/,"Email Manager - Intelligent Gmail Classification
This automation flow is designed to automatically monitor incoming Gmail messages, analyze their content and context using AI, and intelligently classify them with appropriate labels for better email organization and prioritization.
‚öôÔ∏è How It Works (Step-by-Step):
üìß Gmail Monitoring (Trigger)
Continuously monitors your Gmail inbox:
Polls for new emails every minute
Captures all incoming messages automatically
Triggers workflow for each new email received
üìñ Email Content Extraction
Retrieves complete email details:
Full email body and headers
Sender information and recipient lists
Subject line and metadata
Existing Gmail labels and categories
Email threading information (replies/forwards)
üîç Email History Analysis
AI agent checks relationship context:
Searches for previous emails from the same sender
Checks sent folder for prior outbound correspondence
Determines if this is a first-time contact (cold email)
Analyzes conversation thread history
ü§ñ Intelligent Classification Agent
Advanced AI categorization using:
Claude Sonnet 4 for sophisticated email analysis
Context-aware classification based on email history
Content analysis for intent and urgency detection
Header analysis for automated vs. human-sent emails
üè∑Ô∏è Smart Label Assignment
Automatically applies appropriate Gmail labels:
To Respond: Requires direct action/reply
FYI: For awareness, no action needed
Notification: Service updates, policy changes
Marketing: Promotional content and sales pitches
Meeting Update: Calendar-related communications
Comment: Document/task feedback
üìã Structured Processing
Ensures consistent labeling:
Uses structured output parsing for reliability
Returns specific Label ID for Gmail integration
Applies label automatically to the email
Maintains classification accuracy
üõ†Ô∏è Tools Used:
n8n: Workflow automation platform
Gmail API: Email monitoring and label management
Anthropic Claude: Advanced email content analysis
Gmail Tools: Email history checking and search
Structured Output Parser: Consistent AI responses
üì¶ Key Features:
Real-time email monitoring and classification
Context-aware analysis using email history
Intelligent cold vs. warm email detection
Multiple classification categories for organization
Automatic Gmail label application
Header analysis for automated email detection
Thread-aware conversation tracking
üöÄ Ideal Use Cases:
Busy executives managing high email volumes
Sales professionals prioritizing prospect communications
Support teams organizing customer inquiries
Marketing teams filtering promotional content
Anyone wanting automated email organization
Teams needing consistent email prioritization"
üöÄ Transform Podcasts into Viral TikTok Clips with Gemini AI & Auto-Posting ‚úÖ,https://n8n.io/workflows/4568-transform-podcasts-into-viral-tiktok-clips-with-gemini-ai-and-auto-posting/,"üéØ Automatically Create and Post Engaging TikTok Clips (with Audience Retention Videos) from Podcasts Using AI! üöÄ
Effortlessly transform long-form podcast content into highly engaging, viral-ready TikTok clips with this end-to-end automation template.
Designed for content creators already monetizing on TikTok and those looking to start earning from the platform, this workflow streamlines the process of extracting highlights, editing clips, and posting to TikTok, allowing you to maximize reach while minimizing manual effort.
Key Features
üîπ AI-Powered Podcast Highlight Extraction
Automatically identifies the best moments from any podcast video, ensuring each clip is engaging and shareable.
üîπ Smart Video Editing & Captioning
Combines podcast highlights with a copyright-free attention retainer video (e.g., Minecraft parkour, GTA 5 gameplay) for increased audience retention. Auto-generated captions make clips more dynamic and accessible.
üîπ Automated Title Generation
A Large Language Model (LLM) analyzes the clips to generate compelling titles, optimized for TikTok‚Äôs algorithm.
üîπ Hands-Free TikTok Posting
Seamlessly schedules and automatically posts clips to your TikTok account at defined intervals, keeping your audience engaged without manual uploads.
üîπ Fully Automated Workflow
From video download to content publishing 100% FREE, this template eliminates the need for time-consuming video editing, helping you scale your content strategy effortlessly, without having to pay for multiple subscriptions tediously.
Simply find a podcast you like and a cool Minecraft parkour (or any engaging) video, send their YouTube URLs, and let the automation handle everything‚Äîfrom video downloading and audio processing to highlight extraction, editing, captions, and publishing.
How It Works (Step-by-Step Guide)
1Ô∏è‚É£ Provide the YouTube URLs
One for the main podcast video (where highlights will be extracted).
One for the background attention retainer video (e.g., Minecraft parkour, GTA 5 gameplay).
2Ô∏è‚É£ Automation Downloads and Processes the Videos
Downloads both videos.
Extracts audio from the podcast for analysis.
3Ô∏è‚É£ AI Analyzes and Extracts Key Highlights
Detects the most engaging moments from the podcast.
4Ô∏è‚É£ Creates Fully Edited Clips
Merges podcast highlights with the attention retainer video.
Generates captions automatically.
5Ô∏è‚É£ Optimizes for TikTok
Uses AI to generate a compelling title for each clip.
6Ô∏è‚É£ Posts to TikTok Automatically
Uploads clips at your preferred intervals with zero manual effort.
Who Is This For?
‚úÖ Content creators already making money on TikTok
‚úÖ People looking to start earning with TikTok
‚úÖ Podcasters wanting to repurpose content into bite-sized, viral clips
Get Started Today! üöÄ
This AI-driven automation is perfect for scaling your TikTok content effortlessly.
To use this workflow, you‚Äôll just need free accounts on Assembly, Andynocode, and Upload-Posts."
"Automated Invoice Processing with Telegram, GPT-4o, OCR and SAP Integration",https://n8n.io/workflows/4849-automated-invoice-processing-with-telegram-gpt-4o-ocr-and-sap-integration/,"++HOW IT WORKS:++
This workflow automates the processing of invoices sent via Telegram. It extracts the data using LlamaIndex OCR, logs it in Google Sheets, and optionally pushes the structured data to SAP Business One
üîπ 1. Receive Invoice via Telegram:
A user sends a PDF of an invoice through Telegram
A Telegram Trigger node listens for incoming messages and captures the file and metadata
The document is downloaded and prepared for OCR
üîπ 2. OCR with LlamaIndex:
The file is uploaded to the LlamaIndex OCR API.
The workflow polls the API until the processing status returns SUCCESS
Once ready, the parsed content is fetched in Markdown format
üîπ 3. Data Extraction via LLM (editable):
The Markdown content is sent to a language model (LLM) using LangChain
A Structured Output Parser transforms the result into a clean, structured editable JSON
üîπ 4. Save to Google Sheets:
The structured JSON is split into:
Header (main invoice metadata)
Detail (individual line items)
Each part is stored in a dedicated tab within a connected Google Sheets file
üîπ 5. Ask for SAP Confirmation:
The bot replies to the user via Telegram:
""Do you want to send the data to SAP?""
If the user clicks ""Yes"", the next automation path is triggered.
üîπ 6. Push Data to SAP B1:
A connection is made to SAP Business One's Service Layer API
Header and detail data are fetched from Google Sheets
The invoice structure is rebuilt as required by SAP (DocumentLines, CardCode, etc.)
A POST request creates the Purchase Invoice in SAP
A confirmation message with the created DocEntry is sent back to the user on Telegram
++SET UP STEPS:++
Follow these steps to properly configure the workflow before execution:
1Ô∏è‚É£ Create Required Credentials:
Go to Credentials > + New Credential and create the following:
Telegram API (set your bot token get it from BotFather)
Google Sheets
OpenAI
2Ô∏è‚É£ Set Up Environment Variables (Optional but Recommended):
LLAMAINDEX_API_KEY
SAP_USER
SAP_PASSWORD
SAP_COMPANY_DB
SAP_URL
3Ô∏è‚É£ Prepare Google Sheets:
Ensure your Google Spreadsheet has the following:
‚û§ Sheet 1: Header
‚û§ Sheet 2: Details
Contains columns for invoice lines"
AI Personal Assistant,https://n8n.io/workflows/4723-ai-personal-assistant/,"Email Personal Assistant - Comprehensive Communication Manager
This automation flow is designed to proactively monitor email, calendar, and Slack communications, analyze priorities across all channels, and generate a comprehensive daily briefing with actionable tasks for executive productivity management.
‚öôÔ∏è How It Works (Step-by-Step):
‚è∞ Automated Daily Trigger
Runs automatically on weekdays:
Scheduled execution every weekday at 8:00 AM
Manual trigger available for on-demand analysis
Comprehensive daily communication audit
üìß Email Assistant Agent
Analyzes inbox priorities and context:
Scans unread emails across ""To Respond"" and ""FYI"" labels
Checks email history to determine relationship context
Identifies *company-related opportunities and partnerships
Categorizes emails by urgency (High, Medium, Low)
Cross-references with sent emails for follow-up context
üìÖ Follow-Up Assistant Agent
Monitors meeting follow-up requirements:
Reviews last 3 days of calendar meetings
Fetches Fireflies transcripts for recorded sessions
Identifies meetings without post-meeting communication
Flags meetings requiring action items or follow-ups
Checks sent emails and Slack for completed follow-ups
üí¨ Slack Assistant Agent
Tracks Slack communication priorities:
Monitors direct messages and @mentions
Identifies unreplied Slack conversations
Cross-references with email and calendar context
Prioritizes responses based on sender importance
Checks for threaded conversations requiring attention
üéØ Master Orchestrator Agent
Synthesizes all communication data:
Combines reports from all three assistant agents
Cross-references with existing Google Sheets to-do list
Prioritizes tasks by urgency and business impact
Identifies correlations between different communication channels
Creates comprehensive daily action plan
üìä Task Management Integration
Automated tracking and delivery:
Appends new tasks to Google Sheets to-do tracker
Sends personalized daily briefing via Slack DM
Maintains conversation memory for context continuity
Tracks outstanding vs. completed items
üõ†Ô∏è Tools Used:
n8n: Workflow orchestration and scheduling
Claude Sonnet 4 & Opus 4: Multi-agent AI analysis
Gmail API: Email monitoring and history checking
Google Calendar: Meeting tracking and scheduling
Slack API: Message monitoring and user management
Fireflies API: Meeting transcript analysis
Google Sheets: Task tracking and persistence
üì¶ Key Features:
Multi-channel communication monitoring (Email, Calendar, Slack)
AI-powered priority assessment and context analysis
Cross-platform relationship tracking and history
Automated daily briefing generation and delivery
Persistent task tracking with Google Sheets integration
Meeting follow-up verification and flagging
Conversation memory for continuity across sessions
üöÄ Ideal Use Cases:
C-level executives managing multiple communication channels
Sales leaders tracking prospect interactions and follow-ups
Business development professionals managing partnerships
Busy professionals needing communication prioritization
Teams requiring systematic follow-up management
Anyone wanting automated daily productivity briefings"
Auto-Create Podcast from YouTube Transcript using Dumpling AI and GPT-4o,https://n8n.io/workflows/4887-auto-create-podcast-from-youtube-transcript-using-dumpling-ai-and-gpt-4o/,"üîé Who is this for?
This workflow is designed for podcast creators, content marketers, and video producers who want to convert YouTube videos into podcast-ready scripts. It's perfect for anyone repurposing long-form content to reach audio-first audiences without manual effort.
üß† What problem is this workflow solving?
Creating podcast scripts from YouTube videos manually is time-consuming. This workflow automates the process by pulling transcripts, cleaning the text, organizing the dialogue, summarizing the key points, and saving everything in one place. It removes the need for manual transcription, formatting, and structuring.
‚öôÔ∏è What this workflow does
This workflow uses Dumpling AI and GPT-4o to automate the transformation of YouTube video transcripts into polished podcast scripts. Here's how it works:
RSS Feed Trigger
Monitors a YouTube RSS feed for new video uploads. When a new video is detected, the workflow begins automatically.
Get YouTube Transcript (Dumpling AI)
Uses Dumpling AI's get-youtube-transcript endpoint to extract the full transcript from the video URL.
Generate Podcast Script with GPT-4o
GPT-4o receives the transcript and generates a structured JSON output including:
Cleaned transcript with filler words removed
Speaker labels for clarity
A short, engaging podcast title
A concise summary of the episode
Save to Airtable
The structured data (title, summary, cleaned transcript) is saved to Airtable for easy review, editing, or publishing.
This automation is an ideal workflow for repurposing video content into audio-friendly formats, cutting down production time while increasing content output across platforms."
Turn Google Sheets Ideas into AI Videos with GPT-4o and Fal.AI Veo 3,https://n8n.io/workflows/4881-turn-google-sheets-ideas-into-ai-videos-with-gpt-4o-and-falai-veo-3/,"Turn Your Ideas into Videos‚ÄîRight from Google Sheets!
This workflow helps you make cool 8-second videos using Fal.AI and Veo 3, just by typing your idea into a Google Sheet. You can even choose if you want your video to have sound or not. It‚Äôs super easy‚Äîno tech skills needed!
Why use this?
Just type your idea in a sheet‚Äîno fancy tools or uploads.
Get a video link back in the same sheet.
Works with or without sound‚Äîyour choice!
How does it work?
You write your idea, pick the video shape, and say if you want sound (true or false) in the Google Sheet.
n8n reads your idea and asks Fal.AI to make your video.
When your video is ready, the link shows up in your sheet.
What do you need?
A Google account and Google Sheets connected with service account (check this link for reference)
A copy of the following Google Spreadsheet:
Spreadsheet to copy
An OpenAI API key
A Fal.AI account with some money in it
That‚Äôs it! Just add your ideas and let the workflow make the videos for you. Have fun creating!
if you have any questions, just contact me at max@nervoai.com"
Deep Research - Sales Lead Magnet Agent,https://n8n.io/workflows/4721-deep-research-sales-lead-magnet-agent/,"This automation flow is designed to generate comprehensive, research-backed lead magnet articles based on a user-submitted topic, conduct deep research across multiple sources, and automatically create a professional Google Doc ready for LinkedIn sharing.
‚öôÔ∏è How It Works (Step-by-Step):
üìù Chat Input (Entry Point)
A user submits a topic through the chat interface:
Topic for lead magnet content
Target audience (automatically detected)
Company context (when relevant)
üîç Query Builder Agent
An AI agent refines the input by:
Converting the topic into 5 targeted research queries
Determining if topic relates to *company for specialized research
Using structured output parsing for consistent results
üìö Research Leader Agent
Conducts comprehensive research that:
Uses Perplexity API for real-time web research
Integrates *company knowledge base when relevant
Creates detailed table of contents with research insights
Identifies key trends, expert opinions, and case studies
üìã Project Planner Agent
Structures the content by:
Generating professional title and subtitle
Creating 8-10 logical chapter outlines
Developing detailed writing prompts for each section
Ensuring step-by-step actionable guidance
‚úçÔ∏è Research Assistant Team
Multiple AI agents write simultaneously:
Each agent writes one chapter with proper citations
Maintains consistent voice across all sections
Includes real-world examples and implementation steps
Uses both web research and *company knowledge
üìù Editor Agent
Professional content polishing:
Refines tone for authenticity and engagement
Adds image placeholders where appropriate
Ensures proper flow between chapters
Optimizes for LinkedIn lead magnet format
üìÑ Google Docs Creation
Automated document generation:
Creates new Google Doc with formatted content
Sets proper sharing permissions (public link)
Organizes in designated company folder
Returns shareable URL for immediate use
üõ†Ô∏è Tools Used:
n8n: Workflow orchestration platform
Anthropic Claude: Primary AI model for content generation
OpenRouter: Backup AI model options
Perplexity API: Real-time research capabilities
*Company Knowledge Hub: Internal documentation access
Google Docs API: Document creation and formatting
Google Drive API: File management and sharing
üì¶ Key Features:
End-to-end automation from topic to published document
Multi-agent approach ensures comprehensive coverage
Real-time research with proper citations
Company-specific knowledge integration
Professional editing and formatting
Automatic Google Docs creation with sharing
Scalable content generation (3-5 minutes per article)
üöÄ Ideal Use Cases:
B2B companies building thought leadership content
Sales teams creating industry-specific lead magnets
Marketing departments scaling content production
Consultants developing expertise-demonstrating resources
SaaS companies creating feature-focused educational content
Startups establishing market presence without content teams"
ü§ñ AI content generation for Auto Service üöò Automate your social mediaüì≤!,https://n8n.io/workflows/4600-ai-content-generation-for-auto-service-automate-your-social-media/,"Who Is This For?
üöòThis workflow is designed for auto service / car repair businesses looking to streamline their social media marketing with Ai tools and automation via n8n.
Whether you‚Äôre a small garage owner, a car repair shop, an automotive specialist or automechanic - this tool helps you maintain an active online presence without spending hours creating content.
üí™üèº Though this template is set up for Auto Service daily content uploads, but the underlying logic is universal. You can easily adapt it for any niche by editing prompts, adding nodes, and creating or uploading a variety of content to any platform. You can use any LLM and generative AI of your choice. Personally, I prefer the smooth and effective results from ChatGPT 4.1 combined with GPT Image 1. But you can generate videos instead of images for your posts as well üòâ
What Problem Is This Workflow Solving?
ü§¶‚Äç‚ôÇÔ∏è Many auto service businesses struggle with consistently posting engaging content due to time constraints or lack of marketing expertise.
üíéThis workflow solves that by automating the content creation and posting process, ensuring your social media channels stay fresh and active, ultimately attracting more customers.
What This Workflow Does:
Generates daily social media posts tailored specifically to the auto service niche using AI.
Allows easy customization of post and image prompts.
Integrates research links through the Tavily Internet Search tool for relevant content.
Supports starting posts based on reference article links via Google Sheets.
Automatically publishes posts to your connected social media platforms.
Enables scheduled or trigger-based posting for maximum flexibility.
How It Works?
Easy, actually ‚ò∫Ô∏è
AI creates daily social media content made just for Auto Service. You can simply edit prompts for both posts and images, set up news or articles research links via the Tavily Internet Search tool. You can also start the workflow with a reference article link through Google Sheets.
üéØ Consistently posting relevant and actual niche content helps attract new customers, all while leveraging AI and n8n tools to keep the process seamless and cost-effective.
Set Up Steps:
I kept it quick and simple for you ‚ú®
If you‚Äôre happy with the current LLM and image model configurations, just connect your OpenAI API credentials to enable AI content generation.
Link your social media accounts (Facebook, Telegram, X, etc.) for autoposting.
Optionally connect Google Sheets if you want to trigger posts based on sheet updates with reference links.
Customize prompts as needed to match your brand voice, style and marketing tasks.
Choose between:
Scheduled automatic generation and posting at the same time as socials algorythms like it.
Google Sheets trigger with reference.
Manual start.
How to Customize This Workflow to Your Needs?
Switch AI models and edit prompts to better reflect your specific services or promotions.
Add or modify research links in Tavily to keep your posts timely and relevant.
Adjust posting schedules to match peak engagement times for your audience.
Expand or reduce the number of social platforms integrated depending on your marketing strategy.
Use Google Sheets to batch upload ideas or curate specific content topics.
After adjusting a few settings, activate the workflow and let it run.
ü§ñ The system will then automatically publish your content across your selected social platforms ‚Äî saving you time and effort.
üìå You‚Äôll find more detailed tips and additional ai models for customizing ai generation process inside the workflow via sticky notes.
üí¨ Need help? For additional guidance, feel free to message me ‚Äî here‚Äôs my profile in the n8n community for direct contact üëà click!"
Build a Voice AI Chatbot with ElevenLabs and InfraNodus Knowledge Experts,https://n8n.io/workflows/4484-build-a-voice-ai-chatbot-with-elevenlabs-and-infranodus-knowledge-experts/,"Set Up ElevenLabs Voice Chat Agent using Graph RAG Knowledge Graphs as Experts
This workflow creates an AI voice chatbot agent that has access to several knowledge bases at the same time (used as ""experts"").
These knowledge bases are provided using the InfraNodus GraphRAG using the knowledge graphs and providing high-quality responses without the need to set up complex RAG vector store workflows.
We use ElevenLabs to set up a voice agent that can be embedded to any website or used via their API.
The advantages of using GraphRAG instead of the standard vector stores for knowledge are:
Easy and quick to set up (no complex data import workflows needed) and to update with new knowledge
A knowledge graph has a holistic overview of your knowledge base
Better retrieval of relations between the document chunks = higher quality responses
Ability to reuse in other n8n workflows
How it works
This template uses the n8n AI agent node as an orchestrating agent that decides which tool (knowledge graph) to use based on the user's prompt.
The user's prompt is received from the ElevenLabs Conversational AI agent via an n8n Webhook, which also takes care of the voice interaction.
The response from n8n is then sent to the Webhook, which is polled by the ElevenLabs voice agent. This agent processes the response and provides the final answer.
Here's a description step by step:
The user submits a question using ElevenLabs voice interface
The question is sent via the knowledge_base tool in ElevenLabs to the n8n Webhook with the POST request containing the user's prompt and sessionID for Chat Memory node in n8n.
The n8n AI agent node checks a list of tools it has access to. Each tool has a description of the knowledge auto-generated by InfraNodus (we call each tool an ""expert"").
The n8n AI agent decides which tool should be used to generate a response. It may reformulate user's query to be more suitable for the expert.
The query is then sent to the InfraNodus HTTP node endpoint, which will query the graph that corresponds to that expert.
Each InfraNodus GraphRAG expert provides a rich response that takes the whole context into account and provides a response from each expert (graph) along with a list of relevant statements retrieved using a combination or RAG and GraphRAG.
The n8n AI Agent node integrates the responses received from the experts to produce the final answer.
The final answer is sent back to the Webhook endpoint
ElevenLabs conversational AI agent picks up the response arriving from the knowledge_base tool via the webhook and then condenses it for conversational format and transforms text into voice.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Create a separate knowledge graph for each expert (using PDF / content import options) in InfraNodus
For each graph, go to the workflow, paste the name of the graph into the body name field.
Keep other settings intact or learn more about them at the InfraNodus access points page.
Once you add one or more graphs as experts to your flow, add the LLM key to the OpenAI node and launch the workflow
You will also need to set up an ElevenLabs account and to set up a conversational AI agent there. See the Post note in the n8n workflow for a complete step-by-step description or our support article on setting up ElevenLabs AI voice agent
Once the voice AI agent is ready, you might want to combine it with a text AI chatbot workflow so your users have a choice between the text and voice interaction. In that case, you may be interested to use our free open-source website popup chat widget popupchat.dev where you can create an embed code to add to your blog or website and allow the user to choose between the text and voice interaction.
Requirements
An InfraNodus account and API key
An OpenAI (or any other LLM) API key
An ElevenLabs account
FAQ
1. How many ""experts"" should I aim for?
We recommend to aim for the number of experts as the optimal number of people in a team, which is usually 2-7. If you add more experts, your AI orchestrating agent will have troubles choosing the most suitable ""expert"" tool for the user's query. You can mitigate this by specifying in the AI agent description that it can choose maximum 3-7 experts to provide a response.
2. Why use InfraNodus GraphRAG and not standard vector store for knowledge?
First, vector stores are complex to set up and to update. You'd need a separate workflow for that, decide on the vector dimensions, add metadata to your knowledge, etc.
With InfraNodus, you have a complete RAG / GraphRAG solution under the hood that is easy to set up and provides high-quality responses that takes the overall structure and the relations between your ideas into account.
3 Why not use ElevenLabs' own knowledge?
One of the reasons is that you want your knowledge base to be in one place so you can reuse it in other n8n workflows. Another reason is that you will not have such a good separation between the ""experts"" when you converse with the agent. So the answers you get will be based on top matches from all the books / articles you upload, while with the InfraNodus GraphRAG setup you can better control which graphs are consulted as experts and have an explicit way to display this data.
Customizing this workflow
You can use this same workflow with a Telegram bot, so you can interact with it using Telegram. There are many more customizations available on our GitHub repo for n8n workflows.
Check out the complete setup guide for this workflow at https://support.noduslabs.com/hc/en-us/articles/20318967066396-How-to-Build-a-Text-Voice-AI-Agent-Chatbot-with-n8n-Elevenlabs-and-InfraNodus
Also check out the video tutorial with a demo:"
Clone Viral TikToks with AI Avatars & Auto-Post to 9 Platforms using Perplexity & Blotato,https://n8n.io/workflows/4110-clone-viral-tiktoks-with-ai-avatars-and-auto-post-to-9-platforms-using-perplexity-and-blotato/,"Clone a viral TikTok with AI and auto-post it to 9 platforms using Perplexity & Blotato
Who is this for?
This workflow is perfect for:
Content creators looking to repurpose viral content
Social media managers who want to scale short-form content across multiple platforms
Entrepreneurs and marketers aiming to save time and boost visibility with AI-powered automation
What problem is this workflow solving?
Reproducing viral video formats with your own branding and pushing them to multiple platforms is time-consuming and hard to scale. This workflow solves that by:
Cloning a viral TikTok video‚Äôs structure
Generating a new version with your avatar
Rewriting the script, caption, and overlay text
Auto-posting it to 9 social media platforms ‚Äî without manual uploads
What this workflow does
From a simple Telegram message with a TikTok link, the workflow:
Downloads a TikTok video and extracts its thumbnail, audio, and caption
Transcribes the audio and saves original text into Google Sheets
Uses Perplexity AI to suggest a new content idea in the same niche
Rewrites the script, caption, and overlay using GPT-4o
Generates a new video with your avatar using Captions.ai
Adds subtitles and overlay text with JSON2Video
Saves metadata to Google Sheets for tracking
Sends the final video to Telegram for preview
Auto-publishes the video to Instagram, YouTube, TikTok, Facebook, LinkedIn, Threads, X (Twitter), Pinterest, and Bluesky via Blotato
Setup
Connect your Telegram bot to the trigger node.
Add your OpenAI, Perplexity, Cloudinary, Captions.ai, and Blotato API keys.
Make sure your Google Sheet is ready with the appropriate columns.
Replace the default avatar name in the Captions.ai node with yours.
Fill in your social media account IDs in the ""Assign Platform IDs"" node.
Test by sending a TikTok URL to your Telegram bot.
How to customize this workflow to your needs
Change avatar output style: adjust resolution, voice, or avatar ID.
Refine script structure: tweak GPT instructions for different tone/format.
Swap Perplexity with ChatGPT or Claude if needed.
Filter by platform: disable any Blotato nodes you don‚Äôt need.
Add approval step: insert a Telegram confirmation node before publishing.
Adjust subtitle style or overlay text font in JSON2Video.
üìÑ Documentation: Notion Guide
Need help customizing?
Contact me for consulting and support : Linkedin / Youtube"
AI-Powered Multi-Social Media Post Automation: Google Trends & Perplexity AI,https://n8n.io/workflows/4352-ai-powered-multi-social-media-post-automation-google-trends-and-perplexity-ai/,"Overview
This comprehensive n8n workflow automatically transforms trending Google search queries into engaging LinkedIn posts using AI. The system runs autonomously, discovering viral topics, researching content, and publishing professionally formatted posts to grow your social media presence.
Workflow Description
Automate your entire social media content pipeline - from trend discovery to publication. This workflow monitors Google Trends, selects high-potential topics, creates human-like content using advanced AI, and publishes across multiple social platforms with built-in tracking.
Key Features
Automated Trend Discovery: Pulls trending topics from Google Trends API with customizable filters
Intelligent Topic Selection: AI chooses the most relevant trending topic for your niche
Multi-AI Content Generation: Combines Perplexity for research and OpenAI for content curation
Human-Like Writing: Advanced prompts eliminate AI detection markers
LinkedIn Optimization: Proper formatting with Unicode characters, emojis, and engagement hooks
Multi-Platform Support: Ready for LinkedIn, Twitter/X, and Facebook posting
Automated Scheduling: Configurable posting times (default: 6 AM & 6 PM daily)
Performance Tracking: Automatic logging to Google Sheets with timestamps and metrics
Error Handling: Built-in delays and retry mechanisms for API stability
Technical Implementation
Workflow Architecture
Schedule Trigger: Automated execution at specified intervals
Google Trends API: Fetches trending search queries with geographical filtering
Data Processing: JavaScript code node filters high-volume keywords (30+ search volume)
Topic Selection: OpenAI GPT-3.5 evaluates and selects optimal trending topic
Content Research: Perplexity AI researches selected topic for current information
Content Generation: Advanced prompt engineering creates LinkedIn-optimized posts
Content Distribution: Multi-platform posting with platform-specific formatting
Analytics Tracking: Google Sheets integration for performance monitoring
Node Breakdown
Schedule Trigger: Configurable timing for automated execution
HTTP Request (Google Trends): SerpAPI integration for trend data
Set Node: Structures trending data for processing
Code Node: JavaScript filtering for high-volume keywords
OpenAI Node: Intelligent topic selection based on relevance and trend strength
HTTP Request (Perplexity): Advanced AI research with anti-detection prompts
Wait Node: Rate limiting and API respect
Split Out: Prepares content for multi-platform distribution
LinkedIn Node: Authenticated posting with community management
Google Sheets Node: Automated tracking and analytics
Social Media Nodes: Twitter/X, LinkedIn and Facebook ready for activation
Use Cases
Content Creators: Maintain consistent posting schedules with trending content
Marketing Agencies: Scale content creation across multiple client accounts
Business Development: Build thought leadership with timely industry insights
Personal Branding: Establish authority by commenting on trending topics
SEO Professionals: Create content around high-search-volume keywords
Configuration Requirements
API Integrations
SerpAPI: Google Trends data access
Perplexity AI: Advanced content research capabilities
OpenAI: Content curation and topic selection
LinkedIn Community Management API: Professional posting access
Google Sheets API: Analytics and tracking
Authentication Setup
LinkedIn OAuth2 community management credentials
Google Sheets OAuth2 integration
HTTP header authentication for AI services
Customization Options
Industry Targeting: Modify prompts for specific business verticals
Posting Schedule: Adjust timing based on audience activity
Content Tone: Customize voice and style through prompt engineering
Platform Selection: Enable/disable specific social media channels
Trend Filtering: Adjust search volume thresholds and geographic targeting
Content Length: Modify character limits for different platforms
Advanced Features
Anti-AI Detection: Sophisticated prompts create human-like content
Rate Limit Management: Built-in delays prevent API throttling
Error Recovery: Robust error handling with retry mechanisms
Content Deduplication: Prevents posting duplicate content
Engagement Optimization: LinkedIn-specific formatting for maximum reach
Performance Metrics
Time Savings: Eliminates 10+ hours of weekly content creation
Consistency: Maintains regular posting schedule without manual intervention
Relevance: Content always based on current trending topics
Engagement: Optimized formatting increases social media interaction
Scalability: Single workflow manages multiple platform posting
Installation Notes
Import JSON workflow file into n8n instance
Configure all required API credentials
Set up [Google Sheets](Google Sheets) tracking document
Test workflow execution with manual trigger
Enable schedule trigger for automated operation
Best Practices
Monitor API usage to stay within rate limits
Regularly update prompts based on content performance
Review and adjust trending topic filters for your niche
Maintain backup of workflow configuration
Test content output before enabling automation
Support & Updates
Comprehensive setup documentation included
Configuration troubleshooting guide provided
Regular workflow updates for API changes
Community support through n8n forums
Tags
social-media content-automation linkedin ai-generation google-trends perplexity openai marketing trend-analysis content-creation
Compatibility
n8n Version: 1.0+
Node Requirements: Standard n8n installation
External Dependencies: API access to listed services
Hosting: Compatible with cloud and self-hosted n8n instances"
"Voice-Powered Marketing Assistant with ElevenLabs, OpenAI & Content Generation",https://n8n.io/workflows/4888-voice-powered-marketing-assistant-with-elevenlabs-openai-and-content-generation/,"üß† Gwen ‚Äì The AI Voice Marketing Agent
Gwen is your intelligent voice-powered marketing assistant built in n8n. She combines the power of OpenAI, ElevenLabs, and automation workflows to handle content creation, image generation, and voice delivery ‚Äî all from a single agent interface.
This template shows a graphical illustration of how Gwen will work with subworkflows. These subworkflows are modular placeholders and need to be linked into Gwen for full deployment.
‚ú® What Gwen Can Do
üìù Generate Voice-Optimized Blog Posts
Tailored for your target audience with engaging intros, real-time research, and polished structure.
üñºÔ∏è Create AI-Generated Visuals
From simple concepts to detailed image prompts and Google Drive uploads.
üßë‚Äçüé® Edit Images On Demand
Modify past images with a few words ‚Äî powered by OpenAI's image editing API.
üîç Search Image Database
Quickly find past content using title or intent.
üß† Think Tool
Gwen uses this to clarify uncertain tasks or analyze complex requests.
üîä Deliver Results in Natural Voice
With ElevenLabs, Gwen transforms all responses into human-like audio, perfect for marketing, social content, or voice interfaces.
üõ†Ô∏è Setup Instructions
Estimated Time: 15‚Äì30 mins
‚úÖ Step 1: Subworkflows
Import These Workflows
Blog Post, Create Image, Edit Image, Search Images
Connect Them to Gwen
Assign as tools inside the Gwen agent node (Langchain AI Agent in n8n).
üéôÔ∏è Step 2: Enable ElevenLabs Voice Agent
Sign up or log in: https://try.elevenlabs.io
Copy your API key
In the ElevenLabs interface, create a new tool:
Method: POST
URL: https://your-n8n-domain/webhook/042cc868-28b7-42a2-ab65-bc2944fc5a54
Under Body Parameters, add:
prompt ‚Üí value type: LLM Prompt
sessionId ‚Üí value type: Dynamic variable, name: system__conversation_id
Save and connect this tool to your ElevenLabs agent
Run a test and check n8n execution logs to confirm Gwen‚Äôs voice integration is active
üîê Step 3: Credentials to Set
OpenAI ‚Äì For text and image generation
ElevenLabs ‚Äì For voice output
Tavily ‚Äì For real-time research in blog generation
Telegram ‚Äì For sending content to users
Google Sheets ‚Äì To log all outputs like blogs and images"
Automated Investor Intelligence: CrunchBase to Google Sheets Data Harvester,https://n8n.io/workflows/4731-automated-investor-intelligence-crunchbase-to-google-sheets-data-harvester/,"üöÄ Automated Investor Intelligence: CrunchBase to Google Sheets Data Harvester!
Workflow Overview
This cutting-edge n8n automation is a sophisticated investor intelligence tool designed to transform market research into actionable insights. By intelligently connecting CrunchBase, data processing, and Google Sheets, this workflow:
Discovers Investor Insights:
Automatically retrieves latest investor data
Tracks key investment organizations
Eliminates manual market research efforts
Intelligent Data Processing:
Filters investor-specific organizations
Extracts critical investment metrics
Ensures comprehensive market intelligence
Seamless Data Logging:
Automatically updates Google Sheets
Creates real-time investor database
Enables rapid market trend analysis
Scheduled Intelligence Gathering:
Daily automated tracking
Consistent investor insight updates
Zero manual intervention required
Key Benefits
ü§ñ Full Automation: Zero-touch investor research
üí° Smart Filtering: Targeted investment insights
üìä Comprehensive Tracking: Detailed investor intelligence
üåê Multi-Source Synchronization: Seamless data flow
Workflow Architecture
üîπ Stage 1: Investor Discovery
Scheduled Trigger: Daily market scanning
CrunchBase API Integration
Intelligent Filtering:
Investor-specific organizations
Key investment metrics
Most recent data
üîπ Stage 2: Data Extraction
Comprehensive Metadata Parsing
Key Information Retrieval
Structured Data Preparation
üîπ Stage 3: Data Logging
Google Sheets Integration
Automatic Row Appending
Real-Time Database Updates
Potential Use Cases
Venture Capitalists: Investment ecosystem mapping
Startup Scouts: Investor trend analysis
Market Researchers: Comprehensive investment insights
Business Development: Strategic partnership identification
Investment Analysts: Market intelligence gathering
Setup Requirements
CrunchBase API
API credentials
Configured access permissions
Investor organization tracking setup
Google Sheets
Connected Google account
Prepared tracking spreadsheet
Appropriate sharing settings
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced investment trend analysis
üìä Multi-source investor aggregation
üîî Customizable alert mechanisms
üåê Expanded investment stage tracking
üß† Machine learning insights generation
Technical Considerations
Implement robust error handling
Use secure API authentication
Maintain flexible data processing
Ensure compliance with API usage guidelines
Ethical Guidelines
Respect business privacy
Use data for legitimate research
Maintain transparent information gathering
Provide proper attribution
Hashtag Performance Boost üöÄ
#InvestorIntelligence #VentureCapital #MarketResearch #AIWorkflow #DataAutomation #StartupEcosystem #InvestmentTracking #BusinessIntelligence #TechInnovation #StartupFunding
Workflow Visualization
[Daily Trigger]
    ‚¨áÔ∏è
[Fetch Investor Data]
    ‚¨áÔ∏è
[Extract Investor Fields]
    ‚¨áÔ∏è
[Log to Google Sheets]
Connect With Me
Ready to revolutionize your investor research?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your market intelligence with intelligent, automated workflows!"
"Generate Youtube Video Metadata (Timestamps, Tags, Description, ...)",https://n8n.io/workflows/4506-generate-youtube-video-metadata-timestamps-tags-description/,"For Who?
Content Creators
Youtube Automation
Marketing Team
How it works?
1 - Enter the ID of the YTB channel to trigger the workflow when a new video is posted
2 - Apify scrape the last YTB video of the channel
3 - Wait until the dataset is completed in Apify and get it
4 - Verify if Metadata are not already generated and generate them with LLM
5 - Format all the data created and update YTB Video
üì∫ YouTube Video Tutorial: https://youtu.be/HaQPAa6l5bU
SETUP
Setup Input YTB Chanel : Go to the channel's page on YouTube, and look at the URL of the page. The channel ID is the value that comes after channel/ in the URL. Add it after ""?channel_id="" You can also use free tools available to retrieve channel ID.
Setup Output YTB Video Update : Connect your YTB account to your n8n instance thanks to the Google Cloud Console. You can find tutorials by typing ""youtube api Oauth"" on Google.
APIs : For the following third-party integrations, replace ==[YOUR_API_TOKEN]== with your API Token or connect your account via Client ID / Secret to your n8n instance :
Apify : https://docs.apify.com/api/v2/getting-started
Youtube : https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.youtube/?utm_source=n8n_app&utm_medium=node_settings_modal-credential_link&utm_campaign=n8n-nodes-base.youTube#templates-and-examples
üõ†Ô∏è Need Help with Your Workflows ? https://tally.so/r/wayeqB
üìß Contact me : contact.nassercc@gmail.com
üë®‚Äçüíª More Workflows : https://n8n.io/creators/nasser/"
AI-Powered Stock Market Summary Bot,https://n8n.io/workflows/4867-ai-powered-stock-market-summary-bot/,"An automated n8n workflow that analyzes stocks using RSI and MACD, summarizes insights with OpenAI, and sends a Slack-ready market update every hour.
This workflow:
Runs hourly from 6:30 AM to 2:30 PM PT, Mon‚ÄìFri
Checks if the U.S. stock market is open using Alpaca‚Äôs /clock API
Pulls daily stock bars for a list of tickers via Alpaca‚Äôs /v2/stocks/bars
Calculates RSI and MACD using a Python code node
Categorizes each stock as Buy / Hold / Sell
Uses OpenAI Assistant to summarize the results in Slack markdown
Sends the message to a specific Slack user or channel"
Automated video creation using Google V3 and n8n workflow,https://n8n.io/workflows/4877-automated-video-creation-using-google-v3-and-n8n-workflow/,"Who is this for?
Content creators, social media managers, digital marketers, and businesses looking to automate video production without expensive equipment or technical expertise.
What problem is this workflow solving?
Traditional video creation requires cameras, editing software, voice recording equipment, and hours of post-production work. This workflow eliminates all these barriers by automatically generating professional videos with audio using just text prompts.
What this workflow does
This automated workflow takes video ideas from Google Sheets, generates optimized prompts using AI, creates videos through Google's V3 model via Fal AI, monitors the generation progress, and saves the final video URLs back to your spreadsheet for easy access and management.
Setup
Sign up for Fal AI account and obtain API key
Create Google Sheet with video ideas and status columns
Configure n8n with required credentials (Google Sheets, Fal AI API)
Import the workflow template
Set up authentication for all connected services
Test with sample video idea
How to customize this workflow to your needs
Modify the AI prompts to match your brand voice, adjust video styles and camera movements, change polling intervals for video generation status, customize Google Sheet column mappings, and add additional processing steps like thumbnail generation or social media posting."
Extract Instagram Profile Data with Apify and Store in Google Sheets,https://n8n.io/workflows/4587-extract-instagram-profile-data-with-apify-and-store-in-google-sheets/,"Workflow Overview
This cutting-edge n8n automation is a powerful social media intelligence gathering tool designed to transform Instagram profile research into a seamless, automated process. By intelligently combining web scraping, data formatting, and cloud storage technologies, this workflow:
Discovers Profile Insights:
Automatically scrapes Instagram profile data
Captures comprehensive profile metrics
Extracts critical social media intelligence
Intelligent Data Capture:
Retrieves follower counts
Collects biographical information
Captures profile picture and external links
Seamless Data Logging:
Automatically stores data in Google Sheets
Creates a living, updateable database
Enables easy analysis and tracking
Key Benefits
ü§ñ Full Automation: Instant profile intelligence
üí° Comprehensive Insights: Detailed social media metrics
üìä Effortless Tracking: Automated data collection
üåê Multi-Purpose Research: Flexible data gathering
Workflow Architecture
üîπ Stage 1: Trigger & Input
Form-Based Trigger: Manual username submission
Webhook Support: Flexible data entry methods
User-Driven Initiation
üîπ Stage 2: Web Scraping
Apify Integration: Robust Instagram data extraction
Comprehensive Profile Scanning:
Followers count
Following count
Profile biography
Profile picture URL
üîπ Stage 3: Data Formatting
Intelligent Data Mapping
Standardized Data Structure
Preparation for Storage
üîπ Stage 4: Cloud Logging
Google Sheets Integration
Persistent Data Storage
Easy Access and Analysis
Potential Use Cases
Influencer Marketing: Talent identification
Competitive Intelligence: Audience research
Social Media Analysis: Performance tracking
Recruitment: Talent scouting
Brand Partnerships: Collaboration opportunities
Setup Requirements
Apify Account
Instagram scraping actor
API token
Configured scraping parameters
Google Sheets
Connected Google account
Prepared tracking spreadsheet
Appropriate sharing settings
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced profile scoring
üìä Engagement rate calculation
üîî Real-time change alerts
üåê Multi-platform profile tracking
üß† AI-powered insights generation
Technical Considerations
Implement robust error handling
Use exponential backoff for API calls
Maintain flexible data extraction strategies
Ensure compliance with platform terms of service
Ethical Guidelines
Respect user privacy
Use data for legitimate research
Maintain transparent data collection practices
Provide opt-out mechanisms
Connect With Me
Ready to unlock social media insights?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your social media research with intelligent, automated workflows!
#InstagramDataScraping #SocialMediaIntelligence #InfluencerMarketing #DataAutomation #AIResearch #MarketingTechnology #SocialMediaAnalytics #ProfileIntelligence #WebScraping #MarketingTech"
Telegram AI Chatbot Agent with InfraNodus GraphRAG Knowledge Base,https://n8n.io/workflows/4485-telegram-ai-chatbot-agent-with-infranodus-graphrag-knowledge-base/,"Using the knowledge graphs instead of RAG vector stores
This workflow creates a Telegram chatbot agent that has access to several knowledge bases at the same time (used as ""experts"").
These knowledge bases are provided using the InfraNodus GraphRAG using the knowledge graphs and providing high-quality responses without the need to set up complex RAG vector store workflows.
The advantages of using GraphRAG instead of the standard vector stores for knowledge are:
Easy and quick to set up and update (no complex data import workflows or vector stores needed)
A knowledge graph has a holistic view of your knowledge base and knows what it's about
Better retrieval of relations between the document chunks = higher quality responses
How it works
This template uses the n8n AI agent node as an orchestrating agent that decides which tool (knowledge graph) to use based on the user's prompt.
Here's a description step by step:
The user submits a question using the Telegram bot, which is then received in the n8n workflow via the Telegram trigger node.
The AI agent node checks a list of tools it has access to. Each tool has a description of the knowledge it has auto-generated by InfraNodus.
The AI agent decides which tool should be used to generate a response. It may reformulate user's query to be more suitable for the expert.
The query is then sent to the InfraNodus HTTP node endpoint, which will query the graph that corresponds to that expert.
Each InfraNodus GraphRAG expert provides a rich response that takes the whole context into account and provides a response from each expert (graph) along with a list of relevant statements retrieved using a combination or RAG and GraphRAG.
The n8n AI Agent node integrates the responses received from the experts to produce the final answer.
The final answer is sent back to the Telegram bot who delivers it back to the private chat or a Telegram group.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Create a separate knowledge graph for each expert (using PDF / content import options) in InfraNodus
For each graph, go to the workflow, paste the name of the graph into the body name field.
Keep other settings intact or learn more about them at the InfraNodus access points page.
Once you add one or more graphs as experts to your flow, add the LLM key to the OpenAI node
Create a Telegram bot (the instructions are in the workflow Post note) ‚Äî it takes 30 seconds. Get its API key and create the Telegram credentials to use in the Telegram nodes in this workflow.
Requirements
An InfraNodus account and API key
An OpenAI (or any other LLM) API key
A Telegram account
Customizing this workflow
You can use this same workflow with a standard AI chatbot via a URL that can also be embedded to any website. You can also use it with ElevenLabs AI voice agent. There are many more customizations available.
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20174217658396-Using-InfraNodus-Knowledge-Graphs-as-Experts-for-AI-Chatbot-Agents-in-n8n
Also check out the video tutorial with a demo:
Support
If you have any questions, contact us via the support portal at https://support.noduslabs.com or via our Discord channel.
More n8n workflows are available on our support portal: n8n x InfraNodus AI automation workflows."
Personalized LinkedIn Responses with OpenAI GPT & Notion-based Routing,https://n8n.io/workflows/4891-personalized-linkedin-responses-with-openai-gpt-and-notion-based-routing/,"Who is this for?
This workflow is for professionals and teams who want to automate LinkedIn message replies with intelligent, human-like responses ‚Äî without losing control over tone or accuracy. Ideal for founders, sales teams, DevRel, or community managers handling high-volume inbound messages.
What problem is this workflow solving?
Responding to every LinkedIn message manually is slow and inconsistent. Basic AI bots generate replies without context or nuance. This subworkflow solves both problems by using structured message routing from Notion and profile insights from UniPile to craft smart, context-aware responses.
What this workflow does
This workflow takes the sender‚Äôs message and profile (from LinkedIn Auto Message Router with Request Detection) and references your centralized Notion database of message types. It uses that to either match the message to a known response or generate a new one using OpenAI's GPT model ‚Äî all while following professional tone guidelines.
This is the third workflow in a 3-part automation system:
Receives data from LinkedIn Auto Message Router with Request Detection
Uses UniPile LinkedIn Profile Lookup Subworkflow to enrich responses based on follower count or org data
Example Use Case
If a message comes from someone with low reach (e.g., under 1,000 followers), the AI politely deflects a meeting request. If an influencer reaches out, the AI immediately offers a booking link. Your team controls this logic by updating the Notion database ‚Äî no edits to the workflow required.
Setup
Connect this workflow as a subworkflow in your router or Slack approval flow
Store your Notion API key and database ID in n8n
Provide the following parent inputs:
message ‚Äì The LinkedIn message text
sender ‚Äì Name of the sender
chatid ‚Äì Session ID (optional for memory)
linkedinprofile ‚Äì Enriched array with LinkedIn context (follower count, connection info, etc.)
Add your preferred AI model credentials (supports OpenAI, Gemini, or Ollama)
Optional: Customize system prompt to better match your brand voice
How to customize this workflow to your needs
Update the Notion schema to include industry-specific categories or actions
Change the AI tone (e.g., humorous, more corporate, etc.)
Add conditional logic for auto-sending messages without Slack approval
Extend to support multiple platforms (e.g., email, X/Twitter, Instagram DMs)"
Automated Job Hunter: Upwork Opportunity Aggregator & AI-Powered Notifier,https://n8n.io/workflows/4733-automated-job-hunter-upwork-opportunity-aggregator-and-ai-powered-notifier/,"üöÄ Automated Job Hunter: Upwork Opportunity Aggregator & AI-Powered Notifier!
Workflow Overview
This cutting-edge n8n automation is a sophisticated job discovery and notification tool designed to transform freelance job hunting into a seamless, intelligent process. By intelligently connecting Apify, OpenAI, Google Sheets, and Gmail, this workflow:
Discovers Job Opportunities:
Automatically scrapes Upwork job listings
Tracks recent freelance opportunities
Eliminates manual job searching efforts
Intelligent Data Processing:
Filters and extracts key job details
Structures job information
Ensures comprehensive opportunity tracking
AI-Powered Summarization:
Generates concise job summaries
Creates human-readable job digests
Provides quick, actionable insights
Seamless Notification:
Automatically logs jobs to Google Sheets
Sends personalized email digests
Enables rapid opportunity assessment
Key Benefits
ü§ñ Full Automation: Zero-touch job discovery
üí° Smart Filtering: Targeted job opportunities
üìä Comprehensive Tracking: Detailed job market insights
üåê Multi-Platform Synchronization: Seamless data flow
Workflow Architecture
üîπ Stage 1: Job Discovery
Scheduled Trigger: Daily job scanning
Apify Integration: Upwork job scraping
Intelligent Filtering:
Recent job postings
Specific keywords
Relevant opportunities
üîπ Stage 2: Data Extraction
Comprehensive Job Metadata Parsing
Key Information Retrieval
Structured Data Preparation
üîπ Stage 3: AI Summarization
OpenAI GPT Processing
Professional Summary Generation
Contextual Job Insight Creation
üîπ Stage 4: Multi-Platform Distribution
Google Sheets Logging
Gmail Integration
Automated Job Digest Delivery
Potential Use Cases
Freelancers: Opportunity tracking
Job Seekers: Automated job discovery
Recruitment Agencies: Market intelligence
Skill Development Professionals: Trend monitoring
Career Coaches: Client opportunity identification
Setup Requirements
Apify
Upwork scraping actor
API token
Configured scraping parameters
OpenAI API
GPT model access
Summarization configuration
API key management
Google Sheets
Connected Google account
Prepared job tracking spreadsheet
Appropriate sharing settings
Gmail Account
Connected email
Job digest configuration
Appropriate sending permissions
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced job matching algorithms
üìä Multi-platform job aggregation
üîî Customizable alert mechanisms
üåê Expanded job category tracking
üß† Machine learning job recommendation
Technical Considerations
Implement robust error handling
Use secure API authentication
Maintain flexible data processing
Ensure compliance with platform guidelines
Ethical Guidelines
Respect job poster privacy
Use data for legitimate job searching
Maintain transparent information gathering
Provide proper attribution
Hashtag Performance Boost üöÄ
#FreelanceJobHunting #CareerAutomation #JobDiscovery #AIJobSearch #WorkflowAutomation #FreelanceTech #CareerIntelligence #JobMarketInsights #ProfessionalNetworking #TechJobSearch
Workflow Visualization
[Daily Trigger]
    ‚¨áÔ∏è
[Fetch Upwork Jobs]
    ‚¨áÔ∏è
[Format Job Fields]
    ‚¨áÔ∏è
[Log to Google Sheets]
    ‚¨áÔ∏è
[AI Summarization]
    ‚¨áÔ∏è
[Send Email Digest]
Connect With Me
Ready to revolutionize your job hunting strategy?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your job search with intelligent, automated workflows!"
Intelligent Email Organization with AI-Powered Content Classification for Gmail,https://n8n.io/workflows/4557-intelligent-email-organization-with-ai-powered-content-classification-for-gmail/,"This workflow leverages AI to intelligently analyze incoming Gmail messages and automatically apply relevant labels based on the email content.
The default configuration includes the following labels:
Newsletter: Subscription updates or promotional content.
Inquiry: Emails requesting information or responses.
Invoice: Billing and payment-related emails.
Proposal: Business offers or collaboration opportunities.
Action Required: Emails demanding immediate tasks or actions.
Follow-up Reminder: Emails prompting follow-up actions.
Task: Emails containing actionable tasks.
Personal: Non-work-related emails.
Urgent: Time-sensitive or critical communications.
Bank: Banking alerts and financial statements.
Job Update: Recruitment or job-related communications.
Spam/Junk: Unwanted or irrelevant bulk emails.
Social/Networking: Notifications from social platforms.
Receipt: Purchase confirmations and receipts.
Event Invite: Invitations or calendar-related messages.
Subscription Renewal: Reminders for subscription expirations.
System Notification: Technical alerts from services or systems.
You can customize labels and definitions based on your specific use case.
How it works:
The workflow periodically retrieves new Gmail messages.
Only emails without existing labels, regardless of read status, are sent to the AI for analysis.
Email content (subject and body) is analyzed by an AI model to determine the appropriate label.
Labels identified by the AI are applied to each email accordingly.
Note: This workflow performs 100% better than the default Gmail trigger method, which is why the workflow was switched from Gmail trigger to a scheduled workflow. By selectively processing only unlabeled emails, it ensures comprehensive labeling while significantly reducing AI processing costs.
Setup Steps:
Configure credentials for Gmail and your chosen AI service (e.g., OpenAI).
Ensure labels exist in your Gmail account matching the workflow definitions.
Adjust the AI prompt to match your labeling needs.
Optionally customize the polling interval (default: every 2 minutes).
This workflow streamlines your email management, keeping your inbox organized effortlessly while optimizing resource usage."
"Build a PDF Document RAG System with Mistral OCR, Qdrant and Gemini AI",https://n8n.io/workflows/4400-build-a-pdf-document-rag-system-with-mistral-ocr-qdrant-and-gemini-ai/,"This workflow is designed to process PDF documents using Mistral's OCR capabilities, store the extracted text in a Qdrant vector database, and enable Retrieval-Augmented Generation (RAG) for answering questions. Here‚Äôs how it functions:
Once configured, the workflow automates document ingestion, vectorization, and intelligent querying, enabling powerful RAG applications.
Benefits
End-to-End Automation
No manual interaction is needed: documents are read, processed, and made queryable with minimal setup.
Scalable and Modular
The workflow uses subflows and batching, making it easy to scale and customize.
Multi-Model Support
Combines Mistral for OCR, OpenAI for embeddings, and Gemini for intelligent answering‚Äîtaking advantage of the strengths of each.
Real-Time Q&A
With RAG integration, users can query document content through natural language and receive accurate responses grounded in the PDF data.
Light or Full Mode
Users can choose to index full page content or only summarized text, optimizing for either performance or richness.
How It Works
PDF Processing with Mistral OCR:
The workflow starts by uploading a PDF file to Mistral's API, which performs OCR to extract text and metadata.
The extracted content is split into manageable chunks (e.g., pages or sections) for further processing.
Vector Storage in Qdrant:
The extracted text is converted into embeddings using OpenAI's embedding model.
These embeddings are stored in a Qdrant vector database, enabling efficient similarity searches for RAG.
Question-Answering with RAG:
When a user submits a question via a chat interface, the workflow retrieves relevant text chunks from Qdrant using vector similarity.
A language model (Google Gemini) generates answers based on the retrieved context, providing accurate and context-aware responses.
Optional Summarization:
The workflow includes an optional summarization step using Google Gemini to condense the extracted text for faster processing or lighter RAG usage.
Set Up Steps
To deploy this workflow in n8n, follow these steps:
Configure Qdrant Database:
Replace QDRANTURL and COLLECTION in the ""Create collection"" and ""Refresh collection"" nodes with your Qdrant instance details.
Ensure the Qdrant collection is configured with the correct vector size (e.g., 1536 for OpenAI embeddings) and distance metric (e.g., Cosine).
Set Up Credentials:
Add credentials for:
Mistral Cloud API (for OCR processing).
OpenAI API (for embeddings).
Google Gemini API (for chat and summarization).
Google Drive (if sourcing PDFs from Drive).
Qdrant API (for vector storage).
PDF Source Configuration:
If using Google Drive, specify the folder ID in the ""Search PDFs"" node.
Alternatively, modify the workflow to accept PDFs from other sources (e.g., direct uploads or external APIs).
Customize Text Processing:
Adjust chunk size and overlap in the ""Token Splitter"" node to optimize for your document type.
Choose between raw text or summarized content for RAG by toggling between the ""Set page"" and ""Summarization Chain"" nodes.
Test the RAG:
Trigger the workflow manually or via a chat message to verify OCR, embedding, and Qdrant storage.
Use the ""Question and Answer Chain"" node to test query responses.
Optional Sub-Workflows:
The workflow supports execution as a sub-workflow for batch processing (e.g., handling multiple PDFs).
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Generate AI-Powered LinkedIn Posts with Ollama, Image Creation, and Gmail Delivery",https://n8n.io/workflows/4674-generate-ai-powered-linkedin-posts-with-ollama-image-creation-and-gmail-delivery/,"This automation flow is designed to generate professional, research-backed LinkedIn posts based on a user-submitted topic and audience, enhance it with a visually aligned image prompt, and then automatically send the post and image via Gmail and/or publish it to LinkedIn.
‚öôÔ∏è How It Works (Step-by-Step):
üìù Form Submission (Input Trigger)
A user fills out a form with:
 Email

 Topic of the Post

 Target Audience
This form submission acts as the entry point of the workflow (Form Trigger node).
ü§ñ LinkedIn Post Generation Agent
An AI agent is triggered that:
 Uses Tavily API to fetch real-time web search content related to the topic.

 Processes the topic and audience data using an Ollama Chat Model.

 Generates a well-structured LinkedIn post that includes:

     Hook

     Educational insight

     Professional tone

     Source citations

     Hashtags and call to action
üé® Image Prompt Agent
The generated post is passed to a second agent that:
 Extracts the core message.

 Converts it into a graphic prompt using a different Ollama LLM.

 The prompt is written to be used by an image-generation model like DALL¬∑E or Gemma/GPT-based image tools.
üñºÔ∏è Image Generation
The image prompt is passed to a local image generation endpoint (localhost:8098) or OpenAI (api.openai.com) to generate a visual asset.
The base64 image is converted into a file.
üì© Email Dispatch
The post text and the generated image are then sent to the user‚Äôs email address via the Gmail node.
The email includes:
 The full LinkedIn post

 The auto-generated image as an attachment
üîó LinkedIn Publishing (Optional)
If OAuth is enabled, the post can also be published directly to LinkedIn using the LinkedIn node.
üõ†Ô∏è Tools Used:
n8n: Orchestration platform

OpenRouter & Ollama: Local/hosted LLMs for post and prompt generation

Tavily API: Real-time web search

OpenAI or Local Endpoint: Image generation from text

SMTP (Gmail): For sending emails

LinkedIn API: For automated posting
üì¶ Key Features:
End-to-end automation from form to email/post

Uses real-time research for accuracy

Creative visual generation with professional branding intent

Works locally or with cloud APIs

Modular: Can switch tools or endpoints (OpenAI ‚Üî Localhost, Ollama ‚Üî OpenRouter)
üöÄ Ideal Use Cases:
Marketing teams needing fast content turnaround

Personal branding assistants

Founders, freelancers, and executives sharing thought leadership

University or organization-wide communication systems"
AI Telegram Bot Agent: Smart Assistant & Content Summarizer,https://n8n.io/workflows/4457-ai-telegram-bot-agent-smart-assistant-and-content-summarizer/,"Create your own intelligent Telegram bot that summarizes articles and processes commands automatically.
This powerful workflow turns Telegram into your personal AI assistant, handling /help, /summary &lt;URL&gt;, and /img &lt;prompt&gt; commands with intelligent responses - perfect for teams, content creators, and anyone wanting smart automation in their messaging.
üöÄ What It Does
Smart Command Processing: Automatically recognizes and routes /help, /summary, and /img commands to appropriate AI-powered responses.
Article Summarization: Fetches any URL, extracts content, and generates professional 10-12 bullet point summaries using OpenAI.
Image Generation: Processes image prompts and integrates with AI image generation services.
Help System: Provides users with clear command instructions and usage examples.
üéØ Key Benefits
‚úÖ Personal AI Assistant: Get instant article summaries in Telegram
‚úÖ Team Productivity: Share quick content summaries with colleagues
‚úÖ Content Research: Rapidly digest articles and web content
‚úÖ 24/7 Availability: Bot works around the clock without maintenance
‚úÖ Easy Commands: Simple /summary &lt;link&gt; format anyone can use
‚úÖ Scalable: Handles multiple users and requests simultaneously
üè¢ Perfect For
Content Teams & Researchers
Journalists quickly summarizing news articles
Marketing teams researching competitor content
Students processing academic papers and articles
Analysts digesting industry reports
Business Applications
Team Communication: Share article insights in group chats
Research Assistance: Quick content analysis for decision making
Content Curation: Summarize articles for newsletters or reports
Knowledge Sharing: Help teams stay informed efficiently
‚öôÔ∏è What's Included
Complete Bot Workflow: Ready-to-deploy Telegram bot with all commands
AI Integration: OpenAI-powered content summarization and processing
Smart Routing: Intelligent command recognition and response system
Error Handling: Robust system handles invalid commands gracefully
Extensible Design: Easy to add new commands and features
üîß Quick Setup Requirements
n8n Platform: Cloud or self-hosted instance
Telegram Bot Token: Create via @BotFather (free, 5 minutes)
OpenAI API: For content summarization (pay-per-use)
Basic Configuration: Follow 15-minute setup guide
üì± User Experience
Simple Commands:
/help - Show available commands
/summary https://example.com - Get article summary
/img sunset over mountains - Generate image (with supported APIs)
Sample Summary Output:
üìÑ Article Summary:

‚Ä¢ Company reports 40% revenue growth in Q3 2024
‚Ä¢ New AI features driving customer acquisition
‚Ä¢ Expansion into European markets planned for 2025
‚Ä¢ Investment in R&D increased by 25% this quarter
‚Ä¢ Customer satisfaction scores improved to 94%
‚Ä¢ Three new product launches scheduled for next year
‚Ä¢ Remote work policy made permanent post-pandemic
‚Ä¢ Sustainability goals on track to meet 2025 targets
‚Ä¢ Partnership with major tech firm announced
‚Ä¢ Stock price up 15% following earnings report
üé® Customization Options
Command Extensions: Add custom commands for specific workflows
Response Formatting: Customize summary style and length
Multi-Language: Support different languages for international teams
Integration APIs: Connect additional AI services and tools
User Permissions: Control who can use specific commands
Analytics: Track usage patterns and popular content
üè∑Ô∏è Tags & Categories
#telegram-bot #ai-automation #content-summarization #article-processing #team-productivity #openai-integration #smart-assistant #workflow-automation #messaging-bot #content-research #ai-agent #n8n-workflow #business-automation #telegram-integration #ai-powered-bot
üí° Use Case Examples
News Team: Quickly summarize breaking news articles for editorial meetings
Marketing Agency: Research competitor content and industry trends efficiently
Sales Team: Digest industry reports and share insights with prospects
Remote Team: Keep everyone informed with summarized company updates
üìà Expected Results
80% faster content research and analysis
50% more articles processed per day vs manual reading
100% team accessibility through familiar Telegram interface
24/7 availability for global teams across time zones
üõ†Ô∏è Setup & Support
Quick Start: Deploy your bot in 15 minutes with included guide
Video Tutorial: Complete walkthrough available
Template Commands: Pre-built responses and formatting
Expert Support: Direct help from workflow creator
üìû Get Help & Resources
YouTube: https://www.youtube.com/@YaronBeen/videos
üíº Professional Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
üìß Direct Help
Email: Yaron@nofluff.online - Response within 24 hours
Ready to build your intelligent Telegram assistant?
Get this AI Telegram Bot Agent and transform your messaging app into a powerful content processing tool. Perfect for teams, researchers, and anyone who wants AI-powered assistance directly in Telegram.
Stop manually reading long articles. Start getting instant, intelligent summaries with simple commands."
Extract Invoice Data from Google Drive to Sheets with Mistral OCR and Gemini,https://n8n.io/workflows/4868-extract-invoice-data-from-google-drive-to-sheets-with-mistral-ocr-and-gemini/,"Extract data from any PDF or image invoice dropped in Google Drive directly into Google Sheets ‚Äì powered by AI OCR. Free, fully modifiable n8n workflow. Optional add-ons for pro features.
üöÄ What this template does
Stop typing invoice data by hand. Drop a PDF or phone-snapshot into your Invoices Inbox folder in Google Drive and this n8n workflow will:
Auto-OCR the document with the Mistral OCR API
Match any fields you list in Row 1 of your Google Sheet (totally schema-agnostic)
Append a clean, structured row ‚Äì every time
Works with both PDFs and images, in any language supported by Mistral.
Template JSON included, ready to import into self-hosted or n8n Cloud
üëÄ Who‚Äôs this for?
Freelancers & agencies processing client invoices
Small finance teams on Google Workspace
Anyone self-hosting n8n who wants an AI OCR flow without glue-code
No coding skills required ‚Äì but flow tweaking is possible for power users.
üõ† Upcoming PRO Add-Ons
I am also working on PRO add-ons for this template:
Add-On #1 ‚Äì Error Handling & Alerts (ships Jul 2025)‚Ä¢ Flags missing fields, branches to Email/Slack notification; prevents silent failures
Add-On #2 ‚Äì Auto-Currency Converter (ships Jul 2025)‚Ä¢ Detects invoice currency symbol/code ‚Üí converts Total into your base currency via a free FX API
Add-On #3 ‚Äì VAT / GST Breakdown (ships Jul 2025)‚Ä¢ Extracts VAT number, net, tax rate, tax amount, gross ‚Äì ready for EU/UK/AU filings
To pre-order these please see: https://ysqander.gumroad.com/l/N8N-AI-Workflow-Invoice-Data-Extraction-LITE"
"Automate LinkedIn Posts with Claude AI, DALL-E Images & Google Sheets Approval",https://n8n.io/workflows/4766-automate-linkedin-posts-with-claude-ai-dall-e-images-and-google-sheets-approval/,"How it works
This workflow automates your entire LinkedIn content strategy, from ideation to publishing.
Daily Idea Generation: Every day, the workflow uses an advanced AI agent (Anthropic's Claude model) to generate a new, viral-optimized LinkedIn post idea, complete with a title, full text, and a detailed image description. It analyzes your past posts from a Google Sheet to ensure content is fresh and aligns with one of your four content pillars: timeless principles, case studies, growth hacks, or controversial ads.
AI Image Creation: It then uses OpenAI's DALL-E to create a custom, high-quality image based on the generated description and a style reference image you provide, ensuring brand consistency.
Review & Approval Workflow: The generated post and its accompanying image are automatically saved as a new row in a Google Sheet, marked with a ""review"" status. This gives you full editorial control.
Automated Publishing: Once you approve a post by changing its status to ""ready"" in the Google Sheet, a separate part of the workflow picks it up and automatically publishes it to your LinkedIn profile at a scheduled time.
Status Tracking: After publishing, the workflow updates the status in your Google Sheet to ""posted,"" so you always have a clear overview of your content pipeline.
Set up steps
This workflow integrates several services. Follow these steps to get it running:
Copy the Google Sheet: Make a copy of the AI Posts Content Machine Google Sheet template and select your copy in the Get Past Ideas, Save Post, Get Ready Posts, and Update Status nodes.
Set Up Credentials: You will need to create and add API credentials for the following services within the corresponding n8n nodes:
Google Sheets & Google Drive: Create a Google credential.
Anthropic: Add your API key in the Anthropic Chat Model node.
OpenAI: Add your API key as a Header Auth credential in the OpenAI Image node.
Perplexity AI: Add your API key as a Header Auth credential inside the Perplexity Research sub-workflow.
LinkedIn: Create a LinkedIn credential.
Customize Your Content:
In the Idea Generator node, adjust the system prompt to reflect your specific audience, topics, and content style.
Create a folder in Google Drive for your post images. Add a reference image to this folder that defines your desired visual style.
Paste the shareable link to your style reference image into the Image Style node.
Activate the Workflow: Enable the workflow to start the automated content creation and publishing process."
"Build a Knowledge Base Chatbot with OpenAI, RAG and MongoDB Vector Embeddings",https://n8n.io/workflows/4526-build-a-knowledge-base-chatbot-with-openai-rag-and-mongodb-vector-embeddings/,"Who is this for?
This template is designed for internal support teams, product specialists, and knowledge managers in technology companies who want to automate ingestion of product documentation and enable AI-driven, retrieval-augmented question answering.
What problem is this workflow solving?
Support agents often spend too much time manually searching through lengthy documentation, leading to inconsistent or delayed answers. This solution automates importing, chunking, and indexing product manuals, then uses retrieval-augmented generation (RAG) to answer user queries accurately and quickly with AI.
What these workflows do
Workflow 1: Document Ingestion & Indexing
Manually triggered to import product documentation from Google Docs.
Automatically splits large documents into chunks for efficient searching.
Generates vector embeddings for each chunk using OpenAI embeddings.
Inserts the embedded chunks and metadata into a MongoDB Atlas vector store, enabling fast semantic search.
Workflow 2: AI-Powered Query & Response
Listens for incoming user questions (can be extended to webhook).
Converts questions to vector embeddings and performs similarity search on MongoDB vector store.
Uses OpenAI‚Äôs GPT-4o-mini model with retrieval-augmented generation to produce direct, context-aware answers.
Maintains short-term conversation context using a memory buffer node.
Setup
Setting up vector embeddings
Authenticate Google Docs and connect your Google Docs URL containing the product documentation you want to index.
Authenticate MongoDB Atlas and connect the collection where you want to store the vector embeddings. Create a search index on this collection to support vector similarity queries.
Ensure the index name matches the one configured in n8n (data_index).
See the example MongoDB search index template below for reference.
Setting up chat
Configure the AI system prompt in the ‚ÄúKnowledge Base Agent‚Äù node to reflect your company‚Äôs tone, answering style, and any business rules.
Update the workflow description and instructions to help users understand the chat‚Äôs purpose and capabilities.
Connect the MongoDB collection used for vector search in the chat workflow and update the vector search index if needed to match your setup.
Make sure
Both MongoDB nodes (in ingestion and chat workflows) are connected to the same collection, with:
An embedding field storing vector data,
Relevant metadata fields (e.g., document ID, source), and
The same vector index name configured (e.g., data_index).
Search Index Example:
{
""mappings"": {
""dynamic"": false,
""fields"": {
""_id"": {
""type"": ""string""
},
""text"": {
""type"": ""string""
},
""embedding"": {
""type"": ""knnVector"",
""dimensions"": 1536,
""similarity"": ""cosine""
},
""source"": {
""type"": ""string""
},
""doc_id"": {
""type"": ""string""
}
}
}
}"
AI Icebreaker Builder: Scrape Sites with Dumpling AI and Save to Airtable,https://n8n.io/workflows/4885-ai-icebreaker-builder-scrape-sites-with-dumpling-ai-and-save-to-airtable/,"Who is this for?
This workflow is ideal for sales teams, marketers, and virtual assistants who manage outbound campaigns and want to improve their cold outreach personalization. It helps automate the research and writing process for each lead, saving time while improving quality.
What problem is this workflow solving?
Cold outreach often lacks personalization because manually reviewing each lead's website takes time. This workflow eliminates that bottleneck by using AI to auto-generate personalized icebreakers, summaries, and outreach emails based on a lead‚Äôs website‚Äîwithout human research.
What this workflow does
This n8n workflow runs on a schedule and pulls leads from Airtable who don't yet have an ""Ice breaker"" field filled out. For each lead, it does the following:
Trigger: Scheduled daily via the Run Daily to Process New Leads node.
Search Airtable: Finds leads in Airtable where the Ice breaker field is empty using the Search Cold Leads Without Icebreaker node.
Split in Batches: Iterates through each lead one by one using Loop Through Each Lead.
Rate Limiting: Waits briefly before each request using Wait Before Making Request to avoid rate limits.
Scrape Website: Sends each lead‚Äôs website to Dumpling AI's /scrape endpoint via the Scrape Lead Website with Dumpling AI HTTP request.
Generate AI Copy: Sends the scraped content to GPT-4o using the Generate Icebreaker, Summary & Email (GPT-4o) node. It asks the LLM to create:
A short personalized icebreaker
A 2‚Äì3 line website summary
A short email body for cold outreach
Save Results: Updates the original Airtable record with the generated content using the Save AI Output Back to Airtable node.
Sticky Note: Provides an overview of the workflow and usage instructions for future editors or collaborators.
This loop continues for all leads found, updating Airtable with fresh AI-generated outreach content.
Integration Requirements
Airtable (Personal Access Token)
Dumpling AI API Key (Header Auth)
OpenAI (GPT-4o)"
Create a Telegram Customer Support Bot with GPT4-mini and Google Docs Knowledge,https://n8n.io/workflows/4875-create-a-telegram-customer-support-bot-with-gpt4-mini-and-google-docs-knowledge/,"ü§ñ AI Customer Support Agent with Google Docs Knowledge (Telegram + OpenAI)
This no-code workflow turns your Telegram bot into an intelligent, always-on AI support agent that references your business documentation in Google Docs to respond to customer queries‚Äîinstantly and accurately.
Watch full step-by-step video tutorial of the build here:
https://www.youtube.com/@Automatewithmarc
üîß How it works:
Telegram Trigger ‚Äì Captures incoming messages from users on your Telegram bot
Langchain AI Agent (OpenAI GPT) ‚Äì Interprets the message and uses RAG (retrieval-augmented generation) techniques to craft an answer
Google Docs Tool ‚Äì Connects to and retrieves context from your specified Google Doc (e.g. FAQ, SOPs, policies)
Memory Buffer ‚Äì Keeps track of recent chat history for more human-like conversations
Telegram Reply Node ‚Äì Sends the AI-generated response back to the user
üí° Use Cases:
E-commerce customer service
SaaS product onboarding
Internal helpdesk bot for teams
WhatsApp-style support for digital businesses
üß† What makes this powerful:
Supports complex questions by referencing a live Google Doc knowledge base
Works in plain conversational language (no buttons or forms needed)
Runs 24/7 with zero code
Easily extendable to Slack, WhatsApp, or email support
üõ†Ô∏è Tools used:
Telegram Node (trigger + send)
Langchain Agent with OpenAI GPT
Google Docs Tool
Memory Buffer
Sticky Notes for easy understanding"
"Automate Social Media Content with AI for Instagram, Facebook, LinkedIn & X",https://n8n.io/workflows/4637-automate-social-media-content-with-ai-for-instagram-facebook-linkedin-and-x/,"AI Social Media Content Automation ‚Äì n8n Workflow
This workflow is built for creators, solopreneurs, SaaS founders, and agencies looking to automate their social media content process from idea to publication. It combines the power of OpenAI, Google Sheets, and official APIs for Instagram, Facebook, X (Twitter), and LinkedIn to deliver fully automated, brand-consistent social media posts ‚Äî including text, images, hashtags, CTAs, and scheduling.
No more switching tools, rewriting content, or forgetting to post. Just set it up once and let your brand grow automatically.
üîß What the Workflow Does
Generates platform-specific post ideas based on your brand tone.
Selects the best idea (or lets you manually input one).
Writes tailored posts for Instagram, Facebook, X, and LinkedIn.
Auto-generates platform-specific visuals using OpenAI (DALL¬∑E) and uploads to Cloudinary.
Publishes content directly using official platform APIs.
Logs all actions to Google Sheets and optionally sends a summary email via Gmail.
‚öôÔ∏è How It Works
Trigger the workflow manually, on a schedule, or from a chatbot.
AI generates multiple post ideas and filters the best one.
Based on the selected idea, it generates full posts for each platform.
Custom visuals are created using OpenAI and uploaded to Cloudinary.
Final post content is merged and sent to Instagram, Facebook, X, and LinkedIn.
Logs and optional reports are created in Google Sheets and Gmail.
üõ†Ô∏è Setup Steps
Set up your accounts and credentials: OpenAI, Cloudinary, Gmail, Meta, LinkedIn, X, and Google Sheets.
Import the workflow and subworkflow (Get Brand Brief) into n8n.
Replace all placeholders (sheet IDs, access tokens, profile/page IDs, brand brief URLs).
Optionally personalize prompts and output templates to match your brand voice.
Setup time: ~30‚Äì45 minutes (including credentials and testing).
Detailed configuration notes are included in sticky nodes throughout the workflow.
üì¶ Included in Your Package
JSON workflow file (main flow + brand brief subflow)
Google Sheets templates for history logging
Setup documentation (Markdown & PDF)
Placeholder reference sheet"
"Generate Videos with AI, ElevenLabs,PIAPI Shotstack/Creatomate & Post to Youtube",https://n8n.io/workflows/4630-generate-videos-with-ai-elevenlabspiapi-shotstackcreatomate-and-post-to-youtube/,"Auto-Generate Long Videos with AI, ElevenLabs,PIAPI,Shotstack/Creatomate & Post to Youtube
Overview
This n8n automation workflow automates the creation, scripting, production, and posting of YouTube videos. It leverages AI (OpenAI), image generation (PIAPI), video rendering (Shotstack), and platform integrations (Airtable, Google Sheets, YouTube) to streamline the process.
Target Audience
Content creators, video producers, and YouTubers seeking to automate video content creation.
Digital marketing teams managing video campaigns for travel or history niches.
Users familiar with n8n, Airtable, Google Sheets, YouTube, and API integrations.
Problem Solved
Manually managing YouTube video production, from ideation to posting, is time-consuming and prone to errors. This workflow addresses:
Content Sourcing: Generates video ideas using AI based on ""domain of choice"".
Content Formatting: Automatically creates detailed scripts and scene structures.
Visual Production: Produces high-quality images and renders videos with minimal manual input.
Multi-Platform Integration: Publishes to YouTube with status tracking in Airtable and Google Sheets.
Status Tracking: Monitors progress (Generated, Scripted, Produced, Published) across platforms.
Prerequisites
Before setting up, ensure you have:
An n8n instance (self-hosted or cloud)
API credentials for:
OpenAI (for idea generation)
PIAPI.ai (for image and video generation)
ElevenLabs (for audio generation)
Shotstack (for video rendering)
Creatomate (for video rendering)
Airtable & Google Sheets and Drive (for storage and tracking)
Gmail (for notifications)
YouTube API (for posting to YouTube)
How the Automation Works (Step by Step)
Trigger (Schedule Trigger)
Initiates the workflow daily
Connects to ""Generate Idea""
Generate Idea
Uses OpenAI to create a unique 4-minute video idea with a title, description, keywords, scene structure, and intrigue note
Checks Airtable ""Ideas"" table via searchAirtable to avoid duplicates
Connects to ""Parse Ideas""
Parse Ideas
Extracts title, description, and notes from the OpenAI output using JavaScript
Connects to ""Add Ideas""
Add Ideas
Stores the initial idea in Google Sheets ""Ideas"" sheet with fields like ""Idea"", ""Description"", and ""Progress"" (set to ""Generated"")
Connects to ""Store in Airtable""
Store in Airtable
Saves the idea to Airtable ""Ideas"" table with mapped fields and typecast enabled
Connects to ""Generate Script""
Generate Script
Creates a 500-600 word script with 8-10 scenes using OpenAI, based on the latest ""Generated"" idea
Uses Structured Output Parser to format output
Connects to ""Parse Script Output""
Parse Script Output
Formats the script into readable text and JSON, extracting scenes and closing question
Connects to ""Store Script""
Store Script
Saves the script to Google Sheets ""Production"" sheet with ""Script"", ""Scenes"", and ""Status"" (set to ""Scripted"")
Connects to ""Store Script in Airtable""
Store Script in Airtable
Saves the script to Airtable ""Production"" table with mapped fields and typecast enabled
Connects to ""Updated Idea to Scripted""
Updated Idea to Scripted
Updates the idea status to ""Scripted"" in Google Sheets ""Ideas"" sheet
Connects to ""Update Status Ideas Table""
Update Status Ideas Table
Updates the idea status to ""Scripted"" in Airtable ""Ideas"" table, matching by ""Idea ID""
Connects to ""Extract Scenes""
Extract Scenes
Parses scene data from Airtable ""Production"" table for image generation
Connects to ""Text-to-Image""
Text-to-Image
Generates images for each scene using PIAPI
Connects to ""Wait for 4 Min""
Wait for 4 Min
Waits 3 minutes to allow image generation to complete
Connects to ""Get Images""
Trigger (Schedule Trigger1)
Initiates the posting process daily
Connects to ""Search for Latest Ready Video""
Search for Latest Ready Video
Searches Airtable ""Production"" table for the latest video with ""Status"" = ""Ready""
Connects to ""If Ready?""
If Ready?
Checks if a video is ready (condition: Status contains Ready)
If true, connects to ""Download Video"" and ""Update Production Table""
Download Video
Downloads the video file using the VideoURL from Airtable
Connects to ""Post YouTube""
Post YouTube
Uploads the video to YouTube with title and description, using YouTube OAuth2
Update Production Table
Updates the ""Production"" sheet in Google Sheets, setting ""Status"" to ""Published"" and matching by ""Production ID""
Additional Nodes and Processes
OpenAI Chat Model: Powers ""Generate Idea"" and ""Generate Script"" with GPT
Structured Output Parser: Ensures proper JSON output for scripts
Get Images: Retrieves generated images
Extract Narration: Prepares narration for voice generation
Voice Generation: Generates voiceovers
Generate Music Prompt: Creates music prompts using OpenAI
Text-to-Music: Generates music
Build Shotstack Timeline: Constructs video timeline
ShotStack Render Video: Renders the final video
Poll Rendered Videos: Checks video rendering status
Final Video: Updates Airtable ""Production"" with final video details
Final Video Update: Updates Google Sheets ""Production"" with video URL
Music Urls: Adds music URLs to the process
Setup Requirements
Before starting, ensure you have the following
1 n8n Instance
A self-hosted or cloud-based n8n instance to run the workflow
2 API Credentials
OpenAI: API key
Airtable: Personal Access Token (e.g, ""Airtable Personal Access Token
Google Sheets: OAuth2 credentials
PIAPI: HTTP Header Auth key
YouTube: OAuth2 credentials
3 Airtable Configuration
Base: ""Youtube Videos""
Tables:
""Ideas""
""Production""
Share with the Airtable token
4 Google Sheets Configuration
Document: ""Youtube Videos""
Sheets:
""Ideas""
""Production""
Share with the Google Sheets credential email
Shotstack Configuration (Assumed)
API key and endpoint for video rendering (not specified in JSON)
Setup Instructions
Import the Workflow
Import the provided JSON file into your n8n instance
Configure Credentials
Add OpenAI, Airtable, Google Sheets, PIAPI, and YouTube credentials in n8n‚Äôs credential manager
Set Up Airtable
Create or update the ""Ideas"" and ""Production"" tables with the specified fields
Ensure proper permissions with the Airtable token
Configure Google Sheets
Create or update the ""Ideas"" and ""Production"" sheets with the specified columns
Share the document with the Google Sheets credential email
Schedule Triggers
Set ""Schedule Trigger"" to trigger
Test the Workflow
Run manually to verify each node‚Äôs functionality
Check Airtable, Google Sheets, and YouTube for updates
Ensure PIAPI image generation completes (wait 4 minutes)
Monitor and Adjust
Monitor API rate limits (e.g, PIAPI, YouTube)
Adjust wait times if image or video generation delays occur
Verify connections for unlinked nodes
Benefits
Efficiency: Automates the entire process from idea to posting
Scalability: Handles daily video production
Quality: Ensures cinematic scripts and visuals
Tracking: Provides detailed progress monitoring
Notes
Rate Limits: Be aware of PIAPI (150,000 units per task) and YouTube API limits
Timing: Adjust wait nodes (e.g, ""Wait for 4 Min"") based on API response times
Conclusion and Suggestions to Improve the Automation
Conclusion
This workflow streamlines YouTube video production, making it ideal for busy content creators, video producers, and marketing teams. Customize it by adding more platforms, adjusting wait delays for image/video generation, or enhancing notifications for status updates. Share your feedback in the n8n community to help others benefit from this automation.
Suggestions to Improve
Customization: Adjust prompts, timings, and API parameters based on your needs
Expand ""Text-to-Music"" with multiple music styles or integrate a royalty-free music API for variety.
Improve Video Rendering: Optimize ""Build Shotstack Timeline"" and ""ShotStack Render Video"" with predefined templates to reduce rendering time.
Add Analytics Tracking: Include a node to fetch YouTube Analytics post-upload for performance monitoring.
Error Handling: Add error-catching nodes (e.g., ""If"" nodes) to retry failed image or video generations.
Notification System: Incorporate email or Slack notifications for key milestones (e.g., script completion, video upload)."
Automate Business Lead Scraping from Apify to Google Sheets with Data Cleaning,https://n8n.io/workflows/4295-automate-business-lead-scraping-from-apify-to-google-sheets-with-data-cleaning/,"üöÄ Automated Lead Scraper Workflow (Apify + n8n + Google Sheets)
üß† What It Does
This n8n workflow automates the process of scraping leads using Apify, cleaning the extracted data, and exporting it to Google Sheets‚Äîready for use in outreach, prospecting, or CRM pipelines.
üîÑ Workflow Steps
‚úÖ Start ‚Äì Manually triggers the workflow.
üß© Set Variables ‚Äì Stores required Apify credentials:
APIFY_TOKEN: Your Apify token.
APIFY_TASK_ID: The Apify task to run.
üï∏Ô∏è Run Apify Scraper ‚Äì Launches the scraper and fetches the dataset.
üßπ Clean Data ‚Äì Processes scraped results to:
‚úÇÔ∏è Strip non-numeric characters from phone numbers.
‚úâÔ∏è Format emails (lowercase + trimmed).
üìä Export to Google Sheets ‚Äì Appends clean data to your spreadsheet:
üè¢ company name ‚Üí from title
üìû phone ‚Üí cleaned number
üìç address ‚Üí from scraped info
üõ†Ô∏è Requirements
üï∑Ô∏è Apify Account
A valid APIFY_TOKEN
An existing Apify task (APIFY_TASK_ID)
üìó Google Sheets Access
OAuth2 credentials set up in n8n (e.g., ""Google Sheets account 2"")
üö¶ How to Use
‚öôÔ∏è Open the Variables node and plug in your Apify credentials.
üìÑ Confirm the Google Sheets node points to your desired spreadsheet.
‚ñ∂Ô∏è Run the workflow manually from the Start node.
üì• Output
A ready-to-use sheet of cleaned lead data containing:
Company names
Phone numbers
Addresses
üíº Perfect For:
Sales teams doing outbound prospecting
Marketers building lead lists
Agencies running data aggregation tasks"
Auto-classify Gmail emails with AI and apply labels for inbox organization,https://n8n.io/workflows/4876-auto-classify-gmail-emails-with-ai-and-apply-labels-for-inbox-organization/,"Who is this for?
Professionals and individuals who receive high volumes of emails, those who want to automatically organize their Gmail inbox using AI classification.
What problem is this workflow solving?
Manual email sorting is time-consuming and inconsistent. This workflow automatically categorizes incoming emails into 8 predefined labels (To respond, FYI, Comment, Notification, Meeting update, Awaiting reply, Actioned, Marketing) to help maintain inbox zero and prioritize responses.
What this workflow does
Monitors Gmail for new incoming emails
Uses AI to analyze email content and classify into appropriate categories
Automatically applies the corresponding Gmail label
Runs on a schedule to process emails consistently
Setup
Prerequisites
n8n instance (cloud or self-hosted)
Gmail account with API access enabled
Access to an LLM provider (OpenAI, Anthropic Claude, or similar)
Step-by-Step
Configure Gmail Credentials
Create Gmail Labels
Configure LLM Chain
Set Email Polling Schedule
Test the Workflow
Create Gmail Labels
Before running the workflow, create these 8 labels in your Gmail account:
To respond
FYI
Comment
Notification
Meeting update
Awaiting reply
Actioned
Marketing
How to customize this workflow to your needs
Modify Classification Categories
To change the email categories, update two places:
In the AI prompt (Basic LLM Chain node):
1. Your new category - Description of what emails fit here
2. Another category - Description
[... continue with your categories]
In Gmail labels:
Create corresponding labels in your Gmail account with the exact same names and numbering.
Adjust Classification Rules
The AI prompt contains specific rules for each category. To modify:
Edit the ""Key classification rules"" section in the LLM prompt
Add examples of emails that should go into each category
Specify edge cases and how they should be handled
Change Email Sources
Currently monitors all incoming emails. To filter specific emails:
In the Gmail Trigger node, add filters such as:
from:specific-sender@domain.com
subject:contains-keyword
-label:already-processed
You can also change this use Outlook
Modify Polling Frequency
More frequent: Add multiple poll times (e.g., 9 AM, 12 PM, 6 PM)
Less frequent: Change to once daily or weekly
Real-time: Switch to webhook-based triggering (requires Gmail API setup)
I choose daily for cost."
Analyze Crypto News Sentiment for Any Token with GPT-4o and Telegram Alerts,https://n8n.io/workflows/4740-analyze-crypto-news-sentiment-for-any-token-with-gpt-4o-and-telegram-alerts/,"A sentiment intelligence sub-agent for the Binance Spot Market Quant AI Agent. It aggregates crypto news from major sources, filters by token keyword (e.g., BTC, ETH), and produces a Telegram-ready summary including market sentiment and top headlines‚Äîpowered by GPT-4o.
üé• Live Demo:
üõ†Ô∏è Workflow Function
This tool performs the following steps:
üîß Step üìå Description
Webhook Input Accepts { ""message"": ""symbol"" } via HTTP POST
Crypto Keyword Extractor GPT model extracts the valid crypto symbol (e.g., ""SOL"", ""DOGE"", ""ETH"")
RSS News Aggregators Pulls latest headlines from 9+ crypto sources (CoinDesk, Cointelegraph, etc.)
Merge & Filter Articles Keeps only articles containing the specified token
Prompt Builder Creates prompt for GPT with filtered headlines
GPT-4o Summarizer Summarizes news into 3-part response: Summary, Sentiment, Headline Links
Telegram Formatter Converts GPT output into a Telegram-friendly message
Response Handler Returns formatted message to the caller via webhook
üì• Webhook Trigger Format
{
  ""message"": ""ETH""
}
This triggers a full execution of the workflow and returns output like:
üì£ ETH Sentiment: Neutral

‚Ä¢ BlackRock‚Äôs tokenized fund expands to Ethereum mainnet (CoinDesk)  
‚Ä¢ Ethereum fees remain high, analysts call for L2 migration (NewsBTC)  
‚Ä¢ Vitalik warns about centralized risks in staking (Cointelegraph)
üìö Installation Guide
1. Import & Enable
Load the .json into your n8n Editor
Enable webhook trigger in the top-right corner
Ensure it's reachable via POST /webhook/custom-path
2. Required Credentials
OpenAI API Key (GPT-4o capable)
No API keys required for RSS feeds
3. Connect to Quant Agent
Add an HTTP Request node in your main AI agent
Point to this workflow's webhook with body { ""message"": ""symbol"" }
Capture the response to include in your Telegram output
üîç Real Use Cases
Scenario Result
BTC Sentiment before a key event Returns 8‚Äì12 filtered articles with bullish/neutral/bearish tone
Daily pulse for altcoins like DOGE Shows relevant headlines, helpful for intraday trading setups
Telegram chatbot integration Enables user to query sentiment via /sentiment ETH
Macro context for Quant AI outputs Adds emotional/news context to technical-based trade decisions
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected.
No unauthorized rebranding or resale permitted.
üîó For support: LinkedIn ‚Äì Don Jayamaha"
"Personalized AI Tech Newsletter Using RSS, OpenAI and Gmail",https://n8n.io/workflows/3986-personalized-ai-tech-newsletter-using-rss-openai-and-gmail/,"Combine Tech News in a Personalized Weekly Newsletter
This n8n template automates the collection, storage, and summarization of technology news from top sites, turning it into a concise, personalized weekly newsletter.
If you like staying informed but want to reduce daily distractions, this workflow is perfect for you. It leverages RSS feeds, vector databases, and LLMs to read and curate tech content on your behalf‚Äîso you only receive what truly matters.
How it works
A daily scheduled trigger fetches articles from multiple popular tech RSS feeds like Wired, TechCrunch, and The Verge.
Fetched articles are:
Normalized to extract titles, summaries, and publish dates.
Converted to vector embeddings via OpenAI and stored in memory for fast semantic querying.
A weekly scheduled trigger activates the AI summarization flow:
The AI is provided with your interests (e.g., AI, games, gadgets) and the desired number of items (e.g., 15).
It queries the vector store to retrieve relevant articles and summarizes the most newsworthy stories.
The summary is converted into a clean, email-friendly format and sent to your inbox.
How to use
Connect your OpenAI and Gmail accounts to n8n.
Customize the list of RSS feeds in the ‚ÄúSet Tech News RSS Feeds‚Äù node.
Update your interests and number of desired news items in the ‚ÄúYour Topics of Interest‚Äù node.
Activate the workflow and let the automation run on schedule.
Requirements
OpenAI credentials for embeddings and summarization
Gmail (or another email service) for sending the newsletter
Customizing this workflow
Want to use different sources? Swap in your own RSS feeds, or use an API-based news aggregator.
Replace the in-memory vector store with Pinecone, Weaviate, or another persistent vector DB for longer-term storage.
Adjust the agent's summarization style to suit internal updates, industry-specific briefings, or even entertainment recaps.
Prefer chat over email? Replace the email node with a Telegram bot to receive your personalized tech newsletter directly in a Telegram chat."
"Automated Multi-Platform Sales Agent with RAG, CRM & Payment Processing",https://n8n.io/workflows/4859-automated-multi-platform-sales-agent-with-rag-crm-and-payment-processing/,"Turn More Website and Social Media Leads into Sales, Automatically.
This AI workflow instantly connects with new leads from your website, WhatsApp, Instagram, and Facebook, acting as your 24/7 automated sales agent to boost sales.
Who is it for?
This solution is designed for:
Businesses wanting to maximize sales from their online leads.
Sales Teams looking to automate lead engagement and close deals faster.
Companies that need to provide instant, personalized responses on all channels.
Use Case: Turn Conversations into Conversions, 24/7
When a customer messages you on any channel, our AI instantly starts a natural conversation. It understands their needs, suggests the right products, and can even offer a special discount to close the deal. The AI sends payment links directly in the chat and books meetings in your calendar, all while automatically updating your CRM. Every lead is handled perfectly, from start to finish.
How This Workflow Works:
This intelligent system streamlines your entire sales process:
Lead Capture: Engages leads from website forms, web chat, WhatsApp, Instagram, and Facebook.
AI-Powered Sales: Qualifies leads, answers questions, and recommends products using your business information.
Automated Actions: Sends payment links, offers discounts, books meetings, and updates your CRM automatically.
Multi-Channel Communication: Manages the entire conversation on the customer's preferred platform.
How to Set It Up:
Connect your tools: Link your social media, CRM, calendar, and payment gateways.
Create a knowledge base: Give the AI your product details, sales info, and company FAQs.
Personalize the AI: Customize the agent‚Äôs personality and responses to match your brand's voice.
Go Live: Embed the chat on your website and activate it across your social channels to start selling automatically.
DEMO DATABASE -
https://airtable.com/appFUS6RjSdbJhtR0/shrIEUJf9egBqDl8q"
"AI-Generated LinkedIn Posts with OpenAI, Google Sheets & Email Approval Workflow",https://n8n.io/workflows/4005-ai-generated-linkedin-posts-with-openai-google-sheets-and-email-approval-workflow/,"How it works
This workflow automates the process of creating, approving, and optionally posting LinkedIn content from a Google Sheet. Here's a high-level overview:
Scheduled Trigger: Runs automatically based on your defined time interval (daily, weekly, etc.).
Fetch Data from Google Sheets: Pulls the first row from your sheet where Status is marked as Pending.
Generate LinkedIn Post Content: Uses OpenAI to create a professional LinkedIn post using the Post Description and Instructions from the sheet.
Format & Prepare Data: Formats the generated content along with the original instruction and post description for email.
Send for Approval: Sends an email to a predefined user (e.g., marketing team) with a custom form for approval, including a dropdown to accept/reject and an optional field for edits.
(Optional) Image Fetch: Downloads an image from a URL (if provided in the sheet) for future use in post visuals.
Set up steps
You‚Äôll need the following before you start:
A Google Sheet with the following columns: Post Description, Instructions, Image (URL), Status
Access to an OpenAI API key
A connected Gmail account for sending approval emails
Your own Google Sheets and Gmail credentials added in n8n
Steps:
Google Sheet Preparation:
Create a new Google Sheet with the mentioned columns (Post Description, Instructions, Image, Status, Output, Post Link).
Add a row with test data and set Status to Pending.
Credentials:
In n8n, create OAuth2 credentials for:
a. Google Sheets
b. Gmail
c. OpenAI (API Key)
Assign these credentials to the respective nodes in the JSON.
OpenAI Model:
Choose a model like gpt-4o-mini (used here) or any other available in your plan.
Adjust the prompt in the ""Generate Post Content"" node if needed.
Email Configuration:
In the Gmail node, set the recipient email to your own or your team‚Äôs address.
Customize the email message template if necessary.
Schedule the Workflow:
Set the trigger interval (e.g., every morning at 9 AM).
Testing:
Run the workflow manually first to confirm everything works.
Check Gmail for the approval form, respond, and verify the results."
"Automate Email & Calendar Management with Gmail, Google Calendar & GPT-4o AI",https://n8n.io/workflows/4366-automate-email-and-calendar-management-with-gmail-google-calendar-and-gpt-4o-ai/,"Boost your productivity with this AI-powered email and calendar assistant:
This AI-powered template has 2 workflows. It manages your Gmail inbox and Google Calendar, classifies emails with custom labels, and suggests replies and meeting times ‚Äî all fully automated with OpenAI and n8n.
Automatically analyze your Gmail inbox
Suggest replies, priorities, and meeting times
Checks your Google Calendar for conflicts and free slots
Maintain conversation context using Thread History Vector Store
The agent proactively acts using a Tools Agent architecture, with integrated memory and real-time tool invocation. It's perfect for busy professionals who want a personal assistant for communication and scheduling.
Included features:
‚úÖ Do actions on incoming mails 8like Labeling etc)
‚úÖ Summarize and Assist fot the latest emails
‚úÖ Draft replies and schedule meetings contextually
‚úÖ Handle time zone conversion and MessageID referencing
‚úÖ Context retention of last conversation history - using VectorStore
üì¶ Requirements:
Gmail + Google Calendar credentials via n8n credentials
OpenAI API key
n8n VectorStore nodes (or external integration like Pinecone, Qdrant, or Chroma)"
"AI-Powered RAG Document Processing & Chatbot with Google Drive, Supabase, OpenAI",https://n8n.io/workflows/4551-ai-powered-rag-document-processing-and-chatbot-with-google-drive-supabase-openai/,"Who is this for?
This workflow is perfect for:
Businesses and teams who need an automated solution to organize, analyze, and retrieve insights from their internal documents.
Researchers who want to quickly analyze and query large collections of research papers, reports, or datasets.
Customer support teams looking to streamline access to product documentation and support resources.
Legal and compliance professionals needing to reference and query legal documents with confidence.
AI enthusiasts and developers wanting to implement Retrieval-Augmented Generation (RAG) systems without starting from scratch.
What problem is this workflow solving?
Manually organizing, processing, and searching through documents can be time-consuming, error-prone, and inefficient. This workflow solves that by:
Automating document processing from Google Drive, supporting multiple formats like PDFs, CSVs, and Google Docs.
Extracting, chunking, and enhancing document text, preserving context and improving AI comprehension.
Storing vector embeddings in a secure, scalable Supabase vector database, enabling semantic search and retrieval.
Providing an interactive AI chat interface that allows users to ask natural language questions and get precise, document-based answers.
This means teams can quickly access relevant insights from their document repositories‚Äîboosting productivity and ensuring accurate information retrieval.
Key Features
üöÄ End-to-End Document Processing: From Google Drive upload detection to vector embedding and storage.
üîç Semantic Search & Retrieval: Users can ask complex, natural-language questions and receive contextually relevant answers.
ü§ñ AI-Powered Summaries & Metadata: Automatically generates document titles and summaries using Google Gemini AI.
üìù Smart Chunking & Contextual Enhancement: Breaks documents into smart chunks with overlap, preserving context and table integrity.
üîê Secure & Scalable Vector Database: Stores and retrieves embeddings in a Supabase vector store for fast, reliable searches.
üí¨ Conversational AI Interface: Uses OpenAI to power natural, accurate, and cost-effective AI chat interactions.
How does this workflow work?
Monitors Google Drive for new files
Extracts text from PDFs and CSVs (or Google Docs auto-converted)
Splits text into context-preserving chunks
Enhances chunk quality and stores embeddings in Supabase
Enables natural language search and AI-powered chat interactions with the stored documents
Typical Use Cases
üìö Corporate Knowledge Base
üî¨ Research Paper Analysis
üìû Customer Support Document Query
‚öñÔ∏è Legal Document Review and Analysis
üîç Internal Team Documentation Search
Why You‚Äôll Love It
This workflow lets you build a scalable, searchable, and AI-powered document system‚Äîwithout needing to write complex code or manage multiple systems. With this, you can:
Stay organized with automated document processing.
Deliver faster, more accurate answers to user queries.
Reduce manual work and improve productivity.
Gain a competitive edge with cutting-edge AI search capabilities.
Setup Requirements
An n8n instance with Google Drive, Supabase, OpenAI, and Gemini credentials configured.
Access to a Supabase vector store for storing document embeddings.
Configurable chunk size, overlap, and processing limits (default: 1000 characters per chunk, 20 chunks max)."
Binance Spot Market Quant AI Agent | GPT-4o + Telegram (Main Interface),https://n8n.io/workflows/4739-binance-spot-market-quant-ai-agent-or-gpt-4o-telegram-main-interface/,"A professional-grade AI automation system for spot market trading insights on Binance. It analyzes multi-timeframe technical indicators, live price/order data, and crypto sentiment, then delivers fully formatted Telegram-style trading reports.
üé• Live Demo:
üß© Required Workflows
You must install and activate all of the following workflows for the system to function correctly:
‚úÖ Workflow Name üìå Function Description
Binance Spot Market Quant AI Agent Final AI orchestrator. Parses user prompt and generates Telegram-ready reports.
Binance SM Financial Analyst Tool Calls indicator tools and price/order data tools. Synthesizes structured inputs.
Binance SM News and Sentiment Analyst Webhook Tool Analyzes crypto sentiment, gives summary and headlines via POST webhook.
Binance SM Price/24hrStats/OrderBook/Kline Tool Pulls price, order book, 24h stats, and OHLCV klines for 15m‚Äì1d.
Binance SM 15min Indicators Tool Calculates 15m RSI, MACD, BBANDS, ADX, SMA/EMA from Binance kline data.
Binance SM 1hour Indicators Tool Same as above but for 1h timeframe.
Binance SM 4hour Indicators Tool Same as above but for 4h timeframe.
Binance SM 1day Indicators Tool Same as above but for 1d timeframe.
Binance SM Indicators Webhook Tool Technical backend. Handles all webhook logic for each timeframe tool.
‚öôÔ∏è Installation Instructions
Step 1: Import Workflows
Open your n8n Editor UI
Import each workflow JSON file one by one
Activate them or ensure they're called via Execute Workflow
Step 2: Set Credentials
OpenAI API Key (GPT-4o recommended)
Binance endpoints are public (no auth required)
Step 3: Configure Webhook Endpoints
Deploy Binance SM Indicators Webhook Tool
Ensure the following paths are reachable:
/webhook/15m
/webhook/1h
/webhook/4h
/webhook/1d
Step 4: Telegram Integration
Create a Telegram bot using @BotFather
Add your Telegram API token to n8n credentials
Replace the Telegram ID placeholder with your own
Step 5: Final Trigger
Trigger the Binance Spot Market Quant AI Agent manually or from Telegram
The agent:
Extracts the trading pair (e.g. BTCUSDT)
Calls all tools for market data and sentiment
Generates a clean, HTML-formatted Telegram report
üí¨ Telegram Report Output Format
&lt;b&gt;BTCUSDT Market Report&lt;/b&gt;

&lt;b&gt;Spot Strategy&lt;/b&gt;
‚Ä¢ Action: Buy
‚Ä¢ Entry: $63,800 | SL: $61,200 | TP: $66,500
‚Ä¢ Rationale:
‚ÄÉ- MACD Crossover (1h)
‚ÄÉ- RSI Rebound from Oversold (15m)
‚ÄÉ- Sentiment: Bullish

&lt;b&gt;Leverage Strategy&lt;/b&gt;
‚Ä¢ Position: Long 3x
‚Ä¢ Entry: $63,800
‚Ä¢ SL/TP zones same as above

&lt;b&gt;News Sentiment:&lt;/b&gt; Slightly Bullish  
‚Ä¢ ""Bitcoin rallies as ETF inflows surge"" ‚Äì CoinDesk  
‚Ä¢ ""Whales accumulate BTC at key support"" ‚Äì NewsBTC  
üß† System Overview
[Telegram Trigger]
‚Üí [Session + Auth Logic]
‚Üí [Binance Spot Market Quant AI Agent]
‚Üí [Financial Analyst Tool + News Tool]
‚Üí [All Technical Indicator Tools (15m, 1h, 4h, 1d)]
‚Üí [OrderBook/Price/Kline Fetcher]
‚Üí [GPT-4o Reasoning]
‚Üí [Split & Send Message to Telegram]
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected.
No unauthorized rebranding or resale permitted.
üîó For support: LinkedIn ‚Äì Don Jayamaha"
‚ú®ü§ñAutomate Multi-Platform Social Media Content Creation with AI,https://n8n.io/workflows/3066-automate-multi-platform-social-media-content-creation-with-ai/,"Automate Multi-Platform Social Media Content Creation with AI
Who is this for?
Social Media Managers and Digital Marketers seeking to streamline content production across 7+ platforms (X/Twitter, Instagram, LinkedIn, Facebook, TikTok, Threads, YouTube Shorts) using AI-powered automation.
What problem does this solve?
Creating platform-optimized content at scale while maintaining brand consistency across multiple channels, reducing manual work by 80% through AI generation and automated publishing.
What this workflow does
AI Content Generation:
Uses GPT-4/Gemini to create platform-specific posts
Automatically generates hashtags, CTAs, and emoji placement
Supports image/video suggestions and image creation using OpenAI or Pollinations.ai
Uses SERP api to search for relavent content
Approval Workflow:
Sends formatted HTML emails for human review
Implements double-approval system with Gmail integration
Cross-Platform Publishing:
One-click deployment to:
Instagram/Facebook (via Graph API)
X/Twitter (Official API)
LinkedIn (Sales Navigator integration)
Setup
Credentials:
OpenAI API key
Google Gemini API
Social media platform tokens (X, LinkedIn, Facebook)
ImgBB for image hosting
Gmail
SERP API
Telegram
Configuration:
Update all ""your-unique-id"" placeholders in API nodes
Set email recipients in Gmail nodes
Customize AI prompts
Customization:
Adjust character limits per platform
Modify approval thresholds
Add/remove social platforms as needed
How to customize
Content Style: Edit prompt templates in the ""Social Media Content Factory"" agent node
Approval Process: Modify email templates
Analytics: Connect to Google Sheets for performance tracking
Image Generation: Switch between Pollinations.ai/DALL-E/Midjourney"
AI Customer Support Assistant ¬∑ WhatsApp Ready ¬∑ Works for Any Business,https://n8n.io/workflows/3859-ai-customer-support-assistant-whatsapp-ready-works-for-any-business/,"AI Customer-Support Assistant that auto-maps any business site, answers WhatsApp in real time, and lets you earn or save thousands by replacing pricey SaaS chat tools.
‚ö° What the workflow does
Live ‚ÄúAI employee‚Äù - the bot crawls pages on demand (products, policies, FAQs) so you never upload documents or fine-tune a model.
No-code setup - Drop in API keys, paste your domain, publish the webhook‚Äîready in ~15 min.
Chat memory - each conversation turn is written to Supabase/Postgres and automatically replayed into the next prompt, letting the assistant remember context so follow-up questions feel natural and coherent even across long sessions.
WhatsApp ready - Free-form replies inside the 24-hour service window, automatically switches to a template when required (recommended by Meta).
üöÄ Why you‚Äôll love it
Benefit Impact
Zero content training Point the AI Agent at any domain ‚Üí go live.
Save or earn money Replace pricey SaaS chat tools or sell white-label bots to clients.
Channel-agnostic Ships with WhatsApp; swap one node for Telegram, Slack, or web chat.
Flexible voice Adjust tone & language by editing one prompt line.
üß∞ Prerequisites (all free-tier friendly)
OpenAI key
Meta WhatsApp Cloud API number + permanent token (easy setup)
Supabase (or Postgres) URL for chat memory (easy setup)
üõ† 5-step setup
Import the template into n8n.
Add credentials for OpenAI, WhatsApp, and Supabase.
Enter your root domain in the root_url variable.
Point Meta‚Äôs Webhook to the n8n URL.
Hit Execute Trigger and send ‚ÄúHi‚Äù from WhatsApp‚Äîwatch the bot answer with live data.
üîÑ Easy to extend
Voice & language ‚Äì change wording in the System Prompt.
Escalation ‚Äì add an ‚ÄúIf fallback‚Äù branch ‚Üí Zendesk, email, live agents.
Extra channels ‚Äì duplicate the reply node for Telegram or Slack.
Commerce API hooks ‚Äì plug in Shopify, Woo, Stripe for order status or payments.
üí° Monetization ideas
Replace costly SaaS seats. Deploy the bot on your own server and stop paying $300‚Äì$500 every month for third-party ‚ÄúAI support‚Äù platforms.
Sell it as a service. Spin up a branded instance for local shops, clinics, or e-commerce stores and charge each client $300‚Äì$500 per month‚Äîsetup time is under 15 minutes.
Upsell premium coverage (24/7 human hand-off) once the bot handles routine questions.
Embed product links in answers and earn affiliate or upsell revenue automatically.
Spin it up, connect a domain and a phone number, and you‚Äîor your customers‚Äîget enterprise-grade support without code, training, or ongoing licence fees."
Fully Automated AI Video Generation & Multi-Platform Publishing,https://n8n.io/workflows/3442-fully-automated-ai-video-generation-and-multi-platform-publishing/,"Description
This comprehensive n8n automation template orchestrates a complete end-to-end workflow for generating engaging short-form Point-of-View (POV) style videos using multiple AI services and automatically publishing them across major social media platforms. It takes ideas from a Google Sheet and transforms them into finished videos with captions, voiceovers, and platform-specific descriptions, ready for distribution.
Who Is This For?
Content Creators & Agencies: Mass-produce unique short-form video content for various clients or channels with minimal manual effort.
Digital Marketers: Automate video content pipelines to boost online presence and engagement across multiple platforms simultaneously.
Social Media Managers: Schedule and distribute consistent video content efficiently without juggling multiple tools and manual uploads.
Businesses: Leverage AI to create branded video content for marketing, reducing production time and costs.
What Problem Does This Workflow Solve?
Creating and distributing high-quality short-form video content consistently across multiple social networks is incredibly time-consuming and resource-intensive. This workflow tackles these challenges by:
Automating Idea-to-Video Pipeline: Generates video concepts, image prompts, scripts, images, video clips, and voiceovers using AI.
Streamlining Video Assembly: Automatically combines generated assets into a final video using a template.
Generating Platform-Optimized Descriptions: Creates relevant descriptions for posts by transcribing the final video audio.
Automating Multi-Platform Publishing: Uploads the final video and description to TikTok, Instagram, YouTube, Facebook, and LinkedIn simultaneously.
Reducing Manual Workload: Drastically cuts down the time and effort required for video production and distribution.
Centralized Tracking: Updates a Google Sheet with results, costs, and status for easy monitoring.
How It Works
Trigger & Input: Runs on a daily schedule (configurable) and fetches new video ideas from a designated Google Sheet.
AI Content Generation:
Uses OpenAI to generate video captions and image prompts based on the idea.
Uses PiAPI (Flux) to generate images from prompts.
Uses PiAPI (Kling) to generate video clips from the images (Image-to-Video).
Uses OpenAI to generate a voiceover script based on the captions.
Uses ElevenLabs to generate voiceover audio from the script and uploads it to Google Drive.
Video Assembly: Combines the generated video clips, captions, and voiceover audio using a Creatomate template to render the final video.
Description Generation: Uploads the final video to Google Drive, extracts the audio using OpenAI (Whisper), and generates a social media description using OpenAI (GPT).
Multi-Platform Distribution: Uses upload-post.com to upload the final video and generated description to TikTok, Instagram, YouTube, Facebook, and LinkedIn.
Tracking & Notification: Updates the original Google Sheet row with output details (video link, costs, tokens used) and sends a completion notification via Discord.
Setup
Accounts & API Keys: Obtain accounts and generate API keys/credentials for:
n8n
Google Cloud Platform (for Google Sheets & Google Drive APIs + OAuth Credentials)
OpenAI
PiAPI
ElevenLabs
Creatomate
upload-post.com
Discord (Webhook URL)
Google Sheet: Make a copy of the provided Google Sheet Template and connect it in the Load Google Sheet node.
Creatomate Template: Set up a video template in Creatomate (use the provided JSON source code as a base) and note its Template ID.
Configure Nodes:
Enter all API Keys/Credentials in the Set API Keys node and other relevant credential sections (Google nodes, upload-post nodes, etc.).
Configure Google Drive nodes (Folder IDs, Permissions).
Configure the upload-post.com nodes with your user identifier and necessary platform details (e.g., Facebook Page ID).
Customize AI prompts within the OpenAI nodes (Generate Video Captions, Generate Image Prompts, Generate Script, Generate Description...) if desired.
Set the Discord Webhook URL in the Notify me on Discord node.
Enable Google APIs: Ensure Google Drive API and Google Sheets API are enabled in your Google Cloud Project.
Requirements
Accounts: n8n, Google (Sheets, Drive, Cloud Platform), OpenAI, PiAPI, ElevenLabs, Creatomate, upload-post.com, Discord.
API Keys & Credentials: API Keys for OpenAI, PiAPI, ElevenLabs, Creatomate, upload-post.com. Google Cloud OAuth 2.0 Credentials. Discord Webhook URL.
Templates: A configured Google Sheet based on the template, a configured Creatomate video template.
(Potentially) Paid Plans: Some services (OpenAI, PiAPI, Creatomate, upload-post.com) may require paid plans depending on usage volume after free trials/credits are exhausted.
Use this template to build a powerful, automated video content factory, scaling your production and distribution efforts across the social media landscape."
LinkedIn Auto Message Router and Responder with Request Detection,https://n8n.io/workflows/4889-linkedin-auto-message-router-and-responder-with-request-detection/,"Who is this for?
Public-facing professionals (developer advocates, founders, marketers, content creators) who get bombarded with LinkedIn messages that aren't actually for them - support requests when you're in marketing, sales inquiries when you're a devrel, partnership pitches when you handle content, etc.
What problem is this workflow solving?
When you're visible online, people assume you handle everything at your company. You end up spending hours daily playing human router, forwarding messages like ""How do I reset my password?"" or ""What's your enterprise pricing?"" to the right teams. This LinkedIn automation workflow stops you from being your company's unofficial customer service representative.
What this workflow does
This AI-powered LinkedIn DM management workflow automatically assesses incoming LinkedIn messages and routes them intelligently:
Automated Message Assessment: Receives inbound LinkedIn messages via UniPile and looks up sender details from both personal and company LinkedIn profiles.
Smart Route Matching: Compares the message content against your message routing workflow table in Notion, which contains:
Question: ""How can I become an n8n ambassador?""
Description: ""Route here when a user is requesting to become an n8n ambassador. Also when they're asking how they could do more to evangelize n8n in their city, or to start organizing n8n meetups and events in their city.""
Action: ""Tell the user to open the following notion page which has details on ambassador program including how to apply, as well as perks of the program: https://www.notion.so/n8n-Ambassador-Program-d883b2a130e5448faedbebe5139187ea?pvs=21""
AI Response Generation: When a message matches an existing route, this AI assistant generates a personalized response draft based on the ""Action"" instructions from your routing table.
Human-in-the-Loop Approval: Sends the draft response to Slack with approve/reject buttons, so you maintain control while saving time. Draft can be edited from within Slack on desktop and mobile.
Automated LinkedIn Responses: Once approved, sends the reply back via LinkedIn and marks the original message as handled.
The result: You stop being a human switchboard and can focus on your actual job while people still get helpful, timely responses through automated customer service. You can also add routes for things you do handle but get asked about daily (like 'How do I join your beta?' or 'What's your content strategy?') to standardize your responses.
Setup
Sign up for a UniPile account and create a webhook under the Messaging section
Set the callback URL to this workflow's production URL
Generate a UniPile API key with all required scopes and store it in your n8n credentials
Create a Slack app and enable interactive message buttons and webhooks
Here is a slack App manifest template for easy deployment in slack:
{
    ""display_information"": {
        ""name"": ""Request Router"",
        ""description"": ""A bot that alerts when a new linkedin question comes in."",
        ""background_color"": ""#12575e""
    },
    ""features"": {
        ""bot_user"": {
            ""display_name"": ""Request Router"",
            ""always_online"": false
        }
    },
    ""oauth_config"": {
        ""scopes"": {
            ""bot"": [
                ""chat:write"",
                ""chat:write.customize"",
                ""chat:write.public"",
                ""links:write"",
                ""im:history"",
                ""im:read"",
                ""im:write""
            ]
        }
    },
    ""settings"": {
        ""interactivity"": {
            ""is_enabled"": true,
            ""request_url"": ""Your webhook url here""
        },
        ""org_deploy_enabled"": false,
        ""socket_mode_enabled"": false,
        ""token_rotation_enabled"": false
    }
}
Set up your Notion database with the three-column structure (Question, Description, Action)
Configure the AI node with your preferred provider (OpenAI, Gemini, Ollama etc)
Replace placeholder LinkedIn user and organization IDs with your own
How to customize this workflow to your needs
Database Options: Swap Notion with Google Sheets, Airtable, or another database
Filtering Logic: Add custom filters based on keywords, message length, follower count, or business logic
AI Customization: Adjust the system prompt to match your brand tone and response goals
Approval Platform: Replace Slack with email, Discord, or another review platform
Team Routing: Use Slack metadata to route approvals to specific team members based on message category
Enrichment: Add secondary data enrichment using tools like Clearbit or FullContact
Response Rules: Create conditional logic for different response types based on sender profile or message content
Perfect for anyone who's tired of being their company's accidental customer service department while trying to do their real job.
This LinkedIn automation template was inspired by a live build done by Max Tkacz and Angel Menendez for The Studio."
Firecrawl AI-Powered Market Intelligence Bot: Automated News Insights Delivery,https://n8n.io/workflows/4588-firecrawl-ai-powered-market-intelligence-bot-automated-news-insights-delivery/,"Workflow Overview
This cutting-edge n8n automation is a sophisticated market research and intelligence gathering tool designed to transform web content discovery into actionable insights. By intelligently combining web crawling, AI-powered filtering, and smart summarization, this workflow:
Discovers Relevant Content:
Automatically crawls target websites
Identifies trending topics
Extracts comprehensive article details
Intelligent Content Filtering:
Applies custom keyword matching
Filters for most relevant articles
Ensures high-quality information capture
AI-Powered Summarization:
Generates concise, meaningful summaries
Extracts key insights
Provides quick, digestible information
Seamless Delivery:
Sends summaries directly to Slack
Enables instant team communication
Facilitates rapid information sharing
Key Benefits
ü§ñ Full Automation: Continuous market intelligence
üí° Smart Filtering: Precision content discovery
üìä AI-Powered Insights: Intelligent summarization
üöÄ Instant Delivery: Real-time team updates
Workflow Architecture
üîπ Stage 1: Content Discovery
Scheduled Trigger: Daily market research
FireCrawl Integration: Web content crawling
Comprehensive Site Scanning:
Extracts article metadata
Captures full article content
Identifies key information sources
üîπ Stage 2: Intelligent Filtering
Keyword-Based Matching
Relevance Assessment
Custom Domain Optimization:
AI and technology focus
Startup and innovation tracking
üîπ Stage 3: AI Summarization
OpenAI GPT Integration
Contextual Understanding
Concise Insight Generation:
3-point summary format
Captures essential information
üîπ Stage 4: Team Notification
Slack Integration
Instant Information Sharing
Formatted Insight Delivery
Potential Use Cases
Market Research Teams: Trend tracking
Innovation Departments: Technology monitoring
Startup Ecosystems: Competitive intelligence
Product Management: Industry insights
Strategic Planning: Rapid information gathering
Setup Requirements
FireCrawl API
Web crawling credentials
Configured crawling parameters
OpenAI API
GPT model access
Summarization configuration
API key management
Slack Workspace
Channel for insights delivery
Appropriate app permissions
Webhook configuration
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Multi-source crawling
üìä Advanced sentiment analysis
üîî Customizable alert mechanisms
üåê Expanded topic tracking
üß† Machine learning refinement
Technical Considerations
Implement robust error handling
Use exponential backoff for API calls
Maintain flexible crawling strategies
Ensure compliance with website terms of service
Ethical Guidelines
Respect content creator rights
Use data for legitimate research
Maintain transparent information gathering
Provide proper attribution
Workflow Visualization
[Daily Trigger]
    ‚¨áÔ∏è
[Web Crawling]
    ‚¨áÔ∏è
[Content Filtering]
    ‚¨áÔ∏è
[AI Summarization]
    ‚¨áÔ∏è
[Slack Delivery]
Connect With Me
Ready to revolutionize your market research?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your information gathering with intelligent, automated workflows!
#AIResearch #MarketIntelligence #AutomatedInsights #TechTrends #WebCrawling #AIMarketing #InnovationTracking #BusinessIntelligence #DataAutomation #TechNews"
AI Blog Generator for Shopify Product listings: Using GPT-4o and Google Sheets,https://n8n.io/workflows/4735-ai-blog-generator-for-shopify-product-listings-using-gpt-4o-and-google-sheets/,"üß† AI Blog Generator for Shopify Products using GPT-4o
The AI Blog Generator is an advanced automation workflow powered by n8n, integrating GPT-4o and Google Sheets to generate SEO-rich blog articles for Shopify products. It automates the entire process ‚Äî from pulling product data, analyzing images for nutritional information, to producing structured HTML content ready for publishing ‚Äî with zero manual writing.
üí° Key Advantages
üîó Shopify Product Sync
Automatically pulls product data (title, description, images, etc.) via Shopify API.
ü§ñ AI-Powered Nutrition Extraction
Uses GPT-4o to intelligently analyze product images and extract nutritional information.
‚úçÔ∏è SEO Blog Generation
GPT-4o generates blog titles, meta descriptions, and complete articles using both product metadata and extracted nutritional info.
üóÇÔ∏è Structured Content Output
Produces well-formatted HTML with headers, bullet points, and nutrition tables for seamless Shopify blog integration.
üìÑ Google Sheets Integration
Tracks blog creation, manages retries, and prevents duplicate publishing using a centralized Google Sheet.
üì§ Shopify Blog API Integration
Publishes the generated blog to Shopify using a two-step blog + article API call.
‚öôÔ∏è How It Works
Manual Trigger
Initiate the process using a test trigger or a scheduler.
Fetch Products from Shopify
Retrieves all product details including descriptions and images.
Extract Product Images
Splits and processes each image individually.
OCR + Nutrition AI
GPT-4o reads nutrition facts from product images. Skips items without valid info.
Check Existing Logs
References a Google Sheet to avoid duplicates and determine retry status.
AI Blog Generation
Creates a blog with headings, bullet points, intro, and a nutrition table.
Shopify Blog + Article Posting
Uses the Shopify API to publish the blog and its content.
Update Google Sheet
Logs the blog URL, HTML content, errors, and status for future reference.
üõ†Ô∏è Setup Steps
Shopify Node: Connects to your Shopify store and fetches product data.
Split Out Node: Divides product images for individual OCR processing.
OpenAI Node: Uses GPT-4o to extract nutrition data from images.
If Node: Filters for entries with valid nutrition information.
Edit Fields Node: Formats the product data for AI processing.
AI Agent Node: Generates SEO blog content.
Google Sheets Nodes: Reads and updates blog creation status.
HTTP Request Nodes: Posts the blog and article via Shopify‚Äôs API.
üîê Credentials Required
Shopify Access Token ‚Äì For retrieving product data and posting blogs
OpenAI API Key ‚Äì For GPT-4o-based AI generation and image processing
Google Sheets OAuth ‚Äì For accessing the log sheet
üë§ Ideal For
Ecommerce teams looking to automate content for hundreds of products
Shopify store owners aiming to boost organic traffic through blogging
Marketing teams building scalable, AI-driven content workflows
üí¨ Bonus Tip
The workflow is modular. You can easily extend it with internal linking, language translation, or even social media sharing ‚Äî all within the same n8n flow."
Daily News Digest: Summarize RSS Feeds with OpenAI and Deliver to WhatsApp,https://n8n.io/workflows/4709-daily-news-digest-summarize-rss-feeds-with-openai-and-deliver-to-whatsapp/,"This n8n workflow collects and summarizes news from multiple RSS feeds, using OpenAI to generate a concise summary that can be sent to WhatsApp or other destinations. Perfect for automating your daily news digest.
üîÅ Workflow Breakdown:
Schedule Trigger
Start the workflow on your desired schedule (daily, hourly, etc.).
üü® Note: Set the trigger however you wish.
RSS Feeds (My RSS 01‚Äì04)
Fetches articles from four different RSS sources.
üü® Note: You can add as many RSS feeds as you want.
Edit Fields (Edit Fields1‚Äì3)
Normalizes RSS fields (title, link, etc.) to ensure consistency across different sources.
Merge (append mode)
Combines the RSS items into a single unified list.
Filter
Optionally filter articles by keywords, date, or categories.
Limit
Limits the analysis to the 10 most recent articles.
üü® Note: This keeps the result concise and avoids overloading the summary.
Aggregate
Prepares the selected news for summarization by combining them into a single content block.
OpenAI (Message Assistant)
Summarizes the aggregated news items in a clean and readable format using AI.
Send Summary to WhatsApp
Sends the AI-generated summary to a WhatsApp endpoint via webhook (yoururlapi.com). You can replace this with an email service, Google Drive, or any other destination.
üü® Note: You can send it to your WhatsApp API, email, drive, etc.
No Operation (End)
Final placeholder to safely close the workflow. You may expand from here if needed."
"Automate Blog Content Creation with OpenAI, Google Sheets & Email Approval Flow",https://n8n.io/workflows/4371-automate-blog-content-creation-with-openai-google-sheets-and-email-approval-flow/,"Who is this for?
This workflow is perfect for:
Digital marketers who need to scale SEO-optimized content production
Bloggers and content creators who want to maintain consistent publishing schedules
Small business owners who need regular blog content but lack writing resources
What problem is this workflow solving?
Creating high-quality, SEO-optimized blog content consistently is time-consuming and resource-intensive. This workflow solves that by:
Automating the content generation process from topic to final draft
Ensuring quality control through human-in-the-loop approval
Managing topic queues and preventing duplicate content creation
Streamlining the revision process based on human feedback
Organizing and archiving all generated content for future reference
What this workflow does
From topics stored in Google Sheets, this workflow:
Automatically retrieves pending topics from your Google Sheets tracking document
Generates SEO-optimized blog posts (800-1200 words) using OpenAI GPT-4 with structured prompts
Sends content for human approval via email with custom approval forms
Handles revision requests by incorporating feedback while maintaining SEO best practices
Updates topic status to prevent duplicate processing
Add approved generated content in Google Sheets for easy access and management
Routes workflow based on approval decisions (approve, revise, or cancel)
Setup
Copy the Google Sheet template here:
üëâ Automate Blog Content Creation ‚Äì Google Sheet Template
Connect Google Sheets with your topic tracking document (requires ""Topic List"" and ""Generated Content"" sheets)
Add your OpenAI API key to the AI agent nodes for content generation
Configure Gmail for the approval notification system
Set up your topic list in Google Sheets with ""Topic"" and ""Status"" columns
Customize the schedule trigger to run at your preferred intervals
Update email recipient in the approval node to your email address
Test with a sample topic marked as ""Pending"" in your Google Sheet
How to customize this workflow to your needs
Adjust content length: modify the word count requirements in the AI agent prompts
Change writing style: customize the copywriter prompts for different tones (formal, casual, technical)
Add multiple reviewers: extend the approval system to include additional stakeholders
Integrate with CMS: add nodes to automatically publish approved content to WordPress, Webflow, or other platforms
Include keyword research: add Ahrefs or SEMrush nodes to incorporate keyword data
Add image generation: integrate DALL-E or Midjourney for automatic featured image creation
Customize approval criteria: modify the approval form to include specific feedback categories
Add content scoring: integrate readability checkers or SEO analysis tools before approval"
"AI-Powered WhatsApp Chatbot ü§ñüì≤ for Text, Voice, Images & PDFs with memory üß†",https://n8n.io/workflows/3586-ai-powered-whatsapp-chatbot-for-text-voice-images-and-pdfs-with-memory/,"This workflow is a highly advanced multimodal AI assistant designed to operate through WhatsApp. It can understand and respond to text, images, voice messages, and PDF documents by combining OpenAI models with smart logic to adapt to the content received.
üéØ Core Features
üì• 1. Automatic Message Type Detection
Using the Input type node, the bot detects whether the user has sent:
Text
Voice messages
Images
Files (PDF)
Other unsupported content
üí¨ 2. Smart Text Message Handling
Text messages are processed by an OpenAI GPT-4o-mini agent with a customized system prompt.
Replies are concise, accurate, and formatted for mobile readability.
üñºÔ∏è 3. Image Analysis & Description
Images are downloaded, converted to base64, and analyzed by an image-aware AI model.
The output is a rich, structured description, designed for visually impaired users or visual content interpretation.
üéôÔ∏è 4. Voice Message Transcription & Reply
Audio messages are downloaded and transcribed using OpenAI Whisper.
The transcribed text is analyzed and answered by the AI.
Optionally, the AI reply can be converted back to voice using OpenAI's text-to-speech, and sent as an audio message.
üìÑ 5. PDF Document Extraction & Summary
Only PDFs are allowed (filtered via MIME type).
The document‚Äôs content is extracted and combined with the user's message.
The AI then provides a relevant summary or answer.
üß† 6. Contextual Memory
Each user has a personalized session ID with a memory window of 10 interactions.
This ensures a more natural and contextual conversation flow.
How It Works
Thisworkflow is designed to handle incoming WhatsApp messages and process different types of inputs (text, audio, images, and PDF documents) using AI-powered analysis. Here‚Äôs how it functions:
Trigger: The workflow starts with the WhatsApp Trigger node, which listens for incoming messages (text, audio, images, or documents).
Input Routing: The Input type (Switch node) checks the message type and routes it to the appropriate processing branch:
Text: Directly forwards the message to the AI agent for response generation.
Audio: Downloads the audio file, transcribes it using OpenAI, and sends the transcription to the AI agent.
Image: Downloads the image, analyzes it with OpenAI‚Äôs GPT-4 model, and generates a detailed description.
PDF Document: Downloads the file, extracts text, and processes it with the AI agent.
Unsupported Formats: Sends an error message if the input is not supported.
AI Processing: The AI Agent1 node, powered by OpenAI, processes the input (text, transcribed audio, image description, or PDF content) and generates a response.
Response Handling:
For audio inputs, the AI‚Äôs response is converted back into speech (using OpenAI‚Äôs TTS) and sent as a voice message.
For other inputs, the response is sent as a text message via WhatsApp.
Memory: The Simple Memory node maintains conversation context for follow-up interactions.
Setup Steps
To deploy this workflow in n8n, follow these steps:
Configure WhatsApp API Credentials:
Set up WhatsApp Business API credentials (Meta Developer Account).
Add the credentials in the WhatsApp Trigger, Get Image/Audio/File URL, and Send Message nodes.
Set Up OpenAI Integration:
Provide an OpenAI API key in the Analyze Image, Transcribe Audio, Generate Audio Response, and AI Agent1 nodes.
Adjust Input Handling (Optional):
Modify the Switch node (""Input type"") to handle additional message types if needed.
Update the ""Only PDF File"" IF node to support other document formats.
Test & Deploy:
Activate the workflow and test with different message types (text, audio, image, PDF).
Ensure responses are correctly generated and sent back via WhatsApp.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Intelligent AI Digest for Security, Privacy, and Compliance Feeds",https://n8n.io/workflows/4678-intelligent-ai-digest-for-security-privacy-and-compliance-feeds/,"How it works
This workflow acts like your own personal AI assistant, automatically fetching and summarizing the most relevant Security, Privacy, and Compliance news from curated RSS feeds. It processes only the latest articles (past 24 hours), organizes them by category, summarizes key insights using AI, and delivers a clean HTML digest straight to your inbox‚Äîsaving you time every day.
Key Highlights
Handles three independent tracks: Security, Privacy, and Compliance
Processes content from customizable RSS sources (add/remove easily)
Filters fresh articles, removes duplicates, and sorts by recency
Uses AI to summarize and format insights in a digestible format
Sends polished HTML digests via Gmail‚Äîone per category
Fully modular and extensible‚Äîadapt it to your needs
Personalization
You can easily tailor the workflow:
üéØ Customize feeds: Add or remove sources in the following Code nodes:
Fetch Security RSS, Fetch Privacy Feeds, and Fetch Compliance Feeds
üîß Modify logic: Adjust filters, sorting, formatting, or even AI prompts as needed
üß† Bring your own LLM: Works with Gemini, but easily swappable for other LLM APIs
Setup Instructions
Requires Gmail and LLM (e.g., Gemini) credentials
Prebuilt with placeholders for RSS feeds and email output
Designed to be readable, maintainable, and fully adaptable"
ü§ñüí¨ Conversational AI Chatbot with Google Gemini for Text & Image | Telegram,https://n8n.io/workflows/4365-conversational-ai-chatbot-with-google-gemini-for-text-and-image-or-telegram/,"ü§ñüí¨ Conversational AI Chatbot with Google Gemini for Text & Image | Telegram
Overview üìã
Flexible and scalable chatbot template, designed mainly for Spanish conversations but capable of handling English and other languages. Integrates Google Gemini API for text and image generation, and Telegram for messaging. Supports multiple output types with potential for video, audio, and more.
How It Works
Modular Architecture & Extensibility ‚öôÔ∏è
Separate modules for data extraction, intent analysis, and adaptive response generation.
Easily extendable with additional tools, databases, or APIs.
Context & Memory Handling üß†
Session-based volatile memory maintains conversation coherence.
Allows updates or modifications to previous images based on new requests (e.g., add a hat to a dog üê∂).
Intent Detection & Classification üîç
Google Gemini accurately classifies user intent.
Two-step validation ensures context-aware, precise replies.
Content Generation & Output ‚úçÔ∏èüñºÔ∏è
Generates text and images based on intent and context.
Designed for Telegram but adaptable to other platforms and media types (audio, video).
Key Benefits ‚úÖ
Scalable, customizable, and production-ready architecture.
Supports multiple content formats: text, images, audio, and video.
Dynamic context and memory management for smooth, coherent conversations.
Native integration with Google Gemini and Telegram ensures reliable and seamless operation.
Suitable for diverse use cases and easy to extend with new tools or platforms.
Use Cases üíº
Customer support virtual assistants.
On-demand multimedia content creation.
Marketing and user engagement bots.
AI conversational prototypes and pilots.
Requirements üë®‚Äçüíª
n8n instance (self-hosted or cloud)
Google Gemini API credentials
Telegram bot setup (optional but recommended)
Active member of the Polytechnic Artificial Intelligence Club (CIAP) at ESPOL."
"Invoice Processor & Validator with OCR, AI & Google Sheets",https://n8n.io/workflows/4247-invoice-processor-and-validator-with-ocr-ai-and-google-sheets/,"üìù Say goodbye to manual invoice checking!
This smart workflow automates your entire invoice processing pipeline using AI, OCR, and Google Sheets.
‚öôÔ∏è What This Workflow Does:
üì• 1. Reads an invoice PDF
‚Äî Select a local PDF invoice from your machine.
üîç 2. Extracts raw text using OCR
‚Äî Converts scanned or digital PDFs into readable text.
üß† 3. AI Agent processes the text
‚Äî Transforms messy raw text into clean JSON using natural language understanding.
üß± 4. Structures and refines the JSON
‚Äî Converts AI output into a structured, usable format.
üîÑ 5. Splits item-wise data
‚Äî Extracts individual invoice line items with all details.
üÜî 6. Generates unique keys
‚Äî Creates a unique identifier for each item for tracking.
üìä 7. Updates Google Sheet
‚Äî Adds extracted items to your designated sheet automatically.
üìÇ 8. Fetches master item data
‚Äî Loads your internal product master to validate against.
‚úÖ 9. Validates item name & cost
‚Äî Compares extracted items with your official records to verify accuracy.
üìå 10. Updates results per item
‚Äî Marks each item as Valid or Invalid in the sheet based on matching.
üíº Use Case:
Perfect for businesses, freelancers, or operations teams who receive invoices and want to automate validation, detect billing errors, and log everything seamlessly in Google Sheets ‚Äî all using the power of AI + n8n.
üîÅ Fast. Accurate. Zero manual work.
#OCR #AI #Invoices #Automation."
"Multi-Platform AI Sales Agent with RAG, CRM, Calendar & Stripe",https://n8n.io/workflows/4508-multi-platform-ai-sales-agent-with-rag-crm-calendar-and-stripe/,"Turn More Website and Social Media Leads into Sales, Automatically.
This AI workflow instantly connects with new leads from your website, WhatsApp, Instagram, and Facebook to boost your sales.
Who is it for?
This solution is designed for:
Product and service-based businesses committed to maximizing lead conversion and sales.
Sales Teams aiming to automate lead engagement, qualification, and accelerate sales generation.
Companies seeking to provide immediate, personalized, and consistent responses to inquiries across multiple channels.
Use Case: Turn Conversations into Conversions, 24/7
Imagine a potential customer fills out a form on your website or interact with your website chatbot or directly messages your business on WhatsApp, Instagram, or Facebook. Instantly, our intelligent workflow springs into action:
Personalized Welcome: If they filled out a form, they receive a personalized WhatsApp message referencing the key details they provided. If they messaged directly on social media or your website chat, they receive an immediate, welcoming response.
Human-like Engagement: The AI sales agent initiates a natural, human-like conversation to understand their specific goals and needs.
Intelligent Product Matching: Leveraging your product database, the agent intelligently suggests the most suitable products or services.
Expert Consultation & Meeting Booking: For complex needs requiring expert advice, the agent seamlessly books a meeting in your calendar.
Seamless CRM Integration: All interactions, customer details, and meeting information are automatically saved and updated in your CRM.
Overcoming Hesitation (Optional): If a lead shows pricing concerns, the agent can be configured to offer pre-approved discounts or alternative solutions.
Facilitating Transactions (Optional): The workflow can even send payment links to close deals directly within the conversation.
This automated yet personalized approach ensures every lead is nurtured effectively, regardless of where the conversation starts.
How This Workflow Works:
This intelligent system streamlines your sales process through a series of automated yet highly personalized steps:
Lead Capture: A prospect either fills out a form embedded on your website or initiates a conversation directly via your business's WhatsApp, Instagram, Facebook page, or the n8n Chat embedded on your website.
Instant & Personalized Outreach:
Form Submissions: An instant, personalized WhatsApp message is dispatched, incorporating details from their form submission.
Direct Messages (WhatsApp, Instagram, Facebook, n8n Chat): The AI sales agent immediately greets the user and begins a human-like conversation.
AI-Powered Engagement & Qualification: The AI agent engages the lead with relevant questions to understand their needs and qualify them based on predefined criteria. It accesses your product information (from a vector database) to provide accurate and relevant suggestions.
Tailored Call to Action: Based on the conversation and qualification:
Product/Service Recommendation: Offers the most suitable product or service with a clear call to action (e.g., view product, learn more).
Consultation Booking: If expert advice is needed or requested, the agent can schedule a meeting directly into your calendar.
CRM Update: All lead details, conversation history, and scheduled meetings are automatically logged in your CRM.
Handling Objections & Closing (Optional):
Payment Links: Can send direct payment links for purchases.
Discount Offers: If a lead expresses hesitation about pricing, the agent can be programmed to offer a predefined discount to encourage conversion.
Multi-Channel Communication: The entire interaction can seamlessly occur on WhatsApp, Instagram, Facebook, or your website's embedded n8n Chat, providing flexibility for your leads.
How to Set It Up:
Modular Agent Creation: Design distinct workflows for your CRM Agent (managing customer data), Calendar Agent (handling meeting bookings), and Billing Agent (managing payments and discounts).
Connect AI Agent Tools: Integrate each workflow with its specific AI agent communication tool (e.g., call workflow tool).
Multi-Platform App Configuration: Create an application on https://developers.facebook.com and set up WhatsApp, Instagram, and Facebook messaging capabilities, linking them to your app.
Knowledge Base Creation: Develop a vector document (e.g., in PostgreSQL) containing comprehensive details about your products, services, sales techniques, FAQs, and other relevant information for the AI to draw upon.
Credential Integration: Securely connect all necessary credentials for your CRM, calendar, payment gateways, and messaging platforms.
AI Agent Personalization: Edit the system prompt of your AI agent, filling in your specific business details, brand voice, and operational guidelines to ensure responses are accurate, relevant, and on-brand.
Website Form Integration (Recommended): Customize your lead capture form and embed it on your website to directly feed leads into the workflow.
Embeddable Web Chat (Recommended): Integrate the n8n Chat widget onto your website, allowing visitors to directly communicate with your AI sales agent.
DEMO DATABASE - https://airtable.com/appFUS6RjSdbJhtR0/shrIEUJf9egBqDl8q"
Upwork Lead Generation: Extract Client Emails with LinkedIn Scraping and AI,https://n8n.io/workflows/4794-upwork-lead-generation-extract-client-emails-with-linkedin-scraping-and-ai/,"Automated solution to extract and organize contact information from Upwork job postings, enabling direct outreach to potential clients who post jobs matching your expertise.
üöÄ What It Does
Scrapes job postings for contact information
Extracts email addresses and social profiles
Organizes leads in a structured format
Enables direct outreach campaigns
Tracks response rates
üéØ Perfect For
Freelancers looking to expand their client base
Agencies targeting specific industries
Sales professionals in the gig economy
Recruiters sourcing clients
Digital marketing agencies
‚öôÔ∏è Key Benefits
‚úÖ Access to hidden contact information
‚úÖ Expand your client base
‚úÖ Beat the competition to opportunities
‚úÖ Targeted outreach campaigns
‚úÖ Higher response rates
üîß What You Need
Upwork account
n8n instance
Email service (for outreach)
CRM (optional)
üìä Features
Email pattern detection
Social media profile extraction
Company website discovery
Lead scoring system
Outreach tracking
üõ†Ô∏è Setup & Support
Quick Setup
Start collecting leads in 20 minutes with our step-by-step guide
üì∫ Watch Tutorial
üíº Get Expert Support
üìß Direct Help
Take control of your freelance career with direct access to potential clients. Transform how you find and secure projects on Upwork."
"Competitor Price Monitoring with Web Scraping,Google Sheets & Telegram",https://n8n.io/workflows/4640-competitor-price-monitoring-with-web-scrapinggoogle-sheets-and-telegram/,"How it works
Download the google sheet here: Google sheet , upload to google sheets and replace in the google sheets node.
Scheduled trigger: Runs once a day at 8 AM (server time).
Fetch product list: Reads your ‚Äúmaster‚Äù sheet (product_url + last known price) from Google Sheets.
Loop with delay: Iterates over each row (product) one at a time, inserting a short pause (20 s) between HTTP requests to avoid blocking.
Scrape current price: Loads each product_url, extracts the current price via a simple CSS selector.
Compare & normalize: Compares the newly scraped price against the ‚Äúlast_price‚Äù from your sheet, calculates percentage change, and tags items where price_changed == true.
On price change:
Send alert: Formats a Telegram message (‚ÄúPrice Drop‚Äù or ‚ÄúPrice Hike‚Äù) and pushes it to your configured chat.
Log history: Appends a new row to a separate ‚Äúprice_tracking‚Äù tab with timestamp, old price, new price, and % change.
Update master sheet: After a 1 min pause, writes the updated current_price back to your ‚Äúmaster‚Äù sheet so future runs use it as the new baseline.
Set up step
Google Sheets credentials (~5 min)
Create a Google Sheets OAuth credential in n8n.
Copy your sheet‚Äôs ID and ensure you have two tabs:
product_data (columns: product_url, price)
price_tracking (columns: timestamp, product_url, last_price, current_price, price_diff_pct, price_changed)
Paste the sheet ID into both Google Sheets nodes (‚ÄúRead‚Äù and ‚ÄúAppend/Update‚Äù).
Telegram credentials (~5 min)
Create a Telegram Bot token via BotFather.
Copy your chat_id (for your target group or personal chat).
Add those credentials to n8n and drop them into the ‚ÄúTelegram‚Äù node.
Workflow parameters (~5 min)
Verify the schedule in the Schedule Trigger node is set to 08:00 (or adjust to your preferred run time).
In the Loop Over Items node, confirm ‚ÄúBatch Size‚Äù is 1 (to process one URL at a time).
Adjust the Delay to avoid Request Blocking node if your site requires a longer pause (default is 20 s).
In the Parse Data From The HTML Page node, double-check the CSS selector matches how prices appear on your target site.
Once credentials are in place and your sheet tabs match the expected column names, the flow should be ready to activate. Total setup time is under 15 minutes‚Äîdetailed notes are embedded as sticky comments throughout the workflow to help you tweak selectors, change timeouts, or adjust sheet names without digging into code."
Chat with PDF / MD / Text Files using GraphRAG (no vector store needed),https://n8n.io/workflows/4755-chat-with-pdf-md-text-files-using-graphrag-no-vector-store-needed/,"Set up a chat with your documents without the complex vector store setup.
This templates helps you
ingest your PDF / text / MD documents into a knowledge graph
use the graph as the knowledge base for your AI chatbots (and other workflows)
visualize the main topics and gaps in your documents (good for observability and research)
The knowledge base is provided using the InfraNodus GraphRAG with the knowledge graphs offering high-quality responses without the need to set up complex RAG vector store workflows.
The advantages of using GraphRAG instead of the standard vector stores for knowledge are:
Easy and quick to set up and update ‚Äî no complex data import workflows needed
A knowledge graph offers a holistic and interactive view of your knowledge base (accessible via our API or a web interface ‚Äî also shareable)
Better retrieval of relations between the document chunks = higher quality responses
How it works
This template uses the InfraNodus knowledge graph as a knowledge base for your n8n AI agent node.
The knowledge graph contains the documents you can upload using this template from your Google Drive.
When the user asks a question via the chat interface, the agent forwards this question to the InfraNodus knowledge graph, retrieves a response, a summary, and a list of matching statements (based advanced Graph RAG), then delivers the final response back the user.
Here's a description step by step:
Step 1: Upload your documents
Put the PDF / text / MD files you want to chat with into a folder on your Google drive
Authorize access to that folder using the Google drive node in the template.
Add the InfraNodus API key to the InfraNodus Save to Graph HTTP node
Optional: change the name of the graph you want to save the data to in the InfraNodus HTTP node (in the name field of the HTTP post request).
Run the workflow to ingest all the files and save them into the graph
Optional: check the link provided in the Step 1 workflow description to see the visualization of your knowledge base. It will look something like that:
Note: you can replace the PDF to Text convertor node with a better quality PDF convertor from ConvertAPI which respects the original file layout and doesn't split text into small chunks
Step 2: Chat with your documents
Deactive the trigger in the Step 1
Activate the chat trigger in the Step 2
Add your InfraNodus API credentials to Knowledge Base GraphRAG InfraNodus node
Optional: change the graph name in the Knowledge Base node to match the name you provided in the step 1 above
Run the chat and ask the question
Watch the magic
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Requirements
An InfraNodus account and API key
An OpenAI (or any other LLM) API key
A Google Drive OAuth access (follow the n8n instructions)
Optional: ConvertAPI API key for better quality PDF conversion
Customizing this workflow
You can customize this workflow by adding several experts to your AI agent.
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20174217658396-Using-InfraNodus-Knowledge-Graphs-as-Experts-for-AI-Chatbot-Agents-in-n8n
Also check out the video tutorial with a demo:
For support and feedback, please, contact us at https://support.noduslabs.com
To learn more about InfraNodus: https://infranodus.com"
"Search LinkedIn companies, Score with AI and add them to Google Sheet CRM",https://n8n.io/workflows/3904-search-linkedin-companies-score-with-ai-and-add-them-to-google-sheet-crm/,"Search LinkedIn companies, Score with AI and add them to Google Sheet CRM
Who is this for?
This template is ideal for sales teams, business development professionals, and marketers looking to build a targeted prospect database with automatic qualification. Perfect for agencies, consultants, and B2B companies wanting to identify and prioritize the most promising potential clients.
What problem does this workflow solve?
Manually researching companies on LinkedIn, evaluating their fit for your services, and tracking them in your CRM is time-consuming and subjective. This automation streamlines lead generation by automatically finding, scoring, and importing qualified prospects into your database.
What this workflow does
This workflow automatically searches for companies on LinkedIn based on your criteria, retrieves detailed information about each company, filters them based on quality indicators, uses AI to score how well they match your ideal customer profile, and adds them to your Google Sheet CRM while preventing duplicates.
Setup
Create a Ghost Genius API account and get your API key
Configure HTTP Request nodes with Header Auth credentials
Create a copy of the provided Google Sheet template
Set up your Google Sheet and OpenAI credentials following n8n documentation
Customize the ""Set Variables"" node to match your target audience and scoring criteria
How to customize this workflow
Modify search parameters to target different industries, locations, or company sizes
Adjust the follower count threshold based on your qualification criteria
Customize the AI scoring system to align with your specific product or service offering
Add notification nodes to alert you when high-scoring companies are identified"
Automated AI Content Creation & Instagram Publishing from Google Sheets,https://n8n.io/workflows/3840-automated-ai-content-creation-and-instagram-publishing-from-google-sheets/,"Automated AI Content Creation & Instagram Publishing from Google Sheets
This n8n workflow automates the creation and publishing of social media content directly to Instagram, using ideas stored in a Google Sheet. It leverages AI (Google Gemini and Replicate Flux) to generate concepts, image prompts, captions, and the final image, turning your content plan into reality with minimal manual intervention.
Think of this as the execution engine for your content strategy. It assumes you have a separate process (whether manual entry, another workflow, or a different tool) for populating the Google Sheet with initial post ideas (including Topic, Audience, Voice, and Platform). This workflow takes those ideas and handles the rest, from AI generation to final publication.
What does this workflow do?
This workflow streamlines the content execution process by:
Automatically fetching unprocessed content ideas from a designated Google Sheet based on a schedule.
Using Google Gemini to generate a platform-specific content concept (specifically for a 'Single Image' format).
Generating two distinct AI image prompt options based on the concept using Gemini.
Writing an engaging, platform-tailored caption (including hashtags) using Gemini, based on the first prompt option.
Generating a visual image using the first prompt option via the Replicate API (using the Flux model).
Publishing the generated image and caption directly to a connected Instagram Business account.
Updating the status in the Google Sheet to mark the idea as completed, preventing reprocessing.
Who is this for?
Social Media Managers & Agencies: Automate the execution of your content calendar stored in Google Sheets.
Marketing Teams: Streamline content production from planned ideas and ensure consistent posting schedules.
Content Creators & Solopreneurs: Save significant time by automating the generation and publishing process based on your pre-defined ideas.
Anyone using Google Sheets to plan social media content and wanting to automate the creative generation and posting steps with AI.
Benefits
Full Automation: From fetching planned ideas to Instagram publishing, automate the entire content execution pipeline.
AI-Powered Generation: Leverage Google Gemini for creative concepts, prompts, and captions, and Replicate for image generation based on your initial topic.
Content Calendar Execution: Directly turn your Google Sheet plan into published posts.
Time Savings: Drastically reduce the manual effort involved in creating visuals and text for each planned post.
Consistency: Maintain a regular posting schedule by automatically processing your queue of ideas.
Platform-Specific Content: AI prompts are designed to tailor concepts, prompts, and captions for the platform specified in your sheet (e.g., Instagram or LinkedIn).
How it Works
Scheduled Trigger: The workflow starts automatically based on the schedule you set (e.g., every hour, daily).
Fetch Idea: Reads the next row from your Google Sheet where the 'Status' column indicates it's pending (e.g., '0'). It only fetches one idea per run.
Prepare Inputs: Extracts Topic, Audience, Voice, and Platform from the sheet data.
AI Concept Generation (Gemini): Creates a single content concept suitable for a 'Single Image' post on the target platform.
AI Prompt Generation (Gemini): Develops two detailed, distinct image prompt options based on the concept.
AI Caption Generation (Gemini): Writes a caption tailored to the platform, using the first image prompt and other context.
Image Generation (Replicate): Sends the first prompt to the Replicate API (Flux model) to generate the image.
Prepare for Instagram: Formats the generated image URL and caption.
Publish to Instagram: Uses the Facebook Graph API in three steps:
Creates a media container by uploading the image URL and caption.
Waits for Instagram to process the container.
Publishes the processed container to your feed.
Update Sheet: Changes the 'Status' in the Google Sheet for the processed row (e.g., to '1') to mark it as complete.
n8n Nodes Used
Schedule Trigger
Google Sheets (Read & Update operations)
Set (Multiple instances for data preparation)
Langchain Chain - LLM (Multiple instances for Gemini calls)
Langchain Chat Model - Google Gemini (Multiple instances)
Langchain Output Parser - Structured (Multiple instances)
HTTP Request (for Replicate API call)
Wait
Facebook Graph API (Multiple instances for Instagram publishing steps)
Prerequisites
Active n8n instance (Cloud or Self-Hosted).
Google Account with access to Google Sheets.
Google Sheets API Credentials (OAuth2): Configured in n8n.
A Google Sheet structured with columns like Topic, Audience, Voice, Platform, Status (or similar). Ensure your 'pending' and 'completed' statuses are defined (e.g., '0' and '1').
Google Cloud Project with the Vertex AI API enabled.
Google Gemini API Credentials: Configured in n8n (usually via Google Vertex AI credentials).
Replicate Account and API Token.
Replicate API Credentials (Header Auth): Configured in n8n.
Facebook Developer Account.
Instagram Business Account connected to a Facebook Page.
Facebook App with necessary permissions: instagram_basic, instagram_content_publish, pages_read_engagement, pages_show_list.
Facebook Graph API Credentials (OAuth2): Configured in n8n with the required permissions.
Setup
Import the workflow JSON into your n8n instance.
Configure Schedule Trigger: Set the desired frequency (e.g., every 30 minutes, every 4 hours) for checking new ideas in the sheet.
Configure Google Sheets Nodes:
Select your Google Sheets OAuth2 credentials for both Google Sheets nodes.
In 1. Get Next Post Idea..., enter your Spreadsheet ID and Sheet Name. Verify the Status filter matches your 'pending' value (e.g., 0).
In 7. Update Post Status..., enter the same Spreadsheet ID and Sheet Name. Ensure the Matching Columns (e.g., Topic) and the Status value to update match your 'completed' value (e.g., 1).
Configure Google Gemini Nodes: Select your configured Google Vertex AI / Gemini credentials in all Google Gemini Chat Model nodes.
Configure Replicate Node (4. Generate Image...): Select your Replicate Header Auth credentials. The workflow uses black-forest-labs/flux-1.1-pro-ultra by default; you can change this if needed.
Configure Facebook Graph API Nodes (6a, 6c):
Select your Facebook Graph API OAuth2 credentials.
Crucially, update the Instagram Account ID in the Node parameter of both Facebook Graph API nodes (6a and 6c). The template uses a placeholder (17841473009917118); replace this with your actual Instagram Business Account ID.
Adjust Wait Node (6b): The default wait time might be sufficient, but if you encounter errors during publishing (especially with larger images/videos in the future), you might need to increase the wait duration.
Activate the workflow.
Populate your Google Sheet: Ensure you have rows with your content ideas and the correct 'pending' status (e.g., '0'). The workflow will pick them up on its next scheduled run.
This workflow transforms your Google Sheet content plan into a fully automated AI-powered Instagram publishing engine. Start automating your social media presence today!"
AI Chatbot Agent with a Panel of Experts using InfraNodus GraphRAG Knowledge,https://n8n.io/workflows/4402-ai-chatbot-agent-with-a-panel-of-experts-using-infranodus-graphrag-knowledge/,"Using the knowledge graphs instead of RAG vector stores
This workflow creates an AI chatbot agent that has access to several knowledge bases at the same time (used as ""experts"").
These knowledge bases are provided using the InfraNodus GraphRAG using the knowledge graphs and providing high-quality responses without the need to set up complex RAG vector store workflows.
The advantages of using GraphRAG instead of the standard vector stores for knowledge are:
Easy and quick to set up (no complex data import workflows needed)
A knowledge graph has a holistic view of your knowledge base
Better retrieval of relations between the document chunks = higher quality responses
How it works
This template uses the n8n AI agent node as an orchestrating agent that decides which tool (knowledge graph) to use based on the user's prompt.
Here's a description step by step:
The user submits a question using the AI chatbot (n8n interface, in this case, which can be accessed via a URL or embedded to any website)
The AI agent node checks a list of tools it has access to. Each tool has a description of the knowledge it has auto-generated by InfraNodus.
The AI agent decides which tool should be used to generate a response. It may reformulate user's query to be more suitable for the expert.
The query is then sent to the InfraNodus HTTP node endpoint, which will query the graph that corresponds to that expert.
Each InfraNodus GraphRAG expert provides a rich response that takes the whole context into account and provides a response from each expert (graph) along with a list of relevant statements retrieved using a combination or RAG and GraphRAG.
The n8n AI Agent node integrates the responses received from the experts to produce the final answer.
The final answer is sent back to the user's chat (or a webhook endpoint)
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Create a separate knowledge graph for each expert (using PDF / content import options) in InfraNodus
For each graph, go to the workflow, paste the name of the graph into the body name field.
Keep other settings intact or learn more about them at the InfraNodus access points page.
Once you add one or more graphs as experts to your flow, add the LLM key to the OpenAI node and launch the workflow
Requirements
An InfraNodus account and API key
An OpenAI (or any other LLM) API key
Customizing this workflow
You can use this same workflow with a Telegram bot, so you can interact with it using Telegram. There are many more customizations available.
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20174217658396-Using-InfraNodus-Knowledge-Graphs-as-Experts-for-AI-Chatbot-Agents-in-n8n
Also check out the video tutorial with a demo:"
AI YouTube Analytics Agent: Comment Analyzer & Insights Reporter,https://n8n.io/workflows/4459-ai-youtube-analytics-agent-comment-analyzer-and-insights-reporter/,"Transform YouTube comments into actionable insights with automated AI analysis and professional email reports.
This intelligent workflow monitors your Google Sheets for YouTube video IDs, fetches comments using YouTube API, performs comprehensive AI sentiment analysis, and delivers formatted email reports with viewer insights - helping content creators understand their audience and improve engagement.
üöÄ What It Does
Smart Video Monitoring: Watches Google Sheets for new YouTube video IDs marked as ""Pending"" and triggers automated analysis
Complete Comment Collection: Fetches up to 100 top comments per video using YouTube API with relevance-based ordering
AI-Powered Analysis: Uses GPT-4 to analyze comments for sentiment, themes, questions, feedback, and actionable insights
Professional Email Reports: Generates detailed HTML reports with statistics, sentiment breakdown, and improvement recommendations
Automated Status Tracking: Updates spreadsheet status to prevent duplicate processing and maintain organized workflow
üéØ Key Benefits
‚úÖ Deep Audience Insights: Understand what viewers really think about your content
‚úÖ Save Hours of Manual Work: Automated comment analysis vs reading hundreds of comments
‚úÖ Improve Content Strategy: Get actionable feedback for better video performance
‚úÖ Track Sentiment Trends: Monitor positive/negative feedback patterns
‚úÖ Professional Reporting: Receive formatted analysis reports via email
‚úÖ Scalable Analysis: Process multiple videos automatically
üè¢ Perfect For
Content Creators & YouTubers
Individual creators tracking audience engagement
Educational channels analyzing learning feedback
Entertainment creators understanding viewer preferences
Business channels monitoring brand sentiment
Marketing & Business Applications
Brand Monitoring: Track sentiment on branded content and partnerships
Audience Research: Understand viewer demographics and preferences
Content Optimization: Identify what resonates with your audience
Competitor Analysis: Analyze comments on competitor videos (where allowed)
‚öôÔ∏è What's Included
Complete Analytics Workflow: Ready-to-deploy YouTube comment analysis system
Google Sheets Integration: Simple spreadsheet-based video management
YouTube API Integration: Automated comment fetching with proper authentication
AI Analysis Engine: GPT-4 powered sentiment and insight generation
Email Reporting System: Professional HTML-formatted reports
Status Management: Automatic processing tracking and duplicate prevention
üîß Setup Requirements
n8n Platform: Cloud or self-hosted instance
YouTube API Credentials: Google Cloud Console API access
OpenAI API: GPT-4 access for comment analysis
Google Sheets: Video ID management and status tracking
Gmail Account: For receiving analysis reports
üìä Required Google Sheets Structure
| ID | Video Title | YouTube Video ID | Status |
|----|-------------|------------------|---------|
| 1  | My Tutorial | dQw4w9WgXcQ     | Pending |
| 2  | Product Demo| abc123def456    | Mail Sent |
| 3  | Weekly Vlog | xyz789uvw012    | Draft |
Status Options: Draft ‚Üí Pending ‚Üí Mail Sent
üìß Sample Analysis Report
üì∫ YouTube Comments Analysis Report
Video: ""How to Build Your First Website""

üìä Quick Statistics:
‚Ä¢ Total Comments Analyzed: 87
‚Ä¢ Average Likes per Comment: 3.2
‚Ä¢ Total Replies: 156
‚Ä¢ Sentiment Summary: Positive: 65%, Negative: 10%, Neutral: 25%

‚ùì Common Questions:
‚Ä¢ ""What hosting service do you recommend?""
‚Ä¢ ""Can I do this without coding experience?""
‚Ä¢ ""How much does domain registration cost?""

üí° Key Feedback Points:
‚Ä¢ Tutorial pace is perfect for beginners
‚Ä¢ More examples of finished websites requested
‚Ä¢ Viewers want follow-up video on advanced features

üéØ Actionable Insights:
‚Ä¢ Create hosting comparison video
‚Ä¢ Add timestamps for different skill levels
‚Ä¢ Consider beginner-friendly series expansion
üé® Customization Options
Analysis Depth: Adjust AI prompts for different analysis focuses (engagement, education, entertainment)
Comment Limits: Modify maximum comments processed (default: 100, AI analysis: 50)
Report Recipients: Send reports to multiple team members or clients
Custom Metrics: Add specific analysis criteria for your content niche
Multi-Channel: Process videos from multiple YouTube channels
Scheduling: Set up regular analysis of your latest videos
üè∑Ô∏è Tags & Categories
#youtube-analytics #comment-analysis #content-creator-tools #ai-sentiment-analysis #video-insights #audience-research #youtube-api #content-optimization #social-media-analytics #creator-economy #video-marketing #engagement-analysis #content-strategy #ai-reporting #youtube-automation
üí° Use Case Examples
Educational Channel: Analyze tutorial comments to identify confusing concepts and improve teaching methods
Product Reviews: Monitor sentiment on review videos to understand customer satisfaction trends
Entertainment Creator: Track audience reactions to different content formats and optimize future videos"
Build your own N8N Workflows MCP Server,https://n8n.io/workflows/3770-build-your-own-n8n-workflows-mcp-server/,"This n8n template shows you how to create an MCP server out of your existing n8n workflows. With this, any MCP client connected can get more done with powerful end-to-end workflows rather than just simple tools.
Designing agent tools for outcome rather than utility has been a long recommended practice of mine and it applies well when it comes to building MCP servers; In gist, agents to be making the least amount of calls possible to complete a task.
This is why n8n can be a great fit for MCP servers! This template connects your agent/MCP client (like Claude Desktop) to your existing workflows by allowing the AI to discover, manage and run these workflows indirectly.
How it works
An MCP trigger is used and attaches 4 custom workflow tools to discover and manage existing workflows to use and 1 custom workflow tool to execute them.
We'll introduce an idea of ""available"" workflows which the agent is allowed to use. This will help limit and avoid some issues when trying to use every workflow such as clashes or non-production.
The n8n node is a core node which taps into your n8n instance API and is able to retrieve all workflows or filter by tag. For our example, we've tagged the workflows we want to use with ""mcp"" and these are exposed through the tool ""search workflows"".
Redis is used as our main memory for keeping track of which workflows are ""available"". The tools we have are ""add Workflow"", ""remove workflow"" and ""list workflows"". The agent should be able to manage this autonomously.
Our approach to allow the agent to execute workflows is to use the Subworkflow trigger. The tricky part is figuring out the input schema for each but was eventually solved by pulling this information out of the workflow's template JSON and adding it as part of the ""available"" workflow's description. To pass parameters through the Subworkflow trigger, we can do so via the passthrough method - which is that incoming data is used when parameters are not explicitly set within the node.
When running, the agent will not see the ""available"" workflows immediately but will need to discover them via ""list"" and ""search"". The human will need to make the agent aware that these workflows will be preferred when answering queries or completing tasks.
How to use
First, decide which workflows will be made visible to the MCP server. This example uses the tag of ""mcp"" but you can all workflows or filter in other ways.
Next, ensure these workflows have Subworkflow triggers with input schema set. This is how the MCP server will run them.
Set the MCP server to ""active"" which turns on production mode and makes available to production URL.
Use this production URL in your MCP client. For Claude Desktop, see the instructions here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop.
There is a small learning curve which will shape how you communicate with this MCP server so be patient and test. The MCP server will work better if there is a focused goal in mind ie. Research and report, rather than just a collection of unrelated tools.
Requirements
N8N API key to filter for selected workflows.
N8N workflows with Subworkflow triggers!
Redis for memory and tracking the ""available"" workflows.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
If your targeted workflows do not use the subworkflow trigger, it is possible to amend the executeTool to use HTTP requests for webhooks.
Managing available workflows helps if you have many workflows where some may be too similar for the agent. If this isn't a problem for you however, feel free to remove the concept of ""available"" and let the agent discover and use all workflows!"
Automated Weekly Google Calendar Summary via Email with AI ‚ú®üóìÔ∏èüìß,https://n8n.io/workflows/4783-automated-weekly-google-calendar-summary-via-email-with-ai/,"Workflow: Automated Weekly Google Calendar Summary via Email with AI ‚ú®üóìÔ∏èüìß
Get a personalized, AI-powered summary of your upcoming week's Google Calendar events delivered straight to your inbox! This workflow automates the entire process, from fetching events to generating an intelligent summary and emailing it to you.
üåü Overview
This n8n workflow connects to your Google Calendar, retrieves events for the upcoming week (Monday to Sunday, based on the day the workflow runs), uses Google Gemini AI to create a well-structured and insightful summary, and then emails this summary to you. It's designed to help you start your week organized and aware of your commitments.
Key Features:
Automated Weekly Summary: Runs on a schedule (default: weekly) to keep you updated.
AI-Powered Insights: Leverages Google Gemini to not just list events, but to identify important ones and offer a brief weekly outlook.
Personalized Content: Uses your specified timezone, locale, name, and city for accurate and relevant information.
Clear Formatting: Events are grouped by day and displayed chronologically with start and end times. Important events are highlighted.
Email Delivery: Receive your schedule directly in your inbox in a clean HTML format.
Customizable: Easily adapt to your specific calendar, AI preferences, and email settings.
‚öôÔ∏è How It Works: Step-by-Step
The workflow consists of the following nodes, working in sequence:
weekly_schedule (Schedule Trigger):
What it does: Initiates the workflow.
Default: Triggers once a week at 12:00 PM. You can adjust this to your preference (e.g., Sunday evening or Monday morning).
locale (Set Node):
What it does: This is a crucial node for you to configure! It sets user-specific parameters like your preferred language/region (users-locale), timezone (users-timezone), your name (users-name), and your home city (users-home-city). These are used throughout the workflow for correct date/time formatting and personalizing the AI prompt.
date-time (Set Node):
What it does: Dynamically generates various date and time strings based on the current execution time and the locale settings. This is used to define the precise 7-day window (from the current day to 7 days ahead, ending at midnight) for fetching calendar events.
get_next_weeks_events (Google Calendar Node):
What it does: Connects to your specified Google Calendar and fetches all events within the 7-day window calculated by the date-time node.
Requires: Google Calendar API credentials and the ID of the calendar you want to use.
simplify_evens_json (Code Node):
What it does: Runs a small JavaScript snippet to clean up the raw event data from Google Calendar. It removes several fields that aren't needed for the summary (like htmlLink, etag, iCalUID), making the data more concise for the AI.
aggregate_events (Aggregate Node):
What it does: Takes all the individual (and now simplified) event items and groups them into a single JSON array called eventdata. This is the format the AI agent expects for processing.
Google Gemini (LM Chat Google Gemini Node):
What it does: This node is the connection point to the Google Gemini language model.
Requires: Google Gemini (or PaLM) API credentials.
event_summary_agent (Agent Node):
What it does: This is where the magic happens! It uses the Google Gemini model and a detailed system prompt to generate the weekly schedule summary.
The Prompt Instructs the AI to:
Start with a friendly greeting.
Group events by day (Monday to Sunday) for the upcoming week, using the user's timezone and locale.
Format event times clearly (e.g., 09:30 AM - 10:30 AM: Event Summary).
Identify and prefix ""IMPORTANT:"" to events with keywords like ""urgent,"" ""deadline,"" ""meeting,"" etc., in their summary or description.
Conclude with a 1-2 sentence helpful insight about the week's schedule.
Process the input eventdata (the JSON array of calendar events).
Markdown (Markdown to HTML Node):
What it does: Converts the text output from the event_summary_agent (which is generated in Markdown format for easy structure) into HTML. This ensures the email body is well-formatted with proper line breaks, lists, and emphasis.
send_email (Email Send Node):
What it does: Sends the final HTML summary to your specified email address.
Requires: SMTP (email sending) credentials and your desired ""From"" and ""To"" email addresses.
üöÄ Getting Started: Setup Instructions
Follow these steps to get the workflow up and running:
Import the Workflow:
Download the workflow JSON file.
In your n8n instance, go to ""Workflows"" and click the ""Import from File"" button. Select the downloaded JSON file.
Configure Credentials:
You'll need to set up credentials for three services. In n8n, go to ""Credentials"" on the left sidebar and click ""Add credential.""
Google Calendar API:
Search for ""Google Calendar"" and create new credentials using OAuth2. Follow the authentication flow.
Once created, select these credentials in the get_next_weeks_events node.
Google Gemini (PaLM) API:
Search for ""Google Gemini"" or ""Google PaLM"" and create new credentials. You'll typically need an API key from Google AI Studio or Google Cloud.
Once created, select these credentials in the Google Gemini node.
SMTP / Email:
Search for your email provider (e.g., ""SMTP,"" ""Gmail,"" ""Outlook"") and create credentials. This usually involves providing your email server details, username, and password/app password.
Once created, select these credentials in the send_email node.
‚ÄºÔ∏è IMPORTANT: Customize User Settings in the locale Node:
Open the locale node.
Update the following values in the ""Assignments"" section:
users-locale: Set your locale string (e.g., ""en-AU"" for English/Australia, ""en-US"" for English/United States, ""de-DE"" for German/Germany). This affects how dates, times, and numbers are formatted.
users-timezone: Set your timezone string (e.g., ""Australia/Sydney"", ""America/New_York"", ""Europe/London""). This is critical for ensuring event times are displayed correctly for your location.
users-name: Enter your name (e.g., ""Bob""). This is used to personalize the email greeting.
users-home-city: Enter your home city (e.g., ""Sydney""). This can be used for additional context by the AI.
Configure the get_next_weeks_events (Google Calendar) Node:
Open the node.
In the ""Calendar"" parameter, you need to specify which calendar to fetch events from.
The default might be a placeholder like c_4d9c2d4e139327143ee4a5bc4db531ffe074e98d21d1c28662b4a4d4da898866@group.calendar.google.com.
Change this to your primary calendar (often your email address) or the specific Calendar ID you want to use. You can find Calendar IDs in your Google Calendar settings.
Configure the send_email Node:
Open the node.
Set the fromEmail parameter to the email address you want the summary to be sent from.
Set the toEmail parameter to the email address(es) where you want to receive the summary.
You can also customize the subject line if desired.
(Optional) Customize the AI Prompt in event_summary_agent:
If you want to change how the AI summarizes events (e.g., different keywords for important events, a different tone, or specific formatting tweaks), you can edit the ""System Message"" within the event_summary_agent node's parameters.
(Optional) Adjust the Schedule in weekly_schedule:
Open the weekly_schedule node.
Modify the ""Rule"" to change when and how often the workflow runs (e.g., a specific day of the week, a different time).
Activate the Workflow:
Once everything is configured, toggle the ""Active"" switch in the top right corner of the workflow editor to ON.
üì¨ What You Get
You'll receive an email (based on your schedule) with a subject like ""Next Week Calendar Summary : [Start Date] - [End Date]"". The email body will contain:
A friendly greeting.
Your schedule for the upcoming week (Monday to Sunday), with events listed chronologically under each day.
Event times displayed in your local timezone (e.g., 09:30 AM - 10:30 AM: Team Meeting).
Priority events clearly marked (e.g., IMPORTANT: 02:00 PM - 03:00 PM: Project Deadline Review).
A brief, insightful observation about your week's schedule.
üõ†Ô∏è Troubleshooting & Notes
Timezone is Key: Ensure your users-timezone in the locale node is correct. This is the most common source of incorrect event times.
Google API Permissions: When setting up Google Calendar and Gemini credentials, make sure you grant the necessary permissions.
AI Output Varies: The AI-generated summary can vary slightly each time. The prompt is designed to guide it, but LLMs have inherent creativity.
Calendar Event Details: The quality of the summary (especially for identifying important events) depends on how detailed your calendar event titles and descriptions are. Including keywords like ""meeting,"" ""urgent,"" ""prepare for,"" etc., in your events helps the AI.
üí¨ Feedback & Contributions
Feel free to modify and enhance this workflow! If you have suggestions, improvements, or run into issues, please share them in the n8n community.
Happy scheduling!"
"Build & Query RAG System with Google Drive, OpenAI GPT-4o-mini, and Pinecone",https://n8n.io/workflows/4501-build-and-query-rag-system-with-google-drive-openai-gpt-4o-mini-and-pinecone/,"üîç What This Workflow Does
This RAG Pipeline in n8n automates document ingestion from Google Drive, vectorizes it using OpenAI embeddings, stores it in Pinecone, and enables chat-based retrieval using LangChain agents.
Main Functions:
üìÇ Auto-detects new files uploaded to a specific Google Drive folder.
üß† Converts the file into embeddings using OpenAI.
üì¶ Stores them in a Pinecone vector database.
üí¨ Allows a user to query the knowledge base through a chat interface.
ü§ñ Uses a GPT-4o-mini model with LangChain to generate intelligent responses using retrieved context.
‚öôÔ∏è Setup Instructions
Connect Accounts
Ensure these services are connected in n8n:
‚úÖ Google Drive (OAuth2)
‚úÖ OpenAI
‚úÖ Pinecone
You can do this in n8n > Credentials > New and use the matching names from the file:
Google Drive: ""Google Drive account 2""
OpenAI: ""OpenAi success""
Pinecone: ""PineconeApi account 2""
2. Folder Setup
Upload your documents to this folder in Google Drive:
üìÅ Power Folder
The workflow is triggered every minute when a new file is uploaded.
Workflow Overview
A. File Ingestion Path
Google Drive Trigger ‚Äî detects new file.
Google Drive (Download) ‚Äî downloads the new file.
Recursive Text Splitter ‚Äî splits text into chunks.
Default Data Loader ‚Äî loads content as LangChain documents.
OpenAI Embeddings ‚Äî converts text chunks into embeddings.
Pinecone Vector Store ‚Äî stores them in ""ragfile"" index.
B. Chat Retrieval Path
When chat message received ‚Äî
AI Agent ‚Äî LangChain agent managing tools.
OpenAI Chat Model (GPT-4o-mini) ‚Äî generates replies.
Pinecone Vector Store (retrieval) ‚Äî retrieves matching content.
Embeddings OpenAI1 ‚Äî helps match queries to document chunks."
Automated Stock Analysis Reports with Technical & News Sentiment using GPT-4o,https://n8n.io/workflows/3790-automated-stock-analysis-reports-with-technical-and-news-sentiment-using-gpt-4o/,"Stock Analysis Agent (Hebrew, RTL, GPT-4o)
Overview
Get comprehensive stock analysis with this AI-powered workflow that provides actionable insights for your investment decisions. On a weekly basis, this workflow:
Analyzes stock data from multiple sources (Chart-img, Twelve Data API, Alphavantage)
Performs technical analysis using advanced indicators (RSI, MACD, Bollinger Bands, Resistance and Support Levels)
Scans financial news from Alpha Vantage to capture market sentiment
Uses OpenAI's GPT-4o to identify patterns, trends, and trading opportunities
Generates a fully styled, responsive HTML email (with proper RTL layout) in Hebrew
Sends detailed recommendations directly to your inbox
Perfect for investors, traders, and financial analysts who want data-driven stock insights - combining technical indicators with news sentiment for more informed decisions.
Setup Instructions
Estimated setup time:
15 minutes
Required credentials:
OpenAI API Key
Chart-img API Key (free tier)
Twelve Data API Key (free tier)
Alpha Vantage API Key (free tier)
SMTP credentials (for email delivery)
Steps:
Import this template into your n8n instance.
Add your API keys under credentials.
Configure the SMTP Email node with: Host (e.g., smtp.gmail.com), Port (465 or 587), Username (your email), Password (app-specific password or login).
Activate the workflow.
Fill in the Form.
Enjoy! (Check your Spam mailbox)
Customization Tips
Modify the analysis timeframe (daily, weekly, monthly)
Add integrations with trading platforms or portfolio management tools
Adjust the recommendation criteria based on your risk tolerance
Why Use This?
This is more than just stock data. It's an intelligent financial assistant that combines technical analysis with market sentiment to provide actionable recommendations - automatically.
Important Note:
This report is being generated automatically and does not constitute an investment recommendation. Please consult a licensed investment advisor before making any investment decisions."
Generate Landing Page Layouts from Competitor Analysis with GPT-4,https://n8n.io/workflows/4717-generate-landing-page-layouts-from-competitor-analysis-with-gpt-4/,"Who is this for?
This workflow is ideal for SEO specialists, web designers, and digital marketers who want to quickly draft effective landing page layouts by referencing established competitors. It suits users who need a fast, structured starting point for web design while ensuring competitive relevance.
What problem is this workflow solving? / Use case
Designing a high-converting landing page from scratch can be time-consuming. This workflow automates the process of analyzing a competitor‚Äôs website, identifying essential sections, and producing a tailored layout‚Äîhelping users save time and improve their website‚Äôs effectiveness.
What this workflow does
The workflow fetches and analyzes your chosen competitor‚Äôs landing page, using web scraping and structure-detection nodes in n8n. It identifies primary sections like hero banners, service highlights, testimonials, and contact forms, and then generates a simplified, customizable layout suitable for wireframing or initial design.
Setup
Prepare your unique services and target audience profile for customization later.
Gather the competitor‚Äôs landing page URL you wish to analyze.
Run the workflow, inputting your competitor‚Äôs URL when prompted.
How to customize this workflow to your needs
After generating the initial layout, adapt section names and content blocks to highlight your services and brand messaging.
Add or remove sections based on your objectives and audience insights.
Integrate additional nodes for richer analysis, such as keyword extraction or design pattern detection, to tailor the output further."
Scrape business leads from Google Maps using OpenAI and Google Sheets,https://n8n.io/workflows/3443-scrape-business-leads-from-google-maps-using-openai-and-google-sheets/,"Google Maps Data Extraction Workflow for Lead Generation
This workflow is ideal for sales teams, marketers, entrepreneurs, and researchers looking to efficiently gather detailed business information from Google Maps for:
Lead generation
Market analysis
Competitive research
Who Is This Workflow For?
Sales professionals aiming to build targeted contact lists
Marketers looking for localized business data
Researchers needing organized, comprehensive business information
Problem This Workflow Solves
Manually gathering business contact details from Google Maps is:
Tedious
Error-prone
Time-consuming
This workflow automates data extraction to increase efficiency, accuracy, and productivity.
What This Workflow Does
Automates extraction of business data (name, address, phone, email, website) from Google Maps
Crawls and extracts additional website content
Integrates OpenAI to enhance data processing
Stores structured results in Google Sheets for easy access and analysis
Uses Google Search API to fill in missing information
Setup
Import the provided n8n workflow JSON into your n8n instance.
Set your OpenAI and Google Sheets API credentials.
Provide your Google Maps Scraper and Website Content Crawler API keys.
Ensure SerpAPI is configured to enhance data completeness.
Customizing This Workflow to Your Needs
Adjust scraping parameters:
Location
Business category
Country code
Customize Google Sheets output format to fit your current data structure
Integrate additional AI processing steps or APIs for richer data enrichment
Final Notes
This structured approach ensures:
Accurate and compliant data extraction from Google Maps
Streamlined lead generation
Actionable and well-organized data ready for business use
üìÑ Documentation: Notion Guide
Demo Video
üé• Watch the full tutorial here: YouTube Demo"
Build a Chatbot with Reinforced Learning Human Feedback (RLHF) and RAG,https://n8n.io/workflows/4689-build-a-chatbot-with-reinforced-learning-human-feedback-rlhf-and-rag/,"Who is this for?
This template is designed for internal support teams, product specialists, and knowledge managers who want to build an AI-powered knowledge assistant with retrieval-augmented generation (RAG) and reinforcement learning from human feedback (RLHF) via Telegram.
What problem is this workflow solving?
Manual knowledge management and answering support queries can be time-consuming and error-prone. This solution automates importing and indexing official documentation into MongoDB vector search and enhances AI responses with Telegram-based user feedback to continuously improve answer quality.
What these workflows do
Workflow 1: Document ingestion & indexing
Manually triggered workflow imports product documentation from Google Docs.
Documents are split into manageable chunks and embedded using OpenAI embeddings.
Embedded document chunks are stored in MongoDB Atlas vector store to enable semantic search.
Workflow 2: Telegram chat with RLHF feedback loop
Listens for user messages via Telegram bot integration.
Uses vector similarity search on MongoDB to retrieve relevant documentation chunks.
Generates answers with OpenAI GPT-4o-mini model using retrieval-augmented generation.
Sends answers back via Telegram and waits for user feedback (approval or disapproval).
Captures feedback, maps it as positive or negative, and stores it with the conversation data for future model improvement.
Setup
Setting up vector embeddings
Authenticate Google Docs and connect your Google Docs URL containing the product documentation you want to index.
Authenticate MongoDB Atlas and connect the collection where you want to store the vector embeddings. Create a search index on this collection to support vector similarity queries.
Ensure the index name matches the one configured in n8n (data_index).
See the example MongoDB search index template below for reference.
Setting up chat with Telegram RLHF
Create a bot in Telegram with @botFather using the /newbot command.
Connect the MongoDB database and search index used for vector search in the previous workflow. Also create two new collections in MongoDB Atlas: one for feedback and one for chat history. Create a search index for feedback, copying the provided template.
Configure the AI system prompt in the ‚ÄúKnowledge Base Agent‚Äù node, making sure it references all three tools connected (productDocs, feedbackPositive, feedbackNegative) as provided in the template prompt.
Make sure
Product documentation and feedback collections must connect to the same MongoDB database.
There are three distinct MongoDB collections: one for product documentation, one for feedback, and one for chat history (chat history collection can be separate).
Telegram API credentials are valid and webhook URLs are correctly set up.
MongoDB Search Index Templates
Documentation Collection Index
{
""mappings"": {
""dynamic"": false,
""fields"": {
""_id"": {
""type"": ""string""
},
""text"": {
""type"": ""string""
},
""embedding"": {
""type"": ""knnVector"",
""dimensions"": 1536,
""similarity"": ""cosine""
},
""source"": {
""type"": ""string""
},
""doc_id"": {
""type"": ""string""
}
}
}
}
Feedback Collection Index
{
""mappings"": {
""dynamic"": false,
""fields"": {
""prompt"": {
""type"": ""string""
},
""response"": {
""type"": ""string""
},
""text"": {
""type"": ""string""
},
""embedding"": {
""type"": ""knnVector"",
""dimensions"": 1536,
""similarity"": ""cosine""
},
""feedback"": {
""type"": ""token""
}
}
}
}"
AI-Powered Lead Enrichment with Bright Data MCP and Google Sheets,https://n8n.io/workflows/4589-ai-powered-lead-enrichment-with-bright-data-mcp-and-google-sheets/,"üìå HubSpot Lead Enrichment with Bright Data MCP
This template enables natural-language-driven automation using Bright Data's MCP tools, triggered directly by new leads in HubSpot. It dynamically extracts and executes the right tool based on lead context‚Äîpowered by AI and configurable in N8N.
‚ùì What Problem Does This Solve?
Manual lead enrichment is slow, inconsistent, and drains valuable time. This solution automates the process using a no-code workflow that connects HubSpot, Bright Data MCP, and an AI agent‚Äîwithout requiring scripts or technical skills. Perfect for marketing, sales, and RevOps teams.
üß∞ Prerequisites
To use this template, you‚Äôll need:
A self-hosted or cloud instance of N8N
A Bright Data MCP API token
A valid OpenAI API key (or compatible AI model)
A HubSpot account
Either a Private App token or OAuth credentials for HubSpot
Basic familiarity with N8N workflows
‚öôÔ∏è Setup Instructions
1. Set Up Authentication in HubSpot
üîê Option 1: Use a Private App Token (Simple Setup)
Log in to your HubSpot account.
Navigate to Settings ‚Üí Integrations ‚Üí Private Apps.
Create a new Private App with the following scopes:
crm.objects.contacts.read
crm.objects.contacts.write
crm.schemas.contacts.read
crm.objects.companies.read (optional)
Copy the Access Token.
In N8N, create a credential for HubSpot App Token and paste the app token in the field.
Go back to Hubspot Private App settings to setup a webhook.
Copy the url in your workflow's Webhook node and paste it here.
üîÅ Option 2: Use OAuth (Advanced + Secure)
In HubSpot, go to Settings ‚Üí Integrations ‚Üí Apps ‚Üí Create App.
Set your Redirect URL to match your N8N OAuth2 redirect path.
Choose scopes like:
crm.objects.companies.read
crm.objects.contacts.read
crm.objects.deals.read
crm.schemas.companies.read
crm.schemas.contacts.read
crm.schemas.deals.read
crm.objects.contacts.write (conditionally required)
Note the Client ID and Client Secret.
Copy the App ID and the developer API key
In N8N, create a credential for HubSpot Developer API and paste those info from previous step.
Attach these credentials to the HubSpot node in N8N.
2. Setup and obtain API token and other necessary information from Bright Data
In your Bright Data account, obtain the following information:
API token
Web Unlocker zone name (optional)
Browser API username and password string separated by colon (optional)
3. Host SSE server from STDIO command
The methods below will allow you to receive SSE (Server-Sent Events) from Bright Data MCP via a local Supergateway or Smithery
Method 1: Run Supergateway in a separate web service (Recommended)
This method will work for both cloud version and self-hosted N8N.
Signup to any cloud services of your choice (DigitalOcean, Heroku, Hetzner, Render, etc.).
For NPM based installation:
Create a new web service.
Choose Node.js as runtime environment and setup a custom server without repository.
In your server‚Äôs settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token WEB_UNLOCKER_ZONE=optional_zone_name BROWSER_AUTH=optional_browser_auth
Paste the following text as a start command: npx -y supergateway --stdio ""npx -y @brightdata/mcp"" --port 8000 --baseUrl http://localhost:8000 --ssePath /sse --messagePath /message
Deploy it and copy the web server URL, then append /sse into it.
Your SSE server should now be accessible at: https://your_server_url/sse
For Docker based installation:
Create a new web service.
Choose Docker as the runtime environment.
Set up your Docker environment by pulling the necessary images or creating a custom Dockerfile.
In your server‚Äôs settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token WEB_UNLOCKER_ZONE=optional_zone_name BROWSER_ZONE=optional_browser_zone_name
- Use the following Docker command to run Supergateway: docker run -it --rm -p 8000:8000 supercorp/supergateway \ --stdio ""npx -y @brightdata/mcp /"" \ --port 8000
Deploy it and copy the web server URL, then append /sse into it.
Your SSE server should now be accessible at: https://your_server_url/sse
For more installation guides, please refer to https://github.com/supercorp-ai/supergateway.git.
Method 2: Run Supergateway in the same web service as the N8N instance
This method will only work for self-hosted N8N.
a. Set Required Environment Variables
In your server's settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token
WEB_UNLOCKER_ZONE=optional_zone_name
BROWSER_ZONE=optional_browser_zone_name
b. Run Supergateway in Background
npx -y supergateway --stdio ""npx -y @brightdata/mcp"" --port 8000 --baseUrl http://localhost:8000 --ssePath /sse --messagePath /message
Use the command above to execute it through the cloud shell or set it as a pre-deploy command.
Your SSE server should now be accessible at:
http://localhost:8000/sse
For more installation guides, please refer to https://github.com/supercorp-ai/supergateway.git.
Method 3: Configure via Smithery.ai (Easiest)
If you don't want additional setup and want to test it right away, follow these instructions:
Visit https://smithery.ai/server/@luminati-io/brightdata-mcp/tools to:
Signup (if you are new to Smithery)
Create an API key
Define environment variables via a profile
Retrieve your SSE server HTTP URL
4. Connect Google Sheets to N8N
Ensure your Google Sheet:
Contains columns like row_id, first_name, last_name, email, and status.
Is shared with your N8N service account (or connected via OAuth)
In N8N:
Add a Google Sheets Trigger node
Set it to watch for new rows in your lead sheet
5. Import and Configure the N8N Workflow
Import the provided JSON workflow into N8N
Update nodes with your credentials:
Hubspot: Add your API key or connect it via OAuth.
Google Sheets Trigger: Link to your actual sheet
OpenAI Node: Add your API key
Bright Data Tool Execution: Add Bright Data token and SSE URL
üîÑ How It Works
New contact in Hubspot or a new row is added to the Google Sheet
N8N triggers the workflow
AI agent classifies the task (e.g., ‚ÄúFind LinkedIn‚Äù, ‚ÄúGet company info‚Äù)
The relevant MCP tool is called
Results are appended back to the sheet or routed to another destination
Rerun the specific record by specifying status ""needs more enrichment"", or leaving it blank.
üß© Use Cases
B2B Lead Enrichment ‚Äì Add missing fields (title, domain, social profiles)
Email Intelligence ‚Äì Validate and enrich based on email
Market Research ‚Äì Pull company or contact data on demand
CRM Auto-fill ‚Äì Push enriched leads to tools like HubSpot or Salesforce
üõ†Ô∏è Customization
Prompt Tuning ‚Äì Adjust how the AI interprets input data
Column Mapping ‚Äì Customize which fields to pull from the sheet
Tool Logic ‚Äì Add retries, fallback tools, or confidence-based routing
Destination Output ‚Äì Integrate with CRMs, Slack, or webhook endpoints
‚úÖ Summary
This template turns a Google Sheet into an AI-powered lead enrichment engine. By combining Bright Data‚Äôs tools with a natural language AI agent, your team can automate repetitive tasks and scale lead ops‚Äîwithout writing code.
Just add a row, and let the workflow do the rest."
Generate Logos and Images with Consistent Visual Styles using Imagen 3.0,https://n8n.io/workflows/3954-generate-logos-and-images-with-consistent-visual-styles-using-imagen-30/,"This n8n template allows you to use AI to generate logos or images which mimic visual styles of other logos or images. The model used to generate the images is Google's Imagen 3.0.
With this template, users will be able to automate design and marketing tasks such as creating variants of existing designs, remixing existing assets to validate different styles and explore a range of designs which would have been otherwise too expensive and time-consming previously.
How it works
A form trigger is used to capture the source image to reference styles from and a prompt for the target image to generate.
The source image is passed to Gemini 2.0 to be analysed and its visual style and tone extracted as a detailed description.
This visual style description is then combined with the user's initial target image prompt. This final prompt is given to Imagen 3.0 to generate the images.
A quick webpage is put together with the generated images to present back to the user.
If the user provided an email address, a copy of this HTML page will be sent.
How to use
Ensure the workflow is live to share the form publicly.
The source image must be accessible to your n8n instance - either a public image of the internet or within your network.
For best results, select a source image which has strong visual identity as these will allow the LLM to better describe it.
For your prompt, refer to the imagen prompt guide found here: https://ai.google.dev/gemini-api/docs/image-generation#imagen-prompt-guide
Requirements
Gemini for LLM and Imagen model.
Cloudinary for image CDN.
Gmail for email sending.
Customising this workflow
Feel free to swap any of these out for tools and services you prefer.
Want to fully automate? Switch the form trigger for a webhook trigger!"
üéØ Precision Prospecting: Automate LinkedIn Lead Gen with Bright Data,https://n8n.io/workflows/4873-precision-prospecting-automate-linkedin-lead-gen-with-bright-data/,"üéØ Precision Prospecting: Automate LinkedIn Lead Gen with n8n & Bright Data
üìù Overview
This workflow turns n8n into an AI-powered prospector, automatically searching Google for LinkedIn profiles, scraping profile data via Bright Data, and summarizing key details. Ideal for sales and recruitment teams seeking targeted lead lists without manual research.
üé• Workflow in Action
Want to see this workflow in action? You have a chat window output below:
üîë Key Features
AI Chat Trigger: Start prospecting via conversational prompts.
Contextual Memory: Retains the last 20 messages for coherent dialogue.
Automated Google Search: Generates site-restricted queries and fetches the top result.
Bright Data Scraping: Synchronously scrapes LinkedIn profile details by URL.
Intelligent Filtering: Extracts only valid LinkedIn profile links.
Limit Control: Returns a single, most relevant profile per request.
LLM Summary: Uses GPT-4o-mini to interpret and present scraped data.
üöÄ How It Works (Step-by-Step)
Prerequisites:
n8n ‚â• v1.0 with community nodes: install n8n-nodes-brightdata (not verified community node).
API credentials: OpenAI, Bright Data (web unlocker zone ‚Äúweb_unlocker1‚Äù).
Webhook endpoint for chat trigger.
Node Configuration:
When chat message received (chatTrigger): Fires on user prompt.
Simple Memory1 (memoryBufferWindow): Stores the last 20 chat messages.
AI Prospector Agent (agent): Orchestrates search logic.
Get 1 Google Result (brightData): Performs a Google search with site:linkedin.com/in.
Get Links from Body (html): Extracts all &lt;a&gt; hrefs from the search result page.
Extract Links (splitOut): Splits out individual link entries.
Filter only LinkedIn Profiles (filter): Ensures the URL contains ‚Äúlinkedin.com/‚Äù and starts with ‚Äúhttps://‚Äù.
Limit (limit): Restricts output to the first valid profile URL.
Search LinkedIn URI (toolWorkflow): Passes the URL to a secondary workflow to fetch the first link.
Get LinkedIn Profile Data (brightDataTool): Scrapes the profile JSON.
OpenAI Chat Model (lmChatOpenAi): Summarizes and formats the scraped data.
Workflow Logic:
User asks for a person by company & name, company & position, or LinkedIn URL.
Agent builds a Google query (e.g., site:linkedin.com/in bright data cmo) and calls ‚ÄúGet 1 Google Result.‚Äù
Extracted links are filtered and limited to the top valid profile.
If user provided a direct LinkedIn URL, Agent skips search and scrapes immediately.
Scraped profile JSON is passed to GPT-4o-mini to generate a concise summary.
Testing & Optimization:
Trigger via Execute Workflow for dry runs.
Inspect intermediate node outputs in n8n‚Äôs Execution panel.
Adjust maxIterations or memory window length for performance.
Tune Bright Data zone or country settings to optimize scraping speed.
Deployment & Monitoring:
Activate the workflow and expose its webhook URL.
Use n8n‚Äôs built-in Alerts or external monitoring (e.g., Slack notifications) on failures.
Rotate credentials via n8n‚Äôs Credential Vault when needed.
Version-control workflow via duplicates or Git-backed n8n instances.
‚úÖ Pre-requisites
OpenAI Account: API key for GPT-4o-mini.
Bright Data Account: Zone ‚Äúweb_unlocker1‚Äù and dataset gd_l1viktl72bvl7bjuj0.
n8n Version: v1.0+ with community nodes installed.
Permissions: Webhook access, Credential Vault read/write.
üë§ Who Is This For?
Sales teams automating outbound LinkedIn prospecting.
Recruiters sourcing candidates without manual scraping.
Marketing ops looking to enrich CRM with accurate profile data.
üìà Benefits & Use Cases
Efficiency: Reduces hours of manual search and data entry to seconds.
Accuracy: Filters out non-LinkedIn links and ensures high-quality results.
Scalability: Handle multiple prospect requests concurrently via chat or API.
Integration: Easily hook into CRMs or email sequencers downstream.
Workflow created and verified by Miquel Colomer https://www.linkedin.com/in/miquelcolomersalas/ and N8nHackers https://n8nhackers.com"
Anthropic AI Agent: Claude Sonnet 4 and Opus 4 with Think and Web Search tool,https://n8n.io/workflows/4399-anthropic-ai-agent-claude-sonnet-4-and-opus-4-with-think-and-web-search-tool/,"This workflow dynamically chooses between two new powerful Anthropic models ‚Äî Claude Opus 4 and Claude Sonnet 4 ‚Äî to handle user queries, based on their complexity and nature, maintaining scalability and context awareness with Anthropic web search function and Think tool.
Key Advantages
üîÅ Dynamic Model Selection
Automatically routes each user query to either Claude Sonnet 4 (for routine tasks) or Claude Opus 4 (for complex reasoning), ensuring optimal performance and cost-efficiency.
üß† AI Agent with Tool Use
The AI agent can utilize a web search tool to retrieve up-to-date information and a Think tool for complex reasoning processes ‚Äî improving response quality.
üìé Memory Integration
Uses session-based memory to maintain conversational context, making interactions more coherent and human-like.
üßÆ Built-in Calculation Tool
Handles numeric queries using an integrated calculator tool, reducing the need for external processing.
üì§ Structured Output Parser
Ensures outputs are always well-structured and formatted in JSON, which improves consistency and downstream integrations.
üåê Web Search Capability
Supports real-time information retrieval for current events, statistics, or details not available in the AI‚Äôs base knowledge.
Components Overview
Trigger: Listens for new chat messages.
Routing Agent: Analyzes the message and returns the best model to use.
AI Agent: Handles the conversation, decides when to use tools.
Tools:
web_search for internet queries
Think for reasoning
Calculator for math tasks
Models Used:
claude-sonnet-4-20250514: Optimized for general and business logic tasks.
claude-opus-4-20250514: Best for deep, strategic, and analytical queries.
How It Works
Dynamic Model Selection
The workflow begins when a chat message is received. The Anthropic Routing Agent analyzes the user's query to determine the most suitable model (either Claude Sonnet 4 or Claude Opus 4) based on the query's complexity and requirements.
The routing agent uses predefined criteria to decide:
Claude Sonnet 4: Best for standard tasks like real-time workflow routing, data validation, and routine business logic.
Claude Opus 4: Reserved for complex scenarios requiring deep reasoning, advanced analysis, or high-impact decisions.
Query Processing and Response Generation
The selected model processes the query, leveraging tools like web_search for real-time information retrieval, Think for internal reasoning, and Calculator for numerical tasks.
The AI Agent coordinates these tools, ensuring the response is accurate and context-aware. A Simple Memory node retains session context for coherent multi-turn conversations.
The final response is formatted and returned to the user without intermediate steps or metadata.
Set Up Steps
Node Configuration
Trigger: Configure the ""When chat message received"" node to handle incoming user queries.
Routing Agent: Set up the ""Anthropic Routing Agent"" with the system message defining model selection logic. Ensure it outputs a JSON object with prompt and model fields.
AI Model Nodes: Link the ""Sonnet 4 or Opus 4"" node to dynamically use the selected model. The ""Sonnet 3.7"" node powers the routing agent itself.
Tool Integration
Attach the ""web_search"" HTTP tool to enable internet searches, ensuring the API endpoint and headers (e.g., anthropic-version) are correctly configured.
Connect auxiliary tools (Think, Calculator) to the ""AI Agent"" for extended functionality.
Add the ""Simple Memory"" node to maintain conversation history.
Credentials
Provide an Anthropic API key to all nodes requiring authentication (e.g., model nodes, web search).
Testing
Activate the workflow and test with sample queries to verify:
Correct model selection (e.g., Sonnet for simple queries, Opus for complex ones).
Proper tool usage (e.g., web searches trigger when needed).
Memory retention across chat turns.
Deployment
Once validated, set the workflow to active for live interactions.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Daily Startup Intelligence: Crunchbase Updates to Email Digest with GPT,https://n8n.io/workflows/4732-daily-startup-intelligence-crunchbase-updates-to-email-digest-with-gpt/,"üöÄ Automated Startup Intelligence: CrunchBase Updates to Email Digest Workflow!
Workflow Overview
This cutting-edge n8n automation is a sophisticated startup intelligence tool designed to transform market research into actionable insights. By intelligently connecting CrunchBase, AI processing, and Gmail, this workflow:
Discovers Startup Updates:
Automatically retrieves latest company information
Tracks recent organizational changes
Eliminates manual market research efforts
Intelligent Data Processing:
Filters and extracts key company details
Generates AI-powered summaries
Ensures comprehensive market intelligence
Smart Summarization:
Uses AI to create readable company updates
Transforms complex data into digestible insights
Provides professional, context-rich summaries
Seamless Email Distribution:
Automatically sends daily update digests
Delivers insights directly to your inbox
Enables rapid market awareness
Key Benefits
ü§ñ Full Automation: Zero-touch startup research
üí° Smart Filtering: Targeted company insights
üìä Comprehensive Tracking: Detailed market intelligence
üåê Multi-Source Synchronization: Seamless data flow
Workflow Architecture
üîπ Stage 1: Company Discovery
Manual/Scheduled Trigger: Market scanning
CrunchBase API Integration
Intelligent Filtering:
Recent updates
Specific time frames
Key organizational information
üîπ Stage 2: Data Extraction
Comprehensive Metadata Parsing
Key Information Retrieval
Structured Data Preparation
üîπ Stage 3: AI Summarization
OpenAI GPT Processing
Professional Summary Generation
Contextual Insight Creation
üîπ Stage 4: Email Distribution
Gmail Integration
Automated Update Digest
Personalized Delivery
Potential Use Cases
Venture Capitalists: Startup ecosystem tracking
Market Researchers: Industry trend analysis
Startup Founders: Competitive intelligence
Business Strategists: Market opportunity identification
Investors: Real-time company insights
Setup Requirements
CrunchBase API
API credentials
Configured access permissions
Company update tracking setup
OpenAI API
GPT model access
Summarization configuration
API key management
Gmail Account
Connected email
Digest email configuration
Appropriate sending permissions
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced company trend analysis
üìä Multi-source intelligence gathering
üîî Customizable alert mechanisms
üåê Expanded industry tracking
üß† Machine learning insights generation
Technical Considerations
Implement robust error handling
Use secure API authentication
Maintain flexible data processing
Ensure compliance with API usage guidelines
Ethical Guidelines
Respect business privacy
Use data for legitimate research
Maintain transparent information gathering
Provide proper attribution
Hashtag Performance Boost üöÄ
#StartupIntelligence #MarketResearch #AIWorkflow #CompanyUpdates #BusinessIntelligence #TechInnovation #DataAutomation #StartupEcosystem #InvestorInsights #TrendTracking
Workflow Visualization
[Manual/Scheduled Trigger]
    ‚¨áÔ∏è
[Fetch Crunchbase Updates]
    ‚¨áÔ∏è
[Extract Company Details]
    ‚¨áÔ∏è
[AI Summarization]
    ‚¨áÔ∏è
[Send Email Digest]
Connect With Me
Ready to revolutionize your startup intelligence?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your market research with intelligent, automated workflows!"
LinkedIn Job Finder Automation using Bright Data API & Google Sheets,https://n8n.io/workflows/4775-linkedin-job-finder-automation-using-bright-data-api-and-google-sheets/,"üíº LinkedIn Job Finder Automation using Bright Data API & Google Sheets
A comprehensive n8n automation that searches LinkedIn job postings using Bright Data‚Äôs API and automatically organizes results in Google Sheets for efficient job hunting and recruitment workflows.
üìã Overview
This workflow provides an automated LinkedIn job search solution that collects job postings based on your search criteria and organizes them in Google Sheets. Perfect for job seekers, recruiters, HR professionals, and talent acquisition teams.
‚ú® Key Features
üîç Smart Job Search: Form-based input for city, job title, country, and job type
üõç LinkedIn Integration: Uses Bright Data‚Äôs LinkedIn dataset for accurate job posting data
üìä Automated Organization: Populates Google Sheets with structured job data
üìß Real-time Processing: Processes job search requests in real-time
üìà Data Storage: Stores job details including company info, locations, and apply links
üîÑ Batch Processing: Handles multiple job postings efficiently
‚ö° Fast & Reliable: Built-in error handling for scraping
üéØ Customizable Filters: Advanced job filtering based on criteria
üéØ What This Workflow Does
Input
Job Search Criteria: City, job title, country, and optional job type
Search Parameters: Configurable filters and limits
Output Preferences: Google Sheets destination
Processing Steps
Form Submission
Data Request to Bright Data API
Status Monitoring
Data Extraction
Data Filtering
Sheet Update
Error Handling
Output Data Points
Field
Description
Example
Job Title
Position title from posting
Senior Software Engineer
Company Name
Employer company name
Tech Solutions Inc.
Job Detail
Job summary/description
Remote position requiring 5+ years‚Ä¶
Location
Job location
San Francisco, CA
Company URL
Company profile link
View Profile
Apply Link
Direct application link
Apply Now
üöÄ Setup Instructions
Prerequisites
n8n instance (self-hosted or cloud)
Google account with Sheets access
Bright Data account with LinkedIn dataset access
Steps
Import the Workflow: Use JSON import in n8n
Configure Bright Data: Add API credentials and dataset ID
Configure Google Sheets: Create sheet, set credentials, map columns
Update Workflow Settings: Replace placeholders with your actual data
Test & Activate: Submit test form and verify data in Google Sheets
üìñ Usage Guide
Submitting Job Searches
Go to your webhook URL and fill in the form with:
City: e.g., New York
Job Title: e.g., Software Engineer
Country: e.g., US
Job Type: Optional (Full-Time, Remote, etc.)
Understanding Results
Comprehensive job data
Company info and profile links
Direct application links
Location and job descriptions
Customizing Search Parameters
Edit the Create Snapshot ID node to change:
Time range (e.g., ‚ÄúPast month‚Äù)
Result limits
Company filters
üîß Customization Options
More Data Points: Add salary, seniority, applicants, etc.
Custom Form Fields: Add filters for salary, experience, industry
Multiple Sheets: Route results by job type or location
üö® Troubleshooting
Bright Data connection failed: Check API credentials and dataset access
No job data extracted: Verify search parameters and API limits
Google Sheets permission denied: Re-authenticate and check sharing
Form not working: Check webhook URL and field mappings
Filter issues: Review logic and data types
Execution failed: Check logs, retry logic, and network status
üìä Use Cases & Examples
Job Seeker Dashboard: Automate job search and track applications
Recruitment Pipeline: Source candidates and monitor hiring trends
Market Research: Analyze job trends and salary benchmarks
HR Analytics: Support workforce planning and competitive insights
‚öôÔ∏è Advanced Configuration
Batch Processing: Queue multiple searches with delays
Search History: Track and analyze past searches
Tool Integration: Connect to CRM, Slack, databases, BI tools
üìà Performance & Limits
Processing Time: 30‚Äì60 seconds per search
Concurrent Requests: 2‚Äì3 (depends on Bright Data plan)
Data Accuracy: 95%+
Success Rate: 90%+
Daily Capacity: 50‚Äì200 searches
Memory: ~50MB per execution
API Calls: 3‚Äì4 Bright Data + 1 Google Sheets per search
ü§ù Support & Community
n8n Community: community.n8n.io
Documentation: docs.n8n.io
Bright Data Support: Via your Bright Data dashboard
GitHub Issues: Report bugs and request features
üéØ Ready to Use!
Your workflow is ready for automated LinkedIn job searching. Customize it to your recruiting or job search needs.
Webhook URL: https://your-n8n-instance.com/webhook/linkedin-job-finder&lt;/code&gt;&lt;/p&gt;
What Gets Extracted:
* ‚úÖ Job Title * ‚úÖ Company Information * ‚úÖ Location Data * ‚úÖ Job Details * ‚úÖ Application Links * ‚úÖ Processing Timestamps ### Use Cases: * üîç Job Search Automation * üìä Recruitment Intelligence * üìù Market Research * üéØ HR Analytics"
Auto-Publish YouTube Videos to Facebook & Instagram with AI-Generated Captions,https://n8n.io/workflows/4478-auto-publish-youtube-videos-to-facebook-and-instagram-with-ai-generated-captions/,"Automatically turn your YouTube videos into engaging Facebook and Instagram posts with AI-generated captions.
This powerful workflow monitors your YouTube channel for new uploads, generates engaging social media captions using AI, and automatically publishes to both Facebook pages and Instagram business accounts - maximizing your content reach across all major platforms.
üöÄ What It Does
YouTube Channel Monitoring: Watches your YouTube RSS feed hourly for new video uploads and triggers cross-platform publishing
AI Caption Generation: Uses GPT-4 to create engaging, platform-optimized social media captions with emojis and calls-to-action
Facebook Auto-Publishing: Posts directly to your Facebook page with AI-generated captions and video links
Instagram Business Posting: Creates Instagram posts using YouTube thumbnails as images with custom captions
Smart Error Handling: Continues workflow even if one platform fails, ensuring maximum post distribution
üéØ Key Benefits
‚úÖ 3x Content Reach: Automatically distribute YouTube content across major social platforms
‚úÖ Save 5+ Hours Weekly: Eliminate manual cross-posting and caption writing
‚úÖ AI-Optimized Captions: Professional, engaging content tailored for social media
‚úÖ Instant Publishing: New YouTube videos become social posts within 1 hour
‚úÖ Professional Thumbnails: Uses high-quality YouTube thumbnails for Instagram
‚úÖ Set & Forget: Complete automation once configured
üè¢ Perfect For
Content Creators & YouTubers
Individual creators maximizing content distribution
Educational channels expanding social reach
Entertainment creators building multi-platform presence
Business channels driving traffic from social media
Business Applications
Brand Awareness: Increase visibility across all major social platforms
Traffic Generation: Drive social media traffic back to YouTube content
Audience Growth: Reach different demographics on each platform
Content Amplification: Maximize ROI from video content creation
‚öôÔ∏è What's Included
Complete Cross-Platform Workflow: Ready-to-deploy multi-platform publishing system
YouTube RSS Integration: Automatic monitoring of your channel for new uploads
AI Caption Engine: GPT-4 powered social media caption generation
Facebook Publishing: Direct integration with Facebook Pages API
Instagram Business Integration: Full Instagram Business account posting capability
Error-Resistant Design: Robust system continues working even if one platform fails
üîß Setup Requirements
n8n Platform: Cloud or self-hosted instance
YouTube Channel: RSS feed for monitoring new uploads
Meta Developer Account: Facebook app with required permissions
OpenAI API: GPT-4 access for caption generation
Long-lived Access Token: For Facebook and Instagram API access
üìä Required Meta API Permissions
Required Permissions:
‚úÖ pages_manage_posts
‚úÖ pages_read_engagement  
‚úÖ pages_show_list
‚úÖ instagram_content_publish
‚úÖ instagram_basic

Token Type: Long-lived access token
API Version: v22.0
üé® Sample AI Caption Generation
YouTube Video: ""10 Tips for Better Photography""
Generated Social Caption:
üì∏ Just dropped my latest photography tutorial! 

Discover 10 game-changing tips that will transform your photos from amateur to professional. Whether you're just starting out or looking to level up your skills, these techniques will make a huge difference! 

‚ú® What you'll learn:
‚Ä¢ Composition secrets the pros use
‚Ä¢ Lighting techniques for any situation  
‚Ä¢ Camera settings that actually matter

Which tip surprised you the most? Drop a comment below! üëá

Watch the full tutorial: [YouTube Link]

#Photography #Tutorial #PhotoTips #ContentCreator #Learn
üéØ Customization Options
Caption Personalization: Adjust AI prompts for your brand voice, hashtags, and audience
Platform-Specific Formatting: Different caption styles for Facebook vs Instagram
Posting Schedule: Modify monitoring frequency (hourly, daily, custom intervals)
Multiple Channels: Monitor multiple YouTube channels with different posting strategies
Content Filtering: Add filters for specific video types or topics
Brand Integration: Include consistent branding elements and calls-to-action
üè∑Ô∏è Tags & Categories
#youtube-automation #social-media-publishing #cross-platform-marketing #facebook-automation #instagram-business #content-distribution #ai-caption-generation #social-media-management #multi-platform-posting #youtube-marketing #content-amplification #social-media-automation #creator-tools #video-marketing #workflow-automation
üí° Use Case Examples
Educational Creator: Automatically share tutorial videos across platforms to reach students on different social networks
Product Review Channel: Distribute review videos to Facebook and Instagram to drive traffic and affiliate sales
Business Coach: Share valuable content across platforms to build authority and attract consulting clients
Entertainment Channel: Maximize viral potential by instantly distributing funny videos across all major platforms
üõ†Ô∏è Setup & Support
Quick Configuration: Complete setup in 45 minutes with Meta Developer account and API tokens
Template Access Tokens: Instructions for generating long-lived Facebook/Instagram tokens
Channel Integration: Simple YouTube RSS feed configuration for any channel
Testing Protocol: Built-in error handling and testing procedures
üìû Get Help & Resources
YouTube: https://www.youtube.com/@YaronBeen/videos
üíº Social Media Automation Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
üìß Technical Configuration Help
Email: Yaron@nofluff.online - Response within 24 hours"
Build Your First AI Data Analyst Chatbot,https://n8n.io/workflows/3050-build-your-first-ai-data-analyst-chatbot/,"Enhance your data analysis by connecting an AI Agent to your dataset, using n8n tools.
This template teaches you how to build an AI Data Analyst Chatbot that is capable of pulling data from your sources, using tools like Google Sheets or databases. It's designed to be easy and efficient, making it a good starting point for AI-driven data analysis.
You can easily replace the current Google Sheets tools for databases like Postgres or MySQL.
How It Works
The core of the workflow is the AI Agent. It's connected to different data retrieval tools, to get data from Google Sheets (or your preferred database) in many different ways.
Once the data is retrieved, the Calculator tool allows the AI to perform mathematical operations, making your data analysis precise.
Who is this template for
Data Analysts & Researchers: Pull data from different sources and perform quick calculations.
Developers & AI Enthusiasts: Learn to build your first AI Agent with easy dataset access.
Business Owners: Streamline your data analysis with AI insights and automate repetitive tasks.
Automation Experts: Enhance your automation skills by integrating AI with your existing databases.
How to Set Up
You can find detailed instructions in the workflow itself.
Check out my other templates
üëâ https://n8n.io/creators/solomon/"
"Save Time with AI Hiring ‚Äì Automate Screening, Assessments, Alerts & Interviews",https://n8n.io/workflows/4813-save-time-with-ai-hiring-automate-screening-assessments-alerts-and-interviews/,"AI Recruitment Automation Pipeline ‚Äì Resume Parsing, GPT-4 Evaluation, Assessment Triggers & Interview Scheduling
This end-to-end AI-powered HR recruitment workflow automates the entire candidate journey ‚Äî from resume collection and parsing to AI-based evaluation, Talent Acquisition (TA) approvals, assessment dispatch, and interview scheduling. Built in n8n, it leverages OpenAI GPT-4, Google Sheets, Google Drive, Slack, and SMTP to reduce hiring time, improve candidate quality, and eliminate repetitive manual tasks.
Key Features
Smart Resume Intake Form
Collects candidate data: name, email, phone, LinkedIn, job role, and CV (PDF).
Custom-designed UI with branding-ready CSS.
PDF Resume Parsing & Storage
CV is uploaded to a dedicated Google Drive folder.
Resume text is extracted for semantic analysis.
AI-Based Candidate Evaluation (GPT-4 via LangChain)
Extracts: City, Education, Job History, Skills.
Summarizes candidate profile (100 words).
Retrieves and summarizes job description from Google Sheets.
Performs detailed evaluation:
‚úÖ Semantic fit scoring (0‚Äì100%)
‚úÖ Key matches and skill gaps
‚úÖ Soft skills extraction
‚úÖ Red flag detection (job-hopping, missing info)
‚úÖ Final score (1‚Äì10) with rationale
Google Sheets Integration
Logs and updates candidate data at each stage:
CV Submitted ‚Üí Scored ‚Üí Shortlisted ‚Üí Assessment Sent ‚Üí Interview Scheduled ‚Üí Rejected
TA Approval via Email (Send & Wait)
TA receives evaluation summary and gives one-click approve/reject.
‚úÖ Approved ‚Üí Status: Resume Selected
‚ùå Rejected ‚Üí Status: Resume Rejected
Assessment Trigger (Post Approval)
Sends assessment link to shortlisted candidates.
Notifies TA via Slack and Email when assessment is submitted.
Interview Scheduling
Sends Calendly link for self-scheduled interview booking.
Candidate receives detailed next-step instructions.
Status-Based Candidate Emails
Automatically sends:
‚úîÔ∏è Shortlisting confirmation + interview setup
‚ùå Rejection email with branded message
Business Benefits
Save 80%+ time spent on manual resume reviews and coordination
Reduce cost-per-hire by eliminating manual tasks
Enhance hiring accuracy with structured, AI-based decision-making
Scalable recruitment for 100s of candidates per week
Improve candidate experience with instant status updates
Centralize data in Google Sheets for full team visibility
üîß Setup Instructions
1. Google Service Account Setup (One-Time)
Before using Google Sheets or Google Drive in n8n:
Go to Google Cloud Console.
Create a Service Account under your project.
Enable these APIs:
Google Sheets API
Google Drive API
Download the JSON credentials for the service account.
IMPORTANT:
Share your target Google Sheets and Docs with the service account email
(e.g., your-service-account@your-project.iam.gserviceaccount.com).
Add Applicant's Details to Google Sheet
Document: Select the Profiles Google Sheet document.
Sheet: Select the Applicant's Details sheet.
Fields to Map:
EMAIL: {{ $('On form submission').item.json.Email }}
DATE: {{ $now.format('dd-MM-yyyy') }}
NAME: {{ $('On form submission').item.json.Name }}
LINKEDIN URL: {{ $('On form submission').item.json[""LinkedIn Profile URL""] }}
JOB PROFILE: {{ $('On form submission').item.json[""Job Openings""] }}
STATUS: CV SUBMITTED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Extract Applicant's Resume Text
Text:
{{ $('Extract from File').item.json.text }}
Get Job Description from Google Sheet
Document: Profiles
Sheet: Job Openings
Filter:
Column: Job Profile
Value: {{ $('On form submission').item.json[""Job Openings""] }}
Save Evaluation Results in Google Sheets
Document: Profiles
Sheet: Applicant's Details
Column Match On: EMAIL
Fields to Map:
EMAIL: {{ $('On form submission').item.json.Email }}
CITY: {{ $('Applicant\'s Details').item.json.output.City }}
EDUCATIONAL: {{ $('Applicant\'s Details').item.json.output[""Educational Qualification""] }}
JOB HISTORY: {{ $('Applicant\'s Details').item.json.output[""Job History""] }}
SKILLS: {{ $('Applicant\'s Details').item.json.output.Skills }}
SUMMARIZE: {{ $('Summarize Applicant\'s Profile').item.json.response.text }}
SEMANTIC FIT SCORE: {{ $json.output.semantic_fit.score }}
KEY MATCHES: {{ $json.output.semantic_fit.key_matches.toJsonString() }}
KEY GAPS: {{ $json.output.semantic_fit.key_gaps.toJsonString() }}
SEMANTIC FIT CONSIDERATION: {{ $json.output.semantic_fit.consideration }}
SOFT SKILLS: {{ $json.output.soft_skills.toJsonString() }}
EXPERIENCE GAP DETECTED: {{ $json.output.experience_analysis.experience_gap_detected }}
OVER QUALIFICATION DETECTED: {{ $json.output.experience_analysis.overqualification_detected }}
EXPERIENCE ANALYSIS CONSIDERATION: {{ $json.output.experience_analysis.consideration }}
RED FLAGS ISSUES DETECTED: {{ $json.output.red_flags.issues_detected.toJsonString() }}
RED FLAGS CONSIDERATION: {{ $json.output.red_flags.consideration }}
VOTE: {{ $json.output.overall_evaluation.final_vote }}
FINAL CONSIDERATION: {{ $json.output.overall_evaluation.consideration }}
STATUS: CV SCORED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Update Applicant Statuses
Resume Selected
Document: Profiles
Sheet: Applicant's Details
Column Match On: EMAIL
Update:
STATUS: RESUME SELECTED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Resume Rejected
Update:
STATUS: RESUME REJECTED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Assessment Sent
Email: {{ $('Loop to Send Assessment Link to Each Candidate').item.json.EMAIL }}
Update:
STATUS: ASSESSMENT SENT
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Assessment Submitted
Email: {{ $json[""Enter Your Email Address""] }}
Update:
STATUS: ASSESSMENT SUBMITTED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Interview Booked
Email: {{ $json.payload.email }}
Update:
STATUS: INTERVIEW BOOKED
LAST UPDATED DATE: {{ $now.format('dd-MM-yyyy hh:mm:ss') }}
Fetch Applicants with Specific Status
Status: RESUME SELECTED
Document: Profiles
Sheet: Applicant's Details
Filter:
Column: STATUS
Value: RESUME SELECTED
Get Assessment Form URL from Job Profile
Document: Profiles
Sheet: Job Openings
Filter:
Column: Job Profile
Value: {{ $json[""JOB PROFILE""] }}
Trigger on Applicant Status Update
Document: Profiles
Sheet: Applicant's Details
Trigger Settings:
Columns to Watch: STATUS
‚ö†Ô∏è Important Notes
Always use ‚ÄúSelect Document from List‚Äù instead of manually pasting the sheet/document ID.
Share your Sheets/Docs with the Google Service Account email for proper access.
Keep your date formats consistent using {{ $now.format('dd-MM-yyyy hh:mm:ss') }}.
Add credentials for:
Google Drive
Google Sheets
SMTP (for emails)
OpenAI API Key (GPT-4)
Replace placeholders:
Google Sheet & Folder IDs
Calendly Link
Assessment Link
(Optional) Customize GPT-4 prompts for domain-specific scoring
(Optional) Use your Slack webhook for TA notifications
üõ†Ô∏è Tools & Integrations
Form Trigger ‚Äì Candidate form with file upload
Google Drive + Extract PDF ‚Äì CV parsing
Google Sheets ‚Äì Database for all applicant statuses
LangChain GPT-4 Nodes ‚Äì AI profile + job analysis
Email Send & Send & Wait ‚Äì Candidate/TA communication
IF Node ‚Äì Logic for approve/reject
Slack Integration ‚Äì TA notification
Calendly Link ‚Äì Interview scheduling
AI resume screening, GPT-4 recruitment workflow, automated hiring pipeline, semantic fit evaluation, LangChain for HR, resume parsing automation, AI in talent acquisition, assessment workflow automation, interview scheduling automation, candidate shortlisting automation, OpenAI HR integration, Google Sheets recruitment tracker, n8n HR automation template, self-scheduling interviews with Calendly, Slack notifications in recruitment"
Generate and Send AI News Newsletters Automatically with GPT & Gmail,https://n8n.io/workflows/4847-generate-and-send-ai-news-newsletters-automatically-with-gpt-and-gmail/,"SEO-Optimized Description:
Automate your AI newsletter creation and delivery using this ready-to-deploy n8n workflow template. Powered by GPT (OpenAI/Azure) and integrated with Gmail, this workflow generates rich, structured, and engaging AI-focused newsletters and sends them out daily or weekly‚Äîcompletely hands-free.
What It Does:
üì∞ Fetches the latest AI trends and updates using GPT
‚úçÔ∏è Automatically formats news into structured newsletter sections: headlines, tools, stats, tips, and more
üìß Sends HTML email newsletters via Gmail
üïò Runs automatically at your chosen schedule (default: 9 AM daily)
Setup Includes:
Connect your OpenAI or Azure GPT API
Add Gmail SMTP or OAuth credentials
Customize categories, schedule, and email styling
Perfect for:
Tech bloggers, content marketers, AI influencers, and automation enthusiasts who want to send curated AI content to their audience without manual effort."
Convert PDF Documents to AI Podcasts with Google Gemini and Text-to-Speech,https://n8n.io/workflows/4883-convert-pdf-documents-to-ai-podcasts-with-google-gemini-and-text-to-speech/,"Convert PDF documents to AI-generated podcasts with Google Gemini and Text-to-Speech
Transform any PDF document into an engaging, natural-sounding podcast using Google's Gemini AI and advanced Text-to-Speech technology. This automated workflow extracts text content, generates conversational scripts, and produces high-quality audio files.
Who is this for?
This workflow template is perfect for content creators, educators, researchers, and marketing professionals who want to repurpose written content into audio format. Ideal for creating podcast episodes, educational content, or making documents more accessible.
What problem does this solve?
Converting written documents to engaging audio content manually is time-consuming and requires scriptwriting skills. This workflow automates the entire process, turning static PDFs into dynamic, conversational podcasts that sound natural and engaging.
What this workflow does
Extracts text from uploaded PDF documents
Generates podcast script using Google Gemini AI with conversational tone
Converts script to speech using Google's advanced TTS with customizable voices
Processes audio into properly formatted WAV files
Saves final podcast ready for distribution
Setup
Obtain API credentials:
Get Google Gemini API key from AI Studio
Configure credentials in n8n as ""Google Gemini(PaLM) Api account""
Configure voice settings:
Choose from available voices: Kore (professional), Aoede (conversational), Laomedeia (energetic)
Customize script generation prompts if needed
Test the workflow:
Upload a sample PDF file
Verify audio output quality
Adjust voice settings as preferred
How to customize this workflow
Modify script style: Edit the prompt in the ""Generate Podcast Script"" node to change tone, length, or format
Change voice: Update the voice name in ""Prepare TTS Request"" node
Add preprocessing: Insert text cleaning nodes before script generation
Integrate with storage: Connect to Google Drive, Dropbox, or other storage services
Add notifications: Include Slack or email notifications when podcasts are ready
Note: This template requires Google Gemini API access and works best with text-based PDF files under 10MB."
Automated Multilingual Gmail Draft Reply with OpenAI GPT-4o,https://n8n.io/workflows/4870-automated-multilingual-gmail-draft-reply-with-openai-gpt-4o/,"Automated Multilingual Gmail Draft Reply with OpenAI GPT-4o in n8n
Who is this for?
This workflow is ideal for anyone who receives a high volume of Gmail inquiries, especially those providing multilingual customer support or handling diverse client
communications.
What problem is this workflow solving?
Managing frequent emails in multiple languages can be overwhelming. This workflow reduces manual drafting by automatically generating context-aware replies using OpenAI GPT-4o, letting users focus on personalization and quality assurance.
What this workflow does
Monitors your Gmail inbox for new emails with a specific label (e.g., ""Inquiry"").
Uses OpenAI GPT-4o for message assessment and language detection.
Parses information using a JSON parser.
Generates an AI-powered draft reply in the detected language via OpenAI GPT-4o.
Converts the reply to HTML and saves it as a draft in the original Gmail thread for your review.
Setup
Connect your Gmail account and set up relevant labels in both Gmail and the workflow.
Integrate your OpenAI credentials in n8n.
Configure the workflow trigger for your desired labels.
How to customize this workflow to your needs
Adjust label names in both Gmail and the workflow for different email categories.
Define custom starting and ending phrases for draft replies per supported language.
Expand supported languages or modify AI prompt instructions to suit your brand‚Äôs tone."
Automate LinkedIn Profile Search & Cold Email Outreach with OpenAI and Hunter,https://n8n.io/workflows/4831-automate-linkedin-profile-search-and-cold-email-outreach-with-openai-and-hunter/,"Problem
üö® LinkedIn search is BROKEN.
I waste hours on LinkedIn manually filtering profiles, reading summaries, hoping they‚Äôd reply‚Äîtedious, frustrating, inefficient.
I wish LinkedIn had built an agent that would get me to the right profile with one click!
If only I could just type:
""Product managers working on YouTube Shorts' generative AI features.""
and seconds later, have LinkedIn URLs, names, emails, and full context about them.
Introducing LinkGPT, a LinkedIn Agent, an automated agentic workflow powered by n8n.
Prerequisites:
Required accounts/API keys for Hunter.io, Google, and OpenAI.
This would be helpful for:
üéØ Job Seekers: Skip ATS, email hiring managers directly ( I wish I had this when I was recruiting!)
üéØ Recruiters: Reach your dream candidates first
üéØ Founders & Sales Teams: Share your demos with customers directly (this does NOT use the expensive Sales Navigator API)
Step-by-step workflow:
Takes 5-10 minutes to setup.
Generate a Boolean search string for LinkedIn profiles.
Perform authenticated Google searches using your query.
Extract LinkedIn URLs and workplace context from the search results.
Use OpenAI to extract first name, last name, and domain name from the context.
Use Hunter.io to find emails of the contacts.
Append all results to your connected Google Sheet. Columns: first_name, last_name, domain_name, email, context
I put together a clear, step-by-step guide on setting this up yourself.
Sample Query
Sample:
Input: ""I am headed to NYC later this month, whom should I meet with that works in product management for gen AI products?""
Output: List of 10 contacts with first name, last name, workplace, email address and context about them so you can email them."
Extract Invoice Data from Email to Google Sheets using GPT-4o AI Automation,https://n8n.io/workflows/4376-extract-invoice-data-from-email-to-google-sheets-using-gpt-4o-ai-automation/,"Transform your invoice processing from manual data entry into an intelligent automation system. This powerful n8n workflow monitors Gmail for invoice attachments, extracts data using AI-powered analysis, and creates organized Google Sheets with all relevant financial information automatically structured and ready for your accounting workflows.
üîÑ How It Works
This sophisticated 8-step automation eliminates manual invoice processing:
Step 1: Intelligent Email Monitoring
The workflow continuously monitors your Gmail account for emails with specific labels, checking every minute for new invoice attachments that need processing.
Step 2: Attachment Verification
Smart filtering ensures only emails with PDF attachments are processed, preventing unnecessary workflow triggers from text-only emails.
Step 3: Advanced PDF Extraction
The system automatically downloads and converts PDF invoices into readable text, handling various invoice formats and layouts with high accuracy.
Step 4: AI-Powered Data Analysis
GPT-4 processes the extracted text using specialized prompts designed for financial document analysis, identifying and extracting:
Company information and contact details
Invoice numbers, dates, and payment terms
Detailed line items with quantities and pricing
Tax calculations including CGST, SGST, and VAT
Billing and shipping addresses
Payment methods and transaction references
Step 5: Structured Data Formatting
The AI output is automatically formatted into clean, consistent JSON structure with 25+ standardized fields for comprehensive invoice tracking.
Step 6: Dynamic Spreadsheet Creation
Each processed invoice generates a new Google Sheets document with timestamp naming and organized data layout, ready for accounting review.
Step 7: Automated File Organization
Processed spreadsheets are automatically moved to designated Google Drive folders, maintaining organized file structure for easy retrieval and audit trails.
Step 8: Data Population
All extracted invoice data is populated into the spreadsheet with proper formatting, formulas, and structure for immediate use in accounting workflows.
‚öôÔ∏è Setup Steps
Prerequisites
Gmail account with invoice-receiving capability
Google Workspace access for Sheets and Drive
OpenAI API account for data extraction
n8n instance (cloud or self-hosted)
PDF invoices (text-based, not scanned images)
Gmail Configuration Requirements
Label Setup:
Create specific Gmail labels for invoice processing:
üìß Labels to Create:
- ""Invoice-Processing"" (main processing label)
- ""Invoice-Vendors"" (supplier invoices)
- ""Invoice-Clients"" (customer invoices)  
- ""Invoice-Processed"" (completed items)
Email Filter Configuration:
Set up automatic labeling rules:
Emails from known vendors ‚Üí Auto-apply ""Invoice-Processing""
Emails with ""Invoice"" in subject ‚Üí Auto-apply ""Invoice-Processing""
Attachments with PDF extension ‚Üí Auto-apply ""Invoice-Processing""
Configuration Steps
1. Credential Setup
Gmail OAuth2: Full email access including attachments
OpenAI API Key: GPT-4 access for intelligent data extraction
Google Sheets OAuth2: Spreadsheet creation and editing permissions
Google Drive OAuth2: File organization and folder management
2. Google Drive Folder Structure
Create organized folder hierarchy:
üìÅ Invoice Management/
‚îú‚îÄ‚îÄ üìÅ Incoming Invoices/
‚îú‚îÄ‚îÄ üìÅ Processed Invoices/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ 2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ Q1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ Q2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ Q3/
‚îú‚îÄ‚îÄ üìÅ Vendor Invoices/
‚îî‚îÄ‚îÄ üìÅ Client Invoices/
3. AI Extraction Customization
The default AI prompt extracts standard invoice fields but can be customized for:
Regional Tax Systems: GST (India), VAT (EU), Sales Tax (US)
Industry-Specific Fields: Purchase orders, project codes, cost centers
Company Standards: Custom fields, approval workflows, coding requirements
Multi-Currency: Exchange rates, currency conversion, international invoices
4. Data Validation Rules
Implement quality control measures:
Required Field Validation: Ensure critical data is always extracted
Format Standardization: Consistent date formats, number formatting
Duplicate Detection: Identify potentially duplicate invoices
Accuracy Scoring: Confidence levels for extracted data
5. Workflow Activation
Import the workflow JSON into your n8n instance
Configure all credential connections and test each step
Process test invoices to verify accuracy
Activate Gmail trigger for continuous monitoring
üöÄ Use Cases
Accounting Firms & Bookkeepers
Client Service Automation: Process invoices for multiple clients efficiently
Data Entry Elimination: Convert hours of manual work into automated processing
Accuracy Improvement: Reduce human errors in financial data transcription
Scalable Operations: Handle increased client volume without proportional staff increase
Small & Medium Businesses
Accounts Payable Automation: Streamline vendor invoice processing
Cash Flow Management: Quick access to payment due dates and amounts
Expense Tracking: Organized categorization of business expenses
Audit Preparation: Maintain organized, searchable invoice records
Corporate Finance Teams
Procurement Processing: Handle purchase orders and vendor invoices at scale
Multi-Location Operations: Centralize invoice processing across offices
Compliance Management: Ensure consistent data capture for regulatory requirements
Integration Readiness: Prepare data for ERP and accounting system import
Freelancers & Consultants
Client Invoice Tracking: Organize incoming payments and project billing
Expense Management: Categorize business expenses for tax preparation
Cash Flow Monitoring: Track outstanding invoices and payment schedules
Professional Organization: Maintain clean financial records for business growth
E-commerce & Retail
Supplier Invoice Processing: Manage inventory purchasing and cost tracking
Multi-Vendor Operations: Handle invoices from numerous suppliers efficiently
Cost Analysis: Track product costs and supplier performance
Inventory Reconciliation: Match invoice data with purchase orders and receipts
üîß Advanced Customization Options
Multi-Format Invoice Handling
Extend processing capabilities:
- PDF Text-Based: Standard invoice PDFs with selectable text
- Scanned Documents: Add OCR processing for image-based invoices
- Email Body Invoices: Extract data from invoice details in email content  
- Excel Attachments: Process invoices sent as spreadsheet files
- Multi-Page Documents: Handle complex invoices with multiple pages
Intelligent Data Validation
Implement quality assurance features:
Cross-Reference Validation: Compare extracted data against purchase orders
Vendor Database Matching: Verify company details against known vendor lists
Tax Calculation Verification: Validate tax amounts and rates for accuracy
Currency Conversion: Handle multi-currency invoices with real-time exchange rates
Workflow Integration Extensions
Connect to existing business systems:
ERP Integration: Direct data export to SAP, Oracle, or Microsoft Dynamics
Accounting Software: Push data to QuickBooks, Xero, or FreshBooks
Approval Workflows: Add review and approval steps before final processing
Payment Processing: Connect to banking systems for automated payment scheduling
Advanced Analytics & Reporting
Generate business insights:
Vendor Performance Analysis: Track pricing trends and payment terms
Expense Category Reporting: Automated expense categorization and analysis
Cash Flow Forecasting: Predict payment obligations based on due dates
Audit Trail Management: Maintain comprehensive processing logs for compliance
üìä Extracted Data Structure
Standard Invoice Fields (25+ Data Points)
The AI extraction captures comprehensive invoice information:
Header Information:
Billed To (Customer/Company Name)
Invoice Number (Unique Identifier)
Date of Issue (Invoice Creation Date)
Due Date (Payment Deadline)
Line Item Details:
Item Description (Product/Service Details)
Quantity (Number of Items/Hours)
Rate (Unit Price)
Amount (Line Total)
Tax and Financial Calculations:
CGST/SGST Rates and Amounts (Indian GST System)
VAT Calculations (European Tax System)
Subtotal (Pre-tax Amount)
Total Amount (Final Invoice Value)
Company and Contact Information:
Vendor Company Name
Contact Phone/Mobile
Email Address
Website URL
GST Registration Number
PAN Number (Indian Tax ID)
Address Information:
Billing Address
Shipping Address
Place of Supply
Place of Delivery
Payment Details:
Transaction IDs
Payment Mode (Check, Bank Transfer, Card)
Terms and Conditions
Special Instructions
Sample Extracted Data:
{
  ""billed_to"": ""Tech Solutions Inc."",
  ""invoice_number"": ""INV-2024-0156"",
  ""date_of_issue"": ""2024-03-15"",
  ""due_date"": ""2024-04-15"",
  ""item_0_description"": ""Web Development Services"",
  ""item_0_quantity"": 40,
  ""item_0_rate"": 75.00,
  ""item_0_amount"": 3000.00,
  ""tax_0_cgst_rate"": 9,
  ""tax_0_cgst_amount"": 270.00,
  ""tax_0_sgst_rate"": 9,
  ""tax_0_sgst_amount"": 270.00,
  ""subtotal"": 3000.00,
  ""total"": 3540.00,
  ""company_name"": ""Digital Services LLC"",
  ""company_email"": ""billing@digitalservices.com"",
  ""payment_transaction_ids"": ""TXN123456789"",
  ""mode_of_payment"": ""Bank Transfer""
}
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
PDF Extraction Challenges
Scanned Documents: Original workflow handles text-based PDFs only
Complex Layouts: Some invoice formats may require prompt refinement
Multi-Page Invoices: Large invoices might need pagination handling
Password Protection: Encrypted PDFs require manual processing
AI Extraction Accuracy
Field Recognition: Some custom invoice formats may need prompt tuning
Currency Handling: Multi-currency invoices may require specific configuration
Date Formats: International date formats might need standardization
Vendor Variations: Different vendor invoice styles may affect accuracy
Gmail Integration Limitations
Label Management: Ensure consistent labeling for proper processing
Attachment Size: Large PDFs may hit Gmail API limits
Email Volume: High-volume processing may require rate limiting
Security Settings: Corporate Gmail may have additional restrictions
Optimization Strategies
Processing Efficiency
Batch Processing: Group similar invoices for more efficient processing
Template Recognition: Create vendor-specific extraction templates
Quality Scoring: Implement confidence ratings for extracted data
Error Handling: Add fallback processes for failed extractions
Data Quality Assurance
Validation Rules: Implement business logic for data verification
Duplicate Detection: Prevent duplicate invoice processing
Manual Review Queues: Flag uncertain extractions for human review
Audit Logging: Maintain detailed processing logs for troubleshooting
Business Process Integration
Approval Workflows: Add management approval steps for high-value invoices
Exception Handling: Create special processes for unusual invoice types
Reporting Automation: Generate regular summaries of processed invoices
Archive Management: Implement retention policies for processed documents
üìà Success Metrics
Efficiency Improvements
Processing Time: Reduce manual data entry from hours to minutes
Accuracy Rates: Achieve 95%+ data extraction accuracy
Volume Capacity: Process 10-50x more invoices with same resources
Error Reduction: Eliminate manual transcription errors
Business Impact Measurements
Cost Savings: Calculate labor cost reduction from automation
Cash Flow Management: Faster invoice processing enables better payment scheduling
Compliance: Improved audit trails and data consistency
Scalability: Ability to handle business growth without proportional staff increase
üìû Questions & Support
Need help implementing or optimizing your AI Invoice Processor Agent?
üìß Expert Technical Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Specialization: Invoice processing automation, AI data extraction, accounting workflow integration
üé• Comprehensive Training Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup and configuration walkthroughs
Advanced customization for different invoice types
Integration tutorials for popular accounting software
Troubleshooting common extraction and processing issues
Best practices for financial document automation
ü§ù Professional Community & Updates
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing automation consulting and support
Share your invoice processing success stories and ROI results
Access exclusive workflow templates and advanced configurations
Join discussions about financial automation trends and innovations
üí¨ Support Request Guidelines
Include in your support message:
Your current invoice processing volume and types
Specific vendor formats or invoice layouts you handle
Target accounting software or systems for integration
Any technical errors or extraction accuracy issues
Current manual processing workflow and pain points
Ready to eliminate manual invoice processing forever? Deploy this AI Invoice Processor Agent and transform your accounting workflow from tedious data entry into intelligent, automated financial management!"
LinkedIn Profile Extract and Build JSON Resume with Bright Data & Google Gemini,https://n8n.io/workflows/4653-linkedin-profile-extract-and-build-json-resume-with-bright-data-and-google-gemini/,"Who this is for?
The LinkedIn Profile Extract and JSON Resume Builder is a powerful workflow that scrapes professional profile data from LinkedIn using Bright Data's infrastructure, then transforms that data into a clean, structured JSON resume using Google Gemini. The workflow is ideal for automating resume parsing, candidate profiling, or integrating into recruiting platforms.
This workflow is tailored for:
HR professionals & recruiters automating resume screening
Talent acquisition platforms enriching candidate profiles
Developers & AI builders creating resume-parsing AI pipelines
Data scientists working on labor market analytics
Growth hackers profiling prospects via public data
What problem is this workflow solving?
Parsing resumes or LinkedIn profiles into machine-readable formats is often a manual, error-prone process. Most scraping tools either fail due to anti-bot protections or return unstructured HTML that's hard to work with.
This workflow solves that by:
Using Bright Data's Web Unlocker for reliable, CAPTCHA-free LinkedIn scraping
Extracting clean text and structured profile data via Google Gemini LLM
Automatically generating a standards-compliant JSON Resume and Skills
Sending the resume to webhooks or storing it for downstream usage
What this workflow does
Accepts LinkedIn Profile URL and required metadata (Bright Data zone, webhook)
Scrapes LinkedIn profile using Bright Data Web Unlocker
Extracts clean content and skills using Google Gemini LLM
Builds a JSON-formatted resume following the JSON resume schema
Sends the JSON resume via Webhook Notification
Persists the output by saving the file to disk
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Set URL and Bright Data Zone node with the LinkedIn profile, Bright Data Zone and the Webhook notification URL. For testing purposes, you can obtain a webhook url using https://webhook.site/
How to customize this workflow to your needs
Add Language Translation
Insert a translation LLM node to support multilingual profiles.
Generate PDF Resumes
Convert JSON to formatted PDF resumes using an HTML-to-PDF module.
Push to ATS or CRM
Add integration nodes to pipe data into applicant tracking systems (ATS), CRMs, or databases.
Use Alternative LLMs
Swap Gemini with OpenAI or Anthropic Claude if preferred."
Real Estate Intelligence Tracker with Bright Data & OpenAI,https://n8n.io/workflows/4281-real-estate-intelligence-tracker-with-bright-data-and-openai/,"Who this is for
The Real Estate Intelligence Tracker is a powerful automated workflow designed for real estate analysts, investors, proptech startups, and market researchers who need to collect and analyze structured data from real estate listings across the web at scale.
This workflow is tailored for:
Real Estate Analysts - Tracking property prices, locations, and market trends
Investment Firms - Sourcing high-opportunity listings for portfolio decisions
PropTech Developers - Automating listing insights for SaaS platforms
Market Researchers - Extracting insights from competitive housing data
Growth Teams - Monitoring geographic property trends and pricing fluctuations
What problem is this workflow solving?
Collecting structured real estate listing data from property websites is difficult due to bot protections and unstructured HTML content. Manual data collection is slow and error-prone, and traditional scrapers often get blocked or miss context.
This workflow solves:
Automated bypass of anti-bot protection using Bright Data Web Unlocker
Conversion of unstructured HTML content into clean text using a Markdown-to-text LLM pipeline
Structured extraction of key listing data like price, location, property type, and features using OpenAI
Aggregation and delivery of insights to Google Sheets, local storage, and webhook-based alerts
What this workflow does
Convert to Text: Transforms scraped HTML/markdown into clean text using a Basic LLM Chain
Structured Data Extraction: Uses OpenAI GPT-4o with the Information Extractor node to parse property attributes (price, address, area, type, etc.)
Aggregate & Merge: Combines data from multiple pages or listings into a cohesive structure
Outbound Data Handling:
Google Sheets ‚Äì Appends the structured real estate data for further analysis
Save to Disk ‚Äì Persists structured JSON/text data locally
Webhook Notification ‚Äì Sends data alerts or summaries to any third-party platform
Pre-conditions
You need to have a Bright Data account and do the necessary setup as mentioned in the ""Setup"" section below.
You need to have an OpenAI Account.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, Configure the Google Sheet Credentials with your own account. Follow this documentation - Set Google Sheet Credential
In n8n, configure the OpenAi account credentials.
Ensure the URL and Bright Data zone name are correctly set in the Set URL, Filename and Bright Data Zone node.
Set the desired local path in the Write a file to disk node to save the responses.
How to customize this workflow to your needs
Target Multiple Sites or Locations
Update the Bright Data URL node dynamically with a list of regional real estate websites
Loop through different city/state filter URLs
Customize Extracted Fields
Modify the Information Extractor prompt to extract fields like:
Property size, number of bedrooms/bathrooms
Days on market
Nearby amenities or schools
Agent contact details
Integrate with More Destinations
Add nodes to export data to Notion, Airtable, HubSpot, or your custom database
Generate automated reports using PDF generators and email them
Data Quality and Logging
Add validation checks (e.g., missing price or address)
Save intermediate files (markdown, raw HTML, JSON output) to disk for audit purposes"
Build an Image Restoration Service with n8n & Gemini AI Image Editing,https://n8n.io/workflows/4815-build-an-image-restoration-service-with-n8n-and-gemini-ai-image-editing/,"This n8n template demonstrates how to build a simple but effective vintage image restoration service using an AI model with image editing capabilities.
With Gemini now capable of multimodal output, it's a great time to explore this capability for image or graphics automation. Let's see how well it does for a task such as image restoration.
Good to know
At time of writing, each image generated will cost $0.039 USD. See Gemini Pricing for updated info.
The model used in this workflow is geo-restricted! If it says model not found, it may not be available in your country or region.
How it works
Images are imported into our workflow via the HTTP node and converted to base64 strings using the Extract from file node.
The image data is then pipelined to Gemini's Image Generation model. A prompt is provided to instruct Gemini to ""restore"" the image to near new condition - of course, feel free to experiment with this prompt to improve the results!
Gemini's responds with the image as a base64 string and hence, a convert to file node is used to transform the data to binary.
With the restored image as a binary, we can then use this with our Google Drive node to upload it to our desired folder.
How to use
This demonstration uses 3 random images sourced from the internet but any typical image file will work.
Use a webhook node to allow integration from other applications.
Use a telegram trigger for instant mobile service!
Requirements
Google Gemini for LLM/Image generation
Google Drive for Upload Storage
Customising this workflow
AI image editing can be applied to many use-cases not just image restoration. Try using it to add watermarks, branding or modify an existing image for marketing purposes."
Document Q&A Chatbot with Gemini AI and Supabase Vector Search for Telegram,https://n8n.io/workflows/3940-document-qanda-chatbot-with-gemini-ai-and-supabase-vector-search-for-telegram/,"This template creates a Telegram AI Assistant that answers questions based on your documents, powered by Google Gemini and Supabase. Key features include Intelligent HTML Post-processing for rich formatting in Telegram and Adaptive Message Chunking to handle long text responses.
üìπ Watch the Bot in Action
‚ñ∂Ô∏è Click the image above to watch a live demo on YouTube.
This video provides a live demonstration of the bot's core features and how it interacts. See a quick walkthrough of its capabilities and user flow.
How it works:
User uploads a PDF document to a Telegram bot.
The workflow processes the PDF, creates embeddings using Google Gemini, and stores these embeddings in a Supabase vector table.
Users then ask questions to the bot.
The workflow performs a vector search in Supabase to find relevant document chunks based on the user's query.
Google Gemini uses the retrieved relevant chunks to generate an intelligent answer.
The bot sends the formatted answer back to the user on Telegram, utilizing HTML markup for enhanced presentation.
Set up steps:
Setup should take approximately 15-20 minutes.
Import the workflow into your n8n instance.
Configure credentials for Telegram, Google Gemini, and Supabase.
Set up your Supabase vector table using the provided SQL script.
Activate the workflow.
Detailed setup instructions, including how to get API keys and configure nodes, are available in the sticky notes within the workflow itself."
Build an MCP Server with Google Calendar and Custom Functions,https://n8n.io/workflows/3514-build-an-mcp-server-with-google-calendar-and-custom-functions/,"Learn how to build an MCP Server and Client in n8n with official nodes.
‚ö† Requires n8n version 1.88.0 or higher.
In this example, we use Google Calendar and custom functions as two separate MCP Servers, demonstrating how to integrate both native and custom tools.
How it works
The AI Agent connects to two MCP Servers. Each MCP Trigger (Server) generates a URL exposing its tools. This URL is used by an MCP Client linked to the AI Agent.
Whenever you make changes to the tools, there‚Äôs no need to modify the MCP Client. It automatically keeps the AI Agent informed on how to use each tool, even if you change them over time.
That‚Äôs the power of MCP üôå
Who is this template for
Anyone looking to use MCP with their AI Agents.
How to set up
Instructions are included within the workflow itself.
Check out my other templates
üëâ https://n8n.io/creators/solomon/"
"Discover & Analyze TikTok Influencers with Bright Data, Claude AI & Email Outreach",https://n8n.io/workflows/4774-discover-and-analyze-tiktok-influencers-with-bright-data-claude-ai-and-email-outreach/,"üéØ Automated TikTok Influencer Discovery & Analysis
A complete n8n automation that discovers TikTok influencers using Bright Data, evaluates their fit using Claude AI, and sends personalized outreach emails. Designed for marketing teams and brands that need a scalable, intelligent way to find and connect with relevant creators.
üìã Overview
This workflow provides a full-service influencer discovery pipeline: it finds TikTok profiles using search keywords, uses AI to assess alignment with your brand, and initiates contact with qualified influencers. Ideal for influencer marketing, brand partnerships, and campaign planning.
‚ú® Key Features
üîç Keyword-Based Discovery
Locate TikTok influencers by specific niche-related keywords.
üìä Bright Data Integration
Access accurate TikTok profile data from Bright Data‚Äôs datasets.
ü§ñ AI-Powered Analysis
Claude AI evaluates each profile's fit with your brand based on bio, content, and more.
üìß Smart Email Notifications
Sends tailored outreach emails to creators deemed highly relevant.
üìà Data Storage
Google Sheets stores profile details, AI evaluation results, and outreach status.
üéØ Intelligent Filtering
Processes only influencers who meet your criteria (e.g., 5000+ followers, industry match).
‚ö° Fast & Reliable
Uses professional scraping with robust error handling.
üîÑ Batch Processing
Supports bulk influencer processing through a single automated flow.
üéØ What This Workflow Does
Input
Search Keywords ‚Äì TikTok terms for finding niche creators
Business Info ‚Äì Brand description and industry
Collaboration Criteria ‚Äì Follower count minimum, niche alignment
Processing Steps
Form Submission
TikTok Discovery via Bright Data
Data Extraction and Normalization
Save to Google Sheets
Relevance Scoring via Claude AI
Filtering Based on AI Score + Follower Count
Personalized Email Outreach
Output Data Points
Field Description Example
Profile ID TikTok profile identifier tiktoker123456
Username TikTok handle @creativecreator
URL Profile link https://tiktok.com/@creativecreator
Description Creator bio ""Fashion & lifestyle content...""
Followers Total follower count 50,000
Collaboration AI assessment of brand fit ""Highly Relevant""
Analysis 50-word Claude AI relevance summary ""Strong alignment with fashion...""
üöÄ Setup Instructions
Prerequisites
n8n (cloud or self-hosted)
Bright Data account with TikTok access
Google Sheets + Gmail
Anthropic Claude API key
10‚Äì15 minutes setup time
Step-by-Step Setup
Import Workflow via JSON in n8n
Configure Bright Data ‚Äì Add API credentials and dataset ID
Google Sheets ‚Äì Setup credentials and map columns
Claude AI ‚Äì Insert API key and select desired model
Gmail ‚Äì Authenticate Gmail and update mail node settings
Update Variables ‚Äì Replace placeholders with business info
Test & Launch ‚Äì Submit a sample form and verify all outputs
üìñ Usage Guide
Adding Search Keywords
Submit the form with search terms, business description, and industry category to trigger the workflow.
Understanding AI Analysis
Emails are sent only if:
Collaboration status = Highly Relevant
Follower count ‚â• 5000
Industry alignment confirmed
Claude AI returns a 50-word analysis justifying the match
Customizing Filters
Edit the ""Find the Collaborator"" prompt to adjust:
Follower thresholds
Industry relevance
Additional metrics (e.g., engagement rate)
Viewing Results
Google Sheets log includes:
Influencer metadata
AI scores and rationale
Collaboration status
Email delivery timestamp
üîß Customization Options
Add More Fields: Engagement rate, contact email, content themes
Email Personalization: Customize message templates or integrate other mail services
Enhanced Filtering: Use engagement rates, region, content frequency
üö® Troubleshooting
Issue Fix
Bright Data fails Recheck API and dataset ID
No influencer data Adjust keywords or dataset scope
Sheets permission error Re-authenticate and check sharing
Claude fails Validate API key and prompt
Emails not sent Re-auth Gmail or update recipient field
Form not triggering Reconfirm webhook URL and permissions
Advanced Debugging
Check n8n execution logs
Run individual nodes for pinpointing failures
Confirm all data formats
Handle API rate limits
Add error-catch nodes for retries
üìä Use Cases & Examples
Brand Discovery: Fashion, tech, fitness creators
Competitor Insights: Find influencers used by rival brands
Campaign Planning: Build targeted influencer lists
Market Research: Identify creator trends across regions
‚öôÔ∏è Advanced Configuration
Batch Execution: Process multiple keywords with delay logic
Engagement Metrics: Scrape and calculate likes-to-follower ratios
CRM Integration: Sync qualified profiles to HubSpot, Salesforce, or Slack
üìà Performance & Limits
Processing Time: 3‚Äì5 minutes per keyword
Concurrency: 3‚Äì5 simultaneous fetches (depends on plan)
Accuracy: >95% influencer data reliability
Success Rate: 90%+ for outreach and processing"
"Build a Personal Assistant with Google Gemini, Gmail and Calendar using MCP",https://n8n.io/workflows/3905-build-a-personal-assistant-with-google-gemini-gmail-and-calendar-using-mcp/,"Talk to Your Apps: Building a Personal Assistant MCP Server with Google Gemini
Wouldn't it be cool to just tell your computer or phone to ""schedule a meeting with Sarah next Tuesday at 3 PM"" or ""find John Doe's email address"" and have it actually do it? That's the dream of a personal assistant!
With n8n and the power of MCP and AI models like Google Gemini, you can actually build something pretty close to that. We've put together a workflow that shows you how you can use a natural language chat interface to interact with your other apps, like your CRM, email, and calendar.
What You Need to Get Started
Before you dive in, you'll need a few things:
n8n: An n8n instance (either cloud or self-hosted) to build and run your workflow.
Google Gemini Access: Access to the Google Gemini model via an API key.
Credentials for Your Apps: API keys or login details for the specific CRM, Email, and Calendar services you want to connect (like Google Sheets for CRM, Gmail, Google Calendar, etc., depending on your chosen nodes).
A Chat Interface: A way to send messages to n8n to trigger the workflow (e.g., via a chat app node or webhook).
How it Works (In Simple Terms)
Imagine this workflow is like a helpful assistant who sits between you and your computer.
Step 1: You Talk, the AI Agent Listens
It all starts when you send a message through your connected chat interface. Think of this as you speaking directly to your assistant.
Step 2: The Assistant's Brain (Google Gemini)
Your message goes straight to the assistant's ""brain."" In this case, the brain is powered by a smart AI model like Google Gemini. In our template we are using the latest Gemini 2.5 Pro. But this is totally up to you. Experiment and track which model fits the kind of tasks you will pass to the agent. Its job is to understand exactly what you're asking for.
Are you asking to create something?
Are you asking to find information?
Are you asking to update something?
The brain also uses a ""memory"" so it can remember what you've talked about recently, making the conversation feel more natural. We are using the default context window, which is the past 5 interactions.
Step 3: The Assistant Decides What Tool to Use
Once the brain understands your request, the assistant figures out the best way to help you. It looks at the request and thinks, ""Okay, to do this, I need to use one of my tools.""
Step 4: The Assistant's Toolbox (MCP & Your Apps)
Here's where the ""MCP"" part comes in. Think of ""MCP"" (Model Context Protocol) as the assistant's special toolbox. Inside this toolbox are connections to all the different apps and services you use ‚Äì your CRM for contacts, your email service, and your calendar.
The MCP system acts like a manager for these tools, making them available to the assistant whenever they're needed.
Step 5: Using the Right Tool for the Job
Based on what you asked for, the assistant picks the correct tool from the toolbox.
If you asked to find a contact, it grabs the ""Get Contact"" node from the CRM section.
If you wanted to schedule a meeting, it picks the ""Create Event"" node from the Calendar section.
If you asked to draft an email, it uses the ""Draft Email"" node.
Step 6: The Tool Takes Action
Now, the node or set of nodes get to work! It performs the action you requested within the specific app.
The CRM tool finds or adds the contact.
The Email tool drafts the message.
The Calendar tool creates the event.
Step 7: Task Completed!
And just like that, your request is handled automatically, all because you simply told your assistant what you wanted in plain language.
Why This is Awesome
This kind of workflow shows the power of combining AI with automation platforms like n8n. You can move beyond clicking buttons and filling out forms, and instead, interact with your digital life using natural conversation. n8n makes it possible to visually build these complex connections between your chat, the AI brain, and all your different apps.
Taking it Further (Possible Enhancements)
This is just the start! You could enhance this personal assistant by:
Connecting more apps and services (task managers, project tools, etc.).
Adding capabilities to search the web or internal documents.
Implementing more sophisticated memory or context handling.
Getting a notification when the AI agent is done completing each task such as in Slack or Microsoft Teams.
Allowing the assistant to ask clarifying questions if needed. Building a robust prompt for the AI agent.
Ready to Automate Your Workflow?
Imagine the dozens of hours your team could save weekly by automating repetitive tasks through a simple, natural language interface.
Need help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
Create Character-Consistent Images with FLUX Kontext & Post to Social via Upload Post,https://n8n.io/workflows/4798-create-character-consistent-images-with-flux-kontext-and-post-to-social-via-upload-post/,"Create, iterate, and share! Transform a single image through multiple scenes while maintaining consistency.
‚ú® What this workflow does
This template showcases FLUX.1 Kontext - Black Forest Labs' in-context image generation model that excels at maintaining character features across multiple transformations. Combined with the Upload Post community node for effortless multi-platform social media posting, you can create and share compelling visual stories instantly.
The workflow demonstrates FLUX Kontext's core strength: character consistency across multiple image generations. Starting with a single input image, it:
üñºÔ∏è Loads an initial character image (example: a cute animal mascot)
üìù Defines multiple scene transformation prompts
üîÑ Iteratively generates new scenes while preserving exact character features
üéØ Maintains visual consistency by reusing binary data from previous generations
üì± Auto-posts the complete transformation series to multiple social platforms simultaneously
üöÄ Key Features: The Consistency Advantage
Character Preservation: FLUX Kontext's signature feature - maintains character features and style across transformations (requires specific prompting techniques)
Iterative Context Building: Each generation uses the previous image as context, creating visual continuity
Binary Data Reuse: Smart workflow design that feeds output from one generation as input to the next
Multi-Scene Storytelling: Transform your character across different environments while keeping them recognizable
One-Click Multi-Platform Posting: Upload Post* eliminates the tedious process of posting to each platform individually
üì± Why use Upload Post?
Posting the same content to TikTok, Instagram, LinkedIn, YouTube, Facebook, X (Twitter), and Threads individually is time-consuming and error-prone. The Upload Post service* simplifies this process:
‚úÖ Connect once, post everywhere: Link all your social media accounts to Upload Post
‚úÖ Single API call: Post to multiple platforms with one simple node
‚úÖ No more platform juggling: Skip the endless switching between apps and dashboards
‚úÖ Consistent timing: All platforms get your content simultaneously
‚úÖ Trusted by 3,751+ users: Proven solution for content creators and marketers
Instead of spending 30+ minutes manually posting to each platform, Upload Post does it all in seconds with a single n8n node!
üõ†Ô∏è Prerequisites
Required Accounts:
Black Forest Labs API: Create account at dashboard.bfl.ai
Get your API key for FLUX Kontext Pro access
Upload Post Account: Sign up at upload-post.com*
Connect your social media profiles (TikTok, Instagram, LinkedIn, YouTube, Facebook, X/Twitter, Threads)
Get API credentials for automated posting
Free tier available: 10 uploads/month
üí° Perfect For:
Character Designers maintaining brand character integrity across scenes
Social Media Managers creating engaging visual story series without manual posting
Brand Marketers ensuring character consistency across campaigns
Storytellers building visual narratives with consistent protagonists
Agencies managing multiple client accounts efficiently
üîß Customization Options:
Modify transformation prompts to create your own character journey
Adjust iteration steps
Change initial character image
Configure social platform targeting (choose which platforms to post to)
Customize post content and formatting
Experiment with different consistency scenarios
* Affiliate link"
Context-Aware Google Calendar Management with MCP Protocol,https://n8n.io/workflows/4231-context-aware-google-calendar-management-with-mcp-protocol/,"Google Calendar MCP ‚Äì Context-Aware Calendar Operations
This n8n template implements an MCP (Model Context Protocol)-compliant module for managing Google Calendar events in a context-aware, conflict-free manner.
üß† What It Does
This MCP enables structured interaction with Google Calendar based on context and intent, ensuring reliable, reusable operations with awareness of existing data and state.
‚úÖ Core Capabilities
Context-aware event creation
Prevents overlapping by validating time availability before creating new events.
Gap validation
Checks if a time range is busy or free, enabling smarter scheduling decisions.
Conditional updates
Only updates events after confirming their existence and current state.
Safe deletion
Removes events using MCP principles of validation and traceability.
üöÄ How to Use
To use this MCP in your context-aware systems:
Deploy the template in your n8n instance.
Locate the Server node in the workflow ‚Äî it exposes a Server-Sent Events (SSE) URL.
Copy that SSE URL.
Use that URL as the entry point for your MCP client or orchestrator.
This URL acts as the communication bridge, allowing you to interact with the MCP-compliant Google Calendar logic using standard MCP semantics."
"AI-Powered Short-Form Video Generator with OpenAI, Flux, Kling, and ElevenLabs",https://n8n.io/workflows/3121-ai-powered-short-form-video-generator-with-openai-flux-kling-and-elevenlabs/,"Who is this for?
Content creators, digital marketers, and social media managers who want to automate the creation of short-form videos for platforms like TikTok, YouTube Shorts, and Instagram Reels without extensive video editing skills.
What problem does this workflow solve?
Creating engaging short-form videos consistently is time-consuming and requires multiple tools and skills. This workflow automates the entire process from ideation to publishing, significantly reducing the manual effort needed while maintaining content quality.
What this workflow does
This all-in-one solution transforms ideas into fully produced short-form videos through a 5-step process:
Generate video captions from ideas stored in a Google Sheet
Create AI-generated images using Flux and the OpenAI API
Convert images to videos using Kling's API
Add voice-overs to your content with Eleven Labs
Complete the video production with Creatomate by adding templates, transitions, and combining all elements
The workflow handles everything from sourcing content ideas to rendering the final video, and even notifies you on Discord when videos are ready.
Setup (Est. time: 20-30 minutes)
Before getting started, you'll need:
n8n installation (tested on version 1.81.4)
OpenAI API Key (free trial credits available)
PiAPI (free trial credits available)
Eleven Labs (free account)
Creatomate API Key (free trial credits available)
Google Sheets API enabled in Google Cloud Console
Google Drive API enabled in Google Cloud Console
OAuth 2.0 Client ID and Client Secret from your Google Cloud Console Credentials
How to customize this workflow to your needs
Adjust the Google Sheet structure to include additional data like video length, duration, style, etc.
Modify the prompt templates for each AI service to match your brand voice and content style
Update the Creatomate template to reflect your visual branding
Configure notification preferences in Discord to manage your workflow
This workflow combines multiple AI technologies to create a seamless content production pipeline, saving you hours of work per video and allowing you to focus on strategy rather than production."
"Automate Instagram Content Discovery & Repurposing w/ Apify, GPT-4o & Perplexity",https://n8n.io/workflows/4658-automate-instagram-content-discovery-and-repurposing-w-apify-gpt-4o-and-perplexity/,"This workflow creates an end-to-end Instagram content pipeline that automatically discovers trending content from competitor channels, extracts valuable insights, and generates new high-quality scripts for your own content creation. The system helped scale an Instagram channel from 0 to 10,000 followers in just 15 days through intelligent content repurposing.
Benefits:
Complete Content Automation - Monitors competitor Instagram accounts, downloads new reels, and processes them without manual intervention
AI-Powered Script Generation - Uses ChatGPT and Perplexity to analyze content, identify tools/technologies, and rewrite scripts with fresh angles
Smart Duplicate Prevention - Automatically tracks processed content in a database to avoid redundant work
Multi-Platform Intelligence - Combines Instagram scraping, AI transcription, web research, and content generation in one seamless flow
Scalable Content Strategy - Process content from multiple niches and creators to fuel unlimited content ideas
Revenue-Focused Approach - Specifically designed to identify monetizable tools and technologies for business-focused content
How It Works:
Instagram Content Discovery:
Uses Apify's Instagram scraper to monitor specified creator accounts for new reels
Automatically downloads video content and metadata from target accounts
Filters content based on engagement metrics and relevance
Intelligent Processing Pipeline:
Transcribes video content using OpenAI Whisper for accurate text extraction
Filters content using AI to identify tools, technologies, and automation opportunities
Cross-references against existing database to prevent duplicate processing
Enhanced Research & Analysis:
Searches Perplexity AI for additional insights about discovered tools
Generates step-by-step usage guides and implementation instructions
Identifies unique angles and opportunities for content improvement
Script Generation & Optimization:
Creates new, original scripts optimized for your specific audience
Maintains consistent brand voice while adding fresh perspectives
Includes strategic call-to-action elements for audience engagement
Required Google Sheets Database Setup:
Before running this workflow, create a Google Sheets database with these exact column headers:
Essential Columns:
id - Unique Instagram post identifier (primary key for duplicate detection)
timestamp - When the reel was posted
caption - Original reel caption text
hashtags - Hashtags used in the post
videoUrl - Direct link to download the video file
username - Account that posted the reel
scrapedTranscript - Original transcript from video (added by workflow)
newTranscript - AI-generated script for your content (added by workflow)
Additional Tracking Columns:
shortCode - Instagram's internal post code
url - Public Instagram post URL
commentsCount - Number of comments
firstComment - Top comment on the post
likesCount - Number of likes
videoViewCount - View count metrics
videoDuration - Length of video in seconds
Setup Instructions:
Create a new Google Sheet with these column headers in the first row
Name the sheet ""Reels""
Connect your Google Sheets OAuth credentials in n8n
Update the document ID in the workflow nodes
The merge logic relies on the id column to prevent duplicate processing, so this structure is essential for the workflow to function correctly.
Business Use Cases:
Content Creators - Scale content production by 10x while maintaining quality and originality
Marketing Agencies - Offer content research and ideation as a premium service
Course Creators - Identify trending tools and technologies for educational content
Revenue Potential:
This exact system can be sold as a service for $3,000-$5,000 to growing channels or agencies. The automation saves 10+ hours weekly of manual research and content planning.
Difficulty Level: Intermediate
Estimated Build Time: 1-2 hours
Monthly Operating Cost: ~$30 (API usage)
Watch the Complete Build Process
Want to see exactly how this system was built from scratch? Nick Saraev walks through the entire development process in this comprehensive tutorial, including all the debugging, dead ends, and problem-solving that goes into building real automation systems.
üé• Watch: ""The N8N Instagram Parasite System (10K Followers In 15 Days)""
This 1.5-hour deep-dive shows the actual build process - not a polished demo, but real system development with all the thinking and iteration included.
Set Up Steps:
Configure Apify Integration:
Sign up for Apify account and obtain API key
Replace the bearer token in the ""Run Actor Synchronously"" node
Customize the username array with your target Instagram accounts
Set Up AI Services:
Add OpenAI API credentials for transcription and content generation
Configure Perplexity API for enhanced research capabilities
Set up appropriate rate limiting for cost control
Database Configuration:
Create Google Sheets database with provided column structure
Connect Google Sheets OAuth credentials
Configure the merge logic for duplicate detection
Content Filtering Setup:
Customize the AI prompts for your specific niche and requirements
Adjust the filtering criteria for tool/technology detection
Set up the script generation template to match your brand voice
Automation Schedule:
Configure the schedule trigger for daily content monitoring
Set optimal timing based on your content creation workflow
Test the complete flow with a small number of accounts first
Advanced Customization:
Add additional content sources beyond Instagram
Integrate with your existing content management systems
Scale up monitoring to dozens of competitor accounts
More AI Automation Systems:
For more advanced automation tutorials and business systems, check out My YouTube Channel where I share proven automation strategies that generate real revenue."
Generate & Publish SEO-Optimized WordPress Blog Posts with AI,https://n8n.io/workflows/4024-generate-and-publish-seo-optimized-wordpress-blog-posts-with-ai/,"Generate and Publish SEO-Optimized Blog Posts to WordPress
This n8n workflow, BlogBlitz, automates the creation and publishing of SEO-optimized blog posts for WordPress, saving you hours of content creation time. Triggered via Telegram or a scheduled interval, it generates 1,500‚Äì2,500-word articles on Technology, AI, Tech Facts, History, or Tips, complete with catchy titles, slugs, meta descriptions, and realistic featured images. Perfect for bloggers and marketers, BlogBlitz ensures your site stays fresh with high-quality content.
Who is this for?
Bloggers, content marketers, and WordPress site owners who want to automate high-quality, SEO-ready blog content creation without manual writing or formatting.
What problem is this workflow solving?
Manually creating engaging, SEO-optimized blog posts is time-consuming and requires expertise. BlogBlitz solves this by automating the entire process‚Äîfrom generating ideas and writing articles to publishing and notifying you‚Äîkeeping your site active and discoverable.
What this workflow does
Triggers: Runs every 3 hours via a Schedule Trigger or on-demand with a Telegram command (‚Äúgenerate‚Äù).
Generates Content: Uses OpenRouter to pick a category (e.g., Technology, AI) and create a title, slug, focus keyphrase, and meta description.
Writes Articles: OpenAI crafts 1,500‚Äì2,500-word posts with SEO-friendly structure, headings, and a call-to-action.
Adds Visuals: Generates realistic featured images with OpenAI and uploads them to WordPress.
SEO Features: Generate optimized slug, focus keyphrase, meta description
Publishes: Posts articles to WordPress with proper categories and featured images.
Notifies: Sends publish alerts with links to Discord and Telegram.
Setup
n8n Instance: Ensure you have a cloud or self-hosted n8n instance.
Credentials:
WordPress: API access for wp-json/wp/v2 endpoint.
OpenAI: For article and image generation.
OpenRouter: For title and category generation.
Telegram: Bot API for triggers and notifications.
Discord: Webhook for publish alerts.
WordPress Configuration:
Set up categories (Technology [ID:3], AI [ID:4], Tech Fact [ID:7], Tech History [ID:8], Tech Tips [ID:9]).
Ensure an admin user ID is available (default: 1).
Node Setup:
Use the Edit Fields node to centralize variables like category IDs.
Test: Send ‚Äúgenerate‚Äù via Telegram to test the workflow. Check WordPress for the published post.
How to customize this workflow to your needs
Change Categories: Update the WordPress Post Draft node to match your site‚Äôs category IDs.
Adjust Schedule: Modify the Schedule Trigger node for different intervals (e.g., daily).
Tweak Tone: Edit the prompt in the Basic LLM Chain node for a different writing style (e.g., formal or casual).
Add Platforms: Extend notifications to Slack or email by adding nodes.
Image Style: Adjust the OpenAI image node for different sizes or styles (e.g., ‚Äúnatural‚Äù instead of ‚Äúvivid‚Äù).
Pre-Requirements
n8n Instance: Cloud or self-hosted.
Credentials:
WordPress API (wp-json/wp/v2 endpoint).
OpenAI API for text and images.
OpenRouter API for AI content.
Telegram bot API.
Discord Webhook API.
Dependencies: @n8n/n8n-nodes-langchain package.
WordPress Setup: Categories and admin user ID configured.
Network: Stable internet for API calls.
Category: Marketing
Nodes Used: Telegram Trigger, Schedule Trigger, OpenRouter, OpenAI, WordPress, Discord, HTTP Request"
Automated YouTube Video Scheduling & AI Metadata Generation üé¨,https://n8n.io/workflows/3900-automated-youtube-video-scheduling-and-ai-metadata-generation/,"üë• Who Is This For?
Content creators, marketing teams, and channel managers who need to streamline video publishing with optimized metadata and scheduled releases across multiple videos.
üõ† What Problem Does This Solve?
Manual YouTube video publishing is time-consuming and often results in inconsistent descriptions, tags, and scheduling. This workflow fully automates:
Extracting video transcripts via Apify for metadata generation
Creating SEO-optimized descriptions and tags for each video
Setting videos to private during initial upload (critical for scheduling)
Implementing scheduled publishing at strategic times
Maintaining consistent branding and formatting across all content
üîÑ Node-by-Node Breakdown
Step Node Purpose
1 Every Day (Scheduler)
2 Get Videos to Harmonize
3 Get Video IDs (Unpublished)
4 Loop over Video IDs
5 Get Video Data
6 Loop over Videos with Parameter IS
7 Set Videos to Private
8 Apify: Get Transcript
9 Fetch Latest Videos
10 Loop Over Items
11 Generate Description, Tags, etc.
12 AP Clean ID
13 Retrieve Generated Data
14 Adjust Transcript Format
15 Update Video's Metadata
‚öôÔ∏è Pre-conditions / Requirements
n8n with YouTube API credentials configured
Apify account with API access for transcript extraction
YouTube channel with upload permissions
Master templates for description formatting
Videos must be initially set to private for scheduling to work
‚öôÔ∏è Setup Instructions
Import this workflow into your n8n instance.
Configure YouTube API credentials with proper channel access.
Set up Apify integration with appropriate actor for transcript extraction.
Define scheduling parameters in the Every Day node.
Configure description templates with placeholders for dynamic content.
Set default tags and customize tag generation rules.
Test with a single video before batch processing.
üé® How to Customize
Adjust prompt templates for description generation to match your brand voice.
Modify tag selection algorithms based on your channel's SEO strategy.
Create multiple publishing schedules for different content categories.
Integrate with analytics tools to optimize publishing times.
Add notification nodes to alert when videos are successfully scheduled.
‚ö†Ô∏è Important Notes
Videos MUST be uploaded as private initially - the Publish At logic only works for private videos that haven't been published before.
Publishing schedules require videos to remain private until their scheduled time.
Transcript quality affects metadata generation results.
Consider YouTube API quotas when scheduling large batches of videos.
üîê Security and Privacy
API credentials are stored securely within n8n.
Transcripts are processed temporarily and not stored permanently.
Webhook URLs should be protected to prevent unauthorized triggering.
Access to the workflow should be limited to authorized team members only."
"AI Premium Proposal Generator with OpenAI, Google Slides & PandaDoc",https://n8n.io/workflows/4804-ai-premium-proposal-generator-with-openai-google-slides-and-pandadoc/,"AI Proposal Generator System
Categories
Sales Automation
Document Generation
AI Business Tools
This workflow creates a complete AI-powered proposal generation system that transforms simple form inputs into professional, personalized proposals in under 30 seconds and can be deployed during live sales calls, allowing you to send polished proposals before the call even ends.
Benefits
Instant Proposal Generation - Convert 30-second form inputs into professional proposals automatically
High-Value Business Tool - Generates $1,500-$5,000 per client implementation
Live Sales Integration - Generate and send proposals during active sales calls
Complete Automation Pipeline - From form submission to email delivery with zero manual work
Professional Presentation - Produces proposals indistinguishable from manually crafted documents
Dual Platform Support - Works with both Google Slides (free) and PandaDoc (premium) integration
How It Works
Smart Form Interface:
Simple N8N form captures essential deal information
Collects prospect details, problems, solutions, scope, timeline, and budget
Designed for rapid completion during live sales conversations
Advanced AI Processing:
Uses sophisticated GPT-4 prompting with example-based training
Converts basic form inputs into professionally written proposal sections
Applies consistent tone, formatting, and business language automatically
Dynamic Document Generation:
Creates duplicate proposal templates for each new prospect
Replaces template variables with AI-generated personalized content
Maintains professional formatting and visual consistency
Automated Email Delivery:
Sends personalized email with proposal link immediately after generation
Includes professional messaging and clear next steps
Optionally includes invoice for immediate payment processing
Premium PandaDoc Integration:
Advanced version includes built-in payment processing
Combines proposal, agreement, and invoice in single document
Enables immediate signature and payment collection
Business Use Cases
Service-Based Businesses - Generate proposals for consulting, agencies, and professional services
Automation Agencies - Offer proposal generation as a high-value service to clients
Sales Teams - Accelerate proposal creation and improve close rates
Freelancers - Professionalize client interactions with instant custom proposals
Consultants - Streamline business development with automated proposal workflows
B2B Companies - Scale personalized proposal generation across entire sales organization
Difficulty Level: Intermediate
Estimated Build Time: 2-3 hours
Monthly Operating Cost: $20-150 (depending on Google Slides vs PandaDoc)
Watch My Complete Live Build
Want to see me build this entire $2,485 proposal system from scratch? I walk through every component live - including the AI prompting strategies, form design, Google Slides integration, and the advanced PandaDoc setup that enables payment collection.
üé• See My Live Build Process: ""I Built A $2,485 AI Proposal Generator In N8N (Copy This)""
This comprehensive tutorial shows the real development process - including advanced AI prompting, template design, API integrations, and the exact pricing strategy that generates $1,500-$5,000 per client.
Required Template Setup
Google Slides Template: Create a professional proposal template with these variable placeholders (wrapped in double curly braces):
{{proposalTitle}} - Main proposal heading
{{descriptionName}} - Project subtitle/description
{{oneParagraphProblemSummary}} - Problem analysis section
{{solutionHeadingOne}}, {{solutionHeadingTwo}}, {{solutionHeadingThree}} - Solution titles
{{shortScopeTitleOne}} through {{shortScopeTitleThree}} - Scope sections
{{milestoneOneDay}} through {{milestoneFourDay}} - Timeline milestones
{{cost}} - Project pricing
Form Field Requirements: The N8N form must include these exact field labels:
First Name, Last Name, Company Name, Email, Website
Problem (textarea) - Client's current challenges
Solution (textarea) - Your proposed approach
Scope (textarea) - Specific deliverables
Cost - Project pricing
How soon? - Timeline expectations
PandaDoc Setup (Premium): Configure PandaDoc template with token placeholders matching the AI-generated content structure. Template must include pricing tables and signature fields for complete proposal-to-payment automation.
Set Up Steps
Form Design & Integration:
Create N8N form with optimized fields for proposal generation
Design form flow for rapid completion during sales calls
Configure form triggers and data validation
AI Content Generation Setup:
Configure OpenAI API for sophisticated proposal writing
Implement example-based training with input/output pairs
Set up JSON formatting for structured content generation
Google Slides Integration (Free Version):
Create professional proposal templates with variable placeholders
Set up Google Cloud Console API access and credentials
Configure template duplication and text replacement workflows
Email Automation Setup:
Configure Gmail integration for automated proposal delivery
Design professional email templates with proposal links
Set up dynamic content insertion and personalization
PandaDoc Integration (Premium Version):
Set up PandaDoc API for advanced document generation
Configure payment processing and signature collection
Implement proposal-to-payment automation workflows
Testing & Quality Control:
Test complete workflow with various proposal scenarios
Validate AI output quality and professional presentation
Optimize form fields and content generation based on results
Advanced Features
Premium system includes:
Payment Processing Integration: Collect payments immediately after proposal acceptance
Digital Signature Collection: Streamline agreement execution with electronic signatures
Custom Branding: Apply company branding and visual identity automatically
Multi-Template Support: Generate different proposal types based on service offerings
CRM Integration: Automatically sync proposal data with existing sales systems
Why This System Works
The competitive advantage lies in speed and professionalism:
30-second generation time vs. hours of manual proposal writing
Professional presentation that matches or exceeds manual proposals
Live sales integration - send proposals during active conversations
Consistent quality - eliminates human error and formatting inconsistencies
Immediate follow-up - maintain sales momentum with instant delivery
System Architecture
The workflow follows a simple but powerful 6-step process:
Form Trigger - Captures essential deal information
AI Processing - Converts inputs to professional content
Template Duplication - Creates unique document for each prospect
Content Replacement - Populates template with AI-generated content
Email Delivery - Sends proposal with professional messaging
Payment Collection (PandaDoc) - Enables immediate signature and payment
Check Out My Channel
For more high-value automation systems and proven business-building strategies, explore my YouTube channel where I share the exact systems used to build successful automation businesses and scale to $72K+ monthly revenue."
"Summarize Calls & Notify Teams via HubSpot, Slack, Email, WhatsApp",https://n8n.io/workflows/4641-summarize-calls-and-notify-teams-via-hubspot-slack-email-whatsapp/,"This workflow automates the process of handling conversation transcriptions and distributing key information across your organization. Here's what it does:
Trigger: The workflow is initiated via a webhook that receives a transcription (e.g., from a call or meeting).
Summarization & Extraction: Using AI, the transcription is summarized, and key information is extracted ‚Äî such as action items, departments involved, and client details.
Department Notifications: The relevant summarized information is automatically routed to specific departments via email based on content classification.
CRM Sync: The summarized version is saved to the associated contact or deal in HubSpot for future reference and visibility.
**Multi-Channel Alerts: **The summary is also sent via WhatsApp and Slack to keep internal teams instantly informed, regardless of platform.
Use Case:
Ideal for sales, customer service, or operations teams who manage client conversations and want to ensure seamless cross-departmental communication, documentation, and follow-up.
Apps Used:
Webhook (Trigger)
OpenAI (or other AI/NLP for summarization)
HubSpot
Email
Slack
WhatsApp (via Twilio or third-party provider)"
"Auto-create and publish AI social videos with Telegram, GPT-4 and Blotato",https://n8n.io/workflows/3654-auto-create-and-publish-ai-social-videos-with-telegram-gpt-4-and-blotato/,"Auto-create and publish AI social videos with Telegram, GPT-4 and Blotato
‚ö†Ô∏è Disclaimer: This workflow uses Community Nodes and must be run on a self-hosted instance of n8n.
Who is this for?
This template is perfect for social media managers, content creators, AI enthusiasts, and automation pros who want to generate short-form videos (Reels) from a simple Telegram message, then publish them across multiple platforms‚Äîall without video editing or manual uploads.
What problem is this workflow solving?
Creating content is only half the job. The real bottleneck comes in:
Rendering the video,
Adding voice or music,
Writing captions and titles,
Publishing to multiple platforms.
This workflow automates all of that using AI. It saves hours every week and guarantees consistent output.
What this workflow does
This end-to-end automation handles everything from AI video generation to social publishing:
Starts with a Telegram message (text or image prompt)
Generates video using Kling or Blotato, based on the input
Creates music with Piapi and merges it with the video
Generates text overlays and captions with GPT-4
Builds a stylized video using JSON2Video
Logs results to Google Sheets
Sends final output back to Telegram
Auto-posts the video to 9 platforms via Blotato (Instagram, TikTok, YouTube, Facebook, LinkedIn, Threads, Twitter/X, Pinterest, Bluesky)
Setup
Connect your Telegram bot to the trigger node.
Add your OpenAI API key for all GPT nodes.
Set up Kling and Piapi API access (for video and music generation).
Connect your Cloudinary account to upload images.
Link a Google Sheet with columns: Title, Caption, URL.
Set your Blotato API key and fill in the platform-specific account IDs.
How to customize this workflow to your needs
Change prompt formatting to control GPT responses and video tone.
Edit text styling in JSON2Video to match your brand.
Add a Telegram approval step before publishing, if needed.
Disable platforms you don‚Äôt use by deleting their HTTP Request nodes.
Use a Google Sheet filter to only process new rows or drafts.
üìÑ Documentation: Notion Guide"
"Automatic Email Categorization with Gmail, Google Sheets, and AI",https://n8n.io/workflows/4687-automatic-email-categorization-with-gmail-google-sheets-and-ai/,
Automated AI Product Photography and Instagram Post Generator (Deepseek/Segmind),https://n8n.io/workflows/3633-automated-ai-product-photography-and-instagram-post-generator-deepseeksegmind/,"Automatically generate professional-grade product photography and ready-to-use Instagram posts using the power of AI, delivered straight to your Telegram for approval.
Setup is incredibly simple: All you need is your product image URL and a API key from Segmind.
Who is this for?
This template is ideal for:
E-commerce store owners looking to create high-quality product visuals affordably (The estimated cost is approximately $0.10 per product photography and accompanying Instagram post).
Dropshippers needing unique product images to stand out.
Social Media Managers & Marketers seeking to automate content creation for platforms like Instagram.
Small Businesses wanting professional marketing assets without the high cost or effort.
Anyone needing consistent, eye-catching product photos and social media captions on a regular schedule.
What problem is this workflow solving?
Creating professional product images is often expensive and time-consuming, requiring photographers, studios, and editing time. Consistently generating fresh and engaging social media content, especially for visual platforms like Instagram, adds another layer of complexity and effort. This workflow eliminates these bottlenecks by automating both high-quality image generation and relevant caption creation, freeing up your time and budget.
What this workflow does:
This n8n workflow automates the entire process on a schedule you define (e.g., every hour, every day):
Scheduled Start: Triggers automatically based on your chosen time interval (e.g., every hour).
Product Analysis: Takes your input product image URL and uses AI (GPT) to understand the product details.
AI Prompt Generation: Based on the product analysis and your preference (whether to include a human model or not), it uses AI (Deepseek) to craft a sophisticated prompt specifically for generating professional product photography via Segmind.
Instagram Post Creation: Simultaneously, the AI (Deepseek) generates an engaging Instagram post caption, complete with relevant hashtags, tailored to your product.
AI Image Generation: Sends the product image, the generated prompt, and product details to the Segmind API to create a brand new, studio-quality product photograph.
Telegram Validation: Downloads the newly generated product photo and sends both the photo and the generated Instagram post text directly to your specified Telegram chat. This allows you to quickly review and approve the content before using it.
Setup: Get Running in Minutes!
This workflow is designed for maximum ease of setup:
Get Segmind API Key: Sign up or log in to Segmind and grab your API key here: https://cloud.segmind.com/console/api-keys
Enter API Key: In the n8n workflow, click on the SegmindAPIKey node and paste your copied API key into the Value field.
Enter Product Image URL: Click the InputYourImageURL node and paste the web address (URL) of your product image into the Value field.
(Optional) Human Model Preference: Click the ImageInstruction node. Set the HumanModel value to true if you want a human model potentially included in the photo, or false if you want the product showcased alone or in a setting.
Set Your Schedule: Click the Schedule Trigger node. Define how often you want the workflow to run (e.g., under Interval, set 1 and select Hours from the dropdown for hourly runs).
Configure Telegram: Make sure you have a Telegram Bot credential configured in n8n. Then, in the SendProductPhotography and SendInstagramPost nodes, enter the correct Chat ID for where you want to receive the validation messages. [A video guidance is made to help you with telegram setup]
Activate Workflow: Toggle the workflow to ""Active"" in the top right corner of n8n.
That's it! The workflow will now automatically generate and send product photos and Instagram posts to your Telegram at your defined interval.
How to customize this workflow:
While the default setup works great, you can easily tweak it:
Photography Style: Modify the main prompt within the AI Agent1 node to guide the AI towards a specific aesthetic (e.g., ""minimalist background,"" ""bright natural lighting,"" ""dark moody atmosphere"").
Instagram Post Tone: Adjust the instructions within the AI Agent1 node to change the style or focus of the generated Instagram captions.
Schedule: Change the trigger interval in the Schedule Trigger node anytime.
AI Models: Experiment by changing the selected models in the OpenAI, OpenAI Chat Model1 (Deepseek).
Category:
Marketing, Social Media, E-commerce, Automation, AI, Content Creation, Product Photography"
Summarize YouTube Videos into Structured Content Ideas with AI and Airtable,https://n8n.io/workflows/3609-summarize-youtube-videos-into-structured-content-ideas-with-ai-and-airtable/,"Extract the main idea and key takeaways from YouTube videos and turn them into Airtable content ideas
üìù Description
Automatically turn YouTube videos into clear, structured content ideas stored in Airtable. This workflow pulls new video links from Airtable, extracts transcripts using a RapidAPI service, summarizes them with your favourite LLM, and logs the main idea and key takeaways‚Äîkeeping your content pipeline fresh with minimal effort.
‚öôÔ∏è What It Does
Scans Airtable for new YouTube video links every 5 minutes.
Extracts the transcript of the video using a third-party API via RapidAPI.
Summarizes the content to generate a main idea and takeaways.
Updates the original Airtable entry with the insights and marks it as completed.
üõ† Prerequisites
Before using this template, make sure you have:
‚úÖ A RapidAPI account with access to the youtube-video-summarizer-gpt-ai API.
‚úÖ A valid RapidAPI key.
‚úÖ An OpenAI, Claude or Gemini account connected to n8n.
‚úÖ An Airtable account with a base and table ready.
üß∞ Setup Instructions
Clone this template into your n8n workspace.
Open the Get YouTube Sources node and configure your Airtable credentials.
In the Get video transcript node:
Enter your X-RapidAPI-Key under headers.
The API endpoint is pre-configured.
Connect your LLM credentials to the Extract detailed summary node.
(Optional) Adjust the summarization prompt in the LangChain node to better suit your tone.
Set your preferred schedule in the Trigger node.
üìã Airtable Setup
Create a base (e.g., Content Hub) with a table named Ideas and the following columns:
Column Name Type Required Notes
Type Single select ‚úÖ Must be set to Youtube Video
Source URL ‚úÖ The YouTube video URL
Status Checkbox ‚úÖ Leave empty initially; updated after processing
MainIdea Single line text ‚úÖ Summary generated by OpenAI
Key Takeaways Long text ‚úÖ List of takeaways extracted from the transcript
Activate the workflow‚Äîand you're done!"
Create AI News Videos with HeyGen Avatars and Auto-Post to Social Media,https://n8n.io/workflows/3538-create-ai-news-videos-with-heygen-avatars-and-auto-post-to-social-media/,"Automatically generate short AI avatar videos based on trending news, then post them across social media platforms‚Äîfully automated using n8n, HeyGen, ChatGPT, and Blotato.
Tools & Services Used:
n8n (Self-hosted required due to Community Nodes)
HeyGen (for creating AI avatar videos)
OpenAI (for script and caption generation)
Blotato (for social media distribution)
Hacker News (for sourcing trending articles)
Workflow Overview:
This automation performs the following steps:
Fetches trending news from Hacker News
Generates a short script using ChatGPT
Creates an AI avatar video with HeyGen
Generates platform-specific captions
Posts the content across social media with Blotato
Prerequisites:
Make sure you have active accounts and API keys for:
HeyGen
OpenAI
Blotato
How to Use This Template
Step 1: Import the Template
Open your self-hosted n8n instance
Go to ""Workflows"" > ""Create Workflow""
Click the three dots (‚Ä¶) > ""Import from File""
Upload the downloaded JSON file
Step 2: Configure API Keys
Only two nodes need customization:
Setup HeyGen: Add your HeyGen API key, avatar ID, and voice ID
Prepare for Publish: Add your Blotato API key, account IDs, and page IDs
Optional platforms like Pinterest and Bluesky are already disabled by default.
Finding Your HeyGen Avatar & Voice IDs
To customize the AI avatar video:
Log into your HeyGen Dashboard
Navigate to ""Avatars"" ‚Üí choose or create an avatar ‚Üí copy the Avatar ID
Navigate to ""Voices"" ‚Üí select a preferred voice ‚Üí copy the Voice ID
You can upload a custom avatar or voice if you have access to those features in your plan
This allows you to align the video style with your brand‚Äôs tone and personality.
Initial Test Run
For your first run:
Shorten the AI script in the prompt to 5 seconds
Enable only one social media platform
Adjust the wait time to 2 minutes to speed up testing
Verify that the video is created and successfully posted
Once successful, enable more platforms and scale your automation.
Workflow Diagram:
Use Cases
Business owners automating daily content
Creators scaling short-form video production
Freelancers offering automation as a service
Anyone building an AI-driven media workflow
Disclaimer
This workflow uses Community Nodes, which only function on self-hosted n8n instances.
This workflow may not work on n8n Cloud without some modifications."
"Search & Summarize Web Data with Perplexity, Gemini AI & Bright Data to Webhooks",https://n8n.io/workflows/3534-search-and-summarize-web-data-with-perplexity-gemini-ai-and-bright-data-to-webhooks/,"Who this is for?
This workflow is designed for professionals and teams who need real-time, structured insights from Perplexity Search results without manual effort.
What problem is this workflow solving?
This n8n workflow solves the problem of automating Perplexity Search result extraction, cleanup, summarization, and AI-enhanced formatting for downstream use like sending the results to a webhook or another system.
What this workflow does
Automates Perplexity Search via Bright Data
Uses Bright Data‚Äôs proxy-based SERP API to run a Google Search query programmatically.
Makes the process repeatable and scriptable with different search terms and regions/zones.
Cleans and Extracts Useful Content
The Readable Data Extractor uses LLM-based cleaning to remove HTML/CSS/JS from the response and extract pure text data.
Converts messy, unstructured web content into structured, machine-readable format.
Summarizes Search Results
Through the Gemini Flash + Summarization Chain, it generates a concise summary of the search results. Ideal for users who don‚Äôt have time to read full pages of search results.
Formats Data Using AI Agent
The AI Agent acts like a virtual assistant that: - Understands search results
Formats them in a readable, JSON-compatible form
Prepares them for webhook delivery
Delivers Results to Webhook
Sends the final summary + structured search result to a webhook (could be your app, a Slack bot, Google Sheets, or CRM).
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Perplexity Search Request node with the prompt you wish to perform the search.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
1. Change the Perplexity Search Input
Default: It searches a fixed query or dataset.
Customize:
Accept input from a Google Sheet, Airtable, or a form.
Auto-trigger searches based on keywords or schedules.
2. Customize Summarization Style (LLM Output)
Default: General summary using Google Gemini or OpenAI.
Customize:
Add tone: formal, casual, technical, executive-summary, etc.
Focus on specific sections: pricing, competitors, FAQs, etc.
Translate the summaries into multiple languages.
Add bullet points, pros/cons, or insight tags.
3.Choose Where the Results Go
Options:
Email, Slack, Notion, Airtable, Google Docs, or a dashboard.
Auto-create content drafts for WordPress or newsletters.
Feed into CRM notes or attach to Salesforce leads."
"Compare Sequential, Agent-Based, and Parallel LLM Processing with Claude 3.7",https://n8n.io/workflows/3527-compare-sequential-agent-based-and-parallel-llm-processing-with-claude-37/,"This workflow demonstrates three distinct approaches to chaining LLM operations using Claude 3.7 Sonnet. Connect to any section to experience the differences in implementation, performance, and capabilities.
What you'll find:
1Ô∏è‚É£ Naive Sequential Chaining
The simplest but least efficient approach - connecting LLM nodes in a direct sequence. Easy to set up for beginners but becomes unwieldy and slow as your chain grows.
2Ô∏è‚É£ Agent-Based Processing with Memory
Process a list of instructions through a single AI Agent that maintains conversation history. This structured approach provides better context management while keeping your workflow organized.
3Ô∏è‚É£ Parallel Processing for Maximum Speed
Split your prompts and process them simultaneously for much faster results. Ideal when you need to run multiple independent tasks without shared context.
Setup Instructions:
API Credentials: Configure your Anthropic API key in the credentials manager. This workflow uses Claude 3.7 Sonnet, but you can modify the model in each Anthropic Chat Model node, or pick an entirely different LLM.
For Cloud Users: If using the parallel processing method (section 3), replace {{ $env.WEBHOOK_URL }} in the ""LLM steps - parallel"" HTTP Request node with your n8n instance URL.
Test Data: The workflow fetches content from the n8n blog by default. You can modify this part to use a different content or a data source.
Customization: Each section contains a set of example prompts. Modify the ""Initial prompts"" nodes to change the questions asked to the LLM.
Compare these methods to understand the trade-offs between simplicity, speed, and context management in your AI workflows!
Follow me on LinkedIn for more tips on AI automation and n8n workflows!"
Google Search Engine Results Page Extraction and Summarization with Bright Data,https://n8n.io/workflows/3533-google-search-engine-results-page-extraction-and-summarization-with-bright-data/,"Who this is for?
This workflow is designed for professionals and teams who need real-time, structured insights from Google Search results without manual effort.
What problem is this workflow solving?
This n8n workflow solves the problem of automating Google Search result extraction, cleanup, summarization, and AI-enhanced formatting for downstream use like sending the results to a webhook or another system.
What this workflow does
Automates Google Search via Bright Data
Uses Bright Data‚Äôs proxy-based SERP API to run a Google Search query programmatically.
Makes the process repeatable and scriptable with different search terms and regions/zones.
Cleans and Extracts Useful Content
The Google Search Data Extractor uses LLM based cleaning to remove HTML/CSS/JS from the response and extract pure text data.
Converts messy, unstructured web content into structured, machine-readable format.
Summarizes Search Results
Through the Gemini Flash + Summarization Chain, it generates a concise summary of the search results. Ideal for users who don‚Äôt have time to read full pages of search results.
Formats Data Using AI Agent
The AI Agent acts like a virtual assistant that:
Understands search results
Formats them in a readable, JSON-compatible form
Prepares them for webhook delivery
Delivers Results to Webhook
Sends the final summary + structured search result to a webhook (could be your app, a Slack bot, Google Sheets, or CRM).
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Google Search query as you wish by navigating to the Set Google Search Query node.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize This Workflow to your needs
1. Change the Search Input
Default: It searches a fixed query or dataset.
Customize:
Accept input from a Google Sheet, Airtable, or a form.
Auto-trigger searches based on keywords or schedules.
2. Customize Summarization Style (LLM Output)
Default: General summary using Google Gemini or OpenAI.
Customize:
Add tone: formal, casual, technical, executive-summary, etc.
Focus on specific sections: pricing, competitors, FAQs, etc.
Translate the summaries into multiple languages.
Add bullet points, pros/cons, or insight tags.
3.Choose Where the Results Go
Options:
Email, Slack, Notion, Airtable, Google Docs, or a dashboard.
Auto-create content drafts for WordPress or newsletters.
Feed into CRM notes or attach to Salesforce leads."
Smart Gmail Cleaner with AI Validator & Telegram Alerts,https://n8n.io/workflows/3502-smart-gmail-cleaner-with-ai-validator-and-telegram-alerts/,"Automatically clean up your Gmail inbox by deleting unwanted emails, validated by Gemini AI.
Ideal for anyone tired of manual inbox cleanup, this workflow helps you save time while staying in control, with full transparency via Telegram alerts.
How it works
Scans Gmail inbox in adjustable 2-week batches
Uses Gemini AI to decide if an email should be deleted or skipped
Applies a label to skipped emails to avoid rechecking in future runs
Deletes unwanted emails and sends a Telegram message with the AI's reasoning
Also notifies on skipped emails, with explanation included
Set up steps
Connect your Gmail, Gemini AI, and Telegram accounts
Adjust the AI baseline to control sensitivity (e.g. how strict the filtering should be)
Set your batch range (default: last 2 weeks, adjustable)
Define your Telegram chat/channel for notifications
Note: Thanks to n8n's modular design, you can easily switch Gemini for another AI model (like OpenAI, Claude, etc.) or replace Telegram with Discord, Slack, or even email, no code changes needed, just swap the nodes."
AI-powered Student Assistant for Course Information via Twilio SMS,https://n8n.io/workflows/3499-ai-powered-student-assistant-for-course-information-via-twilio-sms/,"This n8n template offers a simple yet capable chatbot assistant who can answer course enquiries over SMS.
Given the right access to data, AI Agents are capable of planning and performing relatively complex research tasks to get their answers. In this example, the agent must first understand the database schema, retrieve lists of values before generating it's own query to search over the database.
Checkout the example database here - https://airtable.com/appO5xvP1aUBYKyJ7/shr8jSFDaghubDOrw
How it works
A Twilio trigger gives us the ability to receive SMS input into our workflow via webhook.
The message is then directed to our AI agent who is instructed to assist the user and use the course database as reference. The database is an Airtable base.
The agent autonomously figures out which tool it needs to use and generates it's own ""filter_by_formula"" query to search over the available courses.
On successful search results, the Agent can then use this information to answer the user's query.
The Agent's output is logged in a second sheet of the Airtable base. We can use this later for analysis and lead gen.
Finally, the response is sent back to the user through SMS using Twilio.
How to use
Ensure your Twilio number is set to forward messages to this workflow's webhook URL.
Configure and update the course database as required. If you're not interested in courses, you can swap this out for inventory, deliveries or any other data relevant to your business.
Ask questions like:
""Can you help me find suitable courses to fill my Wednesday mornings?""
""Which courses are being instructed by profession Lee?""
""I'm interested in creative arts. What courses are available which could be relevant to me?""
Requirements
Twilio for SMS receiving and sending
OpenAI for LLM and Agent
Airtable for Course Database
Customising this workflow
Add additional tools and expand the range of queries the agent is able to answer or assist with.
Not using Airtable? This technique also works with SQL databases like PostgreSQL."
AI YouTube Playlist & Video Analyst Chatbot,https://n8n.io/workflows/3408-ai-youtube-playlist-and-video-analyst-chatbot/,"AI YouTube Playlist & Video Analyst Chatbot
This n8n workflow transforms entire YouTube playlists or single videos into interactive knowledge bases you can chat with. Ask questions and get summaries without needing to watch hours of content.
üåü How it Works
üîó Provide a Link: Start by giving the workflow a URL for a YouTube playlist or a single video.
üìÑ Content Retrieval: The workflow automatically fetches the video details and transcripts for the provided link. For playlists, it can process multiple videos at once (you might be asked how many).
üß† AI Processing: Google's Gemini AI reads through the transcripts, understands the content, and creates summaries.
üíæ Storage & Context: The processed information and summaries are stored in a vector database (Qdrant), making them ready for conversation. Context is managed using Redis, remembering the current video/playlist you're discussing.
üí¨ Chat & Ask: Now, you can ask the AI agent questions about the playlist or video content! Because context is maintained, you can ask follow-up questions (like ""expand on point X"") without needing to provide the URL again.
üõ†Ô∏è Requirements
Community Node: This workflow uses the youtubeTranscripter community node to fetch video transcripts. You'll need to install it in your n8n environment.
Installation: npm install n8n-nodes-youtube-transcription-dmr
Important: Community nodes require a self-hosted n8n instance.
Redis: A Redis instance is required for managing conversation context and status between interactions.
Credentials: You will need API credentials configured in your n8n instance for:
Google Gemini (AI Models)
Qdrant (Vector Store)
Redis (Context Store)
ü§ñ AI Agent Capabilities
Engage with the AI agent to explore the video content. Since the agent remembers the context of your conversation, you can ask detailed follow-up questions naturally:
Get a quick summary of a single video or an entire playlist.
Ask for key takeaways or main topics discussed.
Query for specific information mentioned in the videos.
Ask the agent to elaborate on a specific point previously mentioned.
Understand complex subjects without watching the full duration.
üöÄ Use Cases
üìä Content Analysis: Quickly understand the themes and key points across a playlist or long video.
üìö Research & Learning: Extract insights from educational series or tutorials efficiently.
‚úçÔ∏è Content Creation: Easily repurpose video transcript information into blog posts, notes, or social media content.
‚è±Ô∏è Save Time: Get the essence of video content when you're short on time.
‚ôø Accessibility: Offers a text-based way to interact with and understand video content.
‚ú® Sample Prompts
Please analyze this playlist and tell me the main topics covered: [YouTube Playlist URL]
Summarize the first 5 videos in this playlist: [YouTube Playlist URL]
(Follow-up) Tell me more about the main point in video 3.
What are the key points discussed in this video? [YouTube Video URL]
(Follow-up) Expand on the second key point you mentioned.
Does the video at [YouTube Video URL] mention [specific topic]?"
Automatic Shopify Order Fulfillment Process,https://n8n.io/workflows/3296-automatic-shopify-order-fulfillment-process/,"This workflow automates the Mark as Fulfilled action in Shopify for each order, ensuring a seamless fulfillment process without manual intervention.
How It Works
This workflow retrieves all unfulfilled orders and processes their fulfillment automatically. Since Shopify requires a Fulfillment Order ID (not Order ID) to trigger fulfillment, the workflow follows these steps:
1Ô∏è‚É£ Get all unfulfilled orders using the Shopify node.
2Ô∏è‚É£ Retrieve the Fulfillment Order ID using the ""List Fulfillment Orders"" action.
3Ô∏è‚É£ Create a fulfillment request using ""Mark fulfillment order as fulfilled.""
4Ô∏è‚É£ Handle edge cases, such as partially fulfilled orders or API errors.
This ensures that every valid order is marked as fulfilled efficiently.
üîó Ongoing discussions on this topic: Relevant Shopify API Discussion
Step-by-step
The workflow can be run as requested or on schedule
You can adjust these parameters within the Shopify and filter nodes:
Shopify Admin URL ‚Äì A Global node to customize the Shopify store URL.
To find your Shopify store ID, login into your Shopify admin, then look at the URL in your browser's address bar, the subdomain portion (e.g., example_store_id.myshopify.com) is your store ID (in this case: example_store_id)
Order Filtering ‚Äì Ensures only valid orders are fulfilled, particularly useful if:
You sell digital downloads or gift cards exclusively.
You use third-party fulfillment services for all products.
Credentials
To run this workflow, you'll need:
Shopify API Key ‚Äì Required for authentication.
Who Is This For?
Shopify store owners looking to automate their fulfillment process.
Merchants selling digital or personalized products who need instant fulfillment.
Explore More Templates
üëâ Check out my other n8n templates"
Transform Press Releases (PDF & Word) into Polished Articles with Gmail & OpenAI,https://n8n.io/workflows/3302-transform-press-releases-pdf-and-word-into-polished-articles-with-gmail-and-openai/,"This n8n workflow automates the transformation of press releases into polished articles. It converts the content of an email and its attachments (PDF or Word documents) into an AI-written article/blog post.
What does it do?
This workflow assists editors and journalists in managing incoming press-releases from governments, companies, NGOs, or individuals. The result is a draft article that can easily be reviewed by the editor, who receives it in a reply email containing both the original input and the output, plus an AI-generated self-assessment. This self-assessment represents an additional feedback loop where the AI compares the input with the output to evaluate the quality and accuracy of its transformation.
How does it work?
Triggered by incoming emails in Google, it first filters attachments, retaining only Word and PDF files while removing other formats like JPGs. The workflow then follows one of three paths:
- If no attachments remain, it processes the inline email message directly.
- For PDF attachments, it uses an extractor to obtain the document content.
- For Word attachments, it extracts the text content by a http request.
In each case, the extracted content is then passed to an AI agent that converts the press release into a well-structured article according to predefined prompts. A separate AI evaluation step provides a self-assessment by comparing the output with the original input to ensure quality and accuracy. Finally, the workflow generates a reply email to the sender containing three components: the original input, the AI-generated article, and the self-assessment. This streamlined process helps editors and journalists efficiently manage incoming press releases, delivering draft articles that require minimal additional editing.""
How to set it up
1. Configure Gmail Connection:
Create or use an existing Gmail address
Connect it through the n8n credentials manager
Configure polling frequency according to your needs
Set the trigger event to ""Message Received""
Optional: Filter incoming emails by specifying authorized senders
Enable the ""Download Attachments"" option
2. Set Up AI Integration:
Create an OpenAI account if you don't have one
Create a new AI assistant or use an existing one
Customize the assistant with specific instructions, style guidelines, or response templates
Configure your API credentials in n8n to enable the connection
3. Configure Google Drive Integration:
Connect your Google Drive credentials in n8n
Set the operation mode to ""Upload""
Configure the input data field name as ""data""
-Set the file naming format to dynamic: {{ $json.fileName }}
4. Configure HTTP Request Node:
Set request method to ""POST""
Enter the appropriate Google API endpoint URL
Include all required authorization headers
Structure the request body according to API specifications
Ensure proper error handling for API responses
5. Configure HTTP Request Node 2:
Set request method to ""GET""
Enter the appropriate Google API endpoint URL
Include all required authorization headers
Configure query parameters as needed
Implement response validation and error handling
6. Configure Self-Assessment Node:
Set operation to ""Message a Model""
Select an appropriate AI model (e.g., GPT-4, Claude)
Configure the following prompt in the Message field:
Please analyze and compare the following input and output content:
(for example)
Original Input:
{{ $('HTTP Request3').item.json.data }}
{{ $('Gmail Trigger').item.json.text }}
Generated Output:
{{ $json.output }}
Provide a detailed self-assessment that evaluates:
Content accuracy and completeness
Structure and readability improvements
Tone and style appropriateness
Any information that may have been omitted or misrepresented
Overall quality of the transformation
7. Configure Reply Email Node:
Set operation to ""Send"" and select your Gmail account
Configure the ""To"" field to respond to the original sender: {{ $('Gmail Trigger').item.json.from }}
Set an appropriate subject line: RE: {{ $('Gmail Trigger').item.json.subject }}
Structure the email body with clear sections using the following template:
handlebars
EDITED ARTICLE
{{ $('AI Article Writer 2').item.json.output }}
SELF-ASSESSMENT
Rating: 1 (poor) to 5 (excellent)
{{ $json.message.content }}
ORIGINAL MESSAGE
{{ $('Gmail Trigger').item.json.text }}
ATTACHMENT CONTENT
{{ $('HTTP Request3').item.json.data }}
Note: Adjust the template fields according to the input source (PDF, Word document, or inline message). For inline messages, you may not need the ""ATTACHMENT CONTENT"" section."
üâë Generate Anki Flash Cards for Language Learning with Google Translate and GPT,https://n8n.io/workflows/3195-generate-anki-flash-cards-for-language-learning-with-google-translate-and-gpt/,"Context
Hey! I'm Samir, a Supply Chain Data Scientist from Paris who spent six years in China studying and working while struggling to learn Mandarin.
I know the challenges of mastering a complex language like Chinese and my greatest support was flash cards. Therefore, I designed this workflow to support fellow Mandarin learners by automating flashcard creation using n8n, so they can focus more on learning and less on manual data entry.
üì¨ For business inquiries, you can add me on Here
Who is this template for?
This workflow template is designed for language learners and educators who want to automate the creation of flashcards for Mandarin (or any other language) using Google Translate API, an AI agent for phonetic transcription and generating an illustrative sentence and a free image retrieval API.
Why?
If you use the open-source application Anki, this workflow will help you automatically generate personalized study materials.
How?
Let us imagine you want to learn how to say the word Contract in Mandarin.
The workflow will automatically
Translate the word in Simplified Mandarin (Mandarin: ÂêàÂêå).
Provide the phonetic transcription (Pinyin: H√©t√≥ng)
Generate an example sentence (Example: Êàë‰ª¨Á≠æËÆ¢‰∫Ü‰∏Ä‰ªΩÂêàÂêå.)
Download an illustrative picture (For example, a picture of a contract signature)
All these fields are automatically recorded in a Google Sheet, making it easy to import into Anki and generate flashcards instantly
What do I need to start?
This workflow can be used with the free tier plans of the services used. It does not require any advanced programming skills.
Prerequisite
A Google Drive Account with a folder including a Google Sheet
API Credentials: Google Drive API, Google Sheets API and Google Translate API activated with OAuth2 credentials
A free API key of pexels.com
A google sheet with the columns
Next
Follow the sticky notes to set up the parameters inside each node and get ready to pump your learning skills.
I have detailed the steps in a short tutorial üëá

üé• Check My Tutorial
Notes
This workflow can be used for any language. In the AI Agent prompt, you just need to replace the word pinyin with phonetic transcription.
You can adapt the trigger to operate the workflow in the way you want. These operations can be performed by batch or triggered by Telegram, email, or webhook.
If you want to learn more about how I used Anki flash cards to learn mandarin: üà∑Ô∏è Blog Article about Anki Flash Cards
This workflow has been created with N8N 1.82.1
Submitted: March 17th, 2025"
Compare Local Ollama Vision Models for Image Analysis using Google Docs,https://n8n.io/workflows/3185-compare-local-ollama-vision-models-for-image-analysis-using-google-docs/,"Compare Local Ollama Vision Models for Image Analysis using Google Docs
Process images using locally hosted Ollama Vision Models to extract detailed descriptions, contextual insights, and structured data. Save results directly to Google Docs for efficient collaboration.
Who is this for?
This workflow is ideal for developers, data analysts, marketers and AI enthusiasts who need to process and analyze images using locally hosted Ollama Vision Language Models. It‚Äôs particularly useful for tasks requiring detailed image descriptions, contextual analysis, and structured data extraction.
What problem is this workflow solving? / Use Case
The workflow solves the challenge of extracting meaningful insights from images in exhaustive detail, such as identifying objects, analyzing spatial relationships, extracting textual elements, and providing contextual information. This is especially helpful for applications in real estate, marketing, engineering, and research.
What this workflow does
This workflow:
Downloads an image file from Google Drive.
Processes the image using multiple Ollama Vision Models (e.g., Granite3.2-Vision, Gemma3, Llama3.2-Vision).
Generates detailed markdown-based descriptions of the image.
Saves the output to a Google Docs file for easy sharing and further analysis.
Setup
Ensure you have access to a local instance of Ollama. https://ollama.com/
Pull the Ollama vision models.
Configure your Google Drive and Google Docs credentials in n8n.
Provide the image file ID from Google Drive in the designated node.
Update the list of Ollama vision models
Test the workflow by clicking ‚ÄòTest Workflow‚Äô to trigger the process.
How to customize this workflow to your needs
Replace the image source with another provider if needed (e.g., AWS S3 or Dropbox).
Modify the prompts in the ""General Image Prompt"" node to suit specific analysis requirements.
Add additional nodes for post-processing or integrating results into other platforms like Slack or HubSpot.
Key Features:
Detailed Image Analysis: Extracts comprehensive details about objects, spatial relationships, text elements, and contextual settings.
Multi-Model Support: Utilizes multiple vision models dynamically for optimal performance.
Markdown Output: Formats results in markdown for easy readability and documentation.
Google Drive Integration: Seamlessly downloads images and saves results to Google Docs."
Summarize YouTube Videos & Chat About Content with GPT-4o-mini via Telegram,https://n8n.io/workflows/3156-summarize-youtube-videos-and-chat-about-content-with-gpt-4o-mini-via-telegram/,"Summarize YouTube Videos & Chat About Content with GPT-4o-mini via Telegram
Description
This n8n workflow automates the process of summarizing YouTube video transcripts and enables users to interact with the content through AI-powered question answering via Telegram. It leverages the GPT-4o-mini model to generate summaries and provide insights based on the video‚Äôs transcript.
How It Works
Input: The workflow starts by receiving a YouTube video URL. This can be submitted through:
A Telegram chat message.
A webhook (e.g., triggered by a shortcut on Apple devices).
Transcript Extraction: The URL is processed to extract the video transcript using the custom youtubeTranscripter community node (available here). The transcript is concatenated into a single text and stored in a Google Docs document.
Summarization: The GPT-4o-mini AI model analyzes the transcript and generates a structured summary, including:
A general overview.
Key moments.
Instructions (if applicable).
The summary is then sent back to the user via Telegram.
Interactive Q&A: Users can ask questions about the video content via Telegram. The AI retrieves the stored transcript from Google Docs and provides accurate, context-based answers, which are sent back through Telegram.
Setup Instructions
To configure this workflow, follow these steps:
Import the Workflow: Download the provided JSON template and import it into your n8n instance.
Install the Community Node: Install the youtubeTranscripter community node via npm:
npm install n8n-nodes-youtube-transcription-kasha
Important: This node requires a self-hosted n8n instance due to its external dependencies.
Configure Nodes:
Webhook: Set up the webhook to receive YouTube URLs. Alternatively, configure the Telegram node if using Telegram as the input method.
Google Docs: Provide valid credentials to enable writing the transcript to a Google Docs document.
AI Model: Set up the GPT-4o-mini model for summarization and Q&A functionality.
Test the Workflow: Send a YouTube URL via your chosen input method (Telegram or webhook) and confirm that the summary is generated and delivered correctly.
Customization
Language: Adjust the AI prompts to generate summaries and answers in any desired language.
Output Format: Modify the summary structure by editing the prompt in the summarization node.
Input Methods: Replace the Telegram node with another messaging or input node to adapt the workflow to different platforms.
Who Can Benefit?
This template is perfect for:
Content Creators: Quickly summarize video content for repurposing or review.
Students and Researchers: Extract key insights from educational or informational videos efficiently.
General Users: Interact with video content via AI without needing to watch the full video.
Problem Solved
This workflow simplifies video content consumption by:
Automating the extraction and summarization of key points.
Enabling interactive Q&A to address specific questions without rewatching the video.
Additional Notes
Disclaimer: The youtubeTranscripter community node is required and only works on self-hosted n8n instances due to its reliance on external services.
Apple Users: Enhance your experience with a custom shortcut to share YouTube videos directly to the workflow. Download the shortcut here."
Automatic Weekly Digital PR Stories Suggestions with Reddit and Anthropic,https://n8n.io/workflows/3155-automatic-weekly-digital-pr-stories-suggestions-with-reddit-and-anthropic/,"Introduction
The ""Automatic Weekly Digital PR Stories Suggestions"" workflow is a sophisticated automated system designed to identify trending news stories on Reddit, analyze public sentiment through comment analysis, extract key information from source articles, and generate strategic angles for potential digital PR campaigns.
This workflow leverages the power of social media trends, natural language processing, and AI-driven analysis to deliver curated, sentiment-analyzed news opportunities for PR professionals.
Operating on a weekly schedule, the workflow searches Reddit for posts related to specified topics, filters them based on engagement metrics, and performs a deep analysis of both the content and public reaction.
It then generates comprehensive reports that include story opportunities, audience insights, and strategic recommendations. These reports are automatically compiled, stored in Google Drive, and shared with team members via Mattermost for immediate collaboration.
This workflow solves the time-consuming process of manually monitoring social media for trending stories, analyzing public sentiment, and identifying PR opportunities.
By automating these tasks, PR professionals can focus on strategy development and execution rather than spending hours on research and analysis.
Who is this for?
This workflow is designed for digital PR professionals, content marketers, communications teams, and media relations specialists who need to stay on top of trending stories and public sentiment to develop timely and effective PR campaigns. It's particularly valuable for:
PR agencies managing multiple clients across different industries
In-house PR teams needing to identify media opportunities quickly
Content marketers looking for trending topics to create timely content
Communications professionals monitoring public perception of industry news
Users should have basic familiarity with n8n workflows and the PR strategy development process. While technical knowledge of the integrated APIs is not required to use the workflow, some understanding of Reddit, sentiment analysis, and PR campaign development would be beneficial for interpreting and acting on the generated reports.
What problem is this workflow solving?
Digital PR professionals face several challenges that this workflow addresses:
Information Overload: Manually monitoring social media platforms for trending stories is time-consuming and often results in missed opportunities.
Sentiment Analysis Complexity: Understanding public perception of news stories requires reading through hundreds of comments and identifying patterns, which is labor-intensive and subjective.
Content Extraction: Visiting multiple news sources to read and analyze articles takes significant time.
Strategic Angle Development: Identifying unique PR angles that leverage trending stories and public sentiment requires synthesizing large amounts of information.
Team Collaboration: Sharing findings and insights with team members in a structured format can be cumbersome.
By automating these processes, the workflow enables PR professionals to quickly identify trending stories with PR potential, understand public sentiment, and develop strategic angles based on comprehensive analysis, all while maintaining a structured approach to team collaboration.
What this workflow does
Overview
The workflow automatically identifies trending posts on Reddit related to specified topics, analyzes both the content of linked articles and public sentiment from comments, and generates comprehensive PR strategy reports. These reports include story opportunities, audience insights, and strategic recommendations based on the analysis. The final reports are compiled, stored in Google Drive, and shared with team members via Mattermost.
Process
Topic Selection and Reddit Search:
The workflow starts with a list of topics specified in the ""Set Data"" node
It searches Reddit for posts related to these topics
Posts are filtered based on upvotes and other criteria to focus on trending content
Comment Analysis:
For each post, the workflow retrieves comments
It extracts the top 30 comments based on score
Using Claude AI, it analyzes the comments to understand:
Overall sentiment
Dominant narratives
Audience insights
PR implications
Content Analysis:
The workflow extracts the content of the linked article using Jina AI
It analyzes the content to identify:
Core story elements
Technical aspects
Narrative opportunities
Viral elements
PR Strategy Development:
Based on the combined analysis of comments and content, the workflow generates:
First-mover story opportunities
Trend-amplifier story ideas
Priority rankings
Execution roadmap
Strategic recommendations
Report Generation and Distribution:
The workflow compiles comprehensive reports for each post
Reports are converted to text files
All files are compressed into a ZIP archive
The archive is uploaded to Google Drive
A link to the archive is shared with team members via Mattermost
Setup
To set up this workflow, follow these steps:
Import the Workflow:
Download the workflow JSON file
Import it into your n8n instance
Configure API Credentials:
Reddit: Add a new credential ""Reddit OAuth2 API"" by following the guide at https://docs.n8n.io/integrations/builtin/credentials/reddit/
Anthropic: Add a new credential ""Anthropic Account"" by following the guide at https://docs.n8n.io/integrations/builtin/credentials/anthropic/
Google Drive: Add a new credential ""Google Drive OAuth2 API"" by following the guide at https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/
Configure the ""Set Data"" Node:
Set your interested topics (one per line)
Add your Jina API key (obtain from https://jina.ai/api-dashboard/key-manager)
Configure the Mattermost Node:
Update your Mattermost instance URL
Set your Webhook ID and Channel
Follow the guide at https://developers.mattermost.com/integrate/webhooks/incoming/ for webhook setup
Adjust the Schedule (Optional):
The workflow is set to run every Monday at 6am
Modify the ""Schedule Trigger"" node if you need a different schedule
Test the Workflow:
Run the workflow manually to ensure all connections are working properly
Check the output to verify the reports are being generated correctly
How to customize this workflow to your needs
This workflow can be customized in several ways to better suit your specific requirements:
Topic Selection:
Modify the topics in the ""Set Data"" node to focus on industries or subjects relevant to your PR strategy
Add multiple topics to cover different client interests or market segments
Filtering Criteria:
Adjust the ""Upvotes Requirement Filtering"" node to change the minimum upvotes threshold
Modify the filtering conditions to include or exclude certain types of posts
Analysis Parameters:
Customize the prompts in the ""Comments Analysis,"" ""News Analysis,"" and ""Stories Report"" nodes to focus on specific aspects of the content or comments
Adjust the temperature settings in the Anthropic Chat Model nodes to control the creativity of the AI responses
Report Format:
Modify the ""Set Final Report"" node to change the structure or content of the final reports
Add or remove sections based on your specific reporting needs
Distribution Method:
Replace or supplement the Mattermost notification with email notifications, Slack messages, or other communication channels
Add additional storage options beyond Google Drive
Schedule Frequency:
Change the ""Schedule Trigger"" node to run the workflow more or less frequently
Set up multiple triggers for different topics or clients
Integration with Other Systems:
Add nodes to integrate with your CRM, content management system, or project management tools
Create connections to automatically populate content calendars or task management systems"
Image-Based Data Extraction API using Gemini AI,https://n8n.io/workflows/3149-image-based-data-extraction-api-using-gemini-ai/,"This n8n workflow provides a ready-to-use API endpoint for extracting structured data from images. It processes an image URL using an AI-powered OCR model and returns the extracted details in a structured JSON format.
Use Cases
Document OCR ‚Äì Extract details from ID cards, invoices, receipts, etc.
Text Extraction from Images ‚Äì Process screenshots, scanned documents, and photos.
Automated Form Processing ‚Äì Digitize and capture information from paper forms.
Business Card Data Extraction ‚Äì Extract names, emails, and phone numbers from business cards.
How It Works
Send a GET request with an image URL and define the required extraction parameters.
The image is converted to base64 for processing.
The AI model (Gemini API - Flash Lite) extracts relevant text.
The response returns structured JSON data containing only the requested fields.
Features
‚úîÔ∏è No-Code API Setup ‚Äì Easily integrate into any application.
‚úîÔ∏è Customizable Extraction ‚Äì Modify the request parameters to fit your needs.
‚úîÔ∏è AI-Powered OCR ‚Äì Uses advanced models for accurate text recognition.
‚úîÔ∏è Automated Processing ‚Äì Ideal for document processing and digitization.
Integration
Works with any frontend/backend system that supports API calls.
Can be used for workflow automation in CRM, ERP, and document management solutions.
Supports further customization based on specific OCR requirements."
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync,https://n8n.io/workflows/3035-callforge-05-gongio-call-analysis-with-azure-ai-and-crm-sync/,"CallForge - AI Sales Call Processing & Insights Extraction
Automate sales call analysis with AI-powered insights for sales, marketing, and product teams.
Who is This For?
This workflow is designed for:
‚úÖ Sales teams looking to extract structured insights from Gong call transcripts.
‚úÖ Marketing professionals seeking AI-driven customer pain points & content strategy.
‚úÖ Product teams needing feedback from sales calls to prioritize feature development.
üîç What Problem Does This Workflow Solve?
Manually analyzing Gong.io sales call transcripts is slow, inconsistent, and lacks structured insights.
With CallForge, you can:
‚úî Extract AI-powered insights about use cases, objections, competitors, and next steps.
‚úî Provide structured marketing & product intelligence to enhance strategy.
‚úî Automatically store call insights in Notion and Salesforce for easy access.
‚úî Ensure resilience with automated reruns on failed workflows (handling Notion API limits).
‚úî Improve decision-making with AI-powered competitor and sentiment analysis.
üìå Key Workflow Features
üé§ AI-Powered Transcript Analysis
Uses AI to identify use cases, objections, competitors, and customer pain points.
Categorizes insights for sales, marketing, and product teams.
üìå AI Agent Breakdown
üîπ Sales AI Agent ‚Äì Extracts customer objections, pain points, competitors, and next steps.
üîπ Marketing AI Agent ‚Äì Identifies recurring topics, keyword trends, and content opportunities.
üîπ Product AI Agent ‚Äì Captures feature requests and AI/ML-related references.
üìä Structured Output Processing
Sales Data Processor ‚Üí Stores insights in Notion & Salesforce for sales tracking.
Marketing Data Processor ‚Üí Extracts SEO & content strategy insights for marketing teams.
Product AI Data Processor ‚Üí Logs customer feedback to prioritize feature development.
üí° Competitor & Integration Analysis
Tracks competing products mentioned in calls.
Identifies integration needs, flagging workarounds used by prospects.
üì¢ Real-Time Slack Notifications
Alerts teams on workflow progress and completed call analyses.
üîÑ Failure Resilience & Automated Re-Runs
If a Notion API limit is reached, the process resumes automatically.
üöÄ How This Works
üõ† 1. Trigger & Call Data Processing
The workflow retrieves Gong call transcripts and metadata.
Normalizes data, correcting common mispronunciations like ""n8n.""
ü§ñ 2. AI Agents Analyze the Call
Sales Agent ‚Äì Extracts actionable insights for sales follow-ups.
Marketing Agent ‚Äì Identifies recurring themes and keyword trends.
Product Agent ‚Äì Captures feature requests and AI/ML usage mentions.
üì° 3. Data is Stored in Notion & Salesforce
Logs AI-extracted insights in Notion for structured tracking.
Pushes sales-related data to Salesforce for team accessibility.
üîî 4. Slack Alerts for Teams
Notifies sales, marketing, and product teams about extracted insights.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
üìä Sample Output Data
1Ô∏è‚É£ Sales Insights
{
  ""UseCases"": [
    {
      ""Summary"": ""A manufacturing company wants to automate inventory tracking and reduce manual entry delays."",
      ""DepartmentTags"": [""Operations""],
      ""IndustryTags"": [""Manufacturing""],
      ""ImplementationStatus"": ""Evaluating""
    }
  ],
  ""Objection"": {
    ""ObjectionTags"": [""Feature Limitation""],
    ""Nature"": ""The prospect wanted a deeper integration with their ERP system, which n8n currently lacks.""
  },
  ""CallSummary"": ""The call focused on automation for supply chain processes. The prospect expressed interest but wanted confirmation on ERP integration capabilities."",
  ""NextSteps"": [""Schedule a follow-up demo for ERP integration.""]
}
2Ô∏è‚É£ Marketing Insights
{
  ""MarketingInsights"": [
    {
      ""Tag"": ""Workflow Template Request"",
      ""Summary"": ""The prospect requested a template for automating CRM lead tracking.""
    }
  ],
  ""RecurringTopics"": [
    {
      ""Topic"": ""CRM Integration"",
      ""Mentions"": 3,
      ""Context"": ""Discussed how n8n could sync CRM data automatically.""
    }
  ],
  ""ActionableInsights"": [
    {
      ""RecommendationType"": ""Tutorial"",
      ""Title"": ""How to Automate CRM Lead Tracking with n8n"",
      ""Topic"": ""CRM Integration"",
      ""Rationale"": ""The prospect expressed a need for CRM automation templates.""
    }
  ]
}
3Ô∏è‚É£ Product Feedback
{
  ""ProductFeedback"": [
    {
      ""Sentiment"": ""Positive"",
      ""Feedback"": ""The external speaker praised the simplicity of n8n's UI, making it easier for non-developers to automate tasks.""
    },
    {
      ""Sentiment"": ""Negative"",
      ""Feedback"": ""The external speaker mentioned frustration over the lack of a dedicated ERP integration node.""
    }
  ],
  ""AI_ML_References"": {
    ""Exist"": true,
    ""Context"": ""The external speaker mentioned using AI for automating customer ticket categorization."",
    ""Details"": {
      ""DevelopmentStatus"": ""Building"",
      ""Department"": ""Support"",
      ""RequiresAgents"": true,
      ""RequiresRAG"": false,
      ""RequiresChat"": ""Yes: External App (e.g., Slack)""
    }
  }
}
üîß How to Customize This Workflow
üí° üîó Change Data Storage ‚Äì Swap Notion for Airtable, HubSpot, or another CRM.
üí° üì© Customize Slack Notifications ‚Äì Send alerts via email, webhook, or another channel.
üí° üõ† Modify AI Processing ‚Äì Adjust AI models or processing prompts.
üí° üìä Add More Integrations ‚Äì Sync insights with Pipedrive, HubSpot, or another CRM.
üöÄ Why Use This Workflow?
‚úî Automates Gong call transcript analysis, eliminating manual work.
‚úî Improves collaboration by structuring insights for sales, marketing, and product teams.
‚úî Boosts sales conversions by identifying objections and next steps.
‚úî Enhances marketing and SEO strategy with AI-driven insights.
‚úî Optimizes product roadmap decisions based on customer feedback.
This workflow scales AI-powered sales intelligence for better decision-making, content strategy, and sales enablement. üöÄ"
Automatically Generate Burn-in Video Captions with json2video,https://n8n.io/workflows/3044-automatically-generate-burn-in-video-captions-with-json2video/,"Automatically Add Captions to Your Video
Who Is This For?
This workflow is ideal for content creators, marketers, educators, and businesses that regularly produce video content and want to enhance accessibility and viewer engagement by effortlessly adding subtitles.
What Problem Does This Workflow Solve?
Manually adding subtitles or captions to videos can be tedious and time-consuming. Accurate captions significantly boost viewer retention, accessibility, and SEO rankings.
What Does This Workflow Do?
This automated workflow quickly adds accurate subtitles to your video content by leveraging the Json2Video API.
It accepts a publicly accessible video URL as input.
It makes an HTTP request to Json2Video, where AI analyzes the video, generates captions, and applies them seamlessly.
The workflow returns a URL to the final subtitled video.
The second part of the workflow periodically checks the Json2Video API to monitor the processing status at intervals of 10 seconds.
üëâüèª Try Json2Video for Free üëàüèª
Key Features
Automatic & Synced Captions: Captions are generated automatically and synchronized perfectly with your video.
Fully Customizable Design: Easily adjust fonts, colors, sizes, and more to match your unique style.
Word-by-Word Display: Supports precise, word-by-word captioning for improved clarity and viewer engagement.
Super Fast Processing: Rapid caption generation saves time, allowing you to focus more on creating great content.
Preconditions
To use this workflow, you must have:
A Json2Video API account.
A video hosted at a publicly accessible URL.
Why You Need This Workflow
Adding subtitles to your videos significantly enhances their reach and effectiveness by:
Improving SEO visibility, enabling search engines to effectively index your video content.
Enhancing viewer engagement and accessibility, accommodating viewers who watch without sound or who have hearing impairments.
Streamlining your content production process, allowing more focus on creativity.
Specific Use Cases
Social Media Content: Boost viewer retention by adding subtitles.
Educational Videos: Enhance understanding and improve learning outcomes.
Marketing Videos: Reach broader and more diverse audiences."
Travel AI Agent - AI-Powered Travel Planner,https://n8n.io/workflows/3087-travel-ai-agent-ai-powered-travel-planner/,"Overview
An n8n workflow automating business travel planning via Telegram. Uses AI and APIs to find and book flights/hotels efficiently.
Prerequisites
Telegram Bot (BotFather)
API Keys: OpenAI (transcription), SerpAPI (flights/hotels), DeepSeek (AI processing)
n8n Instance with API access
Setup Instructions
Import Workflow: Upload JSON to n8n.
Configure API Credentials: Set up Telegram, OpenAI, SerpAPI, and DeepSeek keys.
Webhook Activation: Ensure Telegram webhook is active with HTTPS.
Test: Send a Telegram message and verify execution.
Workflow Operation
1. User Input Processing
Telegram bot triggers workflow, extracts text/audio.
OpenAI transcribes voice messages.
AI (DeepSeek) extracts key travel details (locations, dates, accommodation needs).
2. Travel Search
Flights: Uses SerpAPI for flight options (airlines, times, prices).
Hotels: Fetches accommodations with dynamic check-out date.
3. AI Recommendations & Customization
DeepSeek generates structured travel plans.
Users can modify prompts to adjust AI responses for personalized results.
Professional, well-structured responses with links.
4. Response Delivery
Sends travel recommendations via Telegram with clear details.
Use Cases
Ideal for business professionals, executive assistants, frequent travelers, and small businesses.
Customization & Troubleshooting
Adjust memory handling and API calls.
Modify prompts to refine AI output.
Ensure API keys are active and network is accessible."
Avoid Asking Redundant Questions with Dynamically Generated Forms using OpenAI,https://n8n.io/workflows/3062-avoid-asking-redundant-questions-with-dynamically-generated-forms-using-openai/,"Avoid Asking Redundant Questions with Dynamically Generated Forms using OpenAI
Target Audience
This workflow has been built for those who require a form to capture as much data as possible as well as the answers to predefined questions, whilst optimising the user experience by avoiding asking redundant questions.
Use Case
When creating a form to capture information, it can be useful to give the user an opportunity to input a long answer to a large, open-ended question. We then want to drill down to answer specific questions that we require the answer to. When doing this, we don't want to ask duplicate questions. This particular scenario imagines an AI consultancy capturing leads.
What it Does
This workflow requires users to input basic information and then answer an open ended question. The specific questions on the next page will only be those that weren't answered in the open-ended question.
How it Works
The open-ended question (and relevant basic information) is analysed by an LLM to determine which specific questions have not been answered. Chain-of-thought reasoning is utilised and the output structure is specified with the Structured Output Parser.
Those questions that have already been answered are filtered out nodes. The remaining items are then used to generate the last page of the form.
Once the user has filled in the final page of the form, they are shown a form completion page.
Setup
Add your OpenAI credentials
Go to the Get Basic Information node and click Test Step
Complete the form to test the generic use case
Modify the prompt in Analyse Response to fit your use case
Next Steps
Add additional nodes to send an email to the form owner
Add a subsequent LLM call to analyse the form response - those that are qualified should be given the opportunity to book an appointment"
WordPress Auto-Blogging Pro - with DEEP RESEARCH - Content Automation Machine,https://n8n.io/workflows/3041-wordpress-auto-blogging-pro-with-deep-research-content-automation-machine/,"The best content automation template in the market is now even better‚Äîwith ‚Äúdeep research‚Äù on time-sensitive topics! Unlike most n8n content automation templates that are mainly for ‚Äúdemo purposes,‚Äù this advanced n8n workflow is for the real PROs. It conducts in-depth research on the latest information about a topic, then automatically creates and publishes SEO-optimized blog posts to your WordPress website. In addition, it generates a unique image for each chapter and a featured image for the overall article, and it collects internal website links‚Äîseamlessly inserting links throughout each chapter and the entire article. Furthermore, it backs up all content and images to a designated folder in your Google Drive.
This integrated approach not only creates high-quality, comprehensive content for readers but also enhances on-page SEO, improves navigation, and streamlines your content creation process‚Äîsaving you time while ensuring your work is securely stored.
This is a complimentary variation of the n8n WordPress Auto-Blogging Pro for SEO topics. That one specializes in auto-blogging content about established topics that do not need up-to-date information while this one takes it further by incorporating online research into the workflow. Both make up the perfect combo to run your blog on autopilot!


How It Works
Trigger: It activates upon adding a new row to a Google Sheet. Users can control many customizable parameters, such as key topic, number of subtopics, target audience, length, and style.
Initial Research: The workflow initiates preliminary research on the topic to gather insights for the content planning phase.
Structure Planning: Based on the initial research, it plans a detailed structure for the entire article and breaks the article into subtopics strongly tied to the main topic.
In-Depth Research & Writing: For each subtopic, it conducts further research to gather the most up-to-date, relevant information, then uses that research to write each chapter in depth.
Image Generation: It generates and adds images for each chapter, as well as a featured image for the article.
Internal Linking: The system gathers internal website links and strategically embeds them within each chapter and throughout the article, boosting SEO and enhancing user navigation.
Final Assembly & Publishing: All texts, images, and links are combined into one comprehensive article, which is then published directly to your WordPress website. You can also choose to post it as a draft for final review.
Auto Backup to Drive: Get peace of mind knowing all content (blog post and images) is automatically saved to Google Drive, organized in a folder named after the blog post title.
Unique Features
Deep Research: The workflow leverages advanced, real-time research to gather the latest information about a topic. It breaks down the key topic into subtopics and conducts in-depth research on each one, ensuring the most comprehensive and current knowledge.
Full Automation: Designed to be 100% automated, the workflow runs without manual intervention once imported and configured.
Simple Activation: It is easily triggered through the Google Sheets interface‚Äîsimply add a new row to a Google Sheet.
Customization Options: Offers a wide array of options, including topic, category, target audience, word count, number of chapters, length of introduction and conclusion, and writing style. It also allows for the inclusion of calls-to-action (CTAs) and company/product introductions.
Advanced Loop in Loop: Clever looping is used to write each chapter and generate images, ensuring optimal results. ‚ÄúWait‚Äù nodes are added where appropriate to avoid API call rate limits.
Automatic Content Saving: After the blog post is created, all content and images are automatically saved to Google Drive, preventing data loss. The folder is named after the blog post title.
SEO-Optimized Content: It is designed to create SEO-optimized content using seed keywords.
Internal Link Limit: The workflow limits the number of internal links to 20 by default.
Error Catch: ‚ÄúIf‚Äù nodes are strategically used to ensure that the output from AI nodes strictly meets the JSON schema, ensuring a smooth flow of data without interruption.
And many more big and small improvements.
Requirements
Please make sure you have these requirements ready to ensure smooth deployment of this n8n workflow template:
OpenAI API or equivalent for text and image generation
PerplexityAI API or equivalent for online research
A WordPress website (other website platforms will not work!)
Google Sheets for triggering the workflow, or a trigger of your choice
Google Drive and Google Docs for auto backup
Set Up Steps
Install the Workflow Template: Import the JSON files into your n8n instance. There are 2 files: one for the main workflow and one for the research tool (PerplexityAI).
Connect the Workflow with Your Accounts: Link your accounts for OpenAI API, PerplexityAI API, Google Drive, Google Sheets, Google Docs, and WordPress website.
Configure the Google Sheet: Ensure your Google Sheet is set up to trigger the workflow upon adding a new row and that the input data is correctly formatted.
Customize the Inputs: Adjust parameters like topic, target audience, and writing style to match your specific content needs. Optimize prompts for the best results.
Test the Workflow: Use low-cost AI models and image settings initially to ensure everything runs smoothly.
Tailor Further as Needed: Modify workflow elements to perfectly align with your needs and content strategy.
Tips for PROs
Image Generation: The default AI model for image generation is OpenAI‚Äôs Dall-E. However, the outputs of this model are not impressive. Consider using FLUX.1 for better image quality.
Research: PerplexityAI is the tool of choice for research. The default model used in this workflow is ‚Äúsonar‚Äù due to its fast speed and low cost. Feel free to experiment with its other models, including the ‚Äúsonar-deep-research‚Äù model, which is dedicated to deep research.
Triggering: Triggering with a new row on Google Sheets is limited and unreliable. Consider setting an auto trigger with a daily schedule at a specific time. You can even put the whole workflow in a loop to process multiple rows one-by-one from Google Sheets.
Human in the Loop: It is possible to incorporate a human review process. For example, after the article is posted to WordPress as a draft, a human can review the draft. Only after approval will the post be published, and a final copy of the post will be saved to the shared Drive folder."
üê∂ AI Agent for PetShop Appointments (Agente de IA para agendamentos de PetShop),https://n8n.io/workflows/2999-ai-agent-for-petshop-appointments-agente-de-ia-para-agendamentos-de-petshop/,"üê∂ü§ñ AI Agent for Pet Shops ‚Äì Automate Customer Service & Bookings! üêæüí°
Transform Your Pet Shop with AI-Powered Automation! üöÄ
Enhance customer experience and optimize operations with this n8n AI Agent designed for pet shops. üì≤üêæ Automate client interactions, appointment scheduling, and service recommendations‚Äîsaving time and increasing revenue!
üîπ Key Features:
‚úÖ Instant WhatsApp responses ‚Äì AI-powered chatbot handles customer inquiries. üí¨
‚úÖ Automated appointment scheduling ‚Äì Clients can book services hassle-free. üìÖ‚úÇÔ∏è
‚úÖ Personalized reminders ‚Äì Reduce no-shows with automated notifications. üì¢üêæ
‚úÖ Customer data & service history management ‚Äì Track interactions effortlessly. üìäüìÅ
‚úÖ Product & service recommendations ‚Äì Improve sales with smart suggestions. üéÅüê∂
üìå How It Works
1Ô∏è‚É£ The workflow captures customer inquiries via WhatsApp.
2Ô∏è‚É£ AI processes requests, provides information, and offers booking options.
3Ô∏è‚É£ Clients can schedule grooming, vet visits, or other services in seconds.
4Ô∏è‚É£ Automated reminders ensure appointments are remembered.
5Ô∏è‚É£ Customer data is stored for better service personalization.
‚öôÔ∏è Setup & Customization
üîß Connect your WhatsApp API (evolution) for instant messaging.
üîß Integrate with Google Calendar for appointment booking.
üîß Customize reminders, services, and pricing rules to fit your business.
üí° Reduce manual work, improve customer satisfaction, and scale your pet shop with AI automation!
üê∂ü§ñ [PT-BR] Agente de IA para Pet Shops ‚Äì Atendimento e Agendamentos Automatizados! üêæüí°
Transforme Seu Pet Shop com Automa√ß√£o Inteligente! üöÄ
Otimize o atendimento ao cliente e agilize processos com este Agente de IA para n8n. üì≤üêæ Automatize intera√ß√µes, agendamentos e recomenda√ß√µes de servi√ßos‚Äîeconomizando tempo e aumentando as vendas!
üîπ Principais Funcionalidades:
‚úÖ Atendimento autom√°tico no WhatsApp ‚Äì IA responde clientes instantaneamente. üí¨
‚úÖ Agendamento de servi√ßos automatizado ‚Äì Clientes marcam banho, tosa ou consultas facilmente. üìÖ‚úÇÔ∏è
‚úÖ Lembretes personalizados ‚Äì Reduza faltas com notifica√ß√µes autom√°ticas. üì¢üêæ
‚úÖ Gest√£o de clientes e hist√≥rico de servi√ßos ‚Äì Controle dados de forma eficiente. üìäüìÅ
‚úÖ Sugest√£o de produtos e servi√ßos ‚Äì Venda mais com recomenda√ß√µes inteligentes. üéÅüê∂
üìå Como Funciona
1Ô∏è‚É£ O fluxo recebe perguntas dos clientes via WhatsApp.
2Ô∏è‚É£ A IA processa os pedidos e fornece op√ß√µes de agendamento.
3Ô∏è‚É£ O cliente escolhe o servi√ßo desejado e agenda em segundos.
4Ô∏è‚É£ Lembretes autom√°ticos garantem que os clientes n√£o esque√ßam os hor√°rios.
5Ô∏è‚É£ O hist√≥rico do cliente √© salvo para oferecer um atendimento mais personalizado.
‚öôÔ∏è Configura√ß√£o e Personaliza√ß√£o
üîß Conecte sua API do WhatsApp (evolution) para intera√ß√£o autom√°tica.
üîß Integre ao Google Calendar para gerenciar agendamentos.
üîß Personalize valores, servi√ßos e regras de envio de lembretes conforme sua necessidade.
üí° Automatize processos, melhore a experi√™ncia do cliente e escale seu pet shop com IA! üöÄ"
Turn BBC News Articles into Podcasts using Hugging Face and Google Gemini,https://n8n.io/workflows/2972-turn-bbc-news-articles-into-podcasts-using-hugging-face-and-google-gemini/,"Turn BBC News Articles into Podcasts using Hugging Face and Google Gemini
Effortlessly transform BBC news articles into engaging podcasts with this automated n8n workflow.
Who is this for?
This template is perfect for:
Content creators who want to quickly produce podcasts from current events.
Students looking for an efficient way to create audio content for projects or assignments.
Individuals interested in generating their own podcasts without technical expertise.
Setup Information
Install n8n: If you haven't already, download and install n8n from n8n.io.
Import the Workflow: Copy the JSON code for this workflow and import it into your n8n instance.
Configure Credentials:
Gemini API: Set up your Gemini API credentials in the workflow's LLM nodes.
Hugging Face Token: Obtain an access token from Hugging Face and add it to the HTTP Request node for the text-to-speech model.
Customize (Optional):
Filtering Criteria: Adjust the News Classifier node to fine-tune the selection of news articles based on your preferences.
Output Options: Modify the workflow to save the generated audio file to a cloud storage service or publish it to a podcast hosting platform.
Prerequisites
An active n8n instance.
Basic understanding of n8n workflows (no coding required).
API credentials for Gemini and a Hugging Face account with an access token.
What problem does it solve?
This workflow eliminates the manual effort involved in creating podcasts from news articles. It automates the entire process, from fetching and filtering news to generating the final audio file.
What are the benefits?
Time-saving: Create podcasts in minutes, not hours.
Easy to use: No coding or technical skills required.
Customizable: Adapt the workflow to your specific needs and preferences.
Cost-effective: Leverage free or low-cost services like Gemini and Hugging Face.
How does it work?
The workflow fetches news articles from the BBC website.
It filters articles based on their suitability for a podcast.
It extracts the full content of the selected articles.
It uses Gemini LLM to create a podcast script.
It converts the script to speech using Hugging Face's text-to-speech model.
The final podcast audio is ready for use.
Nodes in the Workflow
Fetch BBC News Page: Retrieves the main BBC News page.
News Classifier: Categorizes news articles using Gemini LLM.
Fetch BBC News Detail: Extracts detailed content from suitable articles.
Basic Podcast LLM Chain: Generates a podcast script using Gemini LLM.
HTTP Request: Converts the script to speech using Hugging Face.
Add Story
I'm excited to share this workflow with the n8n community and help content creators and students easily produce engaging podcasts!
Additional Tips
Explore the n8n documentation and community resources for more advanced customization options.
Experiment with different filtering criteria and LLM prompts to achieve your desired podcast style."
Personal Portfolio CV Rag Chatbot - with Conversation Store and Email Summary,https://n8n.io/workflows/2987-personal-portfolio-cv-rag-chatbot-with-conversation-store-and-email-summary/,"Personal Portfolio CV Rag Chatbot - with Conversation Store and Email Summary
Target Audience
This template is perfect for:
Individuals looking to create a working professional and interactive personal portfolio chatbot.
Developers interested in integrating RAG Chatbot functionality with conversation storage.
1. Description
Create a stunning Personal Portfolio CV with integrated RAG Chatbot capabilities, including conversation storage and daily email summaries.
2.Features:
Training: Setup Ingestion stage
Upload your CV to Google Drive and let the Drive trigger updates to read your resume cv and convert it into your vector database (RAG purpose). Modify any parts as needed.
Chat & Track:
Use any frontend/backend interface to call the chat API and chat history API.
Reporting Daily Chat Conversations:
Receive daily automatic summaries of chat conversations. Data stored via NocoDB.
3.Setup Guide:
Step-by-Step Instructions:
Ensure all credentials are ready. Follow the notes provided.
Ingestion:
Upload your CV to Google Drive.
The Drive triggers RAG update in your vector database. You can change the folder name, files and indexname of the vector database accordingly.
Chat:
Use any frontend/backend interface to call the chat API (refer to the notes for details) .
[optional] Use any frontend/backend interface to call the update chat history API (refer to the notes for details).
3.Tracking Chat:
Get daily automatic summaries of chat conversations.Format email conversations report as you like.
You are ready to go!"
Free YouTube Video Analyzer with AI-Powered Summaries & Email Alerts,https://n8n.io/workflows/2964-free-youtube-video-analyzer-with-ai-powered-summaries-and-email-alerts/,"This workflow is designed to analyze YouTube videos by extracting their transcripts, summarizing the content using AI models, and sending the analysis via email.
This workflow is ideal for content creators, marketers, or anyone who needs to quickly analyze and summarize YouTube videos for research, content planning, or educational purposes.
How It Works:
Trigger: The workflow starts with a manual trigger, allowing you to test it by clicking ""Test workflow."" You can also set a YouTube video URL manually or dynamically.
YouTube Video ID Extraction:
The workflow extracts the YouTube video ID from the provided URL using a custom JavaScript function. This ID is necessary for fetching the transcript.
Transcript Generation:
The video ID is sent via an HTTP request to generate the transcript. You need to replace APIKEY with a free API key from the service.
Transcript Validation:
The workflow checks if a transcript exists for the video. If a transcript is available, it proceeds; otherwise, it stops.
Full Text Extraction:
If a transcript exists, the workflow combines all transcript segments into a single text variable for further analysis.
AI-Powered Analysis:
The full transcript is passed to an AI model (DeepSeek, OpenAI, or OpenRouter) for analysis. The AI generates a structured summary, including a title and key points, formatted in markdown.
Email Notification:
The analysis results (title and summary) are sent via email using SMTP credentials. The email contains the structured summary of the video.
Set Up Steps:
YouTube Transcript API:
Obtain a free API key from youtube-transcript.io and replace APIKEY in the ""Generate transcript"" node with your key.
AI Model Configuration:
Configure the AI model nodes (DeepSeek, OpenAI, or OpenRouter) with the appropriate API credentials. You can choose one or multiple models depending on your preference.
Email Setup:
Configure the ""Send Email"" node with your SMTP credentials (e.g., Gmail, Outlook, or any SMTP service). Ensure the email settings are correct to send the analysis results.
Key Features:
Free Tools: Uses youtube-transcript.io for free transcript generation.
AI Models: Supports multiple AI models (DeepSeek, OpenAI, OpenRouter) for flexible analysis.
Email Notifications: Sends the analysis results directly to your inbox.
Customizable: Easily adapt the workflow to analyze different videos or use different AI models."
Automated AI image tagging and writing the keywords into the image file,https://n8n.io/workflows/2995-automated-ai-image-tagging-and-writing-the-keywords-into-the-image-file/,"Welcome to my Automated Image Metadata Tagging Workflow!
This workflow automatically analyzes the image content with the help of AI and writes it directly back into the image file as keywords.
This workflow has the following sequence:
Google Drive trigger (scan for new files added in a specific folder)
Download the added image file
Analyse the content of the image and extract the file as Base64 code
Merge Metadata and Base64 Code
Code Node to write the Keywords into the Metadata (dc:subject)
Convert to file and update the original file in the Google Drive folder
The following accesses are required for the workflow:
Google Drive: Documentation
AI API access (e.g. via OpenAI, Anthropic, Google or Ollama)
You can contact me via LinkedIn, if you have any questions: https://www.linkedin.com/in/friedemann-schuetz"
Chat with your event schedule from Google Sheets in Telegram,https://n8n.io/workflows/2937-chat-with-your-event-schedule-from-google-sheets-in-telegram/,"What it is
Chat with your event schedule from Google Sheets in Telegram:
""When is the next meetup?""
""How many events are there next month?""
""Who presented most often?""
""Which future meetups have no presenters yet?""
This workflow lets you chat with a telegram bot about past, present and future events that are scheduled in a Google Spreadsheet.
(Info: This proof-of-concept was created as a demo for a hackathon of an AI & Developer Meetup in Da Nang (Vietnam) that uses a telegram group to organize)
Who it is for
If you want an easy way for your audience to get information about your events, you can us this workflow for the same purpose, or easily adapt it to your needs and different use-cases where you want to query smaller amounts of tabular data in natural language.
How it works
Upon getting triggered by a chat message to a telegram bot, the schedule of meetups is retrieved from Google Spreadsheets, converted into a markdown table syntax and fed into the system prompt of an LLM (we're using OpenRouter in this example), whose output is posted back as answer into the same telegram chat.
Setup steps
TO REVIEWING IN ACTION
As the reviewer of this workflow, you can temporarily use it via an existing telegram bot, simply point your telegram client to https://t.me/AiDaNangBot and start to ask questions like:
""When is the next meetup?""
""What future meetings do not have presenters?""
""Who presented on Future of Human Relationships?""
To build upon this workflow:
Import the workflow
Customize the Google Docs credentials for your individual access
Create a telegram bot and connect it to the workflow by entering its API token into the credentials used in the telegram trigger node
In the ""Settings"" node, replace the ""scheduleURL"" with the URL of your own copy of the Google Spreadsheet or a copy of the Event Schedule Template Sheet to spin off your own ‚Äì whereby the structure of the spreadsheet doesn't matter, it's just important that you semantically structure your information in dedicated columns clearly labeled in the header row."
Hacker News Job Listing Scraper and Parser,https://n8n.io/workflows/2924-hacker-news-job-listing-scraper-and-parser/,"This automated workflow scrapes and processes the monthly ""Who is Hiring"" thread from Hacker News, transforming raw job listings into structured data for analysis or integration with other systems. Perfect for job seekers, recruiters, or anyone looking to monitor tech job market trends.
How it works
Automatically fetches the latest ""Who is Hiring"" thread from Hacker News
Extracts and cleans relevant job posting data using the HN API
Splits and processes individual job listings into structured format
Parses key information like location, role, requirements, and company details
Outputs clean, structured data ready for analysis or export
Set up steps
Configure API access to Hacker News (no authentication required)
Follow the steps to get your cURL command from https://hn.algolia.com/
Set up desired output format (JSON structured data or custom format)
Optional: Configure additional parsing rules for specific job listing information
Optional: Set up integration with preferred storage or analysis tools
The workflow transforms unstructured job listings into clean, structured data following this pattern:
Input: Raw HN thread comments
Process: Extract, clean, and parse text
Output: Structured job listing data
This template saves hours of manual work collecting and organizing job listings, making it easier to track and analyze tech job opportunities from Hacker News's popular monthly hiring threads."
WordPress Auto-Blogging Pro - Content Automation Machine for SEO topics,https://n8n.io/workflows/2919-wordpress-auto-blogging-pro-content-automation-machine-for-seo-topics/,"The best content automation in the market! This advanced workflow not only creates and publishes SEO-optimized blog posts to your WordPress website but also backs up all content and images to a designated folder in your Google Drive. In addition, It generates a unique image for each chapter and a featured image for the overall article, and it automatically collects internal website links‚Äîseamlessly inserting them throughout each chapter and the entire article. This integrated approach enhances on-page SEO, improves navigation, and streamlines your content creation process, saving you time while ensuring your work is securely stored.

How it works
Triggers upon adding a new row to a Google Sheets.
Generates a full blog post by writing content based on customizable parameters such as topic, target audience, length, style, and seed keyword.
Generates and adds images for each chapter as well as a featured image for the article.
Gathers internal website links and strategically embeds them within each chapter and throughout the article, boosting SEO and enhancing user navigation.
Publishes the blog post directly to your WordPress website.
Saves all content (blog post and images) to Google Drive, organizing them in a folder named after the blog post title.
Unique features
Full Automation: The workflow is designed to be 100% automated. Once imported and configured, it can run without manual intervention.
Simple Activation: It can be easily triggered through the Google Spreadsheets interface. You simply add a new row to a Google Sheet.
Customization Options: Offers a wide array of customization options, including topic, category, target audience, word count, number of chapters, length of introduction and conclusion, and writing style. It also allows for the inclusion of calls-to-action (CTAs) and company/product introductions.
Automatic Content Saving: After writing a blog post, all content and images are automatically saved to Google Drive, preventing data loss. The folder is even named after the title of the blog post.
SEO-Optimized Content: It's designed to create content optimized for SEO using seed keywords.
AI Model Flexibility: It‚Äôs super easy to switch between different AI models through the Open Router node.
Rate Limit Handling: Includes ""Wait"" nodes to avoid rate limits.
Internal Link Limit: Limits the number of internal links to 20 by default.
Set up steps
Install the workflow template: Import the JSON file into your n8n instance.
Connect the workflow to your accounts: This includes linking your WordPress website, Google Drive, and AI models (such as OpenAI GPT-4o).
Configure the Google Sheet: Ensure your Google Sheet is set up to trigger the workflow upon adding a new row and that the input data is correctly formatted.
Customize the workflow: Adjust parameters like topic, target audience, and writing style to match your specific content needs. Optimize prompts for best results.
Test the workflow: Use low-cost AI models and image settings initially to ensure everything runs smoothly.
Tailor Further as Needed: Modify workflow elements to align perfectly with your needs and content strategy."
AI Agent: Find the Right LinkedIn Profiles in Seconds,https://n8n.io/workflows/2898-ai-agent-find-the-right-linkedin-profiles-in-seconds/,"Automate LinkedIn Prospecting with AI Agent and Get Results Straight to Google Sheets!
A practical AI-powered workflow that helps you find relevant LinkedIn profiles using natural language queries and saves the results automatically to Google Sheets.
üéØ What It Actually Does
Accepts natural language requests (e.g., ""Find marketing managers in Paris"")
Uses AI to extract search parameters:
Job titles
Industry
Location
Searches for matching LinkedIn profiles
Saves profile URLs and titles to Google Sheets
üõ†Ô∏è Technical Components
AI Query Processor
Converts natural language to structured search parameters
Handles various input formats and languages
Maintains context for accurate parameter extraction
Search Engine Integration
Uses Google Custom Search API
Handles pagination for multiple results
Filters for actual LinkedIn profile URLs
Data Storage
Automatically saves to Google Sheets
Stores profile titles and URLs
Easy to access and export
üìã Prerequisites
OpenAI API key
Google Custom Search API credentials
Google Sheets access
üí° Adaptation Possibilities
This template can be modified to scrape other websites by:
Changing the search domain in the HTTP Request node
Adjusting the URL filter in the Code node
Modifying the column structure in Google Sheets
Updating the AI prompt for different parameter extraction
Example adaptations:
Twitter profile finder
GitHub repository search
Company website finder
Professional blog discovery
üîß Setup Instructions
API Setup
Configure Google Custom Search API
Set up OpenAI API
Prepare Google Sheets
Workflow Configuration
Import both workflows
Connect your API credentials
Set up your Google Sheet
Test with a sample query
üìù Usage Example
Input: ""Find software developers in London working in fintech""
Output: A Google Sheet containing:
LinkedIn profile URLs of software developers
Profile titles/headlines
Ready for your review and outreach
#AIAgent #WebScraping #Automation #n8n #Workflow #LinkedInProspecting"
Microsoft Outlook AI Email Assistant with contact support from Monday and Airtable,https://n8n.io/workflows/2809-microsoft-outlook-ai-email-assistant-with-contact-support-from-monday-and-airtable/,"Microsoft Outlook AI Email Assistant
Prerequisites
1. Microsoft 365 Login Credentials
Provide your Office 365 credentials to connect Outlook.
2. Monday.com
Generate an API token and have a board with your contact details.
3. Airtable
Obtain an API key (or personal access token) and set up a base to store:
Contacts (populated by the Monday.com sync).
Rules & Categories (used by the AI Email Assistant).
Use this Airtable base as the template: Airtable AI Email Assistant Template. Define your own rules, categories, and delete rules.
4. OpenAI API Key
Sign up for OpenAI if you don‚Äôt already have an account.
Generate a new API key at OpenAI API Keys.
What the System Does
1. Daily Contact Sync (Monday.com ‚Üí Airtable)
Runs once a day to pull the latest contacts from Monday.com and store or update them in Airtable.
2. AI Email Categorisation & Prioritisation
Fetches Outlook emails with filters.
Cleans and processes email content.
Matches emails with known contacts from Airtable.
Uses an AI agent to classify, categorise, and prioritise emails.
Updates Outlook categories and importance based on AI results.
Runs in parallel with Airtable rules & categories retrieval for real-time decision-making.
Workflow 1: Daily Contact Sync (Monday.com ‚Üí Airtable)
Purpose
Keep Airtable‚Äôs Contacts table up to date by pulling new or updated contact data from Monday.com daily.
Steps
Schedule Trigger
Runs at a set interval (daily) to initiate contact syncing.
Monday.com: Get Contacts
Reads the specified board/columns from Monday.com where you store contact details.
Airtable - Contacts
Upserts (adds or updates) the fetched Monday.com data into Airtable‚Äôs Contacts table.
Ensures daily updates reflect changes from Monday.com.
Result
A consolidated contact list in Airtable, ready for AI email categorisation.
Workflow 2: Categorise & Prioritise Outlook Emails
Purpose
Fetches Outlook emails, cleans and processes their content, matches senders with known contacts, and uses AI to categorise and prioritise them.
Steps
1. Get Outlook Emails with Filters
Trigger: Either scheduled (Check Mail Schedule Trigger) or manual (Test Workflow).
Outlook Filters:
Not flagged (flag/flagStatus == 'notFlagged').
Not categorised (not categories/any()).
üîπ Result: A batch of fresh, unprocessed emails ready for processing.
2. Sanitise Email
Convert to Markdown: Strips HTML tags and normalises formatting.
Email Messages Processing: Allows manual removal of signatures, disclaimers, or extra content.
üîπ Result: A clean, AI-friendly email for categorisation.
3. Match Contact
Loop Over Emails: Iterates over each email.
Contact Lookup: Checks Airtable‚Äôs Contacts table (updated daily).
Merge Data: Enriches emails with known client, supplier, or internal team info.
üîπ Result: Enhanced email context for AI processing.
4. AI Agent to Categorise & Prioritise
Retrieve Rules & Categories
Reads Rules, Categories, and Delete Rules from Airtable in parallel.
AI: Analyse Email (Tools Agent)
Uses email text, sender info, and rules to build a structured AI prompt.
OpenAI Chat Model
Processes the AI prompt and outputs:
Category
Subcategory (optional)
Priority level
Short rationale
Structured Output Parser
Ensures AI response is valid JSON format.
üîπ Result:
Each email is labelled, categorised, and prioritised with AI-driven logic.
5. Set Outlook Category & Importance
Set Category: Updates Outlook with the assigned category.
Check Priority Conditions (If Node):
If Action Required or from a VIP, mark as High Priority.
Set Importance: Updates the email's importance flag in Outlook.
üîπ Result:
Outlook is updated with categories & importance based on AI recommendations.
Parallel Processing: Retrieve Rules & Categories
Runs alongside the email categorisation workflow.
Ensures Airtable-based rules are available before AI processing.
Steps
Airtable: Get Rules & Categories
Fetches Rules, Categories, and Delete Rules from Airtable.
Delete Rules (Optional)
If a delete rule matches, the email is removed.
üîπ Result:
A dynamic, updatable rule system ensuring emails are handled properly.
Final Outcome
Daily Contact Sync keeps contacts up to date.
AI-driven email workflow ensures smart categorisation.
Outlook automatically updated with AI-generated categories and importance.
This automated system saves time, ensures efficient inbox management, and allows for customisable rules via Airtable."
Auto-Tag Blog Posts in WordPress with AI,https://n8n.io/workflows/2816-auto-tag-blog-posts-in-wordpress-with-ai/,"How it works:
This workflow automates tagging for WordPress posts using AI:
Fetch blog post content and metadata.
Generate contextually relevant tags using AI.
Verify existing tags in WordPress and create new ones if necessary.
Automatically update posts with accurate and optimized tags.
Set up steps:
Estimated time: ~15 minutes.
Configure the workflow with your WordPress API credentials.
Connect your content source (e.g., RSS feed or manual input).
Adjust tag formatting preferences in the workflow settings.
Run the workflow to ensure proper tag creation and assignment.
This workflow is perfect for marketers and content managers looking to streamline their content categorization and improve SEO efficiency."
Summarize the New Documents from Google Drive and Save Summary in Google Sheet,https://n8n.io/workflows/2754-summarize-the-new-documents-from-google-drive-and-save-summary-in-google-sheet/,"This workflow is created by AI developers at WeblineIndia. It streamlines the process of managing content by automatically identifying and fetching the most recently added Google Doc file from your Google Drive. It extracts the content of the document for processing and leverages an AI model to generate a concise and meaningful summary of the extracted text. The summarized content is then stored in a designated Google Sheet, alongside relevant details like the document name and the date it was added, providing an organized and easily accessible reference for future use. This automation simplifies document handling, enhances productivity, and ensures seamless data management.
Steps :
Fetch the Most Recent Document from Google Drive
Action: Use the Google Drive Node.
Details: List files, filter by date to fetch the most recently added .doc file, and retrieve its file ID and metadata.
Extract Content from the Document
Action: Use the Google Docs Node.
Details: Set the operation to ""Get Content,"" pass the file ID, and extract the document's text content.
Summarize the Document Using an AI Model
Action: Use an AI Model Node (e.g., OpenAI, ChatGPT).
Details: Provide the extracted text to the AI model, use a prompt to generate a summary, and capture the result.
Store the Summarized Content in Google Sheets
Action: Use the Google Sheets Node.
Details: Append a new row to the target sheet with details such as the original document name, summary, and date added.
About WeblineIndia
WeblineIndia specializes in delivering innovative and custom AI solutions to simplify and automate business processes. If you need any help, please reach out to us."
Create a Google Analytics Data Report with AI and sent it to E-Mail and Telegram,https://n8n.io/workflows/2673-create-a-google-analytics-data-report-with-ai-and-sent-it-to-e-mail-and-telegram/,"What this workflow does
This workflow retrieves Google Analytics data from the last 7 days and the same period in the previous year. The data is then prepared by AI as a table, analyzed and provided with a small summary.
The summary is then sent by email to a desired address and, shortened and summarized again, sent to a Telegram account.
This workflow has the following sequence:
time trigger (e.g. every Monday at 7 a.m.)
retrieval of Google Analytics data from the last 7 days
assignment and summary of the data
retrieval of Google Analytics data from the last 7 days of the previous year
allocation and summary of the data
preparation in tabular form and brief analysis by AI.
sending the report as an email
preparation in short form by AI for Telegram (optional)
sending as Telegram message.
Requirements
The following accesses are required for the workflow:
Google Analytics (via Google Analytics API): Documentation
AI API access (e.g. via OpenAI, Anthropic, Google or Ollama)
SMTP access data (for sending the mail)
Telegram access data (optional for sending as Telegram message): Documentation
Feel free to contact me via LinkedIn, if you have any questions!"
AI Agent for project management and meetings with Airtable and Fireflies,https://n8n.io/workflows/2683-ai-agent-for-project-management-and-meetings-with-airtable-and-fireflies/,"Video Guide
I prepared a comprehensive guide detailing how to create a Smart Agent that automates meeting task management by analyzing transcripts, generating tasks in Airtable, and scheduling follow-ups when necessary.
Youtube Link
Who is this for?
This workflow is ideal for project managers, team leaders, and business owners looking to enhance productivity during meetings. It is particularly helpful for those who need to convert discussions into actionable items swiftly and effectively.
What problem does this workflow solve?
Managing action items from meetings can often lead to missed tasks and poor follow-up. This automation alleviates that issue by automatically generating tasks from meeting transcripts, keeping everyone informed about their responsibilities and streamlining communication.
What this workflow does
The workflow leverages n8n to create a Smart Agent that listens for completed meeting transcripts, processes them using AI, and generates tasks in Airtable. Key functionalities include:
Capturing completed meeting events through webhooks.
Extracting relevant meeting details such as transcripts and participants using API calls.
Generating structured tasks from meeting discussions and sending notifications to clients.
Webhook Integration: Listens for meeting completion events to trigger subsequent actions.
API Requests for Data: Pulls necessary details like transcripts and participant information from Fireflies.
Task and Notification Generation: Automatically creates tasks in Airtable and notifies clients of their responsibilities.
Setup
N8N Workflow
Configure the Webhook:
Set up a webhook to capture meeting completion events and integrate it with Fireflies.
Retrieve Meeting Content:
Use GraphQL API requests to extract meeting details and transcripts, ensuring appropriate authentication through Bearer tokens.
AI Processing Setup:
Define system messages for AI tasks and configure connections to the AI chat model (e.g., OpenAI's GPT) to process transcripts.
Task Creation Logic:
Create structured tasks based on AI output, ensuring necessary details are captured and records are created in Airtable.
Client Notifications:
Use an email node to notify clients about their tasks, ensuring communications are client-specific.
Scheduling Follow-Up Calls:
Set up Google Calendar events if follow-up meetings are required, populating details from the original meeting context."
üìö Auto-generate documentation for n8n workflows with GPT and Docsify,https://n8n.io/workflows/2669-auto-generate-documentation-for-n8n-workflows-with-gpt-and-docsify/,"This workflow creates a documentation system for n8n instances using Docsify.js. It serves a dynamic documentation website that allows users to:
View an overview of all workflows in a tabular format
Filter workflows by tags
Access automatically generated documentation for each workflow
Edit documentation with a live Markdown preview
Visualize workflow structures using Mermaid.js diagrams
üì∫ Check out the short 2-min demonstration on LinkedIn. Don't forget to connect!
üîß Key Components
Main Documentation Portal
Serves a Docsify-powered website
Provides a navigation sidebar with workflow tags
Displays workflow status, creation date, and documentation links
Documentation Generator
Uses GPT model to auto-generate workflow descriptions
Creates Mermaid.js diagrams of workflow structures
Maintains consistent documentation format
Live Editor
Split-screen Markdown editor with preview
Real-time Mermaid diagram rendering
Save/Cancel functionality
‚öôÔ∏è Technical Details
Environment Setup
Requires write access to the specified project directory
Uses environment variables for n8n instance URL configuration
Implements webhook endpoints for serving documentation
‚ö†Ô∏è Security Considerations
Note: The current implementation doesn't include authentication for editing. Consider adding authentication for production use.
Dependencies
Docsify.js for documentation rendering
Mermaid.js for workflow visualization
OpenAI GPT for documentation generation
üîç Part of the n8n Observability Series
This workflow is part of a broader series focused on n8n instance observability. Check out these related workflows:
Workflow Dashboard - Get comprehensive analytics of your n8n instance
Visualize Your n8n Workflows with Mermaid.js - Create beautiful workflow visualizations
Each workflow in this series helps you better understand and manage your n8n automation ecosystem!"
Qualifying Appointment Requests with AI & n8n Forms,https://n8n.io/workflows/2580-qualifying-appointment-requests-with-ai-and-n8n-forms/,"This n8n template builds upon a simple appointment request form design which uses AI to qualify if the incoming enquiry is suitable and/or time-worthy of an appointment.
This demonstrates a lighter approach to using AI in your templates but handles a technically difficult problem - contextual understanding! This example can be used in a variety of contexts where figuring out what is and isn't relevant can save a lot of time for your organisation.
How it works
We start with a form trigger which asks for the purpose of the appointment.
Instantly, we can qualify this by using a text classifier node which uses AI's contextual understanding to ensure the appointment is worthwhile. If not, an alternative is suggested instead.
Multi-page forms are then used to set the terms of the appointment and ask the user for a desired date and time.
An acknowledgement is sent to the user while an approval by email process is triggered in the background.
In a subworkflow, we use Gmail with the wait for approval operation to send an approval form to the admin user who can either confirm or decline the appointment request.
When approved, a Google Calendar event is created. When declined, the user is notified via email that the appointment request was declined.
How to use
Modify the enquiry classifier to determine which contexts are relevant to you.
Configure the wait for approval node to send to an email address which is accessible to all appropriate team members.
Requirements
OpenAI for LLM
Gmail for Email
Google Calendar for Appointments
Customising this workflow
Not using Google Mail or Calendar? Feel free to swap this with other services.
The wait for approval step is optional. Remove if you wish to handle appointment request resolution in another way."
CV Screening with OpenAI,https://n8n.io/workflows/2572-cv-screening-with-openai/,"Video Guide
I prepared a detailed guide that showed the whole process of building a resume analyzer.
Who is this for?
This workflow is ideal for recruitment agencies, HR professionals, and hiring managers looking to automate the initial screening of CVs. It is especially useful for organizations handling large volumes of applications and seeking to streamline their recruitment process.
What problem does this workflow solve?
Manually screening resumes is time-consuming and prone to human error. This workflow automates the process, providing consistent and objective analysis of CVs against job descriptions. It helps filter out unsuitable candidates early, reducing workload and improving the overall efficiency of the recruitment process.
What this workflow does
This workflow automates the resume screening process using OpenAI for analysis. It provides a matching score, a summary of candidate suitability, and key insights into why the candidate fits (or doesn‚Äôt fit) the job.
Retrieve Resume: The workflow downloads CVs from a direct link (e.g., Supabase storage or Dropbox).
Extract Data: Extracts text data from PDF or DOC files for analysis.
Analyze with OpenAI: Sends the extracted data and job description to OpenAI to:
Generate a matching score.
Summarize candidate strengths and weaknesses.
Provide actionable insights into their suitability for the job.
Setup
Preparation
Create Accounts:
N8N: For workflow automation.
OpenAI: For AI-powered CV analysis.
Get CV Link:
Upload CV files to Supabase storage or Dropbox to generate a direct link for processing.
Prepare Artifacts for OpenAI:
Define Metrics: Identify the metrics you want from the analysis (e.g., matching percentage, strengths, weaknesses).
Generate JSON Schema: Use OpenAI to structure responses, ensuring compatibility with your database.
Write a Prompt: Provide OpenAI with a clear and detailed prompt to ensure accurate analysis.
N8N Scenario
Download File: Fetch the CV using its direct URL.
Extract Data: Use N8N‚Äôs PDF or text extraction nodes to retrieve text from the CV.
Send to OpenAI:
URL: POST to OpenAI‚Äôs API for analysis.
Parameters:
Include the extracted CV data and job description.
Use JSON Schema to structure the response.
Summary
This workflow provides a seamless, automated solution for CV screening, helping recruitment agencies and HR teams save time while maintaining consistency in candidate evaluation. It enables organizations to focus on the most suitable candidates, improving the overall hiring process."
Upsert huge documents in a vector store with Supabase and Notion,https://n8n.io/workflows/2568-upsert-huge-documents-in-a-vector-store-with-supabase-and-notion/,"Purpose
This workflow adds the capability to build a RAG on living data. In this case Notion is used as a Knowledge Base. Whenever a page is updated, the embeddings get upserted in a Supabase Vector Store.
It can also be fairly easily adapted to PGVector, Pinecone, or Qdrant by using a custom HTTP request for the latter two.
Demo
How it works
A trigger checks every minute for changes in the Notion Database. The manual polling approach improves accuracy and prevents changes from being lost between cached polling intervals.
Afterwards every updated page is processed sequentially
The Vector Database is searched using the Notion Page ID stored in the metadata of each embedding. If old entries exist, they are deleted.
All blocks of the Notion Database Page are retrieved and combined into a single string
The content is embedded and split into chunks if necessary. Metadata, including the Notion Page ID, is added during storage for future reference.
A simple Question and Answer Chain enables users to ask questions about the embedded content through the integrated chat function
Prerequisites
To setup a new Vector Store in Supabase, follow this guide
Prepare a simple Database in Notion with each Database Page containing at least a title and some content in the blocks section. You can of course also connect this to an existing Database of your choice.
Setup
Select your credentials in the nodes which require those
If you are on an n8n cloud plan, switch to the native Notion Trigger by activating it and deactivating the Schedule Trigger along with its subsequent Notion Node
Choose your Notion Database in the first Node related to Notion
Adjust the chunk size and overlap in the Token Splitter to your preference
Activate the workflow
How to use
Populate your Notion Database with useful information and use the chat mode of this workflow to ask questions about it. Updates to a Notion Page should quickly reflect in future conversations."
Visualize your SQL Agent queries with OpenAI and Quickchart.io,https://n8n.io/workflows/2559-visualize-your-sql-agent-queries-with-openai-and-quickchartio/,"Overview
This workflow aims to provide data visualization capabilities to a native SQL Agent.
Together, they can help foster data analysis and data visualization within a team.
It uses the native SQL Agent that works well and adds visualization capabilities thanks to OpenAI‚Äôs Structured Output and Quickchart.io.
How it works
Information Extraction:
The Information Extractor identifies and extracts the user's question.
If the question includes a visualization aspect, the SQL Agent alone may not respond accurately.
SQL Querying:
It leverages a regular SQL Agent: it connects to a database, queries it, and translates the response into a human-readable format.
Chart Decision:
The Text Classifier determines whether the user would benefit from a chart to support the SQL Agent's response.
Chart Generation:
If a chart is needed, the sub-workflow dynamically generates a chart and appends it to the SQL Agent‚Äôs response.
If not, the SQL Agent‚Äôs response is output as is.
Calling OpenAI for Chart Definition:
The sub-workflow calls OpenAI via the HTTP Request node to retrieve a chart definition.
Building and Returning the Chart:
In the ""Set Response"" node, the chart definition is appended to a Quickchart.io URL, generating the final chart image.
The AI Agent returns the response along with the chart.
How to use it
Use an existing database or create a new one.
For example, I've used this Kaggle dataset and uploaded it to a Supabase DB.
Add the PostgreSQL or MySQL credentials.
Alternatively, you can use SQLite binary files (check this template).
Activate the workflow.
Start chatting with the AI SQL Agent.
If the Text Classifier determines a chart would be useful, it will generate one in addition to the SQL Agent's response.
Notes
The full Quickchart.io specifications have not been fully integrated, so there may be some glitches (e.g., radar graphs may not display properly due to size limitations)."
Automatic Background Removal for Images in Google Drive,https://n8n.io/workflows/2529-automatic-background-removal-for-images-in-google-drive/,"This n8n workflow simplifies the process of removing backgrounds from images stored in Google Drive. By leveraging the PhotoRoom API, this template enables automatic background removal, padding adjustments, and output formatting, all while storing the updated images back in a designated Google Drive folder.
This workflow is very useful for companies or individuals that are spending a lot of time into removing the background from product images.
How it Works
The workflow begins with a Google Drive Trigger node that monitors a specific folder for new image uploads.
Upon detecting a new image, the workflow downloads the file and extracts essential metadata, such as the file size.
Configurations are set for background color, padding, output size, and more, which are all customizable to match specific requirements.
The PhotoRoom API is called to process the image by removing its background and adding padding based on the settings.
The processed image is saved back to Google Drive in the specified output folder with an updated name indicating the background has been removed.
Requirements
PhotoRoom API Key
Google Drive API Access
Customizing the Workflow
Easily adjust the background color, padding, and output size using the configuration node.
Modify the output folder path in Google Drive or replace Google Drive with another storage service if needed.
For advanced use cases, integrate further image processing steps, such as adding captions or analyzing content using AI."
Siri AI Agent: Apple Shortcuts powered voice template,https://n8n.io/workflows/2436-siri-ai-agent-apple-shortcuts-powered-voice-template/,"This template demonstrates how to trigger an AI Agent with Siri and Apple Shortcuts, showing a simple pattern for voice-activated workflows in n8n. It's easy to customize‚Äîadd app nodes before the AI Agent step to pass additional context, or modify the Apple Shortcut to send inputs like text, geolocation, images, or files.
Set Up
Basic instructions in template itself.
Requirements
n8n account (cloud or self-hosted)
Apple Shortcuts app on iOS or macOS. Dictation (""Siri"") must be activated. Download the Shortcuts template here.
Key Features:
Voice-Controlled AI: Trigger AI Agent via Siri for real-time voice replies.
Customizable Inputs: Modify Apple Shortcut to send text, images, geolocation, and more.
Flexible Outputs: Siri can return the AI‚Äôs response as text, files, or customize it to trigger CRUD actions in connected apps.
Context-Aware: Automatically feeds the current date and time to the AI Agent, with easy options to pass in more data.
How It Works:
Activate Siri and speak your request.
Siri sends the transcribed text to the n8n workflow via Apple Shortcuts.
AI Agent processes the request and generates a response.
Siri reads the response, or the workflow can return geolocation, files, or even perform CRUD actions in apps.
Inspiration: Custom Use Cases
Tweak this template and make it your own.
Capture Business Cards: Snap a photo of a business card and record a voice note. Have the AI Agent draft a follow-up email in Gmail, ready to send.
Voice-to-Task Automation: Speak a new to-do item, and the workflow will add it to a Notion task board.
Business English on the Fly: Convert casual speech into polished business language, and save the refined text directly to your pasteboard, ready to be pasted into any app. ""It's late because of you"" -> ""There has been a delay, and I believe your input may have contributed to it."""
Automate Image Validation Tasks using AI Vision,https://n8n.io/workflows/2420-automate-image-validation-tasks-using-ai-vision/,"This n8n workflow shows how using multimodal LLMs with AI vision can tackle tricky image validation tasks which are near impossible to achieve with code and often impractical to be done by humans at scale.
You may need image validation when users submitted photos or images are required to meet certain criteria before being accepted. A wine review website may require users only submit photos of wine with labels, a bank may require account holders to submit scanned documents for verification etc.
In this demonstration, our scenario will be to analyse a set of portraits to verify if they meet the criteria for valid passport photos according to the UK government website (https://www.gov.uk/photos-for-passports).
How it works
Our set of portaits are jpg files downloaded from our Google Drive using the Google Drive node.
Each image is resized using the Edit Image node to ensure a balance between resolution and processing speed.
Using the Basic LLM node, we'll define a ""user message"" option with the type of binary (data). This will allow us to pass our portrait to the LLM as an input.
With our prompt containing the criteria pulled off the passport photo requirements webpage, the LLM is able to validate the photo does or doesn't meet its criteria.
A structured output parser is used to structure the LLM's response to a JSON object which has the ""is_valid"" boolean property. This can be useful to further extend the workflow.
Requirements
Google Gemini API key
Google Drive account
Customising this workflow
Not using Gemini? n8n's LLM node works with any compatible multimodal LLM so feel free to swap Gemini out for OpenAI's GPT4o or Antrophic's Claude Sonnet.
Don't need to validate portraits? Try other use cases such as document classification, security footage analysis, people tagging in photos and more."
IT Ops AI SlackBot Workflow - Chat with your knowledge base,https://n8n.io/workflows/2397-it-ops-ai-slackbot-workflow-chat-with-your-knowledge-base/,"Video Demo:
Click here to see a video of this workflow in action.
Summary Description:
The ""IT Department Q&A Workflow"" is designed to streamline and automate the process of handling IT-related inquiries from employees through Slack. When an employee sends a direct message (DM) to the IT department's Slack channel, the workflow is triggered. The initial step involves the ""Receive DMs"" node, which listens for new messages. Upon receiving a message, the workflow verifies the webhook by responding to Slack's challenge request, ensuring that the communication channel is active and secure.
Once the webhook is verified, the workflow checks if the message sender is a bot using the ""Check if Bot"" node. If the sender is identified as a bot, the workflow terminates the process to avoid unnecessary actions. If the sender is a human, the workflow sends an acknowledgment message back to the user, confirming that their query is being processed. This is achieved through the ""Send Initial Message"" node, which posts a simple message like ""On it!"" to the user's Slack channel.
The core functionality of the workflow is powered by the ""AI Agent"" node, which utilizes the OpenAI GPT-4 model to interpret and respond to the user's query. This AI-driven node processes the text of the received message, generating an appropriate response based on the context and information available. To maintain conversation context, the ""Window Buffer Memory"" node stores the last five messages from each user, ensuring that the AI agent can provide coherent and contextually relevant answers.
Additionally, the workflow includes a custom Knowledge Base (KB) tool (see that tool template here) that integrates with the AI agent, allowing it to search the company's internal KB for relevant information. After generating the response, the workflow cleans up the initial acknowledgment message using the ""Delete Initial Message"" node to keep the conversation thread clean. Finally, the generated response is sent back to the user via the ""Send Message"" node, providing them with the information or assistance they requested. This workflow effectively automates the IT support process, reducing response times and improving efficiency.
To quickly deploy the Knowledge Ninja app in Slack, use the app manifest below and don't forget to replace the two sample urls:
{
    ""display_information"": {
        ""name"": ""Knowledge Ninja"",
        ""description"": ""IT Department Q&A Workflow"",
        ""background_color"": ""#005e5e""
    },
    ""features"": {
        ""bot_user"": {
            ""display_name"": ""IT Ops AI SlackBot Workflow"",
            ""always_online"": true
        }
    },
    ""oauth_config"": {
        ""redirect_urls"": [
            ""Replace everything inside the double quotes with your slack redirect oauth url, for example: https://n8n.domain.com/rest/oauth2-credential/callback""
        ],
        ""scopes"": {
            ""user"": [
                ""search:read""
            ],
            ""bot"": [
                ""chat:write"",
                ""chat:write.customize"",
                ""groups:history"",
                ""groups:read"",
                ""groups:write"",
                ""groups:write.invites"",
                ""groups:write.topic"",
                ""im:history"",
                ""im:read"",
                ""im:write"",
                ""mpim:history"",
                ""mpim:read"",
                ""mpim:write"",
                ""mpim:write.topic"",
                ""usergroups:read"",
                ""usergroups:write"",
                ""users:write"",
                ""channels:history""
            ]
        }
    },
    ""settings"": {
        ""event_subscriptions"": {
            ""request_url"": ""Replace everything inside the double quotes with your workflow webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad"",
            ""bot_events"": [
                ""message.im""
            ]
        },
        ""org_deploy_enabled"": false,
        ""socket_mode_enabled"": false,
        ""token_rotation_enabled"": false
    }
}"
Supabase Insertion & Upsertion & Retrieval,https://n8n.io/workflows/2395-supabase-insertion-and-upsertion-and-retrieval/,"This is a demo workflow to showcase how to use Supabase to embed a document, retrieve information from the vector store via chat and update the database.
Setup steps:
set your credentials for Supabase
set your credentials for an AI model of your choice
set credentials for any service you want to use to upload documents
please follow the guidelines in the workflow itself (Sticky Notes)
Feedback & Questions
If you have any questions or feedback about this workflow - Feel free to get in touch at ria@n8n.io"
Actioning Your Meeting Next Steps using Transcripts and AI,https://n8n.io/workflows/2328-actioning-your-meeting-next-steps-using-transcripts-and-ai/,"This n8n workflow demonstrates how you can summarise and automate post-meeting actions from video transcripts fed into an AI Agent.
Save time between meetings by allowing AI handle the chores of organising follow-up meetings and invites.
How it works
This workflow scans for the calendar for client or team meetings which were held online. * Attempts will be made to fetch any recorded transcripts which are then sent to the AI agent.
The AI agent summarises and identifies if any follow-on meetings are required.
If found, the Agent will use its Calendar Tool to to create the event for the time, date and place for the next meeting as well as add known attendees.
Requirements
Google Calendar and the ability to fetch Meeting Transcripts (There is a special OAuth permission for this action!)
OpenAI account for access to the LLM.
Customising the workflow
This example only books follow-on meetings but could be extended to generate reports or send emails."
Invoice data extraction with LlamaParse and OpenAI,https://n8n.io/workflows/2320-invoice-data-extraction-with-llamaparse-and-openai/,"This n8n workflow automates the process of parsing and extracting data from PDF invoices. With this workflow, accounts and finance people can realise huge time and cost savings in their busy schedules.
Read the Blog: https://blog.n8n.io/how-to-extract-data-from-pdf-to-excel-spreadsheet-advance-parsing-with-n8n-io-and-llamaparse/
How it works
This workflow will watch an email inbox for incoming invoices from suppliers
It will download the attached PDFs and processing them through a third party service called LlamaParse.
LlamaParse is specifically designed to handle and convert complex PDF data structures such as tables to markdown.
Markdown is easily to process for LLM models and so the data extraction by our AI agent is more accurate and reliable.
The workflow exports the extracted data from the AI agent to Google Sheets once the job complete.
Requirements
The criteria of the email trigger must be configured to capture emails with attachments.
The gmail label ""invoice synced"" must be created before using this workflow.
A LlamaIndex.ai account to use the LlamaParse service.
An OpenAI account to use GPT for AI work.
Google Sheets to save the output of the data extraction process although this can be replaced for whatever your needs.
Customizing this workflow
This workflow uses Gmail and Google Sheets but these can easily be swapped out for equivalent services such as Outlook and Excel.
Not using Excel? Simple redirect the output of the AI agent to your accounting software of choice."
Gmail AI Auto-Responder: Create Draft Replies to incoming emails,https://n8n.io/workflows/2271-gmail-ai-auto-responder-create-draft-replies-to-incoming-emails/,"This workflow automatically generates draft replies in Gmail.
It's designed for anyone who manages a high volume of emails or often face writer's block when crafting responses.
Since it doesn't send the generated message directly, you're still in charge of editing and approving emails before they go out.
How It Works:
Email Trigger: activates when new emails reach the Gmail inbox
Assessment: uses OpenAI gpt-4o and a JSON parser to determine if a response is necessary.
Reply Generation: crafts a reply with OpenAI GPT-4 Turbo
Draft Integration: after converting the text to html, it places the draft into the Gmail thread as a reply to the first message
Set Up Overview (~10 minutes):
OAuth Configuration (follow n8n instructions here):
Setup Google OAuth in Google Cloud console. Make sure to add Gmail API with the modify scope.
Add Google OAuth credentials in n8n. Make sure to add the n8n redirect URI to the Google Cloud Console consent screen settings.
OpenAI Configuration: add OpenAI API Key in the credentials
Tweaking the prompt: edit the system prompt in the ""Generate email reply"" node to suit your needs
Detailed Walkthrough
Check out this blog post where I go into more details on how I built this workflow.
Reach out to me here if you need help building automations for your business."
Write a WordPress post with AI (starting from a few keywords),https://n8n.io/workflows/2187-write-a-wordpress-post-with-ai-starting-from-a-few-keywords/,"This n8n workflow template allows you to write WordPress posts by just providing a few keywords. It uses AI technology to write the text and to create the post's featured image. The text includes an introduction, chapters, and conclusions. Each chapter is written independently and this allows you to create also very long articles. The workflow uses technologies provided by Open AI: Chat GPT for the text and Dall-E for the image.
I suggest reviewing the created posts before publishing them on your WordPress website.
The article generation might take some minutes as each chapter is created independently.
Features
Easy to use: Easy web interface to start the generation of the WordPress post
AI-powered: Text and image generation is done by artificial intelligence
Long-text ready: Possibility to create very long articles
Configurable: Possibility to provide as many keywords as you want, to choose the number of chapters and the length of the article
Plugs into your WordPress: Easily integrates with your WordPress website
Tweak it as you want: Fine-tune the Open AI prompts and the workflow as you want
Workflow Steps
User form: An n8n form is used to trigger the post creation
Settings: This node is used to set your WordPress URL (which is used later in the workflow)
Article structure: First AI action that writes the introduction, the conclusions, and the chapter structure.
Data check: Check that the data provided by the AI is valid
Chapters split/Chapters text: Splits the data for each chapter in a separate item and generates each chapter's text with AI
Content preparation: Prepares the text for posting merging the introduction, the chapters, and the conclusions. Adds some basic HTML formatting
Draft on WordPress: Creates the draft post on WordPress
Featured image: Creates a featured image and adds it to the post on WordPress
User feedback: Sends a feedback to the user on the n8n form
Getting Started
To deploy and use this template:
Import the workflow into your n8n workspace
Set your WordPress URL in the wordpress_url field in the ""Settings"" node. Include the slash (/) at the end of the URL
Set up your Open AI n8n credentials by following this guide. The Open AI credentials are used by the Open AI nodes (""Create post title and structure"", ""Create chapters text"", and ""Generate featured image"")
Set up your WordPress n8n credentials by following this guide. The WordPress credentials are used by the WordPress and HTTP Request nodes (""Post on Wordpress"", ""Upload media"", and ""Set image ID for the post""). Pay attention that the ""Password"" in the WordPress credentials is not the user's password by the Application Password
How use the workflow to create a WordPress post
Activate the workflow
Open the ""Form"" node and copy the ""Production URL"". This is the public URL of the form to AI-write the post
Open the URL in a browser and fill in the form
Wait a few minutes till you get the feedback in the form that the post was created
Go to WordPress and check the newly created draft post. Review and publish your post!"
"Transcribe Audio Files, Summarize with GPT-4, and Store in Notion",https://n8n.io/workflows/2178-transcribe-audio-files-summarize-with-gpt-4-and-store-in-notion/,"Who is this for?
This workflow template is perfect for content creators, researchers, students, or anyone who regularly works with audio files and needs to transcribe and summarize them for easy reference and organization.
What problem does this workflow solve?
Transcribing audio files and summarizing their content can be time-consuming and tedious when done manually. This workflow automates the process, saving users valuable time and effort while ensuring accurate transcriptions and concise summaries.
What this workflow does
This template automates the following steps:
Monitors a specified Google Drive folder for new audio files
Sends the audio file to OpenAI's Whisper API for transcription
Passes the transcribed text to GPT-4 for summarization
Creates a new page in Notion with the summary
Setup
To set up this workflow:
Connect your Google Drive, OpenAI, and Notion accounts to n8n
Configure the Google Drive node with the folder you want to monitor for new audio files
Set up the OpenAI node with your API key and desired parameters for Whisper and GPT-4
Specify the Notion database where you want the summaries to be stored
How to customize this workflow
Adjust the Google Drive folder being monitored
Modify the OpenAI node parameters to fine-tune the transcription and summarization process
Change the Notion database or page properties to match your preferred structure
With this AI-powered workflow, you can effortlessly transcribe audio files, generate concise summaries, and store them in a structured manner within Notion.
Streamline your audio content processing and organization with this automated template."
Ask questions about a PDF using AI,https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/,"The workflow first populates a Pinecone index with vectors from a Bitcoin whitepaper. Then, it waits for a manual chat message. When received, the chat message is turned into a vector and compared to the vectors in Pinecone. The most similar vectors are retrieved and passed to OpenAI for generating a chat response.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Telegram AI Chatbot,https://n8n.io/workflows/1934-telegram-ai-chatbot/,"The workflow starts by listening for messages from Telegram users. The message is then processed, and based on its content, different actions are taken. If it's a regular chat message, the workflow generates a response using the OpenAI API and sends it back to the user. If it's a command to create an image, the workflow generates an image using the OpenAI API and sends the image to the user. If the command is unsupported, an error message is sent. Throughout the workflow, there are additional nodes for displaying notes and simulating typing actions."
Retrieve Deadlock Game Match Statistics and Send to Telegram,https://n8n.io/workflows/4571-retrieve-deadlock-game-match-statistics-and-send-to-telegram/,"Deadlock Match Stats Bot is an automated workflow for n8n designed to send detailed player statistics from the most recent Deadlock match directly to Telegram. When the user sends the /match command to the Telegram bot, the workflow performs the following steps:
Loads the HTML content of the player's profile page from deadlocktracker.gg using a preconfigured Steam ID.
Extracts the most recent match ID using a regular expression from the embedded JavaScript data.
Loads the HTML page for the specified match.
Parses the match page using cheerio to extract relevant data for each player, including their nickname, selected hero, and current rank.
Formats the collected information into a single message and sends it to the Telegram chat that issued the command."
Switch Between Work and Personal Contexts with GPT-4.1 and iPhone Automation,https://n8n.io/workflows/4345-switch-between-work-and-personal-contexts-with-gpt-41-and-iphone-automation/,"Purpose
The purpose of this automation is to help context switch from office to some side projects or passion gigs so you can be free of distracting thoughts and re-set your perspective.
Benefits
Anyone who works full time and also does something on the side (perhaps a side gig/being a mom/just follow your passion project)
What you need
N8N (lol)
Any LLM API Key (I used OpenAI 4.1)
IPhone (automations and shortcuts)
Template Setup
Setup LLM API key.
Import template file to new workflow.
On Iphone create a new shortcut as per video.
Create automation steps.
Resources
Youtube"
Evaluation metric example: String similarity,https://n8n.io/workflows/4274-evaluation-metric-example-string-similarity/,"AI evaluation in n8n
This is a template for n8n's evaluation feature.
Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow.
By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't.
How it works
This template shows how to calculate a workflow evaluation metric: text similarity, measured character-by-character.
The workflow takes images of hand-written codes, extracts the code and compares it with the expected answer from the dataset.
The images look like this:
The workflow works as follows:
We use an evaluation trigger to read in our dataset
It is wired up in parallel with the regular trigger so that the workflow can be started from either one. More info
We download the image and use AI to extract the code
If we‚Äôre evaluating (i.e. the execution started from the evaluation trigger), we calculate the string distance metric
We pass this information back to n8n as a metric"
Get Colombian Peso to USD Exchange Rate with Telegram Bot and AI Date Recognition,https://n8n.io/workflows/4246-get-colombian-peso-to-usd-exchange-rate-with-telegram-bot-and-ai-date-recognition/,"üìå Request TRM (Colombian Peso to US Dollar exchange rate)
üéØ Objective
Retrieve the TRM (Colombian Peso to US Dollar exchange rate) for a specific date provided by the user via Telegram.
üîÑ Summary Flow
üì® Telegram Message
The user sends a text or audio message to the Telegram Bot.
üß† Date Extraction
AI (OpenAI) is used to identify the date in YYYY-MM-DD format.
üìÖ Validation
If the date is in the future, the user is notified that it is not valid.
üì° TRM Query
The bot queries the open data API from the Colombian government.
üì• Result
‚úÖ If TRM data exists for that date ‚Üí The bot replies to the user.
‚ùå If not ‚Üí It searches up to 10 previous days for available data.
üì§ Final Response
The most recent available value is sent, or a message indicating no information is available.
üîó Data Source
datos.gov.co ‚Äì TRM"
Control Your 3D Printer with GPT-4o and OctoPrint API Conversations,https://n8n.io/workflows/4222-control-your-3d-printer-with-gpt-4o-and-octoprint-api-conversations/,"Ever wanted to just tell your 3d printer what to do remotely?
This game changer let's you converse with OpenAI agents to manage OctoPrint connected 3d printers. Great for remote management and monitoring with easy to read responses.
Query what jobs are available, Load a print job (upload workflow in development, will be released soon for community use), start the print job and request detailed reports on your printers performance as it runs.
Chat > AI > OctoPrint API > Your 3d Printer
++Example 1:++
Human Input: ""What print job is running?""
AI Response:
Current Print Job Details,
Job Name: Stand v1.gcode
File Size: 2,533,233 bytes
Estimated Print Time: Approximately 6 hours 15 minutes (22536 seconds)
Progress:
Completion: 1.97%,
Current Position in File: 49,940 bytes,
Time Spent Printing: 2,611 seconds (~43.5 minutes),
Time Remaining: Approximately 5 hours 34 minutes (20,123 seconds),
Status,
Current State: Printing.""
++Example 2++
Human Input: ""Is my 3d Printer ok?""
AI Response:
""Status: Printing,
Operational: Yes,
++Temperature:++
Bed Temperature:
Actual: 50¬∞C,
Target: 50¬∞C,
Nozzle (Tool 0) Temperature:
Actual: 200¬∞C,
Target: 200¬∞C,
++Connection Status:++
Connection State: Currently connected to the printer,
Port: /dev/ttyUSB0,
Baudrate: 115200,
Summary,
Your 3D printer is currently operational and printing without errors. The bed and nozzle temperatures are both at their target values."""
Tesla 1hour Indicators Tool (Mid-Term Technical Analysis AI),https://n8n.io/workflows/4097-tesla-1hour-indicators-tool-mid-term-technical-analysis-ai/,"üïí Evaluate Tesla (TSLA) price action and market structure on the 1-hour timeframe using 6 real-time indicators.
This sub-agent is designed to feed mid-term technical insights into the Tesla Financial Market Data Analyst Tool. It uses GPT-4.1 to interpret Alpha Vantage indicator data delivered via secure webhooks.
‚ö†Ô∏è This workflow is not standalone and is executed via Execute Workflow.
üîå Requires:
Tesla Quant Technical Indicators Webhooks Tool
Alpha Vantage Premium API Key
üîß Connected Indicators
This tool fetches and analyzes the latest 20 datapoints for:
RSI (Relative Strength Index)
MACD (Moving Average Convergence Divergence)
BBANDS (Bollinger Bands)
SMA (Simple Moving Average)
EMA (Exponential Moving Average)
ADX (Average Directional Index)
üìã Sample Output
{
  ""summary"": ""TSLA is gaining strength on the 1-hour chart. RSI is rising, MACD has crossed bullish, and BBANDS are widening."",
  ""timeframe"": ""1h"",
  ""indicators"": {
    ""RSI"": 62.1,
    ""BBANDS"": {
      ""upper"": 176.90,
      ""lower"": 169.70,
      ""middle"": 173.30,
      ""close"": 176.30
    },
    ""SMA"": 174.20,
    ""EMA"": 175.60,
    ""ADX"": 27.5,
    ""MACD"": {
      ""macd"": 0.84,
      ""signal"": 0.65,
      ""histogram"": 0.19
    }
  }
}
üß† Agent Components
Component Role
1hour Data Pulls Alpha Vantage indicator data via webhook
Tesla 1hour Indicators Agent Interprets signals using structured GPT-4.1 prompt
OpenAI Chat Model GPT-4.1 LLM performs analysis
Simple Memory Maintains session context
üõ†Ô∏è Setup Instructions
Import Workflow into n8n
Name it: Tesla_1hour_Indicators_Tool
Install the Webhook Fetcher Tool
üëâ Required: Tesla_Quant_Technical_Indicators_Webhooks_Tool
This agent expects webhook /1hourData to return pre-cleaned data
Add Credentials
Alpha Vantage Premium API Key (via HTTP Query Auth)
OpenAI GPT-4.1 credentials
Configure for Sub-Agent Use
Triggered only via Execute Workflow from:
üëâ Tesla Financial Market Data Analyst Tool
Inputs:
message (optional)
sessionId (required for memory linkage)
üìå Sticky Notes Overview
üü¢ Trigger Setup ‚Äì Activated only by the parent agent
üìä 1h Webhook Fetcher ‚Äì Calls Alpha Vantage via secured endpoint
üß† AI Agent Summary ‚Äì Interprets trend/momentum from indicator data
üîó GPT Model Notes ‚Äì GPT-4.1 parses and explains technical alignment
üìò Documentation Sticky ‚Äì Embedded in canvas with full walkthrough
üîê Licensing & Support
¬© 2025 Treasurium Capital Limited Company
This tool is part of a proprietary multi-agent AI architecture. No commercial reuse or redistribution permitted.
üîó Author: Don Jayamaha
üîó Templates: https://n8n.io/creators/don-the-gem-dealer/
üöÄ Detect TSLA trend shifts and validate setups with 1-hour technical clarity‚Äîpowered by Alpha Vantage + GPT-4.1.
This tool is required by the Tesla Financial Market Data Analyst Tool."
Create Google Calendar Meetings from Trello Cards with Auto-Meeting Link,https://n8n.io/workflows/4081-create-google-calendar-meetings-from-trello-cards-with-auto-meeting-link/,"How it works
Creates a Google Calendar meeting using a Trello card's due date. When the card is moved to the a specific list, it pulls email addresses from the card's description to invite attendees and then adds the meeting link back to the Trello card. See the video of the automation working here: Trello (Free) + Google Calendar (with n8n)
Who is this template for?
Free Trello users who want to automatically schedule Google Calendar meetings without leaving Trello.
Tools needed
A Google Calendar account
A Trello account (free plan)
N8N
Set up instructions
You can find all the instructions in the workflow or you can watch the video instructions: Instructions Page"
Track Hourly Weather Conditions with OpenWeatherMap and Google Sheets,https://n8n.io/workflows/4066-track-hourly-weather-conditions-with-openweathermap-and-google-sheets/,"This n8n template allows you to monitor hourly weather conditions in a specific city using OpenWeatherMap and log the results to a Google Sheet. It‚Äôs perfect for anyone needing periodic weather tracking‚Äîwhether you're managing logistics, travel planning, or environmental monitoring.
üîß How it works
A Schedule Trigger activates the workflow every hour.
The Get Weather Data from OpenWeatherMap node fetches real-time weather details using the city name you specify.
An IF node checks if the weather description contains ""rain"" or the temperature is below a set threshold.
If the condition is true, the data is formatted with city, temperature, humidity, and conditions.
The Google Sheets node appends this formatted information to your designated sheet.
üë§ Who is it for?
This workflow is ideal for:
Operations teams monitoring weather-sensitive logistics
Researchers collecting climate data
Developers and hobbyists learning how to connect APIs with Google Sheets
üóÇÔ∏è Google Sheet Structure
Your Google Sheet should have the following columns:
city (string)
temperature (K) (number)
humidity (number)
conditions (string)
status (string)
‚öôÔ∏è Setup Instructions
Create a Google Sheet with the above columns.
Set up your Google Service Account credentials in n8n.
Replace the API key in the HTTP Request node with your own OpenWeatherMap credential.
Specify your target city and ensure your OpenWeatherMap account is active.
Adjust the frequency in the Schedule Trigger as needed (default: every hour)."
Enrich Seller Data with Email & Domain Lookup using Bright Data & Google Search,https://n8n.io/workflows/4072-enrich-seller-data-with-email-and-domain-lookup-using-bright-data-and-google-search/,"üîç Email Lookup with Google Search from Postgres Database
This N8N workflow is designed to enrich seller data stored in a Postgres database by performing automated Google search lookups. It uses Bright Data's Web Unlocker to bypass search result restrictions and the HTML Extract node to parse and extract relevant information from webpages.
The main purpose of this workflow is to discover missing contact details, company domains, and secondary emails for businesses or sellers based on existing database entries.
üéØ Problem This Workflow Solves
Manually searching for missing seller or business details‚Äîlike secondary emails, websites, or domain names‚Äîcan be time-consuming and inefficient, especially for large datasets.
This workflow automates the search and data enrichment process, significantly reducing manual effort while improving the quality and completeness of your seller database.
‚úÖ Prerequisites
Before using this template, make sure the following requirements are met:
‚úîÔ∏è A Bright Data account with access to the Web Unlocker or Amazon Scraper API
‚úîÔ∏è A valid Bright Data API key
‚úîÔ∏è An active PostgreSQL database with seller data
‚úîÔ∏è N8N self-hosted instance (recommended for using community nodes like n8n-nodes-brightdata)
‚úîÔ∏è Installed n8n-nodes-brightdata package (custom node for Bright Data integration)
‚öôÔ∏è Setup Instructions
Step 1: Prepare Your Postgres Table
Create a table in Postgres with the following structure (you can adjust field names if needed):
CREATE TABLE sellers (
  seller_id SERIAL PRIMARY KEY,
  seller_name TEXT,
  primary_email TEXT,
  company_info TEXT,
  trade_name TEXT,
  business_address TEXT,
  coc_number TEXT,
  vat_number TEXT,
  commercial_register TEXT,
  secondary_email TEXT,
  domain TEXT,
  seller_slug TEXT,
  source TEXT
);
Step 2: Setup Web Unlocker on Bright Data
Go to your Bright Data dashboard.
Navigate to Proxies & Scraping ‚Üí Web Unlocker.
Create a new zone, selecting Web Unlocker API under Scraping Solutions.
Whitelist your server IP if required.
Step 3: Generate API Key
In the Bright Data dashboard, go to the API section.
Generate a new API key.
In N8N, create HTTP Request Credentials using Bearer Authentication with the API key.
Step 4: Install the Bright Data Node in N8N
In your N8N self-hosted instance, go to Settings ‚Üí Community Nodes.
Search and install n8n-nodes-brightdata.
üîÑ Workflow Functionality
üîÅ Trigger: Can be set to run on a schedule (e.g., daily) or manually.
üì• Read: Fetches seller records from the Postgres table.
üåê Search: Uses Bright Data to perform a Google search based on seller_name, company_info, or trade_name.
üßæ Extract: Parses the HTML content using the HTML Extract node to identify potential websites and email addresses.
üìù Update: Writes enriched data (like domain or secondary_email) back to the Postgres table.
üí° Use Cases
Lead enrichment for e-commerce sellers
Domain and contact info discovery for B2B databases
Email and web domain verification for CRM systems
Market research automation
üõ†Ô∏è Customization Tips
You can enhance the parsing logic in the HTML Extract node to look for phone numbers, LinkedIn profiles, or social media links.
Modify the search query logic to include additional parameters like location or industry for more refined results.
Integrate additional APIs (e.g., Hunter.io, Clearbit) for email validation or social profile enrichment.
Add filtering to skip entries that already have domain or secondary_email."
Find Reddit Prospects & Generate Personalized Responses with Llama3 AI and Google Sheets,https://n8n.io/workflows/4068-find-reddit-prospects-and-generate-personalized-responses-with-llama3-ai-and-google-sheets/,"Turn Reddit into Your Most Powerful Lead Generation Channel
Discover hot prospects on Reddit who are actively discussing problems your product can solve - without spending hours manually searching and responding. This intelligent workflow automatically finds relevant conversations, analyzes intent with Llama3 AI, and crafts personalized responses that drive conversions.
üîç Automated Prospect Discovery
Stop wasting time manually searching Reddit. Let AI find people who genuinely need your solution - while you focus on higher-value tasks. The workflow monitors target subreddits 24/7, ensuring you never miss a potential lead.
üß† Intelligent Relevance Filtering
Not all mentions are quality leads. Our sophisticated Llama3 AI engine analyzes post content, sentiment, and intent to identify true business opportunities. Say goodbye to false positives and low-quality interactions.
üí¨ Personalized Response Generation
Generic outreach gets ignored. This workflow crafts individualized responses that reference specific pain points from each post, positioning your solution as the perfect answer to their problem - dramatically increasing engagement rates.
üìä Complete Activity Tracking
Every interaction is automatically logged in Google Sheets, creating a comprehensive database of your Reddit prospecting activities. Track which posts received responses, measure engagement, and refine your targeting over time.
üîÑ Set It and Forget It
Once configured, the entire process runs autonomously. No more daily Reddit searches or manual outreach - just check your dashboard to see new prospects and conversations.
Perfect For:
SaaS businesses targeting specific use cases
Consultants and agencies looking for clients with specific problems
Product companies monitoring customer pain points
Support teams finding users who need help
Technical Requirements:
Reddit API credentials
Ollama with Llama3 model
Google account for tracking
Get started today and transform Reddit from a time-sink into your most efficient lead generation channel!"
Convert LLM Output into Rich Telegram Messages ‚Äî Auto-Media & Smart Chunking,https://n8n.io/workflows/3961-convert-llm-output-into-rich-telegram-messages-auto-media-and-smart-chunking/,"Telegram Rich Output Helper Workflow
Who is this for?
Builders of Telegram chat‚Äëbots, AI assistants, or notification services who already run n8n and need to convert long, mixed‚Äëmedia answers from an LLM (or any upstream source) into Telegram‚Äëfriendly messages.
Prerequisites
A Telegram bot created with @BotFather.
The bot‚Äôs HTTP API token saved as a Telegram API credential in n8n.
n8n ‚â• 1.0 with the built‚Äëin Telegram node still installed.
A parent workflow that calls this one via Execute Workflow and passes:
chatId ‚Äî the destination chat ID (integer).
output ‚Äî a string that can contain plain text and HTTP links to images, audio, or video.
What the workflow does
Extract Links ‚Äì A JavaScript Code node scans output, deduplicates URLs, and classifies each by file extension.
Link Path
If no media links exist, the text path is used.
Otherwise, each link is routed through a Switch node that triggers the correct Telegram call (sendPhoto, sendAudio, sendVideo) so users get inline previews or players.
Text Path
An IF node checks whether the remaining text exceeds Telegram‚Äôs 1‚ÄØ000‚Äëcharacter limit.
When it does, a Code node slices the text at line boundaries; SplitInBatches then sends the chunks sequentially so nothing is lost.
All branches converge, keeping the whole exchange inside one execution.
Customisation tips
Adjust the character limit ‚Äì edit the first expression in ‚ÄúIf text too long‚Äù.
Filter/enrich links ‚Äì extend the regex or add MIME checks before dispatch.
Captions & keyboards ‚Äì populate additionalFields in the three ‚ÄúSend back‚Äù nodes.
Throughput vs. order ‚Äì tweak the batch size in both SplitInBatches nodes.
With this template in place, your users receive the complete message, playable media, and zero manual formatting ‚Äì all within Telegram‚Äôs API limits."
Automated Replies to X Threads with Airtop Browser Automation,https://n8n.io/workflows/4054-automated-replies-to-x-threads-with-airtop-browser-automation/,"Use Case
Automatically responding to X (formerly Twitter) posts can help you engage with potential customers at scale, saving time while maintaining a personal touch.
What This Automation Does
This automation replies to specified X posts using the following input parameters:
airtop_profile: The name of your Airtop Profile connected to X.
thread_url: The URL of the X post to reply to. Example
reply_text: The message you want to post as a reply.
How It Works
Creates a browser session using Airtop.
Navigates to the specified X post.
Types and submits the reply text.
Setup Requirements
Airtop API Key ‚Äî free to generate.
An Airtop Profile connected to X (requires one-time login).
Next Steps
Combine with X Monitoring: Use this with the X monitoring automation to create a fully automated engagement pipeline.
Extend to Other Platforms: Adapt the automation for use on LinkedIn, Reddit, or any web community.
Read more about this Airtop Automation."
"AI Chatbot Call Center: Demo Call Back (Production-Ready, Part 6)",https://n8n.io/workflows/4050-ai-chatbot-call-center-demo-call-back-production-ready-part-6/,"Workflow Name: üí¨ Demo Call Back
Template was created in n8n v1.90.2
Skill Level: High
Categories: n8n, Chatbot
Stacks
Execute Sub-workflow Trigger node
Chat Trigger node
Redis node
Postgres node
Telegram node
HTTP Request node
If node, Code node, Edit Fields (Set)
Prerequisite
Execute Sub-workflow Trigger: your own node
MiniMax Account (https://www.minimax.io/)
Production Features
Scaling Design for n8n Queue mode in production environment
Optional Provider Data from external Database with Caching Mechanism.
Optional AI Clone Voice Message response via MiniMax API with Multi-Languages support.
Optional Backup Chat Log to Database, so you can use in APP/API building.
Testing Flow with or without dependance on other workflow.
Multi Chatbot (This is a demo for Telegram, you can add WhatsApp, Line, etc)
Error Management
What this workflow does?
This is a n8n Telegram Output Workflow. It will receive message from other Sub-workflow then output to Telegram for Message, or Replay Message and extra Voice Message.
How it works
The Flow Trigger node will wait for the message from other Sub-workflow.
When message is received, it will first check for the matching Provider from the PostgreSQL database.
Then determine if it is a Voice message to Text message.
OPTIONAL. For voice message, use the MiniMax API to generate a voice message, then send it to Telegram.
Finally, send the text to Telegram.
Set up instructions
Pull and Set up the required SQL from our Github repository.
Create you Redis credentials, refer to n8n integration documentation for more information.
Select your Credentials in Provider Cache and Save Provider Cache.
Create you Postgres credentials, refer to n8n integration documentation for more information.
Select your Credentials in Load Member Data, Create Chat Log Input, and Create Chat Log Output.
Create you Telegram credentials, refer to n8n integration documentation for more information.
Select your Credentials in Telegram Voice Output, Telegram Reply Output, and Telegram Output.
AI Clone Voice setup instructions (Optional)
You can clone any voice with MiniMax
Go to https://www.minimax.io/ and create a MiniMax account
Setup the Database with the required variables found in the MiniMax TTS node
That‚Äôs it
How to adjust it to your needs
By default, this template will use the sys_provider table provider information, you could change it for your own design.
The demo use MiniMax API for AI voice cloning, you could implement any other AI your choice.
The Backup Chat Log will backup all chat conversion line by line. You can use it for you own APP/API development."
Connect Airtable Contacts to telli for Automated AI Voice Call Scheduling,https://n8n.io/workflows/3803-connect-airtable-contacts-to-telli-for-automated-ai-voice-call-scheduling/,"Upload your CRM contacts to telli and schedule AI voice-agent calls
Introduction to telli and AI Voice-Agent Calls
telli is an innovative platform that provides AI-powered voice agents capable of making calls and performing tasks tailored to specific customer use cases. These AI voice-agents can handle a wide range of communication tasks, from appointment scheduling to customer support, with remarkable efficiency and natural conversation flow.
This template is designed for businesses and organizations looking to automate their outbound calling processes using telli's AI voice-agents in conjunction with Airtable as their CRM. It solves the problem of manual call scheduling and data transfer between your CRM and calling system, saving time and reducing human error.
Prerequisites
telli account
Airtable base with contact information
n8n instance
Step-by-Step Setup Guide
n8n Setup:
Create a new workflow in n8n.
Add the Airtable node to connect to your CRM table.
telli API Configuration:
Log in to your telli dashboard.
Locate and copy your API key under telli - Settings - API/Webhooks.
Workflow Configuration:
Add two HTTP Request nodes to your n8n workflow.
Set the ""Authorization"" header in both POST requests, replacing the value with your telli API key.
Configure the first request to use the /add-contact endpoint.
Set up the second request to use the /schedule-call endpoint.
Data Mapping:
Map the relevant fields from your Airtable node to the telli API requests.
Testing and Activation:
Run a test execution of your workflow.
Once satisfied with the results, activate the workflow.
API Endpoint Details
Add Contact Endpoint
URL: https://api.telli.com/v1/add-contact
Method: POST
Headers:
Authorization: YOUR-API-KEY
Content-Type: application/json
Payload:
{
  ""external_contact_id"": ""string"",
  ""salutation"": ""string"",
  ""first_name"": ""string"",
  ""last_name"": ""string"",
  ""phone_number"": ""string"",
  ""email"": ""jsmith@example.com"",
  ""contact_details"": {},
  ""timezone"": ""string""
}
Schedule Call Endpoint
URL: https://api.telli.com/v1/schedule-call
Method: POST
Headers:
Authorization: YOUR-API-KEY
Content-Type: application/json
Payload:
{
  ""contact_id"": TELLI-CONTACT-ID,
  ""agent_id"": ""string"",
  ""max_retry_days"": 123,
  ""call_details"": {
    ""message"": ""Hello, this is your friendly reminder!"",
    ""questions"": [
      {
        ""fieldName"": ""email"",
        ""neededInformation"": ""email of the customer"",
        ""exampleQuestion"": ""What is your email address?"",
        ""responseFormat"": ""email string""
      }
    ]
  },
  ""override_from_number"": ""string""
}
Use Cases
This template is versatile and can be applied to various scenarios, including:
- Lead Qualification: Automatically schedule calls to new leads entered in your CRM.
- Appointment Reminders: Set up calls to remind clients of upcoming appointments.
- Customer Feedback: Schedule follow-up calls after product deliveries or service completions.
Uploading Multiple Contacts
For bulk operations, you have two options:
Loop Node: Include a Loop node in your n8n workflow to process multiple contacts sequentially.
Batch Endpoints: Instead of /add-contact and /schedule-call, use telli's batch endpoints:
/add-contacts-batch: Add multiple contacts within an array.
/schedule-calls-batch: Schedule multiple calls at once.
Example of batch endpoint usage:
{
  ""contacts"": [
    {""name"": ""John Doe"", ""phone"": ""+1234567890""},
    {""name"": ""Jane Smith"", ""phone"": ""+1987654321""}
  ]
}
By leveraging this template, you can seamlessly integrate your Airtable CRM with telli's powerful AI voice-agents, automating your outbound calling process and enhancing your customer communication strategy."
Transform Product Photos into 360¬∞ Videos with OpenAI & RunwayML,https://n8n.io/workflows/3957-transform-product-photos-into-360-videos-with-openai-and-runwayml/,"Transform Product Photos into 360¬∞ Videos with OpenAI & RunwayML
Easily convert simple product images into stunning 360¬∞ rotating videos with this AI-powered automation. Perfect for eCommerce stores, marketing teams, and content creators.
What This Workflow Does
This automation takes a product photo, title, and description submitted via a form and automates the entire video creation process:
Uploads product photo to Google Drive.
Generates a photorealistic AI prompt using OpenAI for product enhancement.
Enhances the original product image using the generated prompt for a refined, high-quality visual.
Generates a 360¬∞ rotating product video using RunwayML API.
(Optional) Sends the final video back to the user via email.
Ideal Use Cases
Shopify or WooCommerce product showcase videos.
Automated content creation for social media & ads.
Marketing teams streamlining product media generation.
Small businesses needing zero-effort product videos.
Setup Instructions
Prerequisites
n8n instance running.
Credentials configured for:
Google Drive OAuth2 / Service Account
OpenAI API Key
RunwayML API Key
(Optional) SMTP / Email node setup for notifications.
Steps to Deploy
Import the JSON workflow file into your n8n instance.
Update the Credential Nodes:
Link Google Drive credentials for file uploads.
Link OpenAI credentials for prompt generation.
Link RunwayML credentials for video generation.
Replace placeholders:
{{GDRIVE_FOLDER_ID}}
From_email admin@example.com
Deploy & trigger the workflow via form/webhook.
Customization Options
Prompt Style: Adjust lighting, background, or style in the AI prompt.
Video Duration & Aspect Ratio: Configure video length (10-30s) and formats (1:1, 16:9, etc.).
Rotation Speed & Effects: Modify product spin speed, reflections, shadows.
Output Destination: Save videos to Google Drive or preferred cloud storage.
Email Notifications: Customize email templates for sending final videos to users.
Workflow Highlights
Fully automated end-to-end product videography.
Secure credential management (no hardcoded API keys).
Modular node structure for easy scaling & customization.
Compatible with any eCommerce product visuals.
Tags
eCommerce Product Videos RunwayML OpenAI Automation Google Drive Content Creation Marketing"
Audit & Generate JSON-LD Schema Markup for SEO with GPT-4.1-mini + Gmail,https://n8n.io/workflows/3980-audit-and-generate-json-ld-schema-markup-for-seo-with-gpt-41-mini-gmail/,"Schema Markup Generator
Automatically audit and optimize your site‚Äôs JSON-LD structured data and email a step-by-step implementation guide.
What It Does
Collects a website URL & email via form
Extracts existing JSON-LD, identifies gaps
Generates optimized schema markup (GPT-4.1-mini)
Builds an HTML before/after guide with action items
Emails the full implementation instructions
Prerequisites
n8n instance (self-hosted, Desktop, or Cloud)
Gmail OAuth2 credentials
OpenRouter API key (for GPT-4.1-mini)
Quick Setup
Gmail: Add OAuth2 credential ‚Üí select in ‚ÄúSend Email‚Äù node
OpenRouter: Store API key ‚Üí link in ‚ÄúSchema Markup Agent‚Äù node
Form: Configure fields websiteUrl & emailAddress in your Webhook/Form node
Customization
AI Prompt: Tweak system message to focus on specific schema types or output style
Email Template: Modify HTML header/footer or add sections
Troubleshooting & Limits
Monitor OpenRouter rate limits and Gmail send quotas
Ensure target sites allow scraping for schema extraction"
Multilingual WhatsApp Translator with OpenAI Whisper & GPT-4 and HubSpot Integration,https://n8n.io/workflows/3972-multilingual-whatsapp-translator-with-openai-whisper-and-gpt-4-and-hubspot-integration/,"üåç AI WhatsApp Translator + Voice Transcriber with HubSpot Integration
Hello! I'm Amanda ‚ù§Ô∏è, a passionate automation creator and AI enthusiast.
With over 2 years of experience in n8n and Make.com, I design robust, intelligent agents
that make your operations smarter and smoother.
This translator + transcriber assistant is perfect for multilingual communication via WhatsApp.
It automatically captures voice or text messages, identifies the customer's region and language,
transcribes audio using OpenAI Whisper, and generates friendly, culturally adapted translations
with GPT-4 or GPT-4o. All messages and contacts are saved in your HubSpot CRM‚Äîso no lead gets lost.
‚úÖ What this Workflow Does:
üéß Transcribes audio messages using OpenAI Whisper (accurate + fast).
üåê Translates text & voice into the client‚Äôs native language or a default one.
üáßüá∑ Adapts tone, slang, emojis, and cultural expressions for natural conversation.
üíº Saves contact info & history to HubSpot automatically (name, phone, and translation).
üì¨ Replies via WhatsApp using Evolution API (also works with chatbots).
‚ú® Handles over 80+ countries and phone prefixes to detect preferred languages.
üõ†Ô∏è How to Use It:
Clone the workflow to your n8n instance (cloud or self-hosted).
Configure the Webhook URL in your Evolution API or WhatsApp provider.
Set up credentials for:
OpenAI (for transcription and GPT translation)
HubSpot (to store leads)
Optional: Adjust prompt logic and default languages inside the AI Agent node.
Done! Your AI translator bot is live üí¨üåé
üß† LLMs and Tools Used:
OpenAI GPT-4o / GPT-4 Mini for translation
OpenAI Whisper for transcription
HubSpot CRM integration
Evolution API for WhatsApp messaging
Dynamic language detection based on phone prefix
üí° Ideal For:
Multinational businesses with global customers
Customer support teams with multilingual needs
Agencies serving clients in Latin America, Europe, or Asia
Translating informal messages in real-time
üìÅ Included:
1x Complete n8n Workflow
Preconfigured translation agents
HubSpot contact mapper
Multi-country language index
Ready-to-use voice transcription logic
‚ù§Ô∏è Support Materials and Templates:
iloveflows.gumroad.com
‚òÅÔ∏è Want to use this with n8n Cloud?
Use my affiliate link to get started:
üëâ https://n8n.partnerlinks.io/amanda
üî• Buy a VPS server with a Super Discount: https://hostinger.com/vps"
üìä AI Token Tracker for WhatsApp & Telegram ‚Äì Save AI Usage to Google Sheets,https://n8n.io/workflows/3963-ai-token-tracker-for-whatsapp-and-telegram-save-ai-usage-to-google-sheets/,"üí∏ GPT-4o Token Tracker ‚Äì Measure AI Usage and Cost via WhatsApp or Telegram
Hi! I'm Amanda ‚Äî I create smart, useful AI automations for n8n and Make.
This workflow helps you monitor how many tokens are consumed per conversation with AI Agent Tokens, and logs that usage into Google Sheets ‚Äî so you can track expenses and make smarter decisions about your AI usage.
You can connect it to Telegram or WhatsApp (via Evolution API), and it will automatically capture:
Prompt and completion tokens
Total tokens
Time of interaction
The user/channel
(Optional) Cost estimate in USD
üìä What this workflow does
Logs token usage from LangChain agents
Calculates token totals for each request
Appends the data to a structured Google Sheet
Works with WhatsApp (Evolution API) or Telegram bots
Ideal for tracking AI Agent Tokens usage over time and controlling costs
üß™ Free Spreadsheet Template
You can duplicate and use this template for logging token usage:
üëâ Google Sheets Template
Includes columns like:
Date
Input
Output
Tokens (Prompt / Completion / Total)
Tool name or use case
Estimated cost
üõ†Ô∏è Setup Instructions
Import the workflow to your n8n (Cloud or self-hosted)
Connect your Google Sheets account
Choose your trigger:
WhatsApp (Evolution API webhook)
Telegram (Bot webhook or message input)
Fill out:
AI Agent model used (e.g. gpt-4o)
Cost per 1K tokens (optional, for USD estimation)
Update spreadsheet ID and tab name in the Google Sheets node
(Optional) Connect as a sub-tool to an AI agent for automatic logging
üß∞ Tech Used
OpenRouter
LangChain
Google Sheets
Telegram or WhatsApp webhook (Evolution API)
Nodes: HTTP Request, Set, If, Google Sheets, Merge
‚úÖ Who This Is For
Developers or agencies that want to monitor AI Agent Tokens usage
Freelancers managing AI bots for clients
Teams that need billing visibility and cost control
Anyone who wants transparency in AI token spending
‚úÖ Requirements
OpenAI GPT-4o access
Google Sheets credential
Telegram Bot Token (or Evolution API access for WhatsApp)
n8n (Cloud or self-hosted)
üåê Want this template?
‚ù§Ô∏è Buy more workflows at: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda"
YouTube Comment Sentiment Analysis with Google Gemini AI and Google Sheets,https://n8n.io/workflows/3936-youtube-comment-sentiment-analysis-with-google-gemini-ai-and-google-sheets/,"This workflow automatically collects all comments from a specified YouTube video and analyzes the sentiment of each comment using an AI model (e.g., GPT, Claude or Gemini). The sentiment (Positive, Neutral, or Negative), its strength, and confidence score are extracted and saved into a connected Google Sheet for easy access, reporting, and visualization.
Advantages:
üß† AI-Powered Sentiment Analysis
Uses modern language models (LLMs) to categorize comments with high accuracy.
üì∫ Ideal for YouTube Creators & Marketers
Provides insights into audience perception of videos, campaigns, or products.
üìà Real-Time Feedback Monitoring
Quickly identify trends in viewer sentiment across large comment volumes.
üìä Automatic Reporting
Saves results in Google Sheets for easy sharing or dashboard integration.
üîÅ Handles Pagination
Automatically fetches all comments, even from multi-page videos.
‚öôÔ∏è No-Code Customization
Easily adaptable to other platforms (e.g., TikTok, Instagram) or data sources.
üì• Simple Setup
Requires just a YouTube video ID and API key ‚Äî no coding needed.
üîÅ Loop and Update Logic
Continuously updates sheet with new results, avoiding duplicate processing.
üß© Modular Design
Easy to expand (e.g., reply classification, toxic comment detection, translation).
üí¨ Multi-Language Compatibility
AI can be configured to analyze comments in different languages with minimal setup.
How It Works
Trigger: The workflow starts manually (""When clicking ‚ÄòTest workflow‚Äô"") or can be scheduled.
Fetch Comments: The ""Get API Comments"" node retrieves comments from a YouTube video using the YouTube API.
Process Comments:
Extracts comments and replies via the ""Comments"" node.
Splits them into individual entries (""Split comments"").
Saves raw comments to Google Sheets (""Save comments"").
Sentiment Analysis:
Uses Google Gemini AI (or another model) to classify each comment as Positive, Neutral, or Negative.
Captures strength and confidence metrics for deeper insights.
Update Results: The ""Update sentiment"" node writes the analysis back to Google Sheets, marking processed rows.
Pagination Handling: Checks for multiple pages of comments (""Multipage?"") and loops until all are processed.
Set Up Steps
Prepare Google Sheet:
Clone the template: YouTube Comments Sheet.
Ensure columns exist: VIDEO_ID, COMMENTS, SENTIMENT, STRENGTH, CONFIDENCE, and DO (tracking column).
Configure YouTube API:
Obtain a YouTube API key from Google Developers Console.
Add it to the ""Get API Comments"" node under Youtube Query Auth (parameter: key).
Set Video ID:
Replace the default xxxxxxxx in the ""ID Video"" node with your target YouTube video ID.
AI Integration:
Ensure Google Gemini API credentials are configured in the ""Google Gemini"" node.
Run the Workflow:
Test manually or automate execution (e.g., hourly/daily) to analyze new comments.
Output: A Google Sheet with categorized sentiments, enabling trend analysis and audience engagement tracking.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
üíæ Backup Automation for n8n Workflows to Google Drive (Daily or Manual),https://n8n.io/workflows/3918-backup-automation-for-n8n-workflows-to-google-drive-daily-or-manual/,"üíæ Backup Automation for n8n Workflows to Google Drive ‚Äì No Risk, No Stress
Hi! I‚Äôm Amanda,
I build automation workflows for n8n and Make.
This ready-to-use workflow is designed to automatically export and back up all your n8n workflows to a designated Google Drive folder, organized by date.
It‚Äôs perfect for agencies, developers, and teams that want to protect their automation assets ‚Äî without relying on manual exports or risking losing their work.
‚úÖ What this workflow does
Fetches all your existing workflows from the n8n API
Compiles each workflow as a .json file
Creates a new folder in Google Drive using the current date
Uploads each .json file to that folder
Can be triggered manually or automatically via cron (date-time node)
‚öôÔ∏è How to set it up
Connect your Google Drive in the ‚ÄúFolder Creation‚Äù and ‚ÄúSave to Drive‚Äù nodes
Add your n8n API Key in the ‚ÄúSearch All Workflows‚Äù and ‚ÄúCompiles Individual Data‚Äù nodes
Define your time zone and execution method:
Run manually via the ‚ÄúManual Trigger‚Äù node
Or schedule backups using the ‚ÄúDate & Time‚Äù and ‚ÄúSchedule Trigger‚Äù nodes
(Optional) Change the parent folder ID in the ‚ÄúFolder Creation in Drive‚Äù node to set your destination
Once configured, the workflow will back up your entire workflow list daily ‚Äî or whenever you run it ‚Äî and send the files to your Google Drive!
üß∞ Nodes & Tech Used
Schedule Trigger + Manual Trigger
Date & Time ‚Äì For timestamp folder naming
n8n API ‚Äì To list and fetch each workflow
Google Drive ‚Äì Folder creation and file upload
Merge, Move Binary Data, Set ‚Äì Workflow compilation and formatting
üßë‚Äçüíª Who this is for
n8n developers who want peace of mind
Agencies managing many client automations
Anyone worried about losing their workflow work
Teams needing version control and backup history
üõ†Ô∏è Requirements
Google Drive OAuth2 credential
n8n API Key (from your account settings)
n8n Cloud or self-hosted instance
üåê Want to use this workflow?
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud with my partner link: https://n8n.partnerlinks.io/amanda"
"Generate Riddle Shorts & Post on YouTube with Sonnet 3.5, Pinecone & Creatomate",https://n8n.io/workflows/3907-generate-riddle-shorts-and-post-on-youtube-with-sonnet-35-pinecone-and-creatomate/,"This workflow allows you to generate riddle-themed vertical videos (9:16), render them using Creatomate, and upload them directly to YouTube ‚Äî all automatically. It's optimized for low-cost operation while still integrating AI and automation at key stages.
Here is a sample: https://youtube.com/shorts/svhgti9L6Fw?feature=share
üîÑ Workflow Overview
This is a schedule-triggered workflow that:
Generates riddles with answers using an AI agent.
Checks Pinecone vector store to avoid repeating riddles.
Inserts new riddles into Pinecone under a namespace structured by month/year for easier indexing.
Fetches an unused music track URL for video background.
Generates a video using Creatomate with the riddles and audio.
Uploads the video to YouTube via a custom HTTP request with OAuth.
Updates the worksheet and riddle numbering, and notifies via email.
üîó Integration Guide
üîé Pinecone Setup
You need an active Pinecone account with an index created.
The vector index should be accessible from n8n.
Follow [n8n's Pinecone Node Documentation] for help setting up credentials and connecting.
üìù Google Sheet Setup
Open the sheets template provided in the Gumroad documentation pdf.
Make a copy to your Google Drive.
Set up the required credentials in n8n and link the Google Sheets node to your new copy.
üéß Audio Tracks
Get copyright-free tracks from Royalty Free Music Library.
Ensure files are in .mp3 format for compatibility.
üéûÔ∏è Background Videos (Optional)
For optional background visuals, use free stock videos from Pexels.
üí° Creatomate Setup
Create a Creatomate account at [creatomate.com].
Create a 9:16 aspect ratio template.
Press F12 to open the source editor.
Download the Creatomate JSON file from the Gumroad page and paste its contents into the source editor.
Refresh the page and click ""Use Template"" > API Integration.
Copy the cURL.
From the cURL, extract your Bearer Token and Template ID.
Paste them in the respective fields in the Creatomate Render Node inside n8n.
üìâ YouTube Upload Configuration
Inside n8n, set up a YouTube OAuth credential.
Search for ""YouTube"" in the credentials list and select the OAuth type.
Use the Custom HTTP Request node with multipart/form-data to upload the video using:
Title (e.g., ""Riddles4U #001"")
Description
Video file binary
Category ID (find ID here: https://gist.github.com/dgp/1b24bf2961521bd75d6c)
üîÑ Workflow Logic Detail
Riddle Generation
Checks Pinecone vector store for existing riddles (to avoid duplicates).
Generates 4 new riddles with answers.
Stores them in Pinecone under month/year namespace.
Extracts a numbered title for current riddle post (e.g., Riddles4U #001).
Prepares next riddle title counter (but only updates after YouTube post succeeds).
Track URL Fetching
Pulls the first unused track from Google Sheets.
Render & Upload
Sends the riddles + track to Creatomate for rendering.
Fetches binary video and uploads it directly to YouTube via HTTP.
Note: For video source url, I downloaded a video, uploaded it to drive and used that video drive link. You can add a new sheet and rotate videos too, although youtube likes consistency in shorts so I think it is best to have one very beautiful and calm background video then change to a new one after some months or when you generate like 50 shorts.
Post Actions
Sends email with YouTube video link.
Marks used audio row as ""used"".
Updates to next riddle title.
If all tracks have been used, resets them to ""unused"".
üöÄ Customization Ideas
Add AI-generated voiceover to the riddles.
Replace riddles with trivia, quotes, or jokes.
Integrate TikTok or Instagram upload APIs for multichannel publishing.
‚úâÔ∏è Support
Refer to the comprehensive setup guide included in the Gumroad download folder to configure your template, credentials, and API keys correctly.
This is a beginner-friendly, low-cost automation you can build on for more advanced content pipelines. Happy building!
You can find me on twitter and via email incase you need any custom n8n workflows or wanna chat."
Monitor GitHub Releases with Gemini AI Chinese Translation & Slack Notifications,https://n8n.io/workflows/3452-monitor-github-releases-with-gemini-ai-chinese-translation-and-slack-notifications/,"Overview
This n8n template monitors specified GitHub repositories. When a new release is published, it automatically fetches the information, uses AI (Google Gemini by default) to summarize and translate it into Chinese, and sends a formatted notification to a designated Slack channel.
Core Features:
Automated Monitoring: Checks for updates on a predefined schedule.
Intelligent Processing: Uses AI to extract key information and translate.
Error Handling: Sends an error notification if fetching RSS for a single repository fails, without affecting others.
Duplicate Prevention: Remembers the last processed release ID using Redis to ensure only new content is pushed.
Prerequisites
Slack: Configure your Slack app credentials in n8n.
Redis: Have an available Redis service and configure its credentials in n8n.
AI Provider (Gemini): Configure credentials for Google Gemini (or your chosen AI model) in n8n.
Configuration Instructions
After importing the template, you need to modify the following key nodes:
Cron Trigger:
Adjust the Rule setting to change the update check frequency (default is 0 */10 9-23 * * *, checking every 10 minutes between 9 AM and 11 PM daily).
GitHub Config (Repository List - Code Node):
Edit the JavaScript array within this node's code area.
Modify or add the repositories you want to follow. Each repository object needs a name (custom display name) and github (format: owner/repo).
Example:
{
  ""name"": ""n8n"", // Custom display name
  ""github"": ""n8n-io/n8n"" // GitHub path
},
{
  ""name"": ""LobeChat"",
  ""github"": ""lobehub/lobe-chat""
}
// ... add more repositories
Redis and Redis2 (Redis Connection):
Select your configured Redis credentials in both nodes.
Gemini (AI Model):
Select your configured Google Gemini credentials.
(Optional) Replace with a different supported AI model node and select its credentials.
Information Extractor (AI Processing & Translation):
Main Configuration: Review the System Prompt. By default, it asks the AI to extract information and translate it into Chinese. Modify this prompt if you need a different language or summary style.
Send Message and Send Error (Slack Notifications):
Select your configured Slack credentials in both Slack nodes.
Set the target Channel ID for notifications.
Workflow Overview
Start: Cron Trigger initiates the workflow on schedule.
Load Config: GitHub Config provides the list of repositories to monitor.
Loop: The Loop node iterates through each repository.
Fetch & Check:
The RSS node attempts to fetch the repository's releases feed.
If No Error checks for success:
Failure: Send Error posts an error to Slack, skips this repository.
Success: Continues.
Check for New Release:
The Redis node retrieves the last recorded Release ID for this repository.
The If New node compares the latest Release ID with the recorded ID:
Different IDs (New Release): Proceeds to processing.
Same ID (Already Processed): Skips this repository.
Process & Notify (Only for New Releases):
Information Extractor (with Gemini) extracts, summarizes, and translates the content.
The Code node formats the information into Slack Block Kit.
Send Message sends the formatted message to Slack.
The Redis2 node stores the current Release ID in Redis.
End: The workflow finishes after processing all repositories.
Conclusion
Once configured, this template automates GitHub release monitoring, uses AI to distill key information, and delivers it efficiently to your Slack workspace."
Generate Conversational Twitter/X Threads with GPT-4o AI,https://n8n.io/workflows/3441-generate-conversational-twitterx-threads-with-gpt-4o-ai/,"üßµ Generate Conversational Twitter/X Threads with GPT-4o AI (n8n Workflow)
This workflow uses OpenAI (GPT-4o) and Twitter/X to automatically generate and publish engaging, conversational threads in response to a trigger (e.g., from a chatbot or form).
üöÄ What Does It Do?
Listens for an incoming message (e.g., via webhook or another n8n input).
Uses GPT-4o to craft a narrative-style Twitter thread in a personal, friendly tone.
Publishes the first tweet, then automatically posts each following tweet as a reply‚Äîbuilding a full thread.
üõ†Ô∏è What Do You Need to Configure?
Before using this template, make sure to set up the following credentials:
OpenAI
Add your OpenAI API key in the OpenAI Chat Model node. This is used to generate the thread content.
Twitter/X
Add your Twitter OAuth2 credentials to the First Tweet and Thread Reply nodes. This allows the workflow to publish tweets on your behalf.
‚ú® Who Is This For?
This template is perfect for:
Content creators who want to share ideas regularly
Personal brands looking to grow their presence
Social media managers automating thread creation
üîß How to Customize It
You can easily adjust the tone, structure, or length of the threads by modifying the system prompt in the OpenAI node.
For example:
To create threads with humor, change the prompt to ‚ÄúWrite in a witty and humorous tone.‚Äù
To tailor it for marketing, prompt it with ‚ÄúWrite a persuasive product-focused Twitter thread.‚Äù
You can also integrate this workflow with:
Telegram bots
Web forms (e.g., Typeform, Tally)
CRM tools or newsletter platforms
üìã Sample Output
Prompt sent to the workflow:
‚ÄúTips for growing on Twitter in 2025‚Äù
Generated thread:
++Tweet 1:++
Thinking of growing your presence on Twitter/X in 2024? Here's a thread with the most effective strategies that actually work üßµ
++Reply 1:++
Engage, don‚Äôt broadcast
Twitter is a conversation platform. Reply to others, quote-tweet, and start discussions instead of just posting links.
++Reply 2:++
2. Consistency beats virality
Tweeting regularly builds trust and visibility. You don't need to go viral ‚Äî just show up."
Generate Multiple T-shirt Design Prompts From Images With GPT-4o,https://n8n.io/workflows/3430-generate-multiple-t-shirt-design-prompts-from-images-with-gpt-4o/,"Disclaimer
This Workflow uses a community node.
What It Does
Workflow Demonstration
This powerful workflow eliminates hours of trial and error when trying to craft the perfect prompt for AI-generated images. By simply saving an image of the design you want to model your generated images after into a designated folder on your desktop, the workflow is automatically triggered. Once the image is saved, n8n pulls it into the workflow, where an Analyze Image AI agent powered by GPT-4o analyzes it and generates a detailed text description. This description is then passed to another Prompt/Text Generator AI agent, which uses it as a template to create five entirely new and unique image prompts using the power of GPT-4o. These prompts produce images that share a similar art style with the original, while remaining creatively distinct. In addition, the AI will extract the original phrase and generate nine brand-new phrases that can be incorporated into the image. Finally, all this information is automatically saved into a folder on your computer, ready for you to use whenever you like.
Who This Is For
This workflow is ideal for anyone looking to create high-quality AI-generated images with ease. It‚Äôs especially valuable for those running a print-on-demand business or working in logo and artwork design. By automating the process of crafting image prompts that match a desired style, it saves you hours of manual effort and guesswork. With this streamlined system, you can focus less on prompt engineering and more on what truly matters‚Äîgrowing your business.
How It Works
Once you find an image or design that perfectly matches the style you're aiming for, the rest of the process is seamless. With the workflow running, simply save the image to the designated folder that triggers the automation. This action activates the workflow, and the image is automatically pulled into n8n using the Get Image From File node.
From there, the Analyze Image AI Agent, powered by GPT-4o, takes over. It examines the image in depth‚Äîassessing its overall theme, art style, text phrase, font, aesthetic, and complexity (whether it‚Äôs a simple design or a more detailed composition). Based on this analysis, it generates a detailed description of the image along with the original text phrase it contains.
Once the analysis is complete, a second AI agent, Prompt/Text Generator, receives this information and uses GPT-4o to create five unique image prompts. These prompts are designed to replicate the essence and style of the original image, while still producing completely original results when used with any AI image generator. In addition to the original phrase, the agent also generates nine brand-new text phrases.
By combining each of the five prompts with the ten total text phrases (original + nine new), the workflow gives you the potential to generate 50 unique image variations.
Finally, the Save to File node takes the entire output from the Prompt/Text Generator‚Äîincluding all five image prompts and ten text phrases‚Äîthat was converted to text and automatically saves it into a designated folder on your computer for future use.
After a completed workflow has been saved to the designated folder you must change the name of the file prior to running the workflow again. The workflow will only save the same file name each time.
Set Up Steps
You will need to create a folder on your computer where you would like to save your original image. You will then copy the pathway to this folder into the local file trigger node.
You will need to obtain an Open AI API key from platform.openai.com/api-keys
After you obtain this Open AI API key you will need to connect it to the Analyze Image AI Agent and the Open AI Chat Model connected to the Prompt/Text Generator AI Agent.
You will now need to fund your Open AI account. GPT 4o costs ~$0.01-$0.02 to run the workflow.
Finally create a folder on your computer you wish to have the summarizations saved to. Copy the pathway to this folder into the Save to File node."
Enhance AI Prompts with GPT-4o-mini and Telegram Delivery,https://n8n.io/workflows/3496-enhance-ai-prompts-with-gpt-4o-mini-and-telegram-delivery/,"Workflow Documentation
Description:
This workflow is designed to optimize prompts by enhancing user inputs for clarity and specificity using AI. The workflow takes a user-provided prompt as input and uses a Natural Language Processing (NLP) model to refine and improve the prompt. The optimized prompt is then sent back to the user, ready for use in further workflows or processes.
Setup:
This workflow is suitable for users who want to improve their prompts for better communication and understanding in their workflows.
The workflow utilizes an AI Agent powered by an OpenAI Chat Model to enhance user prompts.
Expected Outcomes:
Users can provide vague or imprecise prompts as input to the workflow.
The AI Agent will refine and optimize the prompt, adding clarity and specific details.
The optimized prompt will be delivered back to the user via Telegram or can be input for the next nodes.
Extra Information:
A. A Telegram node is used to deliver the optimized prompt back to the user.
B. Ensure you have the necessary credentials set up for Telegram and OpenAI accounts.
C. Customize the workflow's settings, such as the AI model used for prompt optimization, to suit your requirements.
D. Activate the workflow once all configurations are set to start optimizing prompts efficiently."
Automatically Create Linear Issues from Gmail Support Request Messages,https://n8n.io/workflows/3899-automatically-create-linear-issues-from-gmail-support-request-messages/,"This n8n template watches a Gmail inbox for support messages and creates an equivalent issue item in Linear.
How it works
A scheduled trigger fetches recent Gmail messages from the inbox which collects support requests.
These support requests are filtered to ensure they are only processed once and their HTML body is converted to markdown for easier parsing.
Each support request is then triaged via an AI Agent which adds appropriate labels, assesses priority and summarises a title and description of the original request.
Finally, the AI generated values are used to create an issue in Linear to be actioned.
How to use
Ensure the messages fetched are solely support requests otherwise you'll need to classify messages before processing them.
Specify the labels and priorities to use in the system prompt of the AI agent.
Requirements
Gmail for incoming support messages
OpenAI for LLM
Linear for issue management
Customising this workflow
Consider automating more steps after the issue is created such as attempting issue resolution or capacity planning."
Convert HTML to PDF and Compress Files with CustomJS API,https://n8n.io/workflows/3869-convert-html-to-pdf-and-compress-files-with-customjs-api/,"This n8n template demonstrates how to convert HTML into a PDF, compress the generated PDF, and return it as a binary response using the PDF Toolkit from www.customjs.space.
Notice
Community nodes can only be installed on self-hosted instances of n8n.
@custom-js/n8n-nodes-pdf-toolkit
What this workflow does
Convert the requested HTML to PDF.
Compress the PDF file.
Use a Code node to handle URLs pointing to PDF files if they exceed 6MB.
Compress the PDF pages.
Requirements
Self-hosted n8n instance
CustomJS API key for compressing PDF files.
HTML Data to convert PDF files
Code node for handling URL that indicates PDF file.
Workflow Steps:
Manual Trigger:
Runs with user interaction.
HTML to PDF:
Request HTML Data
Convert HTML to PDF
Request PDF from URL.
Compress Pages from PDF:
Compress PDF as a binary file.
Usage
Get API key from customJS
Sign up to customJS platform.
Navigate to your profile page
Press ""Show"" button to get API key
Set Credentials for CustomJS API on n8n
Copy and paste your API key generated from CustomJS here.
Design workflow
A Manual Trigger for starting workflow.
HTTP Request Nodes for downloading PDF files.
Code node for handling URL that indicates PDF file.
Compress PDF files.
You can replace logic for triggering and returning results.
For example, you can trigger this workflow by calling a webhook and get a result as a response from webhook. Simply replace Manual Trigger and Write to Disk nodes."
Message Buffer System with Redis and GPT-4 for Efficient Processing,https://n8n.io/workflows/3832-message-buffer-system-with-redis-and-gpt-4-for-efficient-processing/,"Description
This workflow implements a message-batching buffer using Redis for temporary storage and GPT-4 for consolidated response generation. Incoming user messages are collected in a Redis list; once a configurable ‚Äúinactivity‚Äù window elapses or a batch size threshold is reached, all buffered messages are sent to GPT-4 in a single prompt. The system then clears the buffer and returns the consolidated reply.
Key Features
Redis-backed buffer to queue incoming messages per user session
Dynamic wait time (shorter for long messages, longer for short messages)
Batch trigger on inactivity timeout or minimum message count
GPT-4 consolidation: merges all buffered messages into one coherent response
Setup Instructions
Map Input
Rename node to ‚ÄúExtract Session & Message‚Äù
Assign context_id and message from webhook or manual trigger
Compute Wait Time
Rename node to ‚ÄúDetermine Inactivity Timeout‚Äù
JS Code:
const wordCount = $json.message.split(' ').filter(w=&gt;w).length;
return [{ json: {
  context_id: $json.context_id,
  message: $json.message,
  waitSeconds: wordCount &lt; 5 ? 45 : 30
}}];
Buffer Message in Redis
Push into list buffer_in:{{$json.context_id}}
INCR key buffer_count:{{$json.context_id}} with TTL {{$json.waitSeconds + 60}}
Mark Waiting State
GET waiting_reply:{{$json.context_id}} ‚Üí if null, SET it to true with TTL {{$json.waitSeconds}}
Rename nodes to ‚ÄúCheck Waiting Flag‚Äù / ‚ÄúSet Waiting Flag‚Äù
Wait for Inactivity
Wait node: pause for {{$json.waitSeconds}} seconds
Check Batch Trigger
GET keys:
last_seen:{{$json.context_id}}
buffer_count:{{$json.context_id}}
IF both:
buffer_count &gt;= 1
(now ‚Äì last_seen) &gt;= waitSeconds * 1000
Rename node to ‚ÄúTrigger Batch on Inactivity or Count‚Äù
Fetch & Consolidate
GET entire list buffer_in:{{$json.context_id}}
Information Extractor ‚Üí rename to ‚ÄúConsolidate Messages‚Äù
System prompt: ‚ÄúYou are an expert at merging multiple messages into one clear paragraph without duplicates.‚Äù
GPT-4 Chat
OpenAI Chat Model (GPT-4)
Cleanup & Respond
Delete Redis keys:
buffer_in:{{$json.context_id}}
waiting_reply:{{$json.context_id}}
buffer_count:{{$json.context_id}}
Return the consolidated reply to the user
Customization Guidance
Batch Size Trigger: Add an additional IF to fire when buffer_count reaches your desired batch size.
Timeout Policy: Adjust the word-count thresholds or replace with character-count logic.
Multi-Channel Support: Change the trigger from a manual test node to any webhook (e.g., chat, SMS, email).
Error Handling: Insert a fallback branch to catch Redis timeouts or OpenAI API errors and notify users.
üì£ ¬°Con√©ctate con Innovatex!
üîó Encuentra todos nuestros enlaces en Linktree: innovatexiot.carrd.co
üîó Conecta conmigo en LinkedIn: Edison Andr√©s Garc√≠a Herrera"
Error Handling System with PostgreSQL Logging and Rate-Limited Notifications,https://n8n.io/workflows/3882-error-handling-system-with-postgresql-logging-and-rate-limited-notifications/,"Log errors and avoid sending too many emails
Use case
Most of the time, it‚Äôs necessary to log all errors that occur. However, in some cases, a scheduled task or service consuming excessive resources might trigger a surge of errors.
To address this, we can log all errors but limit alerts to a maximum of one notification every 5 minutes.
What this workflow does
This workflow can be configured to receive error events, or you can integrate it before your own error-handling logic.
If used as the primary error handler, note that this flow will only add a database log entry and take no further action. You‚Äôll need to add your own alerts (e.g., email or push notifications). Below is an example of a notification setup I prefer to use.
At the end, there‚Äôs an error cleanup option. This feature is particularly useful in development environments.
If you already have an error-handling workflow, you can call this one as a sub-workflow. Its final steps include cleanup logic to reset the execution state and terminate the workflow.
Setup
Verify all Postgres nodes and credentials when using the 'Error Handling Sample'
How to adjust it to your needs
You can set this workflow as a sub-workflow within your existing error-handling setup.
Alternatively, you can add the ""Error Handling Sample"" at the end of this workflow, which sends email and push notifications.
Configuration Requirements:
‚ö†Ô∏è You must create a database table for this to work!
DDL of this sample:
create table p1gq6ljdsam3x1m.""N8Err""
(
id serial
primary key,
created_at timestamp,
updated_at timestamp,
created_by varchar,
updated_by varchar,
nc_order numeric,
title text,
""URL"" text,
""Stack"" text,
json json,
""Message"" text,
""LastNode"" text
);
alter table p1gq6ljdsam3x1m.""N8Err""
owner to postgres;
create index ""N8Err_order_idx""
on p1gq6ljdsam3x1m.""N8Err"" (nc_order);
by Davi Saranszky Mesquita
https://www.linkedin.com/in/mesquitadavi/"
üöÄ YouTube Comment Sentiment Analyzer with Google Sheets & OpenAI,https://n8n.io/workflows/3855-youtube-comment-sentiment-analyzer-with-google-sheets-and-openai/,"üöÄ YouTube Comment Sentiment Analyzer with Google Sheets & OpenAI
Who Should Use This?
Influencers, marketers, and data teams who need instant insights into audience sentiment‚Äîwithout manual exports or scattered tools.
The Challenge
Manual exports from YouTube Studio
Time-consuming sentiment tagging
Data scattered across multiple platforms
Our workflow automates everything: from fetching comments to logging analysis‚Äîso you can focus on insights, not spreadsheets.
What You‚Äôll Get
Dynamic Input
Read a list of YouTube URLs from your Google Sheet.
Full Comment Harvest
Pull all top-level comments (handles pagination 100/page).
Deep Sentiment Scan
Classify each comment as Positive, Neutral, or Negative using OpenAI.
Smart Formatting
Capture metadata (author, likes, timestamp) alongside sentiment.
Seamless Storage
Append or update rows in your Google Sheet‚Äîready for reporting.
Easy Setup
Prepare Google Sheet
Create a sheet with a video_urls column (full YouTube links).
Add and authorize a Google Sheets Oauth or service-account credential in n8n.
Enable YouTube API
Activate Data API v3 in Google Cloud, grab an API key, and save as an HTTP credential in n8n.
Configure OpenAI
Enter your API key under the ‚ÄúOpenAI Chat‚Äù credential in n8n.
Import the Workflow
Paste the provided JSON into n8n.
Run Manually
Use the Manual Trigger node to start fetching and analyzing comments on demand.
Customize to Your Needs
Filter Comments: Add an IF node to process only comments with specific keywords or minimum likes.
Automate Schedule: Swap the Manual Trigger for a Cron node if you later want periodic runs.
Extended Analysis: Swap sentiment classification for topic extraction, summarization, or translation by tweaking the LLM prompt.
Alternate Destinations: Replace Google Sheets with Airtable, Notion, or any database node.
Tags
YouTube Google Sheets OpenAI Sentiment Analysis n8n Manual Trigger"
Structured Data Extract & Data Mining with Bright Data & Google Gemini,https://n8n.io/workflows/3853-structured-data-extract-and-data-mining-with-bright-data-and-google-gemini/,"Who this is for?
The Structured Data Extract & Data Mining workflow is crafted for researchers, content analysts, SEO strategists, and AI developers who need to transform semi-structured web data (like markdown content or scraped HTML) into actionable structured datasets.
It is ideal for:
Content Analysts - Organizing and mining large volumes of markdown or HTML content.
SEO & Trend Researchers - Exploring topics by location and category.
AI Engineers & NLP Developers - Looking to automate insight extraction from unstructured inputs.
Growth Marketers - Tracking topic-level trends for strategic campaigns.
Automation Specialists - Streamlining workflows from scrape to storage.
What problem is this workflow solving?
Extracting insights from markdown or HTML documents typically requires manual review, formatting, and parsing. This becomes unscalable when dealing with large datasets or when real-time response is needed. Additionally, trend and topic extraction usually involves external tools, custom scripts, and inconsistent formatting.
This workflow solves:
Automatic text extraction from markdown or structured content.
Location and category-based trend mining with semantic grouping.
AI-driven topic extraction and summarization
Real-time notification via webhook with rich structured payloads.
Persistent storage of mined data to disk for audits or further processing.
What this workflow does
Receives input: Sets the URL for the data extraction and analysis.
Uses Bright Data's Web Unlocker to extract content from relevant sites.
A Markdown/Text Extractor node parses the content into clean plaintext
The cleaned data is passed to Google Gemini to:
Identify trends by location and category
Extract key topics and themes
Format the response into structured JSON
The structured insights are sent via Webhook Notification to external systems (e.g., Slack, Web apps, Zapier)
The final output is saved to disk
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Set URL and Bright Data Zone for setting the brand content URL and the Bright Data Zone name.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Update Source : Update the workflow input to read from Google Sheet or Airbase for dynamically tracking multiple brands or topics.
Gemini Prompt Customization :
Extract trends within a custom category (e.g., E-commerce design patterns in the US)
Output topics with popularity metrics
Structure the output as per your database schema (e.g., [{ topic, trend_score, location }])
Webhook Output : Send notifications to -
Slack ‚Äì with AI summaries in rich blocks
Internal APIs ‚Äì for use in dashboards
Zapier/Make ‚Äì for multi-step automation
Persistence
Save output to:
Remote FTP or SFTP storage
Amazon S3, Google Cloud Storage etc."
Generate YouTube Video Summaries with SearchAPI Transcripts and LLM,https://n8n.io/workflows/3828-generate-youtube-video-summaries-with-searchapi-transcripts-and-llm/,"üé• Summarize YouTube Videos using SearchApi & LLM
Who is this for?
This workflow is ideal for content creators, students, digital marketers, educators, and researchers who want to quickly summarize YouTube videos.
What problem does this workflow solve?
Manually extracting important information from lengthy YouTube videos can be tedious and prone to errors. This workflow streamlines the process by automatically fetching video transcripts using SearchApi.io and producing concise, informative summaries through a summarization chain powered by any LLM provider. This allows users to quickly access crucial information without the need for manual transcription or detailed viewing.
What this workflow does
Fetches the complete transcript of a YouTube video using SearchApi.
Combines the retrieved transcript into a single, continuous text.
Utilizes a Summarization Chain with an LLM (e.g., OpenRouter models) to create a concise summary of the video content.
Setup
Install the SearchApi community node:
Open Settings ‚Üí Community Nodes inside your self‚Äëhosted n8n instance.
Fill npm Package Name with @searchapi/n8n-nodes-searchapi.
Accept the risk prompt, and hit Install.
It should now appear as a node when you search for it.
API Configuration:
Set up your SearchApi.io credentials in n8n.
Add your preferred LLM provider credentials (e.g., OpenRouter API).
Input Requirements:
Provide the YouTube video ID (e.g., wBuULAoJxok).
Connect LLM Integration:
Configure the summarization chain with your chosen model and parameters for text splitting.
How to customize this workflow to meet your needs
Adjust the summarization model or modify text-splitter parameters to accommodate different lengths and complexities of video transcripts.
Integrate additional nodes to export summaries directly into your preferred tools, such as Google Drive, Slack, or email.
Customize prompt templates in the summarization chain to obtain various summary styles (bullet points, paragraphs, etc.).
Modify the trigger to suit your workflow.
Example Usage
Input: YouTube video ID (wBuULAoJxok).
Output: A concise, actionable summary that highlights key ideas, recommendations, and insights from the video."
Convert 3-View Drawings to 360¬∞ Videos with GPT-4o-Image and Kling API,https://n8n.io/workflows/3716-convert-3-view-drawings-to-360-videos-with-gpt-4o-image-and-kling-api/,"What this workflow does?
This workflow converts orthographic three-view drawings into 360¬∞ rotation videos through PiAPI's GPT-4o-Image and Kling APIs (unofficial). The workflow could be set with our 3D Figurine Orthographic Views workflow for generation.
Who is the workflow for?
Designers: Generate inspiration into 3D designs and make them spin to gain concrete details in a efficient way.
Online shoppers: Show protential products from all angles in videos and preview overall texture of models.
Content Creators (including toy bloggers): Make fun videos of collectible models.
Step-by-step Instructions
1.Fill in basic params with your X-API-Key of your PiAPI account and 3-View image url.

2.Click test workflow.
3.Get the final video in the last node.
Use Case
Input Image
Output Video"
"AI-Powered YouTube Meta Generator with GPT-4o, Gemini & Content Enrichment",https://n8n.io/workflows/3788-ai-powered-youtube-meta-generator-with-gpt-4o-gemini-and-content-enrichment/,"üéØ AI-Powered Advanced YouTube Meta Generator in n8n
Automatically generate SEO-optimized YouTube Titles, Descriptions, Tags & Hashtags ‚Äì enriched with blog articles, affiliate links, and product recommendations!
Who is this for?
This workflow is built for content creators, affiliate marketers, educators, and agencies who want to instantly enhance their YouTube videos with smart metadata and contextual linking ‚Äî without wasting hours on research or copywriting.
üß† What problem does it solve?
Writing compelling, keyword-rich video metadata is essential for discoverability, engagement, and monetization. But doing this manually for every video is time-consuming. This workflow automates:
Title & description writing
Tag & hashtag generation
Transcript analysis
Affiliate link embedding
Internal link recommendations (related blogs & videos)
All in one flow ‚Äî saving hours per video.
‚öôÔ∏è What this workflow does
üîó Accepts a YouTube video link + optional keywords
üß† Uses Gemini or GPT-4o to analyze the transcript
üìù Auto-generates:
SEO-friendly Title (under 70 chars)
Catchy, benefit-focused Description with timestamps
Tags (450+ chars)
Hashtags (5‚Äì10 optimized)
üîÅ Fetches your blog sitemap and matches relevant articles
üìπ Finds similar past videos using AI
üõç Embeds recommended affiliate links via Airtable
üîß Updates YouTube video with new metadata via API
üõ† Setup
üîå Connect APIs:
YouTube Data API (OAuth 2.0)
Gemini or OpenAI GPT-4o
Airtable (for affiliate links)
Kome AI for transcripts
WordPress sitemap (for internal links)
üìã Deploy this workflow and open the form URL to input:
YouTube Video Link
(Optional) Focus Keywords
üí¨ Connect your accounts and authorize required scopes
üß† AI will handle the rest: from fetching data to publishing metadata.
‚úèÔ∏è How to customize this workflow
Replace Gemini with OpenAI / Claude / DeepSeek in the AI nodes
Point the sitemap node to your own blog
Modify the Airtable structure for affiliate links:
Name, Type, Platform, URL, Keywords
Change tag/hashtag formatting
Modify prompt instructions in AI nodes for brand tone
üìå Sticky Notes Included
‚ö° Form: ‚ÄúEnter Video Link + Optional Keywords‚Äù
üîç Sitemap Extraction: ‚ÄúGet blog URLs for related links‚Äù
üß† AI Logic: ‚ÄúGenerate Metadata‚Äù
‚úÖ Update Metadata: ‚ÄúPublish updated title/description/tags‚Äù
üßæ Completion Confirmation + Attribution to Amjid Ali
üåê Useful Links
üß† Learn n8n Automation
üéì n8n Learning Guidebook
üöÄ n8n Cloud Signup
üîß SyncBricks Tools & Blog
üõç Affiliate Product Hub
üîó Why this workflow?
This workflow is ideal for users exploring n8n AI, working with the n8n API, building AI agents in n8n, deploying via Docker or MCP, integrating with GitHub, automating processes using n8n automation, leveraging tools like n8n chat extension, enhancing YouTube metadata with AI, and incorporating cutting-edge platforms such as Context7, Blotato, and OpenAI, whether using n8n Cloud or setting up self-hosted n8n installations."
CSV to HubSpot Uploader with Dynamic Field Mapping and Google Sheets Integration,https://n8n.io/workflows/3785-csv-to-hubspot-uploader-with-dynamic-field-mapping-and-google-sheets-integration/,"Who is this for?
This n8n workflow is designed for Customer Success Managers (CSM), marketers, sales teams, and data administrators who need to automate the process of uploading and processing CSV data in HubSpot. It is ideal for users who regularly import contact lists, update CRM records, or sync data between systems.
What problem does this workflow solve?
Manually uploading and processing CSV files in HubSpot can be time-consuming and error-prone, especially when dealing with large datasets or complex field mappings. This workflow automates data validation, indexing, and field mapping, reducing manual effort and ensuring data consistency.
What this workflow does
Generating the list of the fields directly from Hubspot API
Indexing: Organizes and prepares CSV data for HubSpot import.
Data Processing: Cleanses and transforms data.
Field Mapping: Maps CSV columns to HubSpot fields dynamically.
Import Execution: Uploads processed data to HubSpot
Setup
Prerequisites:
HubSpot API credentials (Private App token).
Google sheets credentials.
n8n instance (cloud or self-hosted).
Installation:
Import the workflow JSON into n8n.
Configure the HubSpot nodes and the Google Sheets nodes with your API credentials.
Upload your CSV file to the workflow via the form.
Customization
Data Filters: Add nodes to filter/transform data (e.g., deduplication, formatting).
Fields Filters: according to your needs
Add a Hubspot Object: To the list in ""Define array of objects"" node
Workflow Triggers: Set up triggers (e.g., schedule, webhook) for automated runs."
Assign Requests Using AI and Send Reminders Based On NocoDB Kanban Board Status,https://n8n.io/workflows/3784-assign-requests-using-ai-and-send-reminders-based-on-nocodb-kanban-board-status/,"Who is it for?
This is automation for support project manager, which helps not only to keep developres informed but also automatically keep clients in the loop - especially useful if you are managing SLA-like agreement.
It is actually simple incident management board using free Kanban board, that is extended in functionality via N8N.
How It Works?
Script has two entry points.
The first one is incident form. When incident details are provided, automation gets incident definitions from database and pushes both information to AI. AI comparse definitions with client request, refines incident priority and pushed it in NocoDB database.
Second is schedule trigger, which is responsible for regular notificaitons on task status. If task is not picked up or delivered in proper time, then emails or slack messages are being sent both to client and responsible developer.
How to set up?
Clone automation
Create (samples below) two NocoDB tables: one with definitions and second that servers as Kanban board (mind column naming!)
Set up email and slack connection
You should be ready to go
Different incident naming
If your incident level naming is different, you need to update few nodes and few columns in NocoDB. This is because incident naming must be unified through: automation flow, incident definitions and column NocoDB select fields.
So be sure that following is the same:
NocoDB: Incident definitions, column ""Title""
NocoDB: Tasks table, single select fields:
""expected category""
""assigned category""
N8N: Incident Form ""Incident Desired Category""
NocoDB Tables
Incident definitions table
|Title |Definition |Response time|Resolution time|Default assignee|
|single line text|text|number|number|email|
Tasks table
|email|message|expected category|internal notes|assigned category|status|expected response|expected resolution|assignee|assignee slack|
|email|text|single select|text|single select|single select|date and time|date and time|email|slack username|
Use kanban board
Simply set up Kanban view and stack by ""status"" field.
What's More?
That's actually it. I hope that this automation will help your support line be much more streamlined!
There is actually more that you could do with this automation, but it really depends on your needs. For example, you could add Email trigger to handle incoming support requests (but remember to adjust nodes accordingly). Another thing is that you could make different notification schema, depending on your needs (for example I do imagine that you may want a day or two delay before you notify client that task is after due).
Thank you, perfect!
Glad I could help. Visit my profile for other automations for businesses. And if you are looking for dedicated software development, do not hesitate to reach out!"
"Extract, Transform LinkedIn Data with Bright Data MCP Server & Google Gemini",https://n8n.io/workflows/3777-extract-transform-linkedin-data-with-bright-data-mcp-server-and-google-gemini/,"Disclaimer
This template is only available on n8n self-hosted as it's making use of the community node for MCP Client.
Who this is for?
The Extract, Transform LinkedIn Data with Bright Data MCP Server & Google Gemini workflow is an automated solution that scrapes LinkedIn content via Bright Data MCP Server then transforms the response using a Gemini LLM. The final output is sent via webhook notification and also persisted on disk.
This workflow is tailored for:
Data Analysts : Who require structured LinkedIn datasets for analytics and reporting.
Marketing and Sales Teams : Looking to enrich lead databases, track company updates, and identify market trends.
Recruiters and Talent Acquisition Specialists : Who want to automate candidate sourcing and company research.
AI Developers : Integrating real-time professional data into intelligent applications.
Business Intelligence Teams : Needing current and comprehensive LinkedIn data to drive strategic decisions.
What problem is this workflow solving?
Gathering structured and meaningful information from the web is traditionally slow, manual, and error-prone.
This workflow solves:
Reliable web scraping using Bright Data MCP Server LinkedIn tools.
LinkedIn person and company web scrapping with AI Agents setup with the Bright Data MCP Server tools.
Data extraction and transformation with Google Gemini LLM.
Persists the LinkedIn person and company info to disk.
Performs a Webhook notification with the LinkedIn person and company info.
What this workflow does?
This n8n workflow performs the following steps:
Trigger: Start manually.
Input URL(s): Specify the LinkedIn person and company URL.
Web Scraping (Bright Data): Use Bright Data's MCP Server, LinkedIn tools for the person and company data extract.
Data Transformation & Aggregation: Uses the Google LLM for handling the data transformation.
Store / Output: Save results into disk and also performs a Webhook notification.
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.

Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>.
Update the LinkedIn URL person and company workflow.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
Update the file name and path to persist on disk.
How to customize this workflow to your needs
Different Inputs: Instead of static URLs, accept URLs dynamically via webhook or form submissions.
Data Extraction: Modify the LinkedIn Data Extractor node with the suitable prompt to format the data as you wish.
Outputs: Update the Webhook endpoints to send the response to Slack channels, Airtable, Notion, CRM systems, etc."
Paul Graham Essay Search & Chat with Milvus Vector Database,https://n8n.io/workflows/3576-paul-graham-essay-search-and-chat-with-milvus-vector-database/,"Paul Graham Essay Search & Chat with Milvus Vector Database
How It Works
This workflow creates a RAG (Retrieval-Augmented Generation) system using Milvus vector database to search Paul Graham essays:
Scrape & Load: Fetches Paul Graham essays, extracts text, and stores them as vector embeddings in Milvus
Chat Interface: Enables semantic search and AI-powered conversations about the essays
Set Up Steps
Set up Milvus server following the official installation guide, then create a collection
Execute the workflow to scrape essays and load them into your Milvus collection
Chat with the AI agent using the Milvus tool to query and discuss essay content"
AI-Personalized Multi-Product Email Outreach with SMTP Rotation (GPT-4o/o3-mini),https://n8n.io/workflows/3758-ai-personalized-multi-product-email-outreach-with-smtp-rotation-gpt-4oo3-mini/,"Multi Product AI Email Automation with SMTP Rotation & No-Code Workflow(OpenAI GPT-4o or o3-mini)
üöÄ Why This Workflow Matters
True Multi-Offer Campaigns
Promote any mix of products, services or solutions in one run; no separate workflows needed.
SMTP Load Balancing
A Switch node rotates sends across five Gmail/SMTP accounts to spread volume, avoid rate limits and boost deliverability.
Model Flexibility
Supports GPT-4o, GPT-3 or OpenAI o3-mini for query generation and email personalization.
Personalized Outreach
The AI reads each prospect‚Äôs website, then crafts concise (<200 words) HTML emails tailored to their needs, embedding the right offer and link.
SEO & Conversion Focus
Every message uses SEO-smart phrasing, clear hooks and a direct call to action to your specific page.
üõ†Ô∏è How It Works
Define All Offers
In the pinData node, list each product/service with its name, description and URL.
Smart Query Generation
The AI builds 100 targeted Google Maps searches per offer to uncover ideal leads.
Data Extraction
Scrape website content, dedupe URLs and emails, then compile verified email lists.
Dynamic Email Creation
For each lead, the workflow picks an offer, analyzes their site and generates a personalized HTML email with the correct link.
SMTP Rotation & Throttling
A Switch node randomly assigns one of five Gmail/SMTP accounts, then applies a random delay to mimic human sending patterns and manage quotas.
üí° Key Benefits
Scale Effortlessly: Run thousands of personalized emails daily without manual effort.
Protect Sender Reputation: Distribute sends across multiple accounts and built-in delays.
Drive Targeted Traffic: SEO-optimized outreach that resonates with prospects and points them to the right solution.
üìà Ideal Use Cases
Launching multiple products or service lines
Seasonal or promotional campaigns
Account-Based Marketing with varied solutions
Agencies running outreach for diverse clients
‚öôÔ∏è Quick Start
Import the JSON into n8n.
Connect your GPT-4o or o3-mini credentials and five Gmail/SMTP accounts.
Populate pinData with all your offers.
Run the workflow‚Äîlet automation build your pipeline."
Check Which AI Models Are Used in Your Workflows,https://n8n.io/workflows/3718-check-which-ai-models-are-used-in-your-workflows/,"How it works
Fetch all workflows from your n8n instance.
Filter workflows that contain nodes with a modelId setting.
Extract the node names, model IDs, model names, workflow names, and workflow URLs.
Save the extracted information into a connected Google Sheet.
Set up steps
Connect your n8n API credentials.
Connect your Google Sheets account.
Replace ""Your n8n domain"" with your actual domain URL.
Use this Google Sheet template to create a new sheet for results.
Setup typically takes 5 minutes.
Be cautious: if you have over 100 workflows, performance may be impacted.
Notes
Sticky notes inside the workflow provide extra guidance.
This workflow clears old sheet data before writing new results.
Make sure your n8n instance allows API access.
Result Example
Update:
It didn't detect the AI model in tool originally. Now it's fixed!
Update 20250429:
Support 1.91.0 with open node directly!
Optimize the url with node id."
Context-Aware Discord Bot Replies to Mentions & DMs with GPT-4o,https://n8n.io/workflows/3692-context-aware-discord-bot-replies-to-mentions-and-dms-with-gpt-4o/,"If you have any question, or difficulty, feel free to come discuss about it on my Telegram (you might find something there üéÅ)
‚ö†Ô∏è Disclaimer
This workflow uses a community node: n8n-nodes-discord-trigger. Only self-hosted n8n instances can use this workflow.
Community nodes are maintained outside of n8n‚Äôs core team. Please review their code and use responsibly.
Description
This n8n workflow enables a fully autonomous AI assistant within your Discord server. It listens for both public mentions and direct messages (DMs), and replies in real-time using a language model (OpenAI's GPT-4o by default). The assistant can dynamically fetch previous messages for better context, providing smart, coherent, and personalized responses.
üîß Key Features
Dual Trigger: Reacts to both public bot mentions and private DMs.
Context-Aware AI: Dynamically fetches the last 30 messages from either public or private conversations.
Smart Routing: Replies either in the original public channel or via DM, depending on where the message came from.
Custom Prompting: Easily modify the system prompt to define the AI‚Äôs tone, behavior, and personality.
Fully Autonomous: No manual intervention required ‚Äî the bot always replies instantly.
‚úÖ Requirements
An operational n8n instance.
A Discord bot with at least these permissions:
Send Messages
Read Message History
Manage Messages
OpenAI API key (or compatible LLM credentials).
The community package: n8n-nodes-discord-trigger.
üß∞ Setup Instructions
1. Discord Bot Setup
Create and invite your Discord bot to your server with required permissions.
You will find in the next step a complete tutorial to craft your own bot with the right code and permissions for this workflow to work
Set your Discord bot token in n8n's credential manager.
2. Install the Community Node (please read ‚ö†Ô∏è Disclaimer first)
Go to Settings &gt; Community Nodes &gt; Install
Paste: n8n-nodes-discord-trigger and install.
3. Configure the Workflow
Add your Discord bot credentials.
Update the LLM credentials with your OpenAI key or alternative.
Customize the AI agent system prompt if needed.
ü§ñ How It Works
When a public mention is detected, the bot replies directly in the channel, referencing the original message.
When a DM is received, it responds privately.
The AI uses tools to read the latest 30 public or private messages to build accurate context before replying.
üõ† Customization Options
Change LLM: Swap out OpenAI with any LLM that has an API.
Edit Prompt Behavior: Modify the system prompt to control tone and personality.
Adjust Trigger Filters: Choose which roles, channels, or patterns activate the flow.
And if you just need a tutorial on setting up your Discord bot, be my guest ‚û°Ô∏è Here"
Generate Dynamic Images with Text & Templates using ImageKit.,https://n8n.io/workflows/3519-generate-dynamic-images-with-text-and-templates-using-imagekit/,"A Game-Changer in Dynamic Content Generation!
In a world where Templated.io and ApiTemplate.io charge between $29 to $179/month, we‚Äôre flipping the script‚Ä¶
What if you could get all the power ‚Äì for FREE?
We‚Äôre proud to introduce our N8N workflow, a modern, fast, and developer-friendly tool for generating dynamic images with $zero subscription fees.
No pricing tiers.
No usage caps.
No hidden fees.
Just pure value ‚Äì 100% free.
How We Compare:

or
But we are always Free
Note: Pricing is based on 2025-04-01 and the free imagekit plan.
Why Free?
We believe that powerful tools should be accessible to everyone ‚Äî startups, developers, creators, and students alike. Our mission is to empower you to create amazing things without breaking the bank.
What is a template idea, and how can you build one? (With examples)
Follow this page to better understand everything. Add overlays on images
Get Started in Seconds
No credit card. No trials. No limitations.
Just sign up and start creating.
What You Need
n8n (Cloud or self-hosted).
Imagekit just the FREE plan.
How to Generate imagekit API
After signing up, go to the ImageKit dashboard. On the left sidebar, scroll down until you find ""Developer Options"", then click on it.
Choose your preferred method of integration ‚Äì either API keys or Webhooks. In this case, we will proceed with the API option.
On the right side of the screen, you will see your Private Key. This is the API key you'll need for your n8n workflow, along with code samples in different programming languages.
For beginners: If you're not familiar with how to connect using the code from the previous step, simply visit this link: Upload file V1
On the right side of that page, enter your Private Key in the input field labeled ""API Key"", then click on the ""Send API Request"" button.
Once the request is sent, scroll down to view the Shell/cURL code. From this code, copy only the part that looks like this:
--header 'Authorization: Basic ...'
and use it in your n8n workflow.
N8N Workflow Setup
1- Import the workflow into n8n.
2- Add imagekit API.
3- Follow the instructions in the workflow.
4- Test & launch"
Automated Discord Chatbot for chat Interaction in channel using Gemini 2.0 Flash,https://n8n.io/workflows/3456-automated-discord-chatbot-for-chat-interaction-in-channel-using-gemini-20-flash/,"A Discord bot that responds to mentions by sending messages to n8n workflows and returning the responses. Connects Discord conversations with custom automations, APIs, and AI services through n8n.
Full guide on: https://github.com/JimPresting/AI-Discord-Bot/blob/main/README.md
Discord Bot Summary
Overview
The Discord bot listens for mentions, forwards questions to an n8n workflow, processes responses, and replies in Discord.
This workflow is intended for all Discord users who want to offer AI interactions with their respective channels.
What do you need?
You need a Discord account as well as a Google Cloud Project
Key Features
1. Listens for Mentions
The bot monitors Discord channels for messages that mention it.
Optional Configuration: Can be set to respond only in a specific channel.
2. Forwards Questions to n8n
When a user mentions the bot and asks a question:
The bot extracts the question.
Sends the question, along with channel and user information, to an n8n webhook URL.
3. Processes Data in n8n
The n8n workflow receives the question and can:
Interact with AI services (e.g., generating responses).
Access databases or external APIs.
Perform custom logic.
n8n formats the response and sends it back to the bot.
4. Replies to Discord with n8n's Response
The bot receives the response from n8n.
It replies to the user's message in the Discord channel with the answer.
Long Responses: Handles responses exceeding Discord's 2000-character limit by chunking them into multiple messages.
5. Error Handling
Includes error handling for:
Issues with n8n communication.
Response formatting problems.
Manages cases where:
No question is asked.
An invalid response is received from n8n.
6. Typing Indicator
While waiting for n8n's response, the bot sends a ""typing..."" indicator to the Discord channel.
7. Status Update
For lengthy n8n processes, the bot sends a message to the Discord channel to inform the user that it is still processing their request.
Step-by-Step Setup Guide as per Github Instructions
Key Takeaways
You‚Äôll configure an n8n webhook to receive Discord messages, process them with your workflow, and respond.
You‚Äôll set up a Discord application and bot, grant the right permissions/intents, and invite it to your server.
You‚Äôll prepare your server environment (Node.js), scaffold the project, and wire up environment variables.
You‚Äôll implement message‚Äêchunking, ‚Äútyping‚Ä¶‚Äù indicators, and robust error handling in your bot code.
You‚Äôll deploy with PM2 for persistence and know how to test and troubleshoot common issues.
1. n8n: Create & Expose Your Webhook
New Workflow
Log into your n8n instance.
Click Create Workflow (‚ûï), name it e.g. Discord Bot Handler.
Webhook Trigger
Add a node (‚ûï) ‚Üí search Webhook.
Set:
Authentication: None (or your choice)
HTTP Method: POST
Path: e.g. /discord-bot
Click Execute Node to activate.
Copy Webhook URL
After execution, copy the Production Webhook URL.
You‚Äôll paste this into your bot‚Äôs .env.
Build Your Logic
Chain additional nodes (AI, database lookups, etc.) as required.
Format the JSON Response
Insert a Function node before the end:
return {
  json: { answer: ""Your processed reply"" }
};
Respond to Webhook
Add Respond to Webhook as the final node.
Point it at your Function node‚Äôs output (with the answer field).
Activate
Toggle Active in the top‚Äêright and Save.
2. Discord Developer Portal: App & Bot
New Application
Visit the Discord Developer Portal.
Click New Application, name it.
Go to Bot ‚Üí Add Bot.
Enable Intents & Permissions
Under Privileged Gateway Intents, toggle Message Content Intent.
Under Bot Permissions, check:
Read Messages/View Channels
Send Messages
Read Message History
Grab Your Token
In Bot ‚Üí click Copy (or Reset Token).
Store it securely.
Invite Link (OAuth2 URL)
Go to OAuth2 ‚Üí URL Generator.
Select scopes: bot, applications.commands.
Under Bot Permissions, select the same permissions as above.
Copy the generated URL, open it in your browser, and invite your bot.
3. Server Prep: Node.js & Project Setup
Install Node.js v20.x
sudo apt purge nodejs npm
sudo apt autoremove
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt install -y nodejs
node -v    # Expect v20.x.x
npm -v     # Expect 10.x.x
Project Folder
mkdir discord-bot
cd discord-bot
Initialize & Dependencies
npm init -y
npm install discord.js axios dotenv
4. Bot Code & Configuration
Environment Variables
Create .env:
nano .env
Populate:
DISCORD_BOT_TOKEN=your_bot_token
N8N_WEBHOOK_URL=https://your-n8n-instance.com/webhook/discord-bot
# Optional: restrict to one channel
# TARGET_CHANNEL_ID=123456789012345678
Bot Script
Create index.js:
nano index.js
Implement:
Import dotenv, discord.js, axios.
Set up client with MessageContent intent.
On messageCreate:
Ignore bots or non‚Äêmentions.
(Optional) Filter by channel ID.
Extract and validate the user‚Äôs question.
Send ‚Äútyping‚Ä¶‚Äù every 5 s; after 20 s send a status update if still processing.
POST to your n8n webhook with question, channelId, userId, userName.
Parse various response shapes to find answer.
If answer.length ‚â§ 2000, message.reply(answer).
Else, split into ~1900‚Äëchar chunks at sentence/paragraph breaks and send sequentially.
On errors, clear intervals, log details, and reply with an error message.
Login
client.login(process.env.DISCORD_BOT_TOKEN);
5. Deployment: Keep It Alive with PM2
Install PM2
npm install -g pm2
Start & Monitor
pm2 start index.js --name discord-bot
pm2 status
pm2 logs discord-bot
Auto‚ÄêStart on Boot
pm2 startup
# Follow the printed command (e.g. sudo env PATH=$PATH:/usr/bin pm2 startup systemd -u your_user --hp /home/your_user)
pm2 save
6. Test & Troubleshoot
Functional Test
In your Discord server:
@YourBot What‚Äôs the weather like?
Expect a reply from your n8n workflow.
Common Pitfalls
No reply ‚Üí check pm2 logs discord-bot.
Intent Errors ‚Üí verify Message Content Intent in Portal.
Webhook failures ‚Üí ensure workflow is active and URL is correct.
Formatting issues ‚Üí confirm your Function node returns json.answer.
Inspect Raw Data
Search your logs for Complete response from n8n: to debug payload shapes."
3D Figurine Orthographic Views with Midjourney and GPT-4o-image API,https://n8n.io/workflows/3628-3d-figurine-orthographic-views-with-midjourney-and-gpt-4o-image-api/,"What this workflow does?
This workflow primarily uses the GPT-4o API from PiAPI and automatically creates front/side/top views of 3D models from commands.
Who is this forÔºü
3D DesignersÔºö Quickly generate standardized orthographic views for design review
E-commerce OperatorsÔºö Create multi-angle product display images
3D Modeling BeginnersÔºö Instantly produce basic reference views
Step-by-step Instruction
Fill in X-API-Key of your PiAPI account and the image prompt based on your inspiration.
Click Test workflow.
Get the image url in the final node.
OutPut"
Query PostgreSQL Database with Natural Language Using Groq AI Chatbot,https://n8n.io/workflows/3680-query-postgresql-database-with-natural-language-using-groq-ai-chatbot/,"This guide shows you how to deploy a chatbot that lets you query your PostgreSQL database using natural language. You will build a system that accepts chat messages, retains conversation history, constructs dynamic SQL queries, and returns responses generated by an AI model. By following these instructions, you will have a working solution that integrates n8n‚Äôs AI Agent capabilities with PostgreSQL.
Prerequisites
Before you begin, ensure that you have the following:
An active n8n instance (self-hosted or cloud) running version 1.50.0 or later.
Valid PostgreSQL credentials configured in n8n.
API credentials for the Groq Chat Model (or your preferred AI language model).
Basic familiarity with SQL (specifically PostgreSQL syntax) and n8n node concepts such as chat triggers and memory buffers.
Access to the n8n Docs on AI Agents for further reference.
Workflow Setup
Chat Interface & Trigger
When Chat Message Received: This node listens for incoming chat messages via a webhook. When a message arrives, it triggers the workflow immediately.
Conversation Memory
Chat History: This memory buffer node stores the last 10 interactions. It supplies conversation context to the AI Agent, ensuring that responses consider previous messages.
AI Agent Core
AI Agent (Tools Agent): The AI Agent node orchestrates the conversation by receiving the chat input and conversation history. It dynamically generates PostgreSQL-compatible SQL queries based on your requests and coordinates calls to external tools (such as PostgreSQL nodes).
Database Interactions
PostgreSQL Node (Query Execution): This node executes the SQL query generated by the AI Agent against your PostgreSQL database. You reference the query using an expression (e.g., {{$node[""AI Agent""].json.sql_query}}), allowing the agent‚Äôs output to control data retrieval.
PostgreSQL Node (Schema Retrieval): This node (or a dedicated step using the PostgreSQL node) retrieves a list of relevant tables from your PostgreSQL database (e.g., from the public schema, excluding system schemas like pg_catalog or information_schema). The agent uses this information to understand the available tables. This typically involves executing a query like SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';.
PostgreSQL Node (Table Definition Retrieval): This node (or another dedicated step using the PostgreSQL node) fetches detailed metadata (such as column names, data types, and potentially relationships using foreign keys) for a specific table. The table name (and schema if necessary) is supplied dynamically by the AI Agent. This often involves querying information_schema.columns, e.g., SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{{dynamic_table_name}}' AND table_schema = 'public';.
Language Model Processing
Groq Chat Model: This node connects to the Groq Chat API to generate text completions. It processes the combined input (chat message, context, and data fetched from PostgreSQL) and produces the final response.
Guidance & Customization
Sticky Notes: These nodes provide guidance on:
Switching the chat model if you wish to use another provider (e.g., OpenAI or Anthropic).
Adjusting the maximum token count per interaction.
Customizing the SQL queries (ensuring PostgreSQL compatibility) and the context window size.
They help you modify the workflow to suit your environment and requirements.
Workflow Connections
The Chat Trigger passes the incoming message to the AI Agent.
The Chat History node supplies conversation context to the AI Agent.
The AI Agent calls the PostgreSQL nodes as external tools, generating and sending dynamic SQL queries.
The Groq Chat Model processes the consolidated input from the agent and outputs the natural language response delivered to the user.
Testing the Workflow
Send a chat message using the chat interface.
Observe how the AI Agent processes the input and generates a corresponding PostgreSQL SQL query.
Verify that the PostgreSQL nodes execute the query correctly against your database and return data.
Confirm that the Groq Chat Model produces a coherent natural language response based on the query results.
Refer to the sticky notes for guidance if you need to fine-tune any node settings or SQL queries.
Next Steps and References
Customize Your AI Model: Replace the Groq Chat Model with another language model (such as the OpenAI Chat Model) by updating the node credentials and configuration.
Enhance Memory Settings: Adjust the Chat History node‚Äôs context window to retain more or fewer messages based on your needs.
Modify SQL Queries: Update the SQL queries within the PostgreSQL nodes or refine the prompts for the AI Agent to ensure they match your specific database schema and desired data, adhering to PostgreSQL syntax.
Further Reading: Consult the n8n Docs on AI Agents for additional details and examples to expand your workflow‚Äôs capabilities.
Set Up a Website Chatbot: Copy & Paste and replace the placeholders in the following code to embed the chatbot into your personal or company‚Äôs website: View in CodePen ü°•
By following these steps, you will deploy a robust AI chatbot workflow that integrates with your PostgreSQL database, allowing you to query data using natural language."
üß† Build AI Agents with Think-Plan-Act Architecture Using Llama-4 Reasoning,https://n8n.io/workflows/3489-build-ai-agents-with-think-plan-act-architecture-using-llama-4-reasoning/,"üß† A plug-and-play n8n workflow that adds LLM-powered reasoning, planning, and action to your automations ‚Äî with prompts, schemas, and full agent logic included.
Ever wish your n8n flows could think before they act?
Now they can. Say hello to the ultimate agent-based upgrade:
""Think ‚Üí Plan ‚Üí Act"" ‚Äì fully automated. Fully intelligent.‚ö°
üß© What Is This?
This product is a ready-to-use AI-powered workflow template for n8n, featuring a smart ‚ÄúThinking Agent‚Äù that:
üß† Analyzes tasks
üìã Generates a step-by-step plan
üõ†Ô∏è Executes actions intelligently
üßæ Returns structured JSON outputs for your next steps
Inspired by the Hugging Face agent architecture, this template gives your automations a brain before they press the red button. üî¥‚úÖ
A clean 2-step process:
THINK mode ‚Üí Generates structured reasoning (goal, subgoals, tools, assumptions)
ACT mode ‚Üí Executes subgoals step-by-step using tools and AI.
üß¨ What's Inside?
This isn‚Äôt just a flow ‚Äî it‚Äôs a framework. You get:
üîπ Modular Agent Loop
üîπ A demonstration workflow
üìù Requirements
üîπ OpenRouter API Key (or any other provider like OpenAI, groq...)
üîπ Built-in Parsers
üßæ Structured JSON parser for reliable outputs
ü™Ñ Natural language and unstructured parsing fallback
üõ†Ô∏è Plug in your own goals, tools, and OpenRouter keys
üì¶ Includes a demo (e.g., ‚Äúget the weather‚Äù) to get started fast
üí° Why You'll Love It
‚úÖ Plug & Play ‚Äì Drop into any existing n8n flow
‚úÖ Tool-Aware Reasoning ‚Äì Plans include which tool to use and why
‚úÖ Composable ‚Äì Build workflows like agents: Reason ‚Üí Plan ‚Üí Execute
‚úÖ Customizable by anyone ‚Äì Even your intern can tweak the prompts
‚öôÔ∏è Agent customization
üîπ Feel free to customize the agent settings by changing the Config node. Here's the default configuration:
const config = {
  THINK: {
    CONTEXT: ``, // Add the necessary context to the AI.
    CONSTRAINTS: [`Rule 1`, `Rule 2`], // here are the rules that your AI should follow
    TOOLS: [
      {
        tool: """",
        description: """",
      },
    ], // add here the list of the tools that the AI will call in your backend or workflow.
  },
};

return {
  json: {
    config: config,
    input: $input.last().json,
  },
};
After updating the configuration file, you will see the AI reasoning for your input. Feel free to connect with any other node creating smart tasks!"
Scrape Trustpilot Reviews Using Bright Data & GPT-4o-mini for Winning Ad Copy,https://n8n.io/workflows/3610-scrape-trustpilot-reviews-using-bright-data-and-gpt-4o-mini-for-winning-ad-copy/,"üîç Competitor Review Scraper & Ad Copy Generator
(Trustpilot + Bright Data + GPT-4o-mini)
üìå Who It's For
Marketers, business owners, and agencies looking to:
Analyze competitor pain points
Generate high-impact Facebook ad copy
Automate manual data processing
üß© How It Works
This n8n-based workflow combines Bright Data, Google Sheets, and OpenAI to scrape, process, and transform Trustpilot reviews into ready-to-use ad copy.
üîπ Step-by-Step Breakdown
Trigger (Manual Form Submission)
Input required:
Competitor‚Äôs Trustpilot URL
Review timeframe (30d, 3m, 6m, 12m)
Fetch Reviews
Calls Bright Data‚Äôs Dataset API with URL & timeframe
Polls until snapshot is ready
Retrieve & Store
Extracts all reviews
Saves them into a structured Google Sheet
Filter & Aggregate
Filters to only 1‚Äì2 star reviews
Summarizes common negative feedback
Generate Ad Copy
Sends the summary to OpenAI GPT-4o-mini
Produces 3 variations of ad copy targeting pain points
Distribute Insights
Sends ad copy + summary via email to the marketing team
‚úÖ Requirements
-LLM Account
-Google Sheets - Copy this sheet:
https://docs.google.com/spreadsheets/d/1Zi758ds2_aWzvbDYqwuGiQNaurLgs-leS9wjLWWlbUU/edit?gid=0#gid=0
-Bright Data account
‚öôÔ∏è Setup Instructions
**Step 1: Google Sheets
**
Copy this Google Sheets template
Do not change column headers
**Step 2: n8n Credential Setup
**
Google Sheets: OAuth2
Bright Data: Authorization Header
OpenAI: API Key for GPT-4o-mini
**Step 3: Import Workflow
**
Import the .json file into n8n
Configure your sheet + dataset ID
Adjust GPT prompts as needed
**Step 4: Run the Workflow
**
Trigger via form
Receive ad copy + review insights via email
üß† Tips & Best Practices
Bright Data snapshots may take time ‚Äî polling is handled
Focusing on 1‚Äì2 star reviews yields the most actionable pain points
You can customize GPT-4o-mini prompts for tone or vertical
üí¨ Support & Feedback
Need help or customization?
üìß Email: Yaron@nofluff.online
üì∫ YouTube: @YaronBeen
üîó LinkedIn: linkedin.com/in/yaronbeen
üìö Bright Data Docs: docs.brightdata.com/introduction"
"Customer Feedback Analysis with AI, QuickChart & HTML Report Generator",https://n8n.io/workflows/3642-customer-feedback-analysis-with-ai-quickchart-and-html-report-generator/,"Generative Customer Insights from Feedback Data using AI Agents & Charts
This workflow automates the analysis of customer feedback or social media data from Google Sheets using DeepSeek LLM, transforming raw text into structured semantic insights. The workflow also generates data visualizations and produces a final HTML report, ready for email delivery.
Table of Contents
What This Workflow Does
Pre-conditions and Requirements
Step-by-Step Workflow Explanation
Example Results
Customization Guide
What This Workflow Does
This workflow performs automated semantic analysis on unstructured feedback data (from Google Sheets), using LLM-based agents and a sequence of transformations. It achieves:
Prompt proposal generation: AI generates generalizable prompts for various analysis dimensions.
Row-level analysis: Each row of data is evaluated against all prompts.
Output merging and refinement: Raw analysis outputs are merged, deduplicated, and semantically clustered.
Visualization and report generation: QuickChart is used to create graphs, and an HTML report is produced.
Email delivery: The full report is sent automatically via Gmail.
Pre-conditions and Requirements
API Keys
DeepSeek API Key
Gmail OAuth2 (for sending results)
Google Sheet Access
A preformatted Google Sheet containing social listening feedback
The sheet must include at least 20 rows for sample prompt generation
n8n Configuration
Nodes used: Google Sheets, LangChain (LLM/Agent/Parser), Function, Merge, QuickChart (via URL), Gmail
Ensure all credentials are configured properly in n8n‚Äôs credential manager
Step-by-Step Workflow Explanation
Google Sheet Import
Retrieves feedback rows from a specific Google Sheet tab
Filters to the first 20 rows for prompt generation
Prompt Proposal Agent
AI generates 3‚Äì6 row-level analysis prompts in a structured JSON format
Prompts must be agnostic to product names and column headers
Prompt Injection and Pairing
Each row is paired with all prompts
Combined into a single dataset for row-by-row LLM evaluation
First Iteration of Analysis
An LLM answers all injected prompts row-by-row
Output is parsed and transformed into structured fields
Semantic Merging and Refinement
Merged lists of values from all rows
AI clusters synonyms and regenerates improved prompt definitions
Second Iteration of Analysis
The refined prompts are used to re-analyze each row
A new structured output per row is generated and merged into one object
Summarization and Visualization
AI generates summaries per dimension (e.g., sentiment)
QuickChart visualizations are created and URL-encoded
Cross-dimensional insights and a global narrative are generated
Final Report Generation and Emailing
A final HTML report is generated
Sent to the specified email using Gmail node
Example Results
Customization Guide
1. Modify Data Source
Change the Google Sheet ID or sheet tab
Add filters for specific time periods or product names
2. Adjust Prompt Definitions
Refine the initial prompt agent instruction to tailor the type of analysis
3. Swap LLM Models
Replace DeepSeek with OpenAI or another LLM via LangChain nodes
4. Visual Styling
Customize QuickChart configurations to adjust chart types, colors, legends
5. Report Format
Update the final HTML prompt to reflect brand design or restructure sections
6. Add Report Destinations
Replace Gmail with Google Drive upload, Notion page creation, or Slack post
This end-to-end AI-powered social listening workflow enables scalable, repeatable, and customizable insights generation from unstructured customer feedback."
"AI-Powered Upwork Cover Letter Generator ‚Äì Pinecone, Groq, Google Gemin, SerpAPI",https://n8n.io/workflows/3622-ai-powered-upwork-cover-letter-generator-pinecone-groq-google-gemin-serpapi/,"üöÄ Automated Upwork Cover Letter Generator with n8n + MacOS Shortcut + Pinecone Context Retrieval
This n8n automation is designed to streamline the Upwork proposal process by generating highly personalized, context-aware cover letters using your own skills and project data stored in a Pinecone vector store.
With the help of an AI Agent powered by Groq‚Äôs Qwen LLM, and triggered instantly via a MacOS Shortcut, this system takes job descriptions from your clipboard and returns a ready-to-use HTML cover letter‚Äîright on your desktop.
‚öôÔ∏è Workflow Breakdown
üîπ MacOS Shortcut ‚Äì Trigger the Workflow Instantly
The process begins with a MacOS Shortcut that captures job description text from your clipboard and sends it to a custom webhook in n8n.
üîπ Webhook Node ‚Äì Receive and Process Input
The webhook node receives the clipboard data, along with the required credentials for authentication. This node serves as the entry point to the full automation.
üîπ Field Mapping & Pre-Processing
A series of basic logic nodes map and clean up the input fields. These are then passed to an LLM Chain to generate specific questions related to the job description.
üîπ Question Answer Chain + Vector Search (Pinecone)
Using your previously stored skills and project data in a Pinecone vector store, the system retrieves relevant information to answer the generated questions and builds a rich context around the job description.
üîπ AI Agent Node ‚Äì Generate Personalized Cover Letter
With both the job post and contextual data, the AI Agent (powered by Groq‚Äôs Qwen LLM) creates a tailored cover letter.
The agent is equipped with:
üîç Google Search Tool
üß† Vector Store Retriever Tool
üóÉÔ∏è Buffer Memory
This ensures the generated proposal is insightful, relevant, and professional.
üîπ Markdown to HTML ‚Äì Clean Output Conversion
The markdown output from the AI is converted into HTML using a Markdown node, making it easy to paste directly into Upwork or emails.
üîπ Return to Shortcut ‚Äì Display Final Result
The final HTML response is sent back to the MacOS Shortcut, which displays it in a modal window for easy review and copy-paste.
üíº Use Case
This automation is built specifically for freelancers on Upwork (or any freelance platform) who want to:
‚úÖ Save time on repetitive proposal writing
‚úÖ Create job-specific cover letters with context
‚úÖ Stand out with better personalization
‚úÖ Reduce manual effort with automation
Whether you‚Äôre a beginner or a seasoned pro, this tool elevates your workflow while staying simple to use.
üì¶ Setup Instructions
Import Workflow to your n8n instance
Create and Configure MacOS Shortcut (drag-and-drop ready)
Prepare and Embed Your Skills/Project Data into Pinecone
Add API Credentials:
Groq (for Qwen LLM)
Pinecone
n8n Webhook (Basic Auth if needed)
Run the Workflow & Submit Smarter Proposals
Note: This workflow is designed for building and returning Upwork cover letters using job descriptions copied to your clipboard. All generation is context-aware and tailored per submission."
"Create AI-Powered YouTube Shorts with OpenAI, ElevenLabs, 0CodeKit!",https://n8n.io/workflows/3584-create-ai-powered-youtube-shorts-with-openai-elevenlabs-0codekit/,"YouTube Shorts Automation with AI-Powered Video Generation
Overview
This workflow automates the creation of engaging YouTube Shorts by leveraging AI tools to generate scripts, audio, images, and videos, then compiles them into a final video ready for upload. It uses OpenAI for script and image prompt generation, ElevenLabs for text-to-speech, Replicate for image and video generation, Cloudinary for audio storage, and Creatomate for final video rendering. The workflow segments the script into 6-second clips, creates corresponding visuals, and combines them with audio, adding smooth transitions for a polished result.
Why is this workflow useful?
Creating YouTube Shorts manually is time-consuming, requiring scriptwriting, voiceovers, and video editing. This workflow automates the entire process, enabling creators to produce high-quality, engaging Shorts in minutes. It‚Äôs ideal for content creators, marketers, or businesses looking to maintain a consistent social media presence with minimal effort.
Who is this workflow for?
Content Creators: YouTubers or TikTok creators aiming to produce viral Shorts quickly.
Digital Marketers: Businesses promoting products or services through short-form video content.
Hobbyists: Individuals experimenting with AI-driven video production without advanced editing skills.
What will you need to use this workflow?
n8n Account: Free or paid, hosted on n8n Cloud or self-hosted.
OpenAI API Key: For generating scripts and image prompts (GPT-4o-mini model).
ElevenLabs API Key: For text-to-speech conversion.
Replicate API Key: For generating images and videos.
Cloudinary Account: For storing audio files.
Creatomate API Key: For rendering the final video.
Basic JSON Knowledge: To understand and tweak the workflow output if needed.
How does this workflow work?
Trigger: The workflow starts manually via the ""Test Workflow"" button.
Script Generation: OpenAI (GPT-4o-mini) generates a script with an intro (40-70 chars), base (280-350 chars), and CTA (55 chars), plus a title and description with hashtags.
Text-to-Speech: ElevenLabs converts the script into an MP3 audio file with timestamps.
Audio Upload: The audio is uploaded to Cloudinary for storage.
Script Segmentation: A Python script segments the audio into 6-second clips, creating transcription chunks.
Image Prompt Generation: OpenAI generates a visual prompt for each segment in the style of CNSTLL, ensuring simple, animatable visuals.
Image Generation: Replicate creates a 9:16 image for each segment based on the prompt.
Video Generation: Replicate‚Äôs Minimax model generates 5-second video clips from the images.
Video Compilation: Creatomate combines the video clips with the audio, adding fade transitions and a final black shape for a smooth close.
Final Output: The rendered video is retrieved from Creatomate, ready for YouTube Shorts.
Setup instructions
Import the Workflow: Copy the provided JSON into n8n and import it as a new workflow.
Add Credentials:
Configure OpenAI API credentials for the ""Ideator"" and ""Image Prompter"" nodes.
Set up ElevenLabs API key in the ""Script Generator"" node.
Add Replicate API key for the ""Request Image,"" ""Request Video,"" and ""Get Video"" nodes.
Configure Cloudinary credentials for the ""Upload to Cloudinary"" node.
Set Creatomate API key for the ""Editor"" and ""Get Final Video"" nodes.
Test the Workflow:
Click ""Test Workflow"" to run it manually.
Replace &lt;user-query&gt; in the ""Ideator"" node with your video topic (e.g., ""Fitness tips for beginners"").
Verify Outputs: Check each node‚Äôs output, ensuring audio, images, and videos are generated correctly.
Customize (Optional):
Adjust segment duration in the ""HTTP Request"" node‚Äôs Python code (default: 6 seconds).
Modify fade duration or video resolution in the ""Create Editor JSON"" node.
Save and Activate: Save the workflow and activate it for regular use.
Tips for getting the most out of this workflow
Optimize Prompts: Use specific, concise queries in the ""Ideator"" node for better script quality (e.g., ""Minimalist home decor ideas"").
Check API Limits: Ensure your API accounts (OpenAI, ElevenLabs, Replicate, Creatomate) have sufficient credits.
Experiment with Styles: Tweak the CNSTLL style in the ""Image Prompter"" node for unique visuals, but keep prompts simple to avoid AI generation errors.
Monitor Rendering Time: Video rendering in Creatomate may take up to 70 seconds; adjust the ""Rendering"" node‚Äôs wait time if needed.
Add Hashtags: Enhance the generated description with trending hashtags like #YouTubeShorts, #AI, or niche-specific tags.
Troubleshooting
Error in Script Generation: Verify your OpenAI API key and ensure the query is clear.
Audio Issues: Check ElevenLabs API key and confirm the script length fits within API limits.
Image/Video Failures: Ensure Replicate API key is valid and prompts are simple (avoid complex scenes or humans).
Rendering Errors: Confirm Creatomate API key and check JSON structure in the ""Create Editor JSON"" node.
Contact Support: Reach out to n8n Community Forum or respective API providers for persistent issues.
Built with
n8n Nodes: Manual Trigger, OpenAI, HTTP Request, Set, Split Out, Aggregate, Merge, Wait.
External Services: OpenAI (GPT-4o-mini), ElevenLabs (Multilingual v2), Replicate (Flux-Cinestill, Minimax Video-01), Cloudinary, Creatomate.
Custom Code: Python for audio segmentation and JSON rendering.
Example output
For a query like ""Morning routine hacks"":
Script:
Intro: ""Struggling to start your day right?""
Base: ""Try these 3 morning hacks: 5-min meditation to clear your mind, a quick stretch routine to boost energy, and a cold shower to stay alert. Set your day up for success!""
CTA: ""Save this and try it tomorrow!""
Title: ""3 Morning Hacks You NEED to Try!""
Description: ""Kickstart your day with these easy hacks! üßò‚Äç‚ôÄÔ∏èüöø #MorningRoutine #Productivity #YouTubeShorts""
Video: A 15-second Short with three 5-second clips (meditation scene, stretching POV, cold shower close-up), synced with audio and fade transitions."
"Create a RAG System with Paul Essays, Milvus, and OpenAI for Cited Answers",https://n8n.io/workflows/3573-create-a-rag-system-with-paul-essays-milvus-and-openai-for-cited-answers/,"Create a RAG System with Paul Essays, Milvus, and OpenAI for Cited Answers
This workflow automates the process of creating a document-based AI retrieval system using Milvus, an open-source vector database. It consists of two main steps:
Data collection/processing
Retrieval/response generation
The system scrapes Paul Graham essays, processes them, and loads them into a Milvus vector store. When users ask questions, it retrieves relevant information and generates responses with citations.
Step 1: Data Collection and Processing
Set up a Milvus server using the official guide
Create a collection named ""my_collection""
Execute the workflow to scrape Paul Graham essays:
Fetch essay lists
Extract names
Split content into manageable items
Limit results (if needed)
Fetch texts
Extract content
Load everything into Milvus Vector Store
This step uses OpenAI embeddings for vectorization.
Step 2: Retrieval and Response Generation
When a chat message is received, the system:
Sets chunks to send to the model
Retrieves relevant information from the Milvus Vector Store
Prepares chunks
Answers the query based on those chunks
Composes citations
Generates a comprehensive response
This process uses OpenAI embeddings and models to ensure accurate and relevant answers with proper citations.
For more information on vector databases and similarity search, visit Milvus documentation."
üß† FloWatch üëÅÔ∏è Analyze and Diagnose n8n Workflow Errors via OpenAI and Email,https://n8n.io/workflows/3595-flowatch-analyze-and-diagnose-n8n-workflow-errors-via-openai-and-email/,"üß† Analyze and Diagnose n8n Workflow Errors Automatically via OpenAI and Email

‚ö†Ô∏è This template is available on ‚òÅÔ∏è Cloud & üñ•Ô∏è self-hosted n8n instances with the OpenAI node enabled.
üë§ Who is this for?
This workflow is designed for n8n developers, automation engineers, and DevOps teams who want to automatically capture and analyze workflow errors, and receive professional HTML-styled diagnostics directly in their inbox.
üí• What problem does this solve?
Manually troubleshooting failed workflows in n8n can be time-consuming. This template streamlines error detection by:
Capturing workflow failures using the Error Trigger node
Diagnosing root causes with the help of OpenAI
Sending a fully-formatted, human-readable HTML error report via email
Including practical resolutions and next-step suggestions
This helps you or your team resolve issues faster and avoid repeated manual debugging.
‚öôÔ∏è What this workflow does
‚ö° Triggers on any n8n workflow error
üì¶ Extracts relevant error metadata including node, execution ID, and timestamps
üß† Sends error content to OpenAI for analysis and recommendations
üíå Generates an HTML email report with inline styles and clear formatting
üì• Emails the result to a system administrator or support email
üõ†Ô∏è Setup
Install the OpenAI node in your self-hosted n8n instance.
Add your OpenAI API Key securely in credentials.
Configure the SMTP Email node with your email credentials.
Adjust the Error Trigger to monitor specific workflows or all workflows.
Set your preferred admin or dev email address in the final node.
üîß How to customize this workflow to your needs
üß© Use a [Set node] to define your variables, such as:
Default admin email
Workflow filter (optional)
‚úçÔ∏è Customize the prompt sent to OpenAI if you want deeper or more specific analysis
üé® Modify the email HTML styles to match your brand or internal format
üíæ Add additional logging (e.g., to Airtable, Google Sheets, or Notion) for long-term error tracking
üìå Sticky Note
Title: Automated Error Reporter with AI-Powered Diagnosis
Description: Captures any n8n error, sends it to OpenAI, and emails a beautiful HTML report to the administrator with steps to resolve the issue. Requires OpenAI credentials and SMTP configured."
LinkedIn Person ICP Scoring Automation with Airtop & Google Sheets,https://n8n.io/workflows/3476-linkedin-person-icp-scoring-automation-with-airtop-and-google-sheets/,"About The ICP Person Scoring Automation
Sorting through lists of potential leads manually to determine who's truly worth your sales team's time isn't just tedious, it's incredibly inefficient. Without proper qualification, your team might spend hours pursuing prospects who aren't the right fit for your product, while ideal customers slip through the cracks.
How to Automate Identifying Your Ideal Customers
With this automation, you'll learn how to automatically score and prioritize leads using data extracted directly from LinkedIn profiles via Airtop's built-in integration with n8n. By the end, you'll have a fully automated workflow that analyzes prospects and calculates an Ideal Customer Profile (ICP) score, helping your sales team focus on high-potential opportunities.
What You'll Need
A free Airtop API key
A copy of this Google Sheets
Understanding the Process
This automation transforms how you qualify and prioritize leads by extracting real-time, accurate information directly from LinkedIn profiles. Unlike static databases that quickly become outdated, this workflow taps into the most current professional information available.
The workflow in this template:
Uses Airtop to extract comprehensive LinkedIn profile data
Analyzes the data to calculate an ICP score based on AI interest, technical depth, and seniority
Updates your Google Sheet with the enriched data and the ICP score
Person ICP Scoring Workflow
Our person-focused workflow evaluates individual LinkedIn profiles to determine how well they match your ideal customer profile by:
Extracting data for each individual
Analyzing their profile to determine seniority and technical depth
The system then automatically calculates an ICP score based on the following criteria:
AI Interest: beginner-5 pts, intermediate-10 pts, advanced-25 pts, expert-35 pts
Technical Depth: basic-5 pts, intermediate-15 pts, advanced-25 pts, expert-35 pts
Seniority Level: junior-5 pts, mid-level-15 pts, senior-25 pts, executive-30 pts
Setting Up Your Automation
Here's how to get started:
Configure your connections
Connect your Google Sheets account
Add your Airtop API key (obtain from the Airtop dashboard)
Set up your Google Sheet
Ensure your Google Sheet has the necessary columns for input data and result fields
Ensure that columns Linkedin_URL_Person and ICP_Score_Person exist at least
Configure the Airtop module
Set up the Airtop module to use the appropriate LinkedIn extraction prompt
Use our provided prompt that extracts individual profile data
Customization Options
While our templates work out of the box, you might want to customize them for your specific needs:
Modify the ICP scoring criteria: Adjust the point values or add additional criteria specific to your business
Add notification triggers: Set up Slack or email notifications for high-value leads that exceed a certain ICP threshold
Implement batch processing: Modify the workflow to process leads in batches to optimize performance
Add conditional logic: Create different scoring models for different industries or product lines
Integrate with your CRM: Integrate this automation with your preferred CRM to get the details added automatically for you
Real-World Applications
Here's how businesses are using this automation:
AI Sales Platform: A B2B AI company could implement this workflow to process their trade show lead list of contacts. Within hours, they can identify the top 50 prospects based on ICP score.
SaaS Analytics Tool: A SaaS company could implement LinkedIn enrichment to identify which companies fit best. The automation processes weekly leads and categorizes them into high, medium, and low priority tiers, allowing their sales team to focus on the most promising opportunities first.
Best Practices
To get the most out of this automation:
Review and refine your ICP criteria quarterly: What constitutes an ideal customer may evolve as your product and market develop
Create tiered follow-up processes: Develop different outreach strategies based on ICP score ranges
Perform regular data validation: Periodically check the accuracy of the automated scoring against your actual sales results
What's Next?
Now that you've automated your ICP scoring with LinkedIn data, you might be interested in:
Setting up automated outreach sequences based on ICP score thresholds
Creating custom reporting dashboards to track conversion rates by ICP segment
Expanding your scoring model to include additional data sources
Implementing lead assignment automation based on ICP scores
Happy automating!"
Advanced AI-Powered YouTube SEO Optimization & Auto-Update,https://n8n.io/workflows/3528-advanced-ai-powered-youtube-seo-optimization-and-auto-update/,"Optimize Existing YouTube Video Metadata using AI and YouTube API
This n8n workflow automatically enhances the Search Engine Optimization (SEO) of your existing YouTube videos. Provide a video link, and the workflow fetches its current data, uses advanced SEO techniques with AI to generate optimized titles, descriptions, and tags, and then updates the video directly on your YouTube channel.
Who is this for?
This workflow is designed for:
Content Creators & YouTubers:
Who want to improve the visibility and reach of their existing video library without manual effort.
Digital Marketers & Social Media Managers: Who manage YouTube channels and need an efficient way to revitalize older or underperforming content.
Agencies:
Managing multiple client YouTube channels and looking for scalable optimization solutions.
What problem is this workflow solving? / Use Case
Many creators have a backlog of published videos that aren't reaching their full potential due to suboptimal titles, descriptions, or tags. Manually reviewing and updating each video is time-consuming and requires SEO expertise.
This workflow solves that by:
Automating the re-optimization process: Saving significant time and effort.
Leveraging AI for SEO: Generating high-quality, relevant metadata based on the video's content.
Revitalizing existing content: Giving older videos a fresh chance to be discovered by the YouTube algorithm and new viewers.
Improving discoverability: Helping your videos rank better in Youtube and appear more often in recommendations.
What this workflow does
Input:
Takes a YouTube video URL as the starting point (e.g., via a manual trigger or form).
Fetch Data:
Connects to the YouTube API using the provided link to retrieve the video's current title, description, tags, and potentially transcript data (if accessible and configured).
AI Analysis & Generation:
Sends the current metadata and video information to an AI model (like OpenAI, Google AI, or Anthropic - depending on your setup).
The AI analyzes the content and generates new, SEO-optimized suggestions for the:
Video Title
Video Description
Video Tags
Update Video:
Connects to the YouTube API again using appropriate permissions and updates the specific video with the newly generated title, description, and tags, replacing the old ones.
How to customize this workflow to your needs
To use this workflow, you will need to:
Configure Credentials:
YouTube: Set up YouTube API credentials in n8n. You'll likely need OAuth2 credentials that allow editing video metadata (youtube.force-ssl scope). Follow Google Cloud Console instructions to create these.
AI Service: Set up credentials for your chosen AI provider (e.g., OpenAI API Key, Google AI API Key) in n8n.
Connect Credentials: Select your configured YouTube and AI credentials in the respective nodes within the workflow.
(Optional) Review AI Prompts: Check the prompts used in the AI node(s) to ensure they align with your desired tone and optimization strategy."
Generate Company Stories from LinkedIn with Bright Data & Google Gemini,https://n8n.io/workflows/3540-generate-company-stories-from-linkedin-with-bright-data-and-google-gemini/,"Who this is for?
The LinkedIn Company Story Generator is an automated workflow that extracts company profile data from LinkedIn using Bright Data's web scraping infrastructure, then transforms that data into a professionally written narrative or story using a language model (e.g., OpenAI, Gemini). The final output is sent via webhook notification, making it easy to publish, review, or further automate.
This workflow is tailored for:
Marketing Professionals: Seeking to generate compelling company narratives for campaigns.
Sales Teams: Aiming to understand potential clients through summarized company insights.
Content Creators: Looking to craft stories or articles based on company data.
Recruiters: Interested in obtaining concise overviews of companies for talent acquisition strategies.
What problem is this workflow solving?
Manually gathering and summarizing company information from LinkedIn can be time-consuming and inconsistent. This workflow automates the process, ensuring:
Efficiency: Quick extraction and summarization of company data.
Consistency: Standardized summaries for uniformity across use cases.
Scalability: Ability to process multiple companies without additional manual effort.
What this workflow does
The workflow performs the following steps:
Input Acquisition: Receives a company's name or LinkedIn URL as input.
Data Extraction: Utilizes Bright Data to scrape the company's LinkedIn profile.
Information Parsing: Processes the extracted HTML content to retrieve relevant company details.
Summarization: Employs AI Google Gemini to generate a concise company story.
Output Delivery: Sends the summarized content to a specified webhook or email address.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the LinkedIn URL by navigating to the Set LinkedIn URL node.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Input Variations: Modify the Set LinkedIn URL node to accept a different company LinkedIn URL.
Data Points: Adjust the HTML Data Extractor Node to retrieve additional details like employee count, industry, or headquarters location.
Summarization Style: Customize the AI prompt to generate summaries in different tones or formats (e.g., formal, casual, bullet points).
Output Destinations: Configure the output node to send summaries to various platforms, such as Slack, CRM systems, or databases."
Summarize Glassdoor Company Info with Google Gemini and Bright Data Web Scraper,https://n8n.io/workflows/3532-summarize-glassdoor-company-info-with-google-gemini-and-bright-data-web-scraper/,"Who is this for?
This workflow is designed for HR professionals, employer branding teams, talent acquisition strategists, market researchers, and business intelligence analysts who want to monitor, understand, and act upon employee sentiment and company perception on Glassdoor.
It's ideal for organizations that value real-time feedback, are tracking employer brand perception, or need summarized insights for leadership reporting without sifting through thousands of raw reviews.
What problem is this workflow solving?
Manually reviewing and analyzing Glassdoor reviews is tedious, subjective, and not scalable especially for larger companies or those with many subsidiaries.
This workflow:
Automates review collection by making a Glassdoor company request via the Bright Data Web Scrapper API.
Uses Google Gemini to summarize the content.
Sends an actionable summary to HR dashboards, leadership teams, or alert systems via the Webhook notification.
What this workflow does
Makes an HTTP Request to Glassdoor via the Bright Data Web Scrapper API.
Polls the BrightData Glassdoor for the completion of the request.
Downloads the Glassdoor response when a new snapshot is ready.
Sends the prompt to Google Gemini for summarization.
Delivers the summarized insights (strengths, weaknesses, sentiment, patterns) to a configured webhook or dashboard endpoint.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
A webhook or endpoint to receive the summary (e.g., Slack, Notion, or custom HR dashboard).
How to customize this workflow to your needs
Change Summary Focus by updating the Summarization of Glassdoor Response node Summarization methods and prompts to extract specific insights:
Cultural feedback
Leadership issues
Compensation comments
Exit motivation
Update the HTTP Request to Glassdoor node with a specific Glassdoor Company information that you are looking for.
Format the output to produce a customized summary to Markdown or HTML for rich delivery.
Integrate with HR Systems
BambooHR, Workday, SAP SuccessFactors via API.
Google Sheets or Airtable"
Import Google Keep notes to Google Sheets using OpenAI and Google Drive,https://n8n.io/workflows/3517-import-google-keep-notes-to-google-sheets-using-openai-and-google-drive/,"This n8n workflow automates the import of your Google Keep notes into a structured Google Sheet, using Google Drive, OpenAI for AI-powered processing, and JSON file extraction. It's perfect for users who want to turn exported Keep notes into a searchable, filterable spreadsheet ‚Äì optionally enhanced by AI summarization or transformation.
Who is this for?
Researchers, knowledge workers, and digital minimalists who rely on Google Keep and want to better organize or analyze their notes.
Anyone who regularly exports Google Keep notes and wants a clean, automated workflow to store them in Google Sheets.
Users looking to apply AI to process, summarize, or extract insights from raw notes.
What problem is this workflow solving?
Exporting Google Keep notes via Google Takeout gives you unstructured .json files that are hard to read and manage. This workflow solves that by:
Filtering relevant .json files
Extracting note content
(Optionally) applying AI to analyze or summarize each note
Writing the result into a structured Google Sheet
What this workflow does
Google Drive Search: Looks for .json files inside a specified ""Keep"" folder.
Loop: Processes files in batches of 10.
File Filtering: Filters by .json extension.
Download + Extract: Downloads each file and extracts note content from JSON.
Optional Filtering: Only keeps non-archived notes or those meeting content criteria.
AI Processing (optional): Uses OpenAI to summarize or transform the note content.
Prepare for Export: Maps note fields to be written.
Google Sheets: Appends or updates the target sheet with the note data.
Setup
Export your Google Keep notes using Google Takeout:
Deselect all, then choose only Google Keep.
Choose ‚ÄúSend download link via email‚Äù.
Unzip the downloaded archive and upload the .json files to your Google Drive.
Connect Google Drive, OpenAI, and Google Sheets in n8n.
Set the correct folder path for your notes in the ‚ÄúSearch in ‚ÄòKeep‚Äô folder‚Äù node.
Point the Google Sheet node to your spreadsheet
How to customize this workflow to your needs
Skip AI processing: If you don't need summaries or transformations, remove or disable the OpenAI Chat Model node.
Filter criteria: Customize the Filter node to extract only recent notes, or those containing specific keywords.
AI prompts: Edit the Tools Agent or Chat Model node to instruct the AI to summarize, extract tasks, categorize notes, etc.
Field mapping: Adjust the ‚ÄúSet fields for export‚Äù node to control what gets written to the spreadsheet.
Use this template to build a powerful knowledge extraction tool from your Google Keep archive ‚Äì ideal for backups, audits, or data-driven insights."
ü•á Token Estim8r -Sub Workflow to track AI Model Token Usage and cost with JinaAI,https://n8n.io/workflows/3513-token-estim8r-sub-workflow-to-track-ai-model-token-usage-and-cost-with-jinaai/,"Save Your Tokens from Evil King Browser
Image Generated with ideoGener8r
n8n workflow template
üîç Estimate token usage and AI model cost from any workflow in n8n
üôã‚Äç‚ôÇÔ∏è Who is this for?
This workflow is ideal for AI engineers, automation specialists, and business analysts who use OpenAI, Anthropic, or other token-based large language models (LLMs) in their n8n workflows and want to track their usage and accuratley estimate associated costs.
Whether you're prototyping workflows or deploying in production, this tool gives you insight into how many tokens you're using and what that translates to in actual dollars.
üòå What problem is this workflow solving?
n8n users running AI-based workflows often struggle to track how many tokens were used per execution and how much those tokens cost. Without visibility into usage, it‚Äôs easy to lose track of API spending.
This workflow solves that problem by:
Logging token counts and costs to Google Sheets
Supporting prompt and completion token counts
Providing live pricing (optional, via Jina AI API)
‚öôÔ∏è What this workflow does
This template allows you to analyze the token usage and cost of any workflow in n8n. It uses an Execute Workflow node to call the Token Estim8r utility, which:
Estimates prompt and completion tokens
Retrieves model pricing (either statically or live via Jina API)
Calculates the total cost
Logs the data to a connected Google Sheet with timestamp and model info
üõ†Ô∏è Setup Instructions
Create Google Sheet: Copy and paste the CSV template below into a .csv file and upload to Google Sheets:
timestamp, Total Tokens, Prompt Tokens, Completion Tokens, Models Used, Tools Used, Total Cost, Json Array
Set up pricing (optional): In the Get AI Pricing node, add your Jina API Auth Header if you want live pricing.
Select the correct Google Sheet: Ensure your workflow is pointing to the imported sheet.
Attach to your target workflow: Add an Execute Workflow node to the end of your target workflow.
Point to this Token Estim8r: Choose this template as the executed workflow and send {{ $execution.id }} as the input.
Run and view results: Trigger the target workflow and see your token usage and cost data populate in the sheet.
üîß How to customize this workflow to your needs
Change the logging destination: Instead of Google Sheets, connect to Airtable, Notion, or a database.
Support multiple models: Extend the price-mapping logic to cover your own model providers.
Add Slack alerts: Send a notification if a workflow exceeds a token or cost threshold.
Aggregate costs: Create a weekly summary workflow that totals cost by workflow or model.
This utility workflow works across all n8n deployment types and uses only built-in nodes."
"Store Retell transcripts in Sheets, Airtable or Notion from webhook",https://n8n.io/workflows/3504-store-retell-transcripts-in-sheets-airtable-or-notion-from-webhook/,"Automatically store Retell transcripts in Google Sheets/Airtable/Notion from webhook
Overview
This workflow stores the results of a Retell voice call (transcript, analysis, etc.) once it has ended and been analyzed.
It listens for call_analyzed webhook events from Retell and stores the data in Airtable, Google Sheets, and Notion (choose based on your stack).
Useful for anyone building Retell agents who want to keep a detailed history of analyzed calls in structured tools.
Who is it for
For builders of Retell's Voice Agents who want to store call history and essential analytic data.
Prerequisites
Have a Retell AI Account
Create a Retell agent
Associate a phone number with your Retell agent
Set up one of the following:
An Airtable base and table (example: ""Transcripts"")
A Google Sheet with a ‚ÄúTranscripts‚Äù tab
A Notion database with columns to match the transcript fields
Templates:
Airtable
Google Sheets
Notion
How it works
Receives a webhook POST request from Retell when a call has been analyzed.
Filters out any event that is not call_analyzed (Retell sends webhooks for call_started, call_ended and call_analyzed)
Extracts useful fields like:
Call ID, start/end time, duration, total cost
Transcript, summary, sentiment
Stores this data in your preferred tool:
Airtable
Google Sheets
Notion
How to use it
Copy the webhook URL (e.g., https://your-instance.app.n8n.cloud/webhook/poc-retell-analysis) and paste it in your Retell agent under ""Webhook settings"" then ""Agent Level Webhook URL"".
Make sure your Airtable, Google Sheet, or Notion databases are correctly configured to receive the fields.
After each call, once Retell finishes the analysis, this workflow will automatically log the results.
Extension
If you use any ""Post-Call Analysis"" fields, you can add columns to your Airtable, Google Sheet, or Notion database.
Then fetch the data from the call.call_analysis.custom_analysis_data object.
Additional Notes
Phone numbers are extracted depending on the call direction (from_number or to_number).
Cost is converted from cents to dollars before saving.
Dates are converted from timestamps to local ISO strings.
You can remove any of the outputs (Airtable, Google Sheets, Notion) if you're only using one.
üëâ Reach out to us if you're interested in analysing your Retell Agent conversations."
Loading JSON via FTP to Qdrant Vector Database Embedding Pipeline,https://n8n.io/workflows/3495-loading-json-via-ftp-to-qdrant-vector-database-embedding-pipeline/,"üß† This workflow is designed for one purpose only, to bulk-upload structured JSON articles from an FTP server into a Qdrant vector database for use in LLM-powered semantic search, RAG systems, or AI assistants.
The JSON files are pre-cleaned and contain metadata and rich text chunks, ready for vectorization. This workflow handles
Downloading from FTP
Parsing & splitting
Embedding with OpenAI-embedding
Storing in Qdrant for future querying
JSON structure format for blog articles
{
  ""id"": ""article_001"",
  ""title"": ""reseguider"",
  ""language"": ""sv"",
  ""tags"": [""london"", ""resa"", ""info""],
  ""source"": ""alltomlondon.se"",
  ""url"": ""https://..."",
  ""embedded_at"": ""2025-04-08T15:27:00Z"",
  ""chunks"": [
    {
      ""chunk_id"": ""article_001_01"",
      ""section_title"": ""Introduktion"",
      ""text"": ""V√§lkommen till London...""
    },
    ...
  ]
}
üß∞ Benefits
‚úÖ Automated Vector Loading
Handles FTP ‚Üí JSON ‚Üí Qdrant in a hands-free pipeline.
‚úÖ Clean Embedding Input
Supports pre-validated chunks with metadata: titles, tags, language, and article ID.
‚úÖ AI-Ready Format
Perfect for Retrieval-Augmented Generation (RAG), semantic search, or assistant memory.
‚úÖ Flexible Architecture
Modular and swappable: FTP can be replaced with GDrive/Notion/S3, and embeddings can switch to local models like Ollama.
‚úÖ Community Friendly
This template helps others adopt best practices for vector DB feeding and LLM integration."
Sell a Used Car with an AI Agent in Airtop,https://n8n.io/workflows/3483-sell-a-used-car-with-an-ai-agent-in-airtop/,"How to Sell Your Used Car Easily with Airtop
Selling a used car traditionally involves tedious manual steps: listing your vehicle online, filling repetitive forms, answering buyer inquiries, and comparing multiple offers‚Äîoften leading to lost time, inaccuracies, and missed opportunities. Dealers' platforms and marketplace websites can introduce additional friction, occasionally requiring manual interactions and delivering inconsistent offers.
The Sell a Used Car Agent powered by Airtop and n8n eliminates this friction by automating the selling process. Simply input your car's details once, and the automation quickly navigates through popular used car marketplaces like Carvana, CarMax, and others, efficiently fetching competitive buying offers. Leveraging Airtop‚Äôs powerful real-browser automation capabilities, this setup interacts smoothly with the forms and queries specific to each sales platform, eliminating error-prone manual entries and tedious repetition.
This Airtop automation employs sophisticated data extraction features, including structured JSON output, ensuring accurate, formatted data ready for immediate review.
Say goodbye to cumbersome listing processes, and hello to streamlined, effortless car selling.
Who is this Automation for?
Automation engineers streamlining marketplace interactions
Developers building user-friendly car selling apps
Technical teams optimizing inventory resale processes
Dealership IT departments automating valuation systems
Key Benefits
No-code automation setup using Make
Saves significant time with instant multi-platform offers
Real browser sessions ensuring reliable site interactions
Structured JSON output simplifying data integration
Use Cases
Car dealership teams quickly pricing used vehicles
Individuals efficiently comparing offers from online dealers
Developers building consumer-friendly automated car resale apps
Technical consultants streamlining used car valuation processes
How this Airtop Automation Works
When started, Airtop opens automated, real browser sessions to intelligently fill out required car details on leading resale marketplaces (e.g., Carvana and CarMax). It captures instantly generated vehicle offers, structures them into clean JSON output, and returns organized buying proposals. All of this is accomplished without manual intervention.
What You‚Äôll Need
A free Airtop API key
Basic details describing your vehicle (make, model, condition, mileage)
Setting up the workflow
Create a new Airtop connection with your API key:
Enter the information about the car you are selling into the ""Variables"" node. For example:
VIN: 1FTRF17253NB81140
Mileage: 221081
Zip code: 01952
Condition: Perfect, no interior or exterior damages, all tires are inflated, have 2 keys, working battery, has an attached catalytic converter, Airbags not deployed, No flood or fire damage.
Ownership: full clean title with no debt answer yes to all other questions
Run the workflow
Customize the Automation
Add or remove marketplaces based on regional availability or preference
Extend JSON outputs to your internal inventory or sales systems
Schedule regular automation runs to track market pricing trends
Integrate additional valuation platforms for broader market coverage
Automation Best Practices
Regularly review marketplace URLs to maintain accuracy and consistency
Verify structured data to ensure consistent JSON outputs
Maintain clearly documented Make scenarios for easy debugging
Periodically update your car's condition description for accurate pricing
Happy Automating!"
Automate Product Hunt Discovery with Airtop and Slack,https://n8n.io/workflows/3481-automate-product-hunt-discovery-with-airtop-and-slack/,"About The Product Hunt Automation
Staying up-to-date with specific topics and launches on Product Hunt can be time-consuming. Manually checking the site multiple times a day interrupts your workflow and risks missing important launches. What if you could automatically get relevant launches delivered to your Slack workspace?
How to Monitor Product Hunt
In this guide, you'll learn how to create a Product Hunt monitoring system using Airtop's built-in node in n8n. This automation will scan Product Hunt for your chosen topics and deliver the most relevant launches directly to Slack.
What You'll Need
A free Airtop API key
A Slack workspace with permissions to add incoming webhooks
Estimated setup time: 5 minutes
Understanding the Process
The Monitor Product Hunt automation uses Airtop's cloud browser capabilities to access Product Hunt and extract launch information. Here's what happens:
Airtop visits Product Hunt and navigates the page
It searches for and extracts up to 5 launches related to your chosen topic
The information is formatted and sent to your specified Slack channel
This process can run on your preferred schedule, ensuring you never miss relevant launches.
Setting Up Your Automation
We've created a ready-to-use template that handles all the complex parts. Here's how to get started:
Connect your Airtop account by adding the API key you created
Connect your Slack account
Set your prompt in the Airtop node. For this example, we‚Äôve set it to be ‚ÄúExtract up to 5 launches related to AI products‚Äù
Choose your preferred monitoring schedule.
Customization Options
While our template works immediately, you might want to customize it for your specific needs:
Adjust the prompt and the maximum number of launches to monitor
Customize the Slack message format
Change the monitoring frequency
Add filters for particular keywords or companies
Real-World Applications
Here's how teams can use this automation:
A startup's engineering team could track trends in other product‚Äôs tech stack, helping them stay informed about potential issues and improvements.
A product manager can track launches of competitor products, enabling them to gather valuable market insights and user feedback directly from the tech community on that launch.
Best Practices
To get the most out of this automation:
Choose Specific Search Terms: For more relevant results, instead of broad terms like ""AI,"" use specific phrases like ""machine learning for healthcare""
Optimize Scheduling: When setting the monitoring frequency, consider your team's workflow. Running the scenario every 4 hours during working hours often provides a good balance between staying updated and avoiding notification fatigue.
Set Up Error Handling: Enable n8n's error output to alert you if the automation encounters any issues with accessing Product Hunt or sending messages to Slack.
Regular Topic Review: Schedule a monthly review of your monitored topics to ensure they're still relevant to your needs and adjust as necessary.
What's Next?
Now that you've set up your Product Hunt monitor automation, you might be interested in:
Creating a similar monitor for other tech websites
Setting up automated content curation for your team's newsletter
Building a competitive intelligence dashboard using web monitoring
Happy Automating!"
Manage Calendar Events with Slack Using OpenAI-Powered Outlook Assistant,https://n8n.io/workflows/3449-manage-calendar-events-with-slack-using-openai-powered-outlook-assistant/,"This n8n template demonstrates how easy it is to build an Outlook Calendar Assistant powered by an AI agent equipped with Tools.
For teams using Outlook Calendar and Slack who need easier calendar management, this workflow can be a great first step to introducing powerful AI tools into your daily activities.
How it works
A Slack Trigger node is configured to catch ""bot mentions"" events in a designated channel.
The message is parsed using the Edit fields node to extract only the required attributes of the event.
An AI Agent equipped with Outlook Calendar Tools enables question and answer capability for the organisation's shared calendars and events.
The AI agent's response is sent back to Slack as a reply to the user's query.
How to use
The workflow is triggered via @mention-ing the bot followed by the query. eg. ""@bot how many meetings does Paul have to attend to this week?""
To start listening to real mentions, you must activate the workflow and set it to production mode. You must use the production webhook URL for the event subscription.
Some sample queries to try
""What's included in the product team's sprint demo this week?""
""Who's booked room 7 for this Thursday?""
""When is Jim & Nik's sales meeting with Microsoft?""
Requirements
Slack for Chat and Trigger.
To get connected to Slack, see the official n8n docs for Slack Credentials.
Outlook for Agent Tools
To get connected to Outlook, see the official n8n docs for Outlook Credentials.
Customising this workflow
Not using Slack? This template can be modified to work with Teams but requires a little more configuration.
Agents can have any number of tools but an overloaded agent is prone to confusion! If this happens, try splitting into multiple agents serving separate needs."
Auto-Generate and Post Social Media Content to Bluesky using Groq LLM,https://n8n.io/workflows/3455-auto-generate-and-post-social-media-content-to-bluesky-using-groq-llm/,"Automatically Generate Content and Post to Bluesky with LLM Workflow
Unlock the power of automation with the Automatically Generate Content and Post to Bluesky with LLM Workflow, an n8n-powered solution that uses Large Language Model (LLM) APIs to create engaging content and seamlessly posts it to Bluesky. Say goodbye to manual content creation and posting‚Äîstreamline your social media presence in minutes!
How It Works
This workflow harnesses an LLM API (like Openai or your preferred model) to generate tailored content based on your input prompts. Once the content is created, it‚Äôs formatted to fit Bluesky‚Äôs requirements (e.g., under 300 characters), authenticated via the Bluesky API, and posted automatically. Built-in checks ensure error-free execution, making your content strategy seamless and efficient.
Set Up Steps (10-15 Minutes)
Authenticate Bluesky: Add your Bluesky API credentials (username and app password in your Bluesky settings).
Configure LLM API: Connect your chosen LLM API (e.g., Groq) with your API key.
Set Prompt: Define a system prompt in the LLM node (e.g., ‚ÄúWrite a witty Bluesky post about AI automation‚Äù).
Test Workflow: Run it manually to verify content generation and posting.
Schedule (Optional): Set a cron node to automate posts (e.g., daily at 9 AM).
Total setup time: 10-15 minutes, depending on familiarity with n8n.
Features
LLM-Powered Content: Generate unique, high-quality posts using any LLM API.
Bluesky Automation: Posts directly to Bluesky with proper formatting and authentication.
Character Limit Safety: Ensures posts stay under 300 characters with built-in validation.
Error Handling: Stops execution on errors to prevent unwanted posts.
Customizable Scheduling: Post instantly or schedule content at your preferred intervals.
Scalable Design: Easily adapt for multiple platforms or content types.
Perfect for Creators Who Want To
Save time by automating content creation and posting.
Maintain a consistent Bluesky presence without manual effort.
Experiment with AI-generated content tailored to their brand.
Focus on strategy and engagement rather than repetitive tasks.
Suggested Enhancements
Multi-Platform Support: Extend the workflow to post to Twitter, Mastodon, or LinkedIn simultaneously.
Content Variety: Add nodes to generate images (e.g., via DALL-E) alongside text for richer posts.
Analytics Integration: Track post performance with a Bluesky API feedback loop.
Dynamic Prompts: Incorporate trending topics or keywords from the web for timely content.
Approval Step: Add a manual review node before posting for extra control."
Batch Process Prompts with Anthropic Claude API,https://n8n.io/workflows/3409-batch-process-prompts-with-anthropic-claude-api/,"This workflow template provides a robust solution for efficiently sending multiple prompts to Anthropic's Claude models in a single batch request and retrieving the results. It leverages the Anthropic Batch API endpoint (/v1/messages/batches) for optimized processing and outputs each result as a separate item.
Core Functionality & Example Usage Included
This template includes:
The Core Batch Processing Workflow: Designed to be called by another n8n workflow.
An Example Usage Workflow: A separate branch demonstrating how to prepare data and trigger the core workflow, including examples using simple strings and n8n's Langchain Chat Memory nodes.
Who is this for?
This template is designed for:
Developers, data scientists, and researchers who need to process large volumes of text prompts using Claude models via n8n.
Content creators looking to generate multiple pieces of content (e.g., summaries, Q&As, creative text) based on different inputs simultaneously.
n8n users who want to automate interactions with the Anthropic API beyond single requests, improve efficiency, and integrate batch processing into larger automation sequences.
Anyone needing to perform bulk text generation or analysis tasks with Claude programmatically.
What problem does this workflow solve?
Sending prompts to language models one by one can be slow and inefficient, especially when dealing with hundreds or thousands of requests. This workflow addresses that by:
Batching: Grouping multiple prompts into a single API call to Anthropic's dedicated batch endpoint (/v1/messages/batches).
Efficiency: Significantly reducing the time required compared to sequential processing.
Scalability: Handling large numbers of prompts (up to API limits) systematically.
Automation: Providing a ready-to-use, callable n8n structure for batch interactions with Claude.
Structured Output: Parsing the results and outputting each individual prompt's result as a separate n8n item.
Use Cases:
Bulk content generation (e.g., product descriptions, summaries).
Large-scale question answering based on different contexts.
Sentiment analysis or data extraction across multiple text snippets.
Running the same prompt against many different inputs for research or testing.
What the Core Workflow does
(Triggered by the 'When Executed by Another Workflow' node)
Receive Input: The workflow starts when called by another workflow (e.g., using the 'Execute Workflow' node). It expects input data containing:
anthropic-version (string, e.g., ""2023-06-01"")
requests (JSON array, where each object represents a single prompt request conforming to the Anthropic Batch API schema).
Submit Batch Job: Sends the formatted requests data via POST to the Anthropic API /v1/messages/batches endpoint to create a new batch job. Requires Anthropic credentials.
Wait & Poll: Enters a loop:
Checks if the processing_status of the batch job is ended.
If not ended, it waits for a set interval (10 seconds by default in the 'Batch Status Poll Interval' node).
It then checks the batch job status again via GET to /v1/messages/batches/{batch_id}. Requires Anthropic credentials.
This loop continues until the status is ended.
Retrieve Results: Once the batch job is complete, it fetches the results file by making a GET request to the results_url provided in the batch status response. Requires Anthropic credentials.
Parse Results: The results are typically returned in JSON Lines (.jsonl) format. The 'Parse response' Code node splits the response text by newlines and parses each line into a separate JSON object, storing them in an array field (e.g., parsed).
Split Output: The 'Split Out Parsed Results' node takes the array of parsed results and outputs each result object as an individual item from the workflow.
Prerequisites
An active n8n instance (Cloud or self-hosted).
An Anthropic API account with access granted to Claude models and the Batch API.
Your Anthropic API Key.
Basic understanding of n8n concepts (nodes, workflows, credentials, expressions, 'Execute Workflow' node).
Familiarity with JSON data structures for providing input prompts and understanding the output.
Understanding of the Anthropic Batch API request/response structure.
(For Example Usage Branch) Familiarity with n8n's Langchain nodes (@n8n/n8n-nodes-langchain) if you plan to adapt that part.
Setup
Import Template: Add this template to your n8n instance.
Configure Credentials:
Navigate to the 'Credentials' section in your n8n instance.
Click 'Add Credential'.
Search for 'Anthropic' and select the Anthropic API credential type.
Enter your Anthropic API Key and save the credential (e.g., name it ""Anthropic account"").
Assign Credentials: Open the workflow and locate the three HTTP Request nodes in the core workflow:
Submit batch
Check batch status
Get results
In each of these nodes, select the Anthropic credential you just configured from the 'Credential for Anthropic API' dropdown.
Review Input Format: Understand the required input structure for the When Executed by Another Workflow trigger node. The primary inputs are anthropic-version (string) and requests (array). Refer to the Sticky Notes in the template and the Anthropic Batch API documentation for the exact schema required within the requests array.
Activate Workflow: Save and activate the core workflow so it can be called by other workflows.
‚û°Ô∏è Quick Start & Input/Output Examples: Look for the Sticky Notes within the workflow canvas! They provide crucial information, including examples of the required input JSON structure and the expected output format.
How to customize this workflow
Input Source: The core workflow is designed to be called. You will build another workflow that prepares the anthropic-version and requests array and then uses the 'Execute Workflow' node to trigger this template. The included example branch shows how to prepare this data.
Model Selection & Parameters: Model (claude-3-opus-20240229, etc.), max_tokens, temperature, and other parameters are defined within each object inside the requests array you pass to the workflow trigger. You configure these in the workflow calling this template.
Polling Interval: Modify the 'Wait' node ('Batch Status Poll Interval') duration if you need faster or slower status checks (default is 10 seconds). Be mindful of potential rate limits.
Parsing Logic: If Anthropic changes the result format or you have specific needs, modify the Javascript code within the 'Parse response' Code node.
Error Handling: Enhance the workflow with more specific error handling for API failures (e.g., using 'Error Trigger' or checking HTTP status codes) or batch processing issues (batch.status === 'failed').
Output Processing: In the workflow that calls this template, add nodes after the 'Execute Workflow' node to process the individual result items returned (e.g., save to a database, spreadsheet, send notifications).
Example Usage Branch (Manual Trigger)
This template also contains a separate branch starting with the Run example Manual Trigger node.
Purpose: This branch demonstrates how to construct the necessary anthropic-version and requests array payload.
Methods Shown: It includes steps for:
Creating a request object from a simple query string.
Creating a request object using data from n8n's Langchain Chat Memory nodes (@n8n/n8n-nodes-langchain).
Execution: It merges these examples, constructs the final payload, and then uses the Execute Workflow node to call the main batch processing logic described above. It finishes by filtering the results for demonstration.
Note: This branch is for demonstration and testing. You would typically build your own data preparation logic in a separate workflow. The use of Langchain nodes is optional for the core batch functionality.
Notes
API Limits: According to the Anthropic API documentation, batches can contain up to 100,000 requests and be up to 256 MB in total size. Ensure your n8n instance has sufficient resources for large batches.
API Costs: Using the Anthropic API, including the Batch API, incurs costs based on token usage. Monitor your usage via the Anthropic dashboard.
Completion Time: Batch processing time depends on the number and complexity of prompts and current API load. The polling mechanism accounts for this variability.
Versioning: Always include the anthropic-version header in your requests, as shown in the workflow and examples. Refer to Anthropic API versioning documentation."
üå≤ AI Agent for Sustainability Report Audit with Gmail and GPT-40,https://n8n.io/workflows/3420-ai-agent-for-sustainability-report-audit-with-gmail-and-gpt-40/,"Tags: Sustainability, CSRD, Reporting, ESG, Compliance, Automation
Context
Hey! I'm Samir, a Supply Chain Engineer and Data Scientist from Paris, founder of
LogiGreen Consulting
We help companies automate sustainability workflows using AI, Data Analytics, and No-Code tools like N8N.
Sustainability Reporting meets Automation with n8n!
üì¨ For business inquiries, you can add me on Here
What is a CSRD XHTML Report?
Under the Corporate Sustainability Reporting Directive (CSRD), companies must publish their ESG disclosures in a machine-readable XHTML format, embedding XBRL tags that make the report structured and standardized.
These files must follow strict formatting and tagging rules to ensure compliance, traceability, and accessibility for both regulators and analysts.
Who is this template for?
This workflow is designed for sustainability teams, ESG consultants, or developers who want to automatically check the structure and format of CSRD reports submitted in XHTML.
How does it work?
This N8N workflow automates the audit process:
üì§ Input Node ‚Üí Uploads or fetches the XHTML file via URL or Webhook.
üß™ Validates Structure ‚Üí Uses a custom code node to parse HTML and identify required tags (e.g., &lt;ix:nonNumeric&gt;, namespaces).
üìã Outputs a Report ‚Üí Returns a summary report of errors, warnings, and key metadata (like entity name, reporting period).
üì§ Export Option ‚Üí Save the results in Google Sheets or send via email.
Prerequisite
A sample XHTML file that you can find in my GitHub Repository
Google Sheets API and OpenAI API credentials
Next Steps
Follow the sticky notes inside each node to adjust parsing rules or extend validation to specific XBRL tags relevant to your sector (e.g., GHG emissions, water usage).
**üì∫ Check my complete tutorial to understand how to use it: **

üé• Check My Tutorial
üöÄ Interested in combining CSRD compliance with automation and analytics? Let‚Äôs connect on LinkedIn
Notes
This workflow includes an example XHTML file to test the validator.
You can plug this into your internal systems or even extend it with AI to auto-summarize the sustainability report.
This workflow has been created with N8N 1.82.1
Submitted: April 3rd, 2025"
AI-Powered Product Research & Comparison with GPT-4o and SerpAPI,https://n8n.io/workflows/3315-ai-powered-product-research-and-comparison-with-gpt-4o-and-serpapi/,"What It Does
This powerful workflow can take hours of difficult research attempting to identify the perfect product to buy and condense it into a few short minutes. Simply typing the name of an item you wish to purchase into a chat message will initiate the workflow to begin its search process to identify 5 top of the line products for you to purchase. The initial chat with prompt the Item Finder AI agent utilizing the power of GPT-4o in combination with SerpAPI to conduct a search and find 5 top end items that match the prompt. It will then send the name of each item to a separate Reviewer Tools Agent that will gather information on each item. Each Reviewer Tools Agent will again harness the power of GPT-4o and SerpAPI to give a detailed description of the item features, lowest price available, options of retailers available to purchase the item, a summary of the reviews and overall star ratings.
Who This Is For
This is the perfect workflow for anyone who is interested in making a big-time purchase and wants to have all the information presented in a quick organized fashion to make the best decision possible. Rather than searching through multiple stores and reading through hundreds of reviews and product description, you can now utilize the power of GPT-4o and SerpAPI to conduct that search for you and bring all the information to you. Whether you are searching for a TV with the best screen quality, a work laptop that will have the power and memory you need, or that unique item for your business that is difficult to find and even harder to find the cheapest price, this workflow is exactly what you need.
How It Works
Begin by opening the chat trigger node and typing the name of an item you would like to purchase and would like it to do an in-depth search for. Example: golf driver, gaming desktop computer, or mid-size three row SUV.
This will trigger the Item Finder Tools Agent to begin to utilize GPT-4o to begin to create search queries based off the item you wish to purchase. It will then use these search queries with SerpAPI to identify 5 high quality, modern items. After it has obtained the names of five items it will then in a structured output, send each item to a separate Reviewer Tools Agent.
Once the Reviewer Tools Agent has received the name of the item it will then begin an in-depth review of that item. Each Reviewer Tools Agent will again use the power of GPT-4o connected with SerpAPI to obtain the most up to date information on each item from the internet. For each item you will be provided with:
Detailed description of item features
It will go in depth and give features that make that item unique. Qualities that will allow you to compare one items features to another.
Lowest Price available
Online retail stores available for purchase
Summary of reviews
It will give both the pros and cons discussed by various reviewers in regard to the item.
Overall star rating
After each of the Reviewer Tools Agent is complete with its review of the item, they will all send the information along to a Compiler Tools Agent. This tools agent uses GPT-4o-mini to arrange the data and present it in a concise, organized fashion. This allows for easy readability.
Set Up Steps
Set up steps:
You will need to obtain an Open AI API key from platform.openai.com/api-keys
After you obtain this Open AI API key you will need to connect it to the Open AI Chat Model for all of the Tools agents (Item finder, Reviewer 1, Reviewer 2, Reviewer 3, Reviewer 4, Reviewer 5, and Compiler).
You will now need to fund your Open AI account. GPT 4o costs ~$0.06 to run the workflow.
Next you will need to create a SerpAPI account at https://serpapi.com/users/sign_up
After you create an account you will need to obtain a SerpAPI key.
You will then need to use this key to connect to the SerpAPI tool for each of the tools agents (Item finder, Reviewer 1, Reviewer 2, Reviewer 3, Reviewer 4, and Reviewer 5)
Tip: SerpAPI will allow you to run 100 free searches each month. This workflow uses ~8-15 SerpAPI searches per run. If you would like to utilize the workflow more than that each month, create multiple SerpAPI accounts and have an API key for each account. When you utilize all 100 free searches for an account, switch to the API key for another account within the workflow."
"Custom Deal Recommendations by Email using Forms, Bright Data & GPT-4o-mini",https://n8n.io/workflows/3304-custom-deal-recommendations-by-email-using-forms-bright-data-and-gpt-4o-mini/,"This n8n workflow template automates the process of collecting and delivering the ""Top Deals of the Day"" from MediaMarkt, tailored to user preferences. By combining user-submitted forms, Bright Data web scraping, GPT-4o-mini deal generation, and email delivery, this workflow sends personalized product recommendations straight to a user‚Äôs inbox.
‚ö†Ô∏è Note: This workflow uses community nodes (Bright Data and Document Generator) which only work on self-hosted n8n instances.
üöÄ What It Does
Collects user preferences via a form (categories + email)
Scrapes MediaMarkt‚Äôs deals page using Bright Data
Uses GPT-4o-mini (OpenAI) to recommend top deals
Generates a structured HTML email using a template
Sends the personalized deals directly via email
üß© Community Node Integration
We created and used the following community nodes:
Bright Data ‚Äì To scrape MediaMarkt deals using proxy-based scraping
Document Generator ‚Äì To generate a templated HTML document from deal data
These nodes are not available in n8n Cloud and require self-hosted n8n.
üõ†Ô∏è Step-by-Step Setup
Install Community Nodes
Make sure you're on a self-hosted n8n instance. Install:
n8n-nodes-brightdata
n8n-nodes-document-generator
Configure Credentials
Bright Data API Key (Proxy + Scraping setup)
OpenAI API Key (GPT-4o-mini access)
SMTP Credentials for sending emails
Customize the Form
Adapt the form node to collect desired categories and email addresses. Typical categories include appliances, phones, laptops, etc.
Design Your HTML Template
In the Document Generator node, you can tweak the HTML/CSS to change how deals appear in the final email.
Test the Workflow
Submit the form with test data and check that the entire flow‚Äîfrom scraping to email‚Äîexecutes as expected.
üß† How It Works: Workflow Overview
User Interaction via Form
Users select product categories and enter their email. This triggers the workflow.
Data Extraction via Bright Data
Bright Data scrapes the MediaMarkt offers page and returns HTML content.
HTML Parsing
Key elements like product names, prices, and links are extracted for processing.
GPT-4o-mini Recommendation Generation
The extracted data is sent to OpenAI (GPT-4o-mini), which filters, ranks, and enhances deals based on the user‚Äôs preferences.
Data Structuring & Split
The result is split into individual deal items to be formatted.
HTML Document Creation
Document Generator populates a clean HTML template with the top recommended deals.
Email Delivery
The final document is emailed via SMTP to the user with a friendly message.
üì® Final Output
Users receive a custom HTML email featuring a curated list of top MediaMarkt deals based on their selected categories.
üîê Credentials Used
Bright Data API ‚Äì Web scraping with proxy support
OpenAI API ‚Äì Generating personalized recommendations
SMTP ‚Äì Sending personalized deal emails
‚ú® Customization Tips
Change the Data Source: You can adapt this to scrape other e-commerce sites.
Update the Email Template: Make it match your branding or include images.
Extend the Form: Add preferences like price range or specific brands.
Add Scheduling: Use Cron to run the workflow daily or weekly.
‚ùìQuestions?
Template and node created by Miquel Colomer and n8nhackers.com.
Need help customizing or deploying? Contact us for consulting and support."
Smart Email Classifier & Auto-Responder with AI,https://n8n.io/workflows/3242-smart-email-classifier-and-auto-responder-with-ai/,"This n8n workflow automates email management by classifying incoming messages, drafting replies, and sending alerts‚Äîall powered by AI.
Features
üöÄ AI-Powered Email Categorization
Classifies emails into Spam, Important, Promotion, Notification, Personal, Call Request, Needs Reply.
Uses GPT-4o to determine whether an email requires a response.
‚úâÔ∏è Automated Smart Replies
Generates context-aware responses using AI.
Supports email threading for seamless conversations.
üîî Real-Time Notifications
Telegram Alerts for important emails.
Gmail Drafts auto-generated for quick replies.
üõ†Ô∏è Google Calendar Integration
Schedules follow-ups based on email content.
Setup Instructions
Connect Gmail, OpenAI, Telegram, and Google Calendar.
Set up classification categories and notification preferences.
Customize AI response styles if needed.
Run the workflow‚Äîwatch it organize your inbox effortlessly.
Who Should Use This?
Busy Professionals: Focus on high-priority emails.
Customer Support: Manage inquiries with quick replies.
Sales Teams: Respond to leads instantly.
Keep your inbox under control with this automation! üöÄ"
Generate AI-Ready llms.txt Files from Screaming Frog Website Crawls,https://n8n.io/workflows/3219-generate-ai-ready-llmstxt-files-from-screaming-frog-website-crawls/,"This workflow helps you generate an llms.txt file (if you're unfamiliar with it, check out this article) using a Screaming Frog export.
Screaming Frog is a well-known website crawler.
You can easily crawl a website. Then, export the ""internal_html"" section in CSV format.
How It Works:
A form allows you to enter:
The name of the website
A short description
The internal_html.csv file from your Screaming Frog export
Once the form is submitted, the workflow is triggered automatically, and you can download the llms.txt file directly from n8n.
Downloading the File
Since the last node in this workflow is ""Convert to File"", you will need to download the file directly from the n8n UI.
However, you can easily add a node (e.g., Google Drive, OneDrive) to automatically upload the file wherever you want.
AI-Powered Filtering (Optional):
This workflow includes a text classifier node, which is deactivated by default.
You can activate it to apply a more intelligent filter to select URLs for the llms.txt file.
Consider modifying the description in the classifier node to specify the type of URLs you want to include.
How to Use This Workflow
Crawl the website you want to generate an llms.txt file for using Screaming Frog.
Export the ""internal_html"" section in CSV format.
In n8n, click ""Test Workflow"", fill in the form, and upload the internal_html.csv file.
Once the workflow is complete, go to the ""Export to File"" node and download the output.
That's it! You now have your llms.txt file!
Recommended Usage:
Use this workflow directly in the n8n UI by clicking 'Test Workflow' and uploading the file in the form."
Automate Video Creation with Luma AI Dream Machine and Airtable (Part 2),https://n8n.io/workflows/3201-automate-video-creation-with-luma-ai-dream-machine-and-airtable-part-2/,"Automate Video Creation with Luma AI Dream Machine and Airtable (Part 2)
Description
This is the second part of the Luma AI Dream Machine automation. It captures the webhook response from Luma AI after video generation is complete, processes the data, and automatically updates Airtable with the video and thumbnail URLs. This completes the end-to-end automation for video creation and tracking.
üëâ Airtable Base Template
üëâ Tutorial Video
Setup
1. Luma AI Setup
Ensure you‚Äôve created an account with Luma AI and generated an API key.
Confirm that the API key has permission to manage video requests.
2. Airtable Setup
Make sure your Airtable base includes the following fields (set up in Part 1):
Use the Airtable Base Template linked above to simplify setup.
Generation ID ‚Äì To match incoming webhook data.
Status ‚Äì Workflow status (e.g., ""Done"").
Video URL ‚Äì Stores the generated video URL.
Thumbnail URL ‚Äì Stores the thumbnail URL.
3. n8n Setup
Ensure that the n8n workflow from Part 1 is set up and configured.
Import this workflow and connect it to the webhook callback from Luma AI.
How It Works
1. Webhook Trigger
The Webhook node listens for a POST response from Luma AI once video generation is finished.
The response includes:
Video URL ‚Äì Direct link to the video.
Thumbnail URL ‚Äì Link to the video thumbnail.
Generation ID ‚Äì Used to match the record in Airtable.
2. Process Webhook Data
The Set node extracts the video data from the webhook response.
The If node checks if the video URL is valid before proceeding.
3. Store in Airtable
The Airtable node updates the record with:
Video URL ‚Äì Direct link to the video.
Thumbnail URL ‚Äì Link to the video thumbnail.
Status ‚Äì Marked as ""Done.""
Uses the Generation ID to match and update the correct record.
Why This Workflow is Useful
‚úÖ Automates the completion step for video creation
‚úÖ Ensures accurate record-keeping by matching generation IDs
‚úÖ Simplifies the process of managing and organizing video content
‚úÖ Reduces manual effort by automating the update process
Next Steps
Future Enhancements ‚Äì Adding more complex post-processing, video trimming, and multi-platform publishing."
Daily AI News Briefing and Summarization with Google Gemini and Telegram,https://n8n.io/workflows/3173-daily-ai-news-briefing-and-summarization-with-google-gemini-and-telegram/,"Stay ahead with personalized AI news delivered straight to your Telegram! This powerful n8n workflow automates your daily news consumption, leveraging AI to bring you the most relevant trends in AI, (or cryptocurrency, the stock market) and more ‚Äì all from two free news API sources (GNewsAPI and NewsAPI).
What's Included:
The n8n Workflow
Video Guidance for Telegram Integration: A clear and concise video tutorial demonstrating how to set up the Telegram integration within your n8n instance.
Who is this for?
This template is ideal for:
Individuals wanting to stay updated on the latest AI trends and advancements.
Tech enthusiasts eager to track the latest AI breakthroughs and applications.
Anyone seeking a free, automated, and highly customizable news briefing delivered directly to their Telegram.
Individuals who want to save time and stay informed without the hassle of manual news searching.
What problem is this workflow solving?
Keeping up with the constant influx of information in fast-paced sectors like AI, crypto, and the stock market can be overwhelming and time-consuming. This n8n workflow solves this problem by automatically gathering news from two free news APIs: News API and GNews API, filtering it with AI, and delivering a concise, personalized briefing directly to your Telegram, ensuring you're always in the know without the manual effort.
What this workflow does:
This workflow automates the following steps using two free news API sources (GNewsAPI and NewsAPI):
Automated News Gathering: Fetches the latest news on your chosen topics from two distinct free API sources.
Intelligent AI Filtering: Employs an AI agent to identify and extract the most pertinent news articles related to your specified interests (default: AI, but easily customizable).
Concise AI Summarization: The AI agent creates brief summaries of the key information from the top news articles.
Direct Telegram Delivery: Sends a daily digest of the summarized news directly to your Telegram account for convenient access.
Effortless Customization: Allows you to easily tailor the news topics to focus on AI, cryptocurrency, stock market updates, or any other area you need to monitor.
Setup:
Easy Telegram Integration (Video Guidance Included): Follow our step-by-step video tutorial to seamlessly integrate Telegram with this n8n workflow, enabling automated news delivery to your preferred chat.
Free API Keys: This workflow utilizes the free tiers of two popular news APIs: GNewsAPI and NewsAPI. You will need to obtain your own free API keys for these services and input them into the respective HTTP Request nodes within n8n. Clear instructions on how to get these free keys are provided.
First Run: Activate the workflow and execute it once to verify that all connections are established and functioning correctly.
How to customize this workflow:
Tailor Your News Topics: Easily customize the news you receive by modifying the search queries in the ""News Source: GNewsAPI"" and ""News Source: NewsAPI"" nodes. For example, change q=AI to q=Bitcoin for crypto news or q=Tesla stock for stock market updates.
Adjust the Language: Change the lang=en parameter in the API URLs to receive news in your preferred language.
Set Your Delivery Schedule: Modify the trigger time in the ""Trigger workflow at 6am everyday"" node to have your personalized news briefing delivered at the time that suits you best.
Expand Your Sources: For even more comprehensive news coverage, you can explore and integrate additional free news APIs by adding more HTTP Request nodes to the workflow.
Category:
News Automation, AI, Cryptocurrency, Stock Market, Information, Personal Productivity, Free Resources"
Generate a YouTube Bedtime Story using OpenAI,https://n8n.io/workflows/2930-generate-a-youtube-bedtime-story-using-openai/,"Automated YouTube Bedtime Story Video Generator (No Code) n8n Workflow
Free Support:
Setting up and getting the workflow tailord to your needs.
One small free adjustment included.
Transform your creative ideas into enchanting bedtime story videos with this fully automated n8n workflow. Designed for content creators, digital marketers, and storytellers, this tool streamlines the process of turning audio recordings into professional, engaging YouTube videos.
Samautomation.work
You will have the best and cheapest video rendering service online. Check the website for more information. You can customize every video and change all these settings:
Change the aspact ratio to 16:9 for long form content. Check the API docs for more information.
""settings"": {
  ""aspect_ratio"": ""9:16"",
  ""background_volume"": 0.15,
  ""voice_volume"": 1.0,
  ""font"": ""bangers"",
  ""font_size"": 40,
  ""text_color"": ""blue"",
  ""stroke_color"": ""white"",
  ""stroke_width"": 10,
  ""disable_captions"": ""true""
More information on:
Samautomation.work
support
Contact our whatsapp support if you have any questions, we are ready to assist with customizing the template if needed.
Key Features
Seamless Audio-to-Text Conversion:
Utilize OpenAI Whisper to convert audio recordings into accurate transcripts, ensuring every word of your story is captured.
AI-Powered Image Generation:
Automatically generate beautiful, thematic visuals using advanced AI image tools that create captivating 16:9 backgrounds tailored for bedtime stories.
Dynamic Video Compilation:
Merge images, voiceovers, and background music into a polished video with a few clicks. Customize video length, style, and audio levels to match your vision.
Automated YouTube Integration:
Upload your videos directly to YouTube complete with autogenerated titles, descriptions, and metadata. Manage your content easily with real-time updates.
Efficient Workflow Management:
Track your projects via Google Sheets and receive instant notifications through Telegram, keeping you always in the loop.
Cost Efficiency:
Each video costs only 25 cents to produce, making this solution incredibly affordable for large-scale content creation.
How It Works
Capture & Transcribe:
Record your bedtime story and let OpenAI Whisper transcribe the audio into text.
Generate Stunning Visuals:
Use integrated AI image generation to create dreamy visuals that complement your narrative.
Compile Your Video:
Combine the visuals, transcripts, and background music into a complete video ready for distribution.
Publish & Manage:
Automatically upload your video to YouTube, with all details saved and tracked in Google Sheets for seamless management.
Who Is It For?
This workflow is perfect for:
Content Creators looking to effortlessly produce engaging bedtime story videos.
Digital Marketers aiming to boost video content output without heavy manual intervention.
Storytellers & Educators who want to bring their narratives to life with a professional finish.
Small Businesses & Startups seeking scalable video production solutions.
Setup Requirements
API keys for OpenAI, Google Cloud Storage, and YouTube.
Basic familiarity with n8n and API integration.
A passion for storytelling and creative video production.
Video Rendering with samautomation.work
Start automating your video creation process today and captivate your audience with enchanting bedtime stories. With this workflow, you can save time, enhance productivity, and focus on what you do best‚Äîtelling magical stories, all while keeping your production cost at just 25 cents per video!
Ready to transform your content game?
Get your Automated YouTube Bedtime Story Video Generator now and start creating charming videos in minutes!
Demo video
Check out the link to the demo video of the end result."
AI-Powered Technical Analyst with Perplexity R1 Research,https://n8n.io/workflows/3160-ai-powered-technical-analyst-with-perplexity-r1-research/,"Leverage the latest AI technology to analyze financial charts and make informed trading decisions with our Technical Analysis AI Agent. This powerful workflow combines Claude Sonnet 3.7 vision capabilities with Perplexity deep reasoning and up-to-date internet information to deliver comprehensive market analysis.
Key Capabilities:
Visual Chart Analysis - AI vision technology examines technical charts to identify key price points, volume patterns, and trend indicators
Fundamental Research Integration - Combines technical analysis with real-time fundamental data using DeepSeek R1 reasoning
Fully Cited Reports - Fundamental analysis backed by verifiable sources for confident decision-making
Automated Email Delivery - Receive complete analysis reports directly to your inbox
How It Works:
This workflow orchestrates multiple AI components to analyze financial instruments:
The Technical Analysis Leader coordinates the entire analysis process
Chart analysis tool identifies the appropriate exchange and downloads Trading View charts
AI Vision examines the chart for technical indicators including RSI, volume patterns, and support/resistance levels
Perplexity tool conducts fundamental research using DeepSeek R1 reasoning capabilities
All data is synthesized into a comprehensive report with trading recommendations
Results can be automatically emailed for reference
Setup Instructions:
Quick start video included in the template.
Get API key from OpenRouter.ai to access the Sonnet 3.7 model
Get API key from chart-img.com to access tradingview charts
Connect the Gmail node for email delivery functionality
IMPORTANT DISCLAIMER:
This tool provides technical analysis for informational purposes only and should not be construed as investment advice. This AI-powered technical analysis tool is designed to assist with market analysis but should not be used as the sole basis for any investment decision"
Summarize Google Drive Documents with Mistral AI and Send via Gmail,https://n8n.io/workflows/3109-summarize-google-drive-documents-with-mistral-ai-and-send-via-gmail/,"This workflow automates document summarization directly from Google Drive, processes the content using Mistral AI, and delivers a clean, styled summary via Gmail. It's ideal for professionals who need quick insights from lengthy documents without manually reading through them.
‚úÖ Key Features:
Google Drive Integration: Fetches a file (PDF/DOCX) from your Drive.
AI Summarization: Uses Mistral AI to extract key points efficiently.
Styled Email Output: Delivers a formatted, easy-to-read summary to your inbox with a timestamp.
Error Handling: Built to skip corrupted files or missing credentials.
üîß Nodes Breakdown:
1Ô∏è‚É£ Manual Trigger ‚Äî Starts the workflow manually for easy testing.
2Ô∏è‚É£ Google Drive Node ‚Äî Downloads a specified file from Google Drive (supports PDF/DOCX).
3Ô∏è‚É£ Mistral Cloud Chat Model Node ‚Äî Connects to Mistral AI for summarization.
4Ô∏è‚É£ Summarization Chain Node ‚Äî Breaks the file into chunks, processes content, and generates a concise summary.
5Ô∏è‚É£ Gmail Node ‚Äî Sends the styled summary directly to the user‚Äôs inbox, with custom formatting and current time in the Lagos timezone.
Extra Features:
Dynamic Time Formatting: Supports Lagos timezone (easily adjustable).
HTML Styling: Beautiful email formatting with headers, icons, and line breaks for clarity.
Custom Email Sender Name: Branded output (e.g., ""Swot.AI"").
Future Expansion: Can extend to WhatsApp or Slack with minor tweaks.
Use Cases:
Legal teams summarizing contracts.
Content creators extracting highlights from research papers.
Business analysts getting insights from reports on-the-go.
Customization Tips:
Change the timezone (Africa/Lagos) to match your preferred location.
Add error-handling nodes for missing files or API failures.
Swap Mistral AI with OpenAI for different summarization behavior.
Change the ""Send To"" address(email to receive the Summarized texts) with your personal preffered address.
Change the ""Sender Name"" from Swot.AI to your preferred Sender Name.
Why To Use This Workflow?
This automation saves hours of manual reading. It‚Äôs perfect for personal productivity, legal analysis, content creation, or business reporting.
With clean formatting and a professional email summary ‚Äî your team will get instant insights in seconds!
I can make this much better and build others, If Interested: Swot.ai25@gmail.com"
"CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI",https://n8n.io/workflows/3036-callforge-06-automate-sales-insights-with-gongio-notion-and-ai/,"CallForge - AI-Powered Sales Call Data Processor
Automate sales call analysis and store structured insights in Notion with AI-powered intelligence.
Who is This For?
This workflow is ideal for:
‚úÖ Sales teams looking to automate call insight processing.
‚úÖ Sales operations managers managing AI-driven call analysis.
‚úÖ Revenue teams using Gong, Fireflies.ai, Otter.ai, or similar transcription tools.
It streamlines sales call intelligence, ensuring that insights such as competitor mentions, objections, and customer pain points are efficiently categorized and stored in Notion for easy access.
üîç What Problem Does This Workflow Solve?
Manually reviewing and documenting sales call takeaways is time-consuming and error-prone.
With CallForge, you can:
‚úî Identify competitors mentioned in sales calls.
‚úî Capture objections and customer pain points for follow-up.
‚úî Track sales call outcomes and categorize insights automatically.
‚úî Store structured sales intelligence in Notion for future reference.
‚úî Improve sales strategy with AI-driven, automated call analysis.
üìå Key Features & Workflow Steps
üéôÔ∏è AI-Powered Call Data Processing
This workflow processes AI-generated sales call insights and structures them in Notion databases:
Triggers automatically when AI call analysis data is received.
Extracts competitor mentions from the call transcript and logs them in Notion.
Identifies and categorizes sales objections for better follow-ups.
Processes integration mentions, capturing tools or platforms referenced in the call.
Extracts customer use cases, categorizing pain points and feature requests.
Aggregates all extracted insights and updates relevant Notion databases.
üìä Notion Database Integration
Competitors ‚Üí Logs mentioned competitors for sales intelligence.
Objections ‚Üí Tracks and categorizes common objections from prospects.
Integrations ‚Üí Captures third-party tools & platforms discussed in calls.
Use Cases ‚Üí Stores customer challenges & product feature requests.
üõ† How to Set Up This Workflow
1. Prepare Your AI Call Analysis Data
Ensure AI-generated sales call data is passed into the workflow.
Compatible with Gong, Fireflies.ai, Otter.ai, and other AI transcription tools.
2. Connect Your Notion Database
Set up Notion databases for:
üîπ Competitors (tracks competing products)
üîπ Objections (logs customer objections & concerns)
üîπ Integrations (captures mentioned platforms & tools)
üîπ Use Cases (categorizes customer pain points & feature requests)
3. Configure n8n API Integrations
Connect your Notion API key in n8n under ‚ÄúNotion API Credentials.‚Äù
Set up webhook triggers to receive data from your AI transcription tool.
Test the workflow using a sample AI-generated call transcript.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
üîß How to Customize This Workflow
üí° Modify Notion Data Structure ‚Äì Adjust fields to match your company‚Äôs CRM setup.
üí° Enhance AI Data Processing ‚Äì Align fields with different AI transcription providers.
üí° Expand with CRM Integration ‚Äì Sync insights with HubSpot, Salesforce, or Pipedrive.
üí° Add Notifications ‚Äì Send alerts via Slack, email, or webhook when key competitor mentions or objections are detected.
‚öôÔ∏è Key Nodes Used in This Workflow
üîπ If Nodes ‚Äì Checks if AI-generated data includes competitors, integrations, objections, or use cases.
üîπ Notion Nodes ‚Äì Creates or updates entries in Notion databases.
üîπ Split Out & Aggregate Nodes ‚Äì Processes multiple insights and consolidates AI outputs.
üîπ Wait Nodes ‚Äì Ensures smooth sequencing of API calls and database updates.
üîπ HTTP Request Node ‚Äì Sends AI-extracted insights to Notion for structured storage.
üöÄ Why Use This Workflow?
‚úî Eliminates manual data entry and speeds up sales intelligence processing.
‚úî Ensures structured and categorized sales insights for decision-making.
‚úî Improves team collaboration with AI-powered competitor tracking & objections logging.
‚úî Seamlessly integrates with Notion to centralize and manage sales call insights.
‚úî Scalable for teams using n8n Cloud or self-hosted deployments.
This workflow empowers sales teams with automated AI insights, streamlining sales strategy and follow-ups with minimal effort. üöÄ"
CallForge - 04 - AI Workflow for Gong.io Sales Calls,https://n8n.io/workflows/3034-callforge-04-ai-workflow-for-gongio-sales-calls/,"CallForge - AI Gong Sales Call Processing Workflow
Automate your Gong.io sales call analysis with AI-driven insights, real-time tracking, and structured CRM integration.
Who is This For?
This workflow is designed for:
‚úÖ Sales teams looking to automate sales call processing.
‚úÖ Revenue operations (RevOps) professionals managing high volumes of call data.
‚úÖ AI-driven sales intelligence teams using Gong.io for data-driven insights.
What Problem Does This Workflow Solve?
Manually managing and analyzing large volumes of Gong call data is time-consuming and error-prone.
With CallForge, you can:
‚úî Automate call processing to scale AI-driven insights.
‚úî Integrate with Notion to track and organize sales call data efficiently.
‚úî Get real-time Slack updates to stay informed on call processing progress.
‚úî Handle API failures gracefully, allowing easy reruns if a rate limit is hit.
‚úî Ensure AI-ready analysis, feeding structured call data into an AI-powered system.
What This Workflow Does
1. Triggers on New Gong Calls
Captures new Gong calls and retrieves metadata, call summaries, and participant details.
2. Compares Calls Against Notion Database
Checks whether the call has already been processed and stored in Notion.
Prevents duplicate entries from being added.
3. Creates a Parent Notion Record for AI Processing
Stores call details such as date, title, URL, company name, sales rep, and opportunity details in Notion.
Links calls to Salesforce Opportunity (SF Opp) data.
Assigns sales representatives and customer information to each call.
4. Loops Through Calls for Processing
Ensures resilience by allowing failed runs to restart where they left off.
Processes calls one at a time to prevent Notion rate limits.
5. Sends Call Data to an AI Processor
Extracts structured call details and sends them to an AI-powered analysis workflow.
Allows multiple AI agents to process and extract structured data from calls.
6. Provides Real-Time Slack Alerts
Posts a progress update in Slack when the queue starts processing.
Sends real-time call progress notifications.
Sends a completion alert once all calls are processed.
How to Set Up This Workflow
1. Connect Your APIs
üîπ Gong API Credentials ‚Äì Ensure you have valid Gong API credentials in n8n.
üîπ Notion Database ‚Äì Provide access to a Notion database for storing call insights.
üîπ Slack Integration ‚Äì Configure a Slack channel for progress alerts.
üîπ AI Processing Workflow ‚Äì Connect an AI-powered call processing workflow for final analysis.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
How to Customize This Workflow
üí° Modify Call Storage ‚Äì Swap Notion for a different CRM or database (e.g., HubSpot, Airtable, Salesforce).
üí° Change AI Processing ‚Äì Integrate a custom AI model for analyzing sales conversations.
üí° Customize Slack Notifications ‚Äì Adjust Slack messages or send alerts via email instead.
üí° Expand with More Integrations ‚Äì Connect with Salesforce, Pipedrive, or HubSpot for further enrichment.
Why Use CallForge?
üöÄ Automate Gong call tracking for seamless sales intelligence.
üìä Improve sales operations with structured, AI-powered insights.
‚ö° Get real-time updates and keep your team informed instantly.
Start optimizing your Gong call processing today!"
Extract Pay Slip Data with Line Chatbot and Gemini to Google Sheets,https://n8n.io/workflows/3093-extract-pay-slip-data-with-line-chatbot-and-gemini-to-google-sheets/,"Workflow Overview:
Extract text from image using AI is worth because you need no code. It incorporates Google Gemini 2.0 Flash model for important text extraction from image. If you code without AI, you have to use multiple condition and may cause a lot of bug but with Google Gemini, you don't need any coding and if the Pay Slip is different, Gemini will extract it automatically.
Workflow description:
User uses Line Messaging API to send Pay Slip image or message to the chatbot, create Line Business ID from here: Line Business
Classify the message which is image or text
If the message is Pay Slip image, it will process using Gemini 2.0 Flash EXP and extract important information and response in JSON format without coding by using the following prompt: Analyze image and then return in JSON Response that has the only following value: Status, From, To, Date, Amount

To get Google AI Studio API Key, you can find from the following link: Google AI Studio API Key
Create Google Sheets which include the fileds (Status, From, To, Date, Amount) that we have created related to the AI prompt Google Sheets as the following example:
If the message is text, it will process using Gemini 2.0 Flash EXP model as the AI Assistant else if the message is image, it will extract the important fields then reply to the User and insert into Google Sheets
Key Features:
Extract text from image with No Code Without N8N, we have to write code to extract text from image, but with N8N and Google Gemini 2.0 Flash EXP together, we don't need to code and it will process all slip vendors or other document vendors.
Multipurpose Chatbot this chatbot accept both text and image so we don't have to create many chatbot accounts
Reduce human error this workflow let any officer to verify document status when the job ends
Note: You can change the information by changing your prompt and also Google Sheets Column names relatively."
Generate AI Prompts with Google Gemini and store them in Airtable,https://n8n.io/workflows/3027-generate-ai-prompts-with-google-gemini-and-store-them-in-airtable/,"This workflow is designed to generate prompts for AI agents and store them in Airtable.
It starts by receiving a chat message, processes it to create a structured prompt, categorizes the prompt, and finally stores it in Airtable.
2. Setup Instructions
Prerequisites
AI model eg Gemini, openAI etc
Airtable base and table or other storage tool
Step-by-Step Guide
Clone the Workflow
Copy the provided workflow JSON and import it into your n8n instance.
Configure Credentials
Set up the Google Gemini(PaLM) API account credentials.
Set up the Airtable Personal Access Token account credentials.
Map Airtable Base and Table
Create a copy of the Prompt Library in Airtable.
Map the Airtable base and table in the Airtable node.
Customize Prompt Template
Edit the 'Create prompt' node to customize the prompt template as needed.
Configuration Options
Prompt Template: Customize the prompt template in the 'Create prompt' node to fit your specific use case.
Airtable Mapping: Ensure the Airtable base and table are correctly mapped in the Airtable node.
4. Running and Troubleshooting
Running the Workflow
Trigger the Workflow: Send a chat message to trigger the workflow.
Monitor Execution: Use the n8n interface to monitor the workflow execution.
Check Completion: Verify that the prompt is stored in Airtable and check the chat interface for the result.
Troubleshooting Tips
API Issues: Ensure that the APIs and Airtable credentials are correctly configured.
Data Mapping: Verify that the Airtable base and table are correctly mapped.
Prompt Template: Check the prompt template for any errors or inconsistencies.
Use Case Examples
This workflow is particularly useful in scenarios where you want to automate the generation and management of AI agent prompts.
Here are some examples:
Rapid Prototyping of AI Agents:
Quickly generate and test different prompts for AI agents in various applications.
Content Creation: Generate prompts for AI models that create blog posts, articles, or social media content.
Customer Service Automation: Develop prompts for AI-powered chatbots to handle customer inquiries and support requests.
Educational Tools: Create prompts for AI tutors or learning assistants.
Industries/Professionals:
Software Development: Developers building AI-powered applications.
Marketing: Marketers automating content creation and social media management.
Customer Service: Customer service managers implementing AI-driven chatbots.
Education: Educators creating AI-based learning tools.
Practical Value:
Time Savings: Automates the prompt generation process, saving significant time and effort.
Improved Prompt Quality: Leverages Google Gemini and structured prompt engineering principles to generate more effective prompts.
Centralized Prompt Management: Stores prompts in Airtable for easy access, organization, and reuse.
4. Running and Troubleshooting
Running the Workflow:
Activate the workflow in n8n.
Send a chat message to the webhook URL configured in the ""When chat message received"" node.
Monitor the workflow execution in the n8n editor.
Monitoring Execution:
Check the execution log in n8n to see the data flowing through each node and identify any errors.
Checking for Successful Completion:
Verify that a new record is created in your Airtable base with the generated prompt, name, and category.
Confirm that the ""Return results"" node sends back confirmation of the prompt in the chat interface.
Troubleshooting Tips:
Error: 400: Bad Request in the Google Gemini nodes:
Cause: Invalid API key or insufficient permissions.
Solution: Double-check your Google Gemini API key and ensure that the API is enabled for your project.
Error: Airtable node fails to create a record:
Cause: Invalid Airtable credentials, incorrect Base ID or Table ID, or mismatched column names.
Solution: Verify your Airtable API key, Base ID, Table ID, and column names. Ensure that the data types in n8n match the data types in your Airtable columns.
Follow me on Linkedin for more"
"Android to N8N Automation | Save Links to with Readeck, Openrouter, SerpAPI",https://n8n.io/workflows/3117-android-to-n8n-automation-or-save-links-to-with-readeck-openrouter-serpapi/,"This workflow is for automating and centralizing your bookmarking process using AI-powered tagging and seamless integration between your Android device and a self-hosted Read Deck platform (https://readeck.org/en/). This workflow eliminates manual entry, organizes links with smart AI-generated tags, and ensures your bookmarks are always accessible, searchable, and secure.
How It Works
üì± Android Shortcut Integration
Use the HTTP Shortcuts app to create a 1-tap trigger that sends URLs and titles from your Android phone directly to n8n.
ü§ñ AI-Powered Tagging & Processing
Leverage ChatGPT-4 to analyze content context and auto-generate relevant tags (e.g., ‚ÄúTech Tutorials,‚Äù ‚ÄúProductivity Tools‚Äù).
Extract clean titles and URLs from messy shared data (even from apps like Twitter or Reddit).
üîó Readeck Integration
Automatically save processed bookmarks to your self-hosted Readeck-like platform with structured metadata (title, URL, tags).
‚ö° Silent Automation
It runs in the background‚Äîno pop-ups or interruptions.
üîí Pro Security
Optional authentication (API tokens, headers) to protect your data.
Use Case
Perfect for researchers, content creators, or anyone drowning in tabs who wants to:
Save articles, videos, or social posts in one click.
Organize bookmarks with AI-generated tags.
Build a personal knowledge base that‚Äôs always accessible.
Tutorial
1Ô∏è‚É£ Set Up Android Shortcut
Install ""HTTP Shortcuts"" and configure it to send data to your n8n webhook.
Enable ‚ÄúShare Menu‚Äù to trigger bookmarks from any app.
2Ô∏è‚É£ Configure n8n Workflow
Import the template and add your Read Deck API token (or similar service).
3Ô∏è‚É£ Test & Scale
Share a link from your phone‚Äîwatch it appear in Read Deck instantly!
Add error handling or notifications for advanced use.
Note: For self-hosted platforms, ensure your instance is publicly accessible (or use a VPN).
Why Choose This Workflow?
Zero Manual Entry: Save hours of copying/pasting.
AI Organization: Say goodbye to chaotic bookmark folders.
Privacy First: Host your data on your terms.
Transform your bookmarking chaos into a streamlined system‚Äîtry ‚ÄúSave: Bookmark‚Äù today! üöÄ"
Analyze Meta Ad Library Video Ads with Gemini and store results in Google Sheets,https://n8n.io/workflows/3069-analyze-meta-ad-library-video-ads-with-gemini-and-store-results-in-google-sheets/,"Meta Ads Analyzer
This n8n template builds an automated system to scrape, analyze, and extract insights from Meta advertising content. The workflow uses AI to perform deep analysis of video ads and organize the results in a structured format.
How it works
The workflow connects to Facebook's Ad Library to scrape video ads based on a specified page ID.
Videos are filtered by reach and processed through Google's Gemini AI to analyze their content.
Each video is systematically analyzed for its hook, transcript, advertising format, concept, and narrative structure.
Results are processed through an AI agent that structures the data into standardized fields.
All analysis is saved to a Google Sheet for easy access and further processing.
How to use
Once you've set up your Credentials and configured the output:
Configure the Google Sheets connection for data output.
Specify the Meta Ad Library URL you want to analyze.
Adjust the maximum number of ads to scrape and analyze based on your needs.
Click ""Test Workflow"" to start the analysis process.
Requirements
Apify account (for Meta Ad Library scraping)
Google Gemini
Google Sheets
Customizing this workflow
Modify the AI prompts in the Settings node to extract different information from the videos.
Adjust the output formats in the AI Agent and Structured Output Parser nodes.
Change the Google Sheets mapping to match your desired output structure.
Increase the number of ads analyzed for more comprehensive research."
Generate ü§ñüß† AI-powered video üé• from image and upload it on Google Drive,https://n8n.io/workflows/3088-generate-ai-powered-video-from-image-and-upload-it-on-google-drive/,"This workflow automates the process of generating AI-powered video from image and then generates a video that is uploaded to Google Drive.
This workflow is a powerful tool for automating the creation of AI-generated videos from images, saving time and ensuring a seamless process from input to final output.
Below is a breakdown of the workflow:
1. How It Works
The workflow is designed to create videos from images using AI and manage the generated content. Here's how it works:
Form Submission:
The workflow starts with a Form Trigger node, where users submit a form with the following fields:
Description: The text description for the video.
Duration (in seconds): The length of the video.
Aspect Ratio: The aspect ratio of the video (e.g., 16:9, 9:16, 1:1).
Image URL: The URL of the image to be used for video generation.
Set Data:
The Set Data node organizes the form inputs into a structured format for further processing.
Create Video:
The Create Video node sends a POST request to generate the video.
The request includes the description, image URL, duration, and aspect ratio.
Wait and Check Status:
The Wait 60 sec. node pauses the workflow for 60 seconds to allow the video generation process to complete.
The Get Status node checks the status of the video generation by querying the API.
Completed?:
The Completed? node checks if the video generation is complete. If not, the workflow loops back to wait and check again.
Retrieve and Upload Video:
Once the video is generated, the Get Url Video node retrieves the video URL.
The Get File Video node downloads the video file.
The Upload Video node uploads the video to a specified folder in Google Drive.
Watch the resulting video
2. Set Up Steps
To set up and use this workflow in n8n, follow these steps:
API Key:
Create an account on account and obtain your API Key.
In the Create Video node, set up HTTP Header Authentication:
Name: Authorization
Value: Key YOURAPIKEY
Google Drive Integration:
Set up Google Drive credentials in n8n for the Upload Video node.
Specify the folder ID in Google Drive where the generated videos will be uploaded.
Form Configuration:
The Form Trigger node is pre-configured with fields for:
Description: The text description for the video.
Duration (in seconds): The length of the video.
Aspect Ratio: Choose between 16:9, 9:16, or 1:1.
Image URL: The URL of the image to be used for video generation.
Customize the form fields if needed.
Test the Workflow:
Submit the form with the required details (description, duration, aspect ratio, and image URL).
The workflow will:
Generate the video using the API.
Check the status until the video is ready.
Upload the video to Google Drive.
Optional Customization:
Modify the workflow to include additional features, such as:
Adding more aspect ratio options.
Sending notifications when the video is ready.
Integrating with other storage services (e.g., Dropbox, AWS S3)."
Generate & Upload an Audio Summary of a WordPress (or Woocommerce) Article,https://n8n.io/workflows/3028-generate-and-upload-an-audio-summary-of-a-wordpress-or-woocommerce-article/,"This workflow automates the process of summarizing or transcribing a WordPress article, converting the text into speech using Eleven Labs API, and uploading the resulting MP3 file back to WordPress.
How It Works
Trigger ‚Äì The workflow starts manually when the user clicks ‚ÄúTest Workflow‚Äù.
Retrieve Article ‚Äì It fetches a WordPress article based on a given post ID.
Summarize or Transcribe ‚Äì An LLM (GPT-4o-mini) generates either:
‚Ä¢ A summary of the article, or
‚Ä¢ A full transcription, depending on the chosen prompt.
Generate Speech ‚Äì The processed text (summary or transcription) is converted into an MP3 audio file using Eleven Labs API.
Upload MP3 to WordPress ‚Äì The generated MP3 file is uploaded to WordPress.
Update WordPress Post ‚Äì The article is updated with an embedded audio player, allowing users to listen to the summary or transcription.
Set Up Steps
1. WordPress API Credentials
‚Ä¢ Configure your WordPress API credentials in n8n.
2. Eleven Labs API Key
‚Ä¢ Obtain an API Key from Eleven Labs and configure it in n8n.
3. Choose Between Summary or Transcription
‚Ä¢ Modify the AI prompt to either generate a summary or keep the full transcription.
4. Test the Workflow
‚Ä¢ Run the workflow and ensure the MP3 file is correctly generated and uploaded.
üí° Customization Options
‚Ä¢ Modify the AI prompt to switch between a summary and a transcription.
‚Ä¢ Change the voice model in Eleven Labs for different speech styles.
‚Ä¢ Adjust output format to higher/lower quality MP3.
üöÄ This automation improves content accessibility and engagement by allowing users to listen to a summarized or full version of the article.
Phil | Inforeole"
Get Daily Exercise Plan with Flex Message via LINE,https://n8n.io/workflows/2988-get-daily-exercise-plan-with-flex-message-via-line/,"The YogiAI workflow automates sending daily yoga pose reminders and related information via Line Push Messages . This automation leverages data from a Google Sheets database containing yoga pose details such as names, image URLs, and links to ensure users receive personalized and engaging content every day.
Purpose
Provide users with daily yoga pose suggestions tailored to their practice.
Deliver visually appealing and informative content through Line's Flex Messages, including images and clickable links.
Log user interactions and preferences back into Google Sheets to refine future recommendations.
Key Features
Automated Daily Reminders : Sends a curated list of yoga poses at a scheduled time (21:30 Bangkok time).
Dynamic Content Generation : Uses AI to rewrite and format messages in a user-friendly manner, complete with emojis and clear instructions.
Integration with Google Sheets : Pulls data from a predefined Google Sheet and logs interactions for continuous improvement.
Customizable Messaging : Ensures JSON outputs are properly formatted for Line‚Äôs Flex Message API, allowing for interactive and visually rich content.
Data Source
Google Sheets Structure
The workflow relies on a Google Sheet structured as follows:
PoseName : The name of the yoga pose.
uri : The image URL representing the pose.
url : A clickable link directing users to more information about the pose.
Sample Data Layout
Supine Angle
https://example.com/SupineAngle-tn146.png
https://example.com/pose/SupineAngle
Warrior II
https://example.com/WarriorII-tn146.png
https://example.com/pose/WarriorII
*Note : Ensure that you update the Google Sheet with your own data. Refer to this sample sheet for reference. *
Scheduled Trigger
The workflow is triggered daily at 21:30 (9:30 PM) Bangkok Time (Asia/Bangkok) . This ensures timely delivery of reminders to users, keeping them engaged with their yoga practice.
Workflow Process
Data Retrieval
Node: Get PoseName
Fetches yoga pose details from the specified range in the Google Sheet.
Content Generation
Node: WritePosesToday
Utilizes Azure OpenAI to craft user-friendly text, complete with emojis and clear instructions.
Node: RewritePosesToday
Formats the AI-generated text specifically for Line messaging, ensuring compatibility and visual appeal.
JSON Formatting
Node: WriteJSONflex
Generates JSON structures required for Line‚Äôs Flex Messages, enabling carousel displays of yoga pose images and links.
Node: Fix JSON
Ensures all JSON outputs are correctly formatted before being sent via Line.
Message Delivery
Node: Line Push with Flex Bubble
Sends the final message, including both text and Flex Message carousels, directly to users via Line Push Messages.
Logging Interactions
Nodes: YogaLog & YogaLog2
Logs each interaction back into Google Sheets to track which poses were sent and how often they appear, refining future recommendations.
Setup Prerequisites
Google Sheets Account : Set up a Google Sheet with the required structure and populate it with your yoga pose data.
Line Developer Account : Create a Line channel to obtain necessary credentials for sending push messages.
Azure OpenAI Account : Configure access to Azure OpenAI services for generating and formatting content.
Intended Audience
This workflow is ideal for:
Yoga Instructors : Seeking to engage students with daily pose suggestions.
Fitness Enthusiasts : Looking to maintain consistency in their yoga practice.
Content Creators : Interested in automating personalized and visually appealing content distribution."
"Automatically create YouTube short videos using Elevenlabs, Hailuo AI",https://n8n.io/workflows/2914-automatically-create-youtube-short-videos-using-elevenlabs-hailuo-ai/,"AI-Powered Workflow for Automated Video Creation
Overview
This workflow is designed for individuals and businesses looking to streamline the creation of engaging promotional videos. Whether you're marketing a product or developing a personal brand, this AI-driven system automates the entire video production process, from script generation to final video upload.
Key Features
üéôÔ∏è AI-Generated Voiceovers with ElevenLabs
Using ElevenLabs, this workflow converts text-based scripts into natural-sounding voiceovers, making your videos more engaging and professional.
‚úçÔ∏è Enhanced Prompt Engineering with Together AI
To ensure high-quality content, Together AI refines and optimizes prompts, helping generate more compelling narratives and animations.
üé• Image-to-Video Conversion with Hailuo AI
With Hailuo AI, static images are transformed into dynamic animated videos, allowing for visually appealing content without the need for manual editing.
üß† Intelligent Processing with OpenAI
Various inference tasks, such as text generation and content refinement, are handled by OpenAI, ensuring smooth and efficient video production.
Automated Video Upload & Sharing
üìä Google Sheets Integration
Once the video is created, its downloadable URL is automatically uploaded to a Google Sheets document, making it easy to access and manage content. Add video title, video url, video status columns for tracking and archiving.
üì¢ Multi-Platform Distribution
Users can download and upload the generated videos to major social media platforms, including Facebook, YouTube, Instagram, and TikTok, maximizing reach and engagement.
Who Can Benefit?
This workflow is ideal for:
Marketers looking to create fast, automated promotional videos
Content creators who want to streamline video production
Businesses seeking an efficient way to generate engaging product showcases
By leveraging the power of AI, this workflow simplifies video creation, saving time while delivering high-quality content. üöÄ"
Automate Crypto News Posting to X & Telegram with AI Summarization,https://n8n.io/workflows/2961-automate-crypto-news-posting-to-x-and-telegram-with-ai-summarization/,"Automate Crypto News Posting to X & Telegram with AI Summarization
This n8n template automates the process of curating and sharing the latest cryptocurrency news on X (formerly Twitter) and Telegram. By leveraging AI for content summarization, this workflow allows you to effortlessly maintain an active social media presence, keeping your audience informed about the dynamic crypto market without manual effort.
Who is this for?
This template is ideal for:
Content Creators & Marketers: Aiming to consistently share valuable news and engage their audience without manual content curation.
Crypto Influencers & Educators: Looking to provide timely news updates to their followers across multiple platforms.
Crypto Communities & DAOs: Seeking to automate news dissemination within their Telegram channels and wider X audience.
Anyone interested in automated news monitoring and sharing.
What problem is this workflow solving?
Manually tracking, summarizing, and posting crypto news across different social media platforms is time-consuming and requires constant effort. This workflow eliminates these manual tasks, allowing users to:
Save Time & Effort: Automate the entire news curation and posting process.
Maintain Consistent Presence: Ensure a regular flow of valuable crypto news updates on X and Telegram.
Increase Audience Engagement: Provide timely and summarized news to keep your audience informed and engaged.
Focus on Strategy: Free up time to focus on broader content strategy and audience growth instead of repetitive manual posting.
What this workflow does:
This workflow automates the following key steps:
Scheduled News Retrieval: Uses a Schedule Trigger to run every 90 minutes (configurable), initiating the news gathering process.
Real-time Crypto News Aggregation: Fetches the latest cryptocurrency news from the CryptoPanic API.
Recent News Filtering: Filters news articles to include only those published within the last 30 minutes, ensuring timely updates.
Content Extraction from News URLs: Visits individual news URLs and extracts the full article content.
AI-Powered Content Summarization: Leverages GPT or other LLMs to extract the core content from news articles.
Content Aggregation: Merges content from multiple news articles into a single input for summarization.
AI-Driven Social Media Content Generation: Utilizes GPT or other LLMs to summarize the aggregated news and create two distinct outputs:
Concise & Engaging X Post: Optimized for Twitter's character limit, designed to be attention-grabbing.
Detailed Telegram Report: A more comprehensive summary suitable for a Telegram channel or group.
Automated Posting to X (Twitter): Automatically posts the generated X summary to your connected Twitter account.
Automated Delivery to Telegram: Automatically sends the detailed Telegram report to your specified Telegram chat ID.
Setup:
To get started, you will need to configure the following services and credentials:
CryptoPanic API Token:
Obtain a free API token from the CryptoPanic website: https://cryptopanic.com/
In n8n, navigate to the ""HTTP Request"" node (named ""HTTP Request"").
In the node parameters, locate the ""URL"" field and replace ""YOURTOKEN"" in the URL with your obtained CryptoPanic API token.
OpenAI API Key:
Obtain an API key from OpenAI: https://platform.openai.com/
For Content Extraction: In n8n, connect your OpenAI account to the ""ContentExtraction GPT3.5"" node (named ""ContentExtraction GPT3.5""). Use your OpenAI API key for the credentials.
For News Summarization & Social Media Content Generation: In n8n, connect your OpenAI account to the ""Summary news GPT"" node (named ""Summary news GPT""). Use your OpenAI API key for the credentials.
X (Twitter) Developer Credentials:
Create a developer account and project on the X Developer Portal: https://developer.twitter.com/
Obtain the necessary API keys and tokens for your X app.
In n8n, connect your X Developer account credentials to the ""X"" node (named ""X"").
Telegram Bot and Chat ID:
Create a Telegram bot using BotFather on Telegram. Obtain your bot's API token.
Obtain the Chat ID of the Telegram chat where you want to send news reports.
In n8n, connect your Telegram Bot API token to the ""Telegram"" node (named ""Telegram"").
In the ""Telegram"" node parameters, replace ""YOUR_TELEGRAM_CHAT_ID"" with your Telegram Chat ID.
How to customize this workflow:
Adapt to ANY Topic: Change the ""HTTP Request"" node to use a news API for your desired topic (AI, Sports, World News, etc.). Critically, adjust the prompts in the ""Summary news GPT"" node to be relevant to your chosen topic so the AI generates appropriate summaries and social media content.
Adjust Scheduling Frequency: Modify the ""Schedule Trigger"" node to change how often the workflow runs and posts news.
Adjust Scheduling Frequency: Modify the ""Schedule Trigger"" node to change the frequency of news updates (e.g., change the interval from 90 minutes to a different value).
Modify News Filtering: Customize the Python code in the ""Extract Meta"" node to adjust the news filtering criteria. You can change the time window (currently 30 minutes) or filter based on other criteria from the CryptoPanic API response.
Experiment with GPT Models: In the ""Summary news GPT"" node, try different OpenAI models (e.g., gpt-4, gpt-3.5-turbo-16k) to see how they affect the summarization quality and output. Note that more advanced models may incur higher API costs.
Customize AI Prompts: Fine-tune the system and user prompts in the ""Summary news GPT"" node to alter the tone, style, or format of the generated X and Telegram content. You can adjust the persona of the AI blogger, the desired level of detail in summaries, or specific keywords to include.
Extend to Other Platforms: Add nodes to post to other social media platforms like LinkedIn, Facebook, or Discord by adapting the ""Summary news GPT"" prompts and integrating relevant n8n nodes for those platforms.
Category:
Marketing, Social Media, AI, News Automation, Content Creation
Workflow by: Tianyi (muzi)
n8n Creators Profile: https://n8n.io/creators/muzi/"
üé¶üöÄ YouTube Video Comment Analysis Agent,https://n8n.io/workflows/2965-youtube-video-comment-analysis-agent/,"üé¶üöÄ YouTube Video Comment Analysis Agent
This n8n workflow is designed to help YouTube creators analyze video details and comments to generate a comprehensive and actionable report. The workflow provides insights into video performance, audience engagement, and viewer feedback, helping creators identify trends, interests, and opportunities for future content creation.
‚ú® Key Features
Video Performance Analysis: Extracts metrics like views, likes, and comments to evaluate the video's success.
Comment Sentiment Analysis: Determines the tone of comments (positive, neutral, or negative) to understand audience sentiment.
Recurring Themes Detection: Identifies common topics or questions in comments to highlight viewer interests.
Engagement Drivers: Pinpoints what aspects of the video resonated most with viewers.
Actionable Recommendations: Offers strategies for creating follow-up content or improving future videos.
Keyword Suggestions: Extracts frequently mentioned terms for better discoverability on YouTube.
Collaboration Opportunities: Suggests potential partnerships based on viewer feedback or related channels.
üõ†Ô∏è How to Use
Set Up Workflow Variables:
Add your GOOGLE_API_KEY and the VIDEO_ID of the YouTube video you want to analyze in the ""Workflow Variables"" node.
Ensure your Google API key has access to the YouTube Data API.
Run the Workflow:
Trigger the workflow manually or through another workflow using the ""Execute Workflow Trigger"" node.
The workflow will fetch video details and comments using pagination to ensure all data is captured.
Generate Insights:
The workflow processes video details and comments to create a detailed report with actionable insights.
Outputs include sentiment analysis, engagement drivers, content opportunities, and audience profiling.
View or Share Results:
The report is converted into Markdown and can be emailed via Gmail or saved to Google Drive as a document.
üåü Value from This Workflow
Gain a deeper understanding of your audience's preferences and feedback.
Identify trends and engagement drivers to replicate success in future videos.
Discover new content opportunities based on viewer questions and suggestions.
Improve discoverability by leveraging keyword suggestions extracted from comments.
Build stronger connections with your audience by addressing their needs effectively."
Fetch Keyword From Google Sheet and Classify Them Using AI,https://n8n.io/workflows/2945-fetch-keyword-from-google-sheet-and-classify-them-using-ai/,"Who is this template for
This template is for marketers, SEO specialists, or content managers who need to analyze keywords to identify which ones contain references to a specific area or topic, in this case ‚Äì IT software, services, tools, or apps.
Use case
Automating the process of scanning a large list of keywords to determine if they reference known IT products or services (like ServiceNow, Salesforce, etc.), and updating a Google Sheet with this classification. This helps in categorizing keywords for targeted SEO campaigns, content creation, or market analysis.
How this workflow works
Fetches keyword data from a Google Sheet
Processes keywords in batches to prevent rate limiting
Uses an AI agent (OpenAI) to analyze each keyword and determine if it contains a reference to an IT service/software
Updates the original Google Sheet with the results in a ""Service?"" column
Continues processing until all keywords are analyzed
Set up steps
Connect your Google Sheets account credentials
Set the Google Sheet document ID (currently using ""Copy of Sheet1 1"")
Configure the OpenAI API credentials for the AI agent
Adjust the batch size (currently 6) if needed based on your API rate limits
Ensure the Google Sheet has the required columns: ""Number"", ""Keyword"", and ""Service?""
The AI agent's prompt is highly customizable to match different identification needs. For example, instead of looking for IT software/services, you could modify the prompt to identify:
Industry-specific terms (healthcare, finance, education)
Geographic references (cities, countries, regions)
Product categories (electronics, clothing, food)
Competitor brand mentions
Here's how you could modify the prompt for different use cases:
Copy
// For identifying educational content keywords
""Check the keyword I provided and define if this keyword relates to educational content, courses, or learning materials and return yes or no.""

// For identifying local service keywords
""Check the keyword I provided and determine if it contains location-specific terms (city names, neighborhoods, regions) that suggest local service intent and return yes or no.""

// For identifying competitor mentions
""Check the keyword I provided and determine if it mentions any of our competitors (CompetitorA, CompetitorB, CompetitorC) and return yes or no."""
Bitrix24 Chatbot Application Workflow example with Webhook Integration,https://n8n.io/workflows/2923-bitrix24-chatbot-application-workflow-example-with-webhook-integration/,"Use Case
Automate chat interactions in Bitrix24 with a customizable bot that can handle various events and respond to user messages.
What This Workflow Does
Processes incoming webhook requests from Bitrix24
Handles authentication and token validation
Routes different event types (messages, joins, installations)
Provides automated responses and bot registration
Manages secure communication between Bitrix24 and external services
Setup Instructions
Configure Bitrix24 webhook endpoints
Set up authentication credentials
Customize bot responses and behavior
Deploy and test the workflow"
ü§ñüßë‚Äçüíª AI Agent for Top n8n Creators Leaderboard Reporting,https://n8n.io/workflows/2942-ai-agent-for-top-n8n-creators-leaderboard-reporting/,"This n8n workflow is designed to automate the aggregation, processing, and reporting of community statistics related to n8n creators and workflows. Its primary purpose is to generate insightful reports that highlight top contributors, popular workflows, and key trends within the n8n ecosystem. Here's how it works and why it's important:
How It Works
Data Retrieval:
The workflow fetches JSON data files from a GitHub repository containing statistics about creators and workflows.
It uses HTTP requests to access these files dynamically based on pre-defined global variables.
Data Processing:
The data is parsed into separate streams for creators and workflows.
It processes the data to identify key metrics such as unique weekly and monthly inserters/visitors.
Ranking and Filtering:
The workflow sorts creators by their weekly inserts and workflows by their popularity.
It selects the top 10 creators and top 50 workflows for detailed analysis.
Report Generation:
Using AI tools like GPT-4 or Google Gemini, the workflow generates a Markdown report summarizing trends, contributors, and workflow statistics.
The report includes tables with detailed metrics (e.g., unique visitors, inserters) and insights into why certain workflows are popular.
Distribution:
The report is saved locally or uploaded to Google Drive.
It can also be shared via email or Telegram for broader accessibility.
Automation:
A schedule trigger ensures the workflow runs daily or as needed, keeping the reports up-to-date.
Why It's Important
Community Insights: This workflow provides actionable insights into the n8n community by identifying impactful contributors and popular workflows. This fosters collaboration and innovation within the ecosystem.
Time Efficiency: By automating data collection, processing, and reporting, it saves significant time and effort for community managers or administrators.
Recognition of Contributors: Highlighting top creators encourages engagement and recognizes individuals driving value in the community.
Trend Analysis: The workflow helps uncover patterns in usage, enabling better decision-making for platform improvements or feature prioritization.
Scalability: With its modular design, this workflow can be easily adapted to include additional metrics or integrate with other tools."
Detect hallucinations using specialised Ollama model bespoke-minicheck,https://n8n.io/workflows/2922-detect-hallucinations-using-specialised-ollama-model-bespoke-minicheck/,"Fact-Checking Workflow Documentation
Overview
This workflow is designed for automated fact-checking of texts. It uses AI models to compare a given text with a list of facts and identify potential discrepancies or hallucinations.
Components
1. Input
The workflow can be initiated in two ways:
a) Manually via the ""When clicking 'Test workflow'"" trigger
b) By calling from another workflow via the ""When Executed by Another Workflow"" trigger
Required inputs:
facts: A list of verified facts
text: The text to be checked
2. Text Preparation
The ""Code"" node splits the input text into individual sentences
Takes into account date specifications and list elements
3. Fact Checking
Each sentence is individually compared with the given facts
Uses the ""bespoke-minicheck"" Ollama model for verification
The model responds with ""Yes"" or ""No"" for each sentence
4. Filtering and Aggregation
Sentences marked as ""No"" (not fact-based) are filtered
The filtered results are aggregated
5. Summary
A larger language model (Qwen2.5) creates a summary of the results
The summary contains:
Number of incorrect factual statements
List of incorrect statements
Final assessment of the article's accuracy
Usage
Ensure the ""bespoke-minicheck"" model is installed in Ollama (ollama pull bespoke-minicheck)
Prepare a list of verified facts
Enter the text to be checked
Start the workflow
The results are output as a structured summary
Notes
The workflow ignores small talk and focuses on verifiable factual statements
Accuracy depends on the quality of the provided facts and the performance of the AI models
Customization Options
The summarization function can be adjusted or removed to return only the raw data of the issues found
The AI models used can be exchanged if needed
This workflow provides an efficient method for automated fact-checking and can be easily integrated into larger systems or editorial workflows."
Use OpenRouter in n8n versions <1.78,https://n8n.io/workflows/2897-use-openrouter-in-n8n-versions-less178/,"What it is:
In version 1.78, n8n introduced a dedicated node to use the OpenRouter service, which lets you to use a lot of different LLM models and providers and change models on the fly in an agentic workflow.
For prior n8n versions, there's a workaround to make OpenRouter accessible, by using the OpenAI node with a OpenRouter-specific BaseURL.
This trivial workflow demonstrates this for version before 1.78, so that you can use different LLM model dynamically with the available n8n nodes for OpenAI LLM and OpenAI credentials.
What you can do:
Use any of the OpenRouter models
Have the model even dynamically configured or changing (by some external config, some rule, or some specific chat message)
Setup steps:
Import the workflow
Ensure you have registered and account, purchased some credits and created and API key for OpenRouter.ai
Configure the ""OpenRouter"" credentials with your own credentials, using an OpenAI type credential, but making sure in the credential's config form its ""Base URL"" is set to _https://openrouter.ai/api/v1_ so OpenRouter is used instead of OpenAI.
Open the ""Settings"" node and change the model value to any valid model id from the OpenRouter models list or even have the model property set dynamically"
"AI-Powered Crypto Analysis Using OpenRouter, Gemini, and SerpAPI",https://n8n.io/workflows/2906-ai-powered-crypto-analysis-using-openrouter-gemini-and-serpapi/,"This n8n automation is designed to analyze cryptocurrency trends by extracting, processing, and interpreting candlestick charts using AI-powered agents. The workflow enhances technical analysis by integrating real-time market data, ensuring traders receive accurate and actionable insights.
Workflow Breakdown:
üîπ 1. Chat Node ‚Äì Provide Crypto Information
Users enter a crypto symbol in the required format (EXCHANGE:SYMBOL), such as BINANCE:BTCUSDT. This ensures the workflow retrieves the correct market data.
üîπ Retrieve Daily Candlestick Chart
Once the input is received, the workflow fetches the full-day candlestick chart for the selected crypto, providing a macro-level market trend.
üîπ AI Agent ‚Äì Analyze Daily Chart
The first AI agent, powered by Google Gemini 2.0 Flash via OpenRouter, analyzes the daily candlestick pattern to detect trends and potential market signals.
üîπ Fetch 5-Minute Candlestick Chart
To refine the analysis, the workflow retrieves a 5-minute interval candlestick chart, allowing for real-time market movement evaluation.
üîπ AI Agent ‚Äì Advanced Candlestick Analysis
This AI agent combines the 5-minute chart with the daily analysis to provide an in-depth market prediction. Here‚Äôs where the real magic happens‚ÄîAI interprets short-term trends in the context of long-term movements.
üîπ Shared Windows Buffer ‚Äì Store Intermediate Results
The Windows Buffer temporarily stores analysis results, ensuring seamless data flow between AI agents for a more structured interpretation.
üîπ Serp API ‚Äì Retrieve Crypto News
To add fundamental analysis, the Serp API tool fetches the latest crypto-related news from the web, providing additional market context.
üîπ Chat Window ‚Äì Deliver Final Insights
Once all data points are processed, the final market analysis is displayed in the chat window, combining technical and fundamental analysis for a more comprehensive trading strategy.
Use Case:
This automation simplifies crypto market analysis by integrating AI-driven technical and fundamental insights. It‚Äôs ideal for:
‚úÖ Traders looking for automated market insights
‚úÖ Analysts seeking structured candlestick interpretations
‚úÖ Developers wanting to integrate AI-powered trading analysis into applications
By automating candlestick chart analysis, this workflow enhances decision-making and reduces manual effort, making it a valuable tool for anyone involved in cryptocurrency trading.
Setup Instructions:
1Ô∏è‚É£ Import the workflow to your n8n instance
2Ô∏è‚É£ Prepare & add credentials:
OpenRouter (Google Gemini 2.0 Flash) Get a free API key from https://openrouter.ai/
Serp API (for news retrieval) Get a free API key from https://serpapi.com/
Chart Img (For candlestick chart) Get a free API key from https://chart-img.com/
3Ô∏è‚É£ Run the workflow and get AI-powered crypto insights!
NOTE
Remember: Not all LLM models are capable of analyzing image data, so choose your model wisely.
Limitations: All free services come with usage limits. For example, OpenRouter has a daily limit, and once it's consumed, the workflow will stop processing further requests.
Disclaimer
This workflow is designed purely for educational and research purposes. It does not provide financial advice. üöÄ"
Slack slash commands AI Chat Bot,https://n8n.io/workflows/2905-slack-slash-commands-ai-chat-bot/,"This is a response chatbot in public channels through slash commands. I explain more in detail through the YouTube video, but it's only available in Korean.
How it works?
When you request the created slash command in Slack, the request comes to the webhook. Then, the Switch Node branches appropriately according to each slash command request. Here, a slash command called /ask is connected to the chatbot, and the chatbot generates answers to the questions asked. The final node responds to the channel.
Set up steps
Create a Slack app.
Add chat:write permission in Slack OAuth&Permissions>Scopes.
Create a Command in Slack Slash Commands menu and enter the n8n Webhook node's URL.
Complete creating the Slash Commands.
Enter the created command in the Switch node.
<br />
<br />
Ïä¨ÎûòÏãú Ïª§Îß®ÎìúÎ•º ÌÜµÌïú Í≥µÍ∞ú Ï±ÑÎÑêÏóêÏÑúÏùò ÏùëÎãµ Ï±óÎ¥á ÏûÖÎãàÎã§. Ïú†ÌäúÎ∏å ÏòÅÏÉÅÏóê Îçî ÏûêÏÑ∏ÌïòÍ≤å ÏÑ§Î™Ö ÎìúÎ¶ΩÎãàÎã§.
ÏÑ§Î™Ö
Ïä¨ÎûôÏóê ÏÉùÏÑ±Ìïú Ïä¨ÎûòÏãú Ïª§Îß®ÎìúÎ•º Ïä¨ÎûôÏóêÏÑú ÏöîÏ≤≠ÌïòÎ©¥ ÏõπÌõÖÏóê ÏöîÏ≤≠Ïù¥ Îì§Ïñ¥ÏòµÎãàÎã§. Ïù¥ÌõÑ Switch NodeÏóêÏÑú Í∞Å Ïä¨ÎûòÏãú Ïª§Îß®ÎìúÏùò ÏöîÏ≤≠Ïóê Îî∞Îùº ÏïåÎßûÍ≤å Î∂ÑÍ∏∞Ìï©ÎãàÎã§. Ïó¨Í∏∞ÏóêÏÑúÎäî /askÎùºÎäî Ïä¨ÎûòÏãú Ïª§Îß®ÎìúÍ∞Ä Ï±óÎ¥áÏúºÎ°ú Ïó∞Í≤∞ÎêòÏñ¥ ÏûàÍ≥†, Ï±óÎ¥áÏóêÏÑú ÏßàÎ¨∏Ìïú ÎÇ¥Ïö©Ïùò ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§. ÎßàÏßÄÎßâ ÎÖ∏ÎìúÏóêÏÑú Ï±ÑÎÑêÎ°ú ÏùëÎãµÏùÑ Ìï©ÎãàÎã§.
ÏÑ§Ï†ï Î∞©Î≤ï
Slack Ïï±ÏùÑ ÎßåÎìúÏÑ∏Ïöî.
Slack OAuth&Permissions>Scopes ÏóêÏÑú chat:write Í∂åÌïúÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.
Slack Slash Commands Î©îÎâ¥ÏóêÏÑú CommandÎ•º ÏÉùÏÑ±ÌïòÍ≥†, n8n Webhook ÎÖ∏ÎìúÏùò urlÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.
Slash Slash Commands ÏÉùÏÑ±ÏùÑ ÏôÑÎ£åÌïòÏÑ∏Ïöî.
Switch ÎÖ∏ÎìúÏóê ÏÉùÏÑ±Ìïú Ïª§Îß®ÎìúÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî."
Arxiv Paper summarization with ChatGPT,https://n8n.io/workflows/2904-arxiv-paper-summarization-with-chatgpt/,"Webhook | Paper Summarization
Who is this for?
This workflow is designed for researchers, students, and professionals who frequently read academic papers and need concise summaries. It is useful for anyone who wants to quickly extract key information from research papers hosted on arXiv.
What problem is this workflow solving?
Academic papers are often lengthy and complex, making it time-consuming to extract essential insights. This workflow automates the process of retrieving, processing, and summarizing research papers, allowing users to focus on key findings without manually reading the entire paper.
What this workflow does
This workflow extracts the content of an arXiv research paper, processes its abstract and main sections, and generates a structured summary. It provides a well-organized output containing the Abstract Overview, Introduction, Results, and Conclusion, ensuring that users receive critical information in a concise format.
Setup
Ensure you have n8n installed and configured.
Import this workflow into your n8n instance.
Configure an external trigger using the Webhook node to accept paper IDs.
Test the workflow by providing an arXiv paper ID.
(Optional) Modify the summarization model or output format according to your preferences.
How to customize this workflow to your needs
Adjust the HTTPRequest node to fetch papers from other sources beyond arXiv.
Modify the Summarization Chain node to refine the summary output.
Enhance the Reorganize Paper Summary step by integrating additional language models.
Add an email or Slack notification step to receive summaries directly.
Workflow Steps
Webhook receives a request with an arXiv paper ID.
Send an HTTP request using ""Request to Paper Page"" to fetch the HTML content of the paper.
Extract the abstract and sections using ""Extract Contents"".
Split out all sections using ""Split out All Sections"" to process individual paragraphs.
Clean up text using ""Remove useless links"" to remove unnecessary elements.
Summarize extracted content using ""Summarization Chain"".
Aggregate summarized content using ""Aggregate summarized content"".
Reorganize the paper summary into structured sections using ""Reorganize Paper Summary"".
Extract key information using ""Content Extractor"" to classify data into Abstract Overview, Introduction, Results, and Conclusion.
Respond to the webhook with the structured summary.
Note: This workflow is designed for use with arXiv research papers but can be adapted to process papers from other sources."
Fetch Dynamic Prompts from GitHub and Auto-Populate n8n Expressions in Prompt,https://n8n.io/workflows/2893-fetch-dynamic-prompts-from-github-and-auto-populate-n8n-expressions-in-prompt/,"Who Is This For?
This workflow is designed for AI engineers, automation specialists, and content creators who need a scalable system to dynamically manage prompts stored in GitHub. It eliminates manual updates, enforces required variable checks, and ensures that AI interactions always receive fully processed prompts.
üöÄ What Problem Does This Solve?
Manually managing AI prompts can be inefficient and error-prone. This workflow:
‚úÖ Fetches dynamic prompts from GitHub
‚úÖ Auto-populates placeholders with values from the setVars node
‚úÖ Ensures all required variables are present before execution
‚úÖ Processes the formatted prompt through an AI agent
üõ† How This Workflow Works
This workflow consists of three key branches, ensuring smooth prompt retrieval, variable validation, and AI processing.
1Ô∏è‚É£ Retrieve the Prompt from GitHub (HTTP Request ‚Üí Extract from File ‚Üí SetPrompt)
The workflow starts manually or via an external trigger.
It fetches a text-based prompt stored in a GitHub repository.
The Extract from File Node retrieves the content from the GitHub file.
The SetPrompt Node stores the prompt, making it accessible for processing.
üìå Note:
The prompt must contain n8n expression format variables (e.g., {{ $json.company }}) so they can be dynamically replaced.
2Ô∏è‚É£ Extract & Auto-Populate Variables (Check All Prompt Vars ‚Üí Replace Variables)
A Code Node scans the prompt for placeholders in the n8n expression format ({{ $json.variableName }}).
The workflow compares required variables against the setVars node:
‚úÖ If all variables are present, it proceeds to variable replacement.
‚ùå If any variables are missing, the workflow stops and returns an error listing them.
The Replace Variables Node replaces all placeholders with values from setVars.
üìå Example of a properly formatted GitHub prompt:
Hello {{ $json.company }}, your product {{ $json.features }} launches on {{ $json.launch_date }}.
This ensures seamless replacement when processed in n8n.
3Ô∏è‚É£ AI Processing & Output (AI Agent ‚Üí Prompt Output)
The Set Completed Prompt Node stores the final, processed prompt.
The AI Agent Node (Ollama Chat Model) processes the prompt.
The Prompt Output Node returns the fully formatted response.
üìå Optional: Modify this to use OpenAI, Claude, or other AI models.
‚ö†Ô∏è Error Handling: Missing Variables
If a required variable is missing, the workflow stops execution and provides an error message:
‚ö†Ô∏è Missing Required Variables: [""launch_date""]
This ensures no incomplete prompts are sent to AI agents.
‚úÖ Example Use Case
üìú GitHub Prompt File (Using n8n Expressions)
Hello {{ $json.company }}, your product {{ $json.features }} launches on {{ $json.launch_date }}.
üîπ Variables in setVars Node
{
  ""company"": ""PropTechPro"",
  ""features"": ""AI-powered Property Management"",
  ""launch_date"": ""March 15, 2025""
}
‚úÖ Successful Output
Hello PropTechPro, your product AI-powered Property Management launches on March 15, 2025.
üö® Error Output (If Missing launch_date)
‚ö†Ô∏è Missing Required Variables: [""launch_date""]
üîß Setup Instructions
1Ô∏è‚É£ Connect Your GitHub Repository
Store your prompt in a public or private GitHub repo.
The workflow will fetch the raw file using the GitHub API.
2Ô∏è‚É£ Configure the SetVars Node
Define the required variables in the SetVars Node.
Make sure the variable names match those used in the prompt.
3Ô∏è‚É£ Test & Run
Click Test Workflow to execute.
If variables are missing, it will show an error.
If everything is correct, it will output the fully formatted prompt.
‚ö° How to Customize This Workflow
üí° Need CRM or Database Integration?
Connect the setVars node to an Airtable, Google Sheets, or HubSpot API to pull variables dynamically.
üí° Want to Modify the AI Model?
Replace the Ollama Chat Model with OpenAI, Claude, or a custom LLM endpoint.
üìå Why Use This Workflow?
‚úÖ No Manual Updates Required ‚Äì Fetches prompts dynamically from GitHub.
‚úÖ Prevents Broken Prompts ‚Äì Ensures required variables exist before execution.
‚úÖ Works for Any Use Case ‚Äì Handles AI chat prompts, marketing messages, and chatbot scripts.
‚úÖ Compatible with All n8n Deployments ‚Äì Works on Cloud, Self-Hosted, and Desktop versions."
Ask AI about your Meta Ads - ask questions about your Facebook Ads insights,https://n8n.io/workflows/2870-ask-ai-about-your-meta-ads-ask-questions-about-your-facebook-ads-insights/,"Chat with an AI and ask questions about your Meta Ads metrics.
Get key metrics like impressions, spend, reach, conversions, CTR, CPC‚Ä¶ you name it.

Who is this template for?
Marketing professionals who want an AI assistant to analyze their Meta Ads data.
You can also replace the Telegram node with another chat app, like WhatsApp, Slack, or Discord.
Which languages are supported?
Any of the 85+ languages supported by OpenAI.
How to set up
Instructions are included within the workflow itself.
Check out my other templates
üëâ https://n8n.io/creators/solomon/"
Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API,https://n8n.io/workflows/2865-automate-pinterest-analysis-and-ai-powered-content-suggestions-with-pinterest-api/,"Automate Pinterest Analysis & AI-Powered Content Suggestions With Pinterest API
This workflow automates the collection, analysis, and summarization of Pinterest Pin data to help marketers optimize content strategy. It gathers Pinterest Pin performance data, analyzes trends using an AI agent, and delivers actionable insights to the Marketing Manager via email.
This setup is ideal for content creators and marketing teams who need weekly insights on Pinterest trends to refine their content calendar and audience engagement strategy.
Prerequisites
Before setting up this workflow, ensure you have the following:
Pinterest API Access & Developer Account
Sign up at Pinterest Developers and obtain API credentials.
Ensure you have access to both Organic and Paid Pin data.
Airtable Account & API Key
Create an account at Airtable and set up a database.
Obtain an API key from Account Settings.
AI Agent for Trend Analysis
An AI-powered agent (such as OpenAI's GPT or a custom ML model) is required to analyze Pinterest trends.
Ensure integration with your workflow automation tool (e.g., Zapier, Make, or a custom Python script).
Email Automation Setup
Configure an SMTP email service (e.g., Gmail, Outlook, SendGrid) to send the summarized results to the Marketing Manager.
Step-by-Step Guide to Automating Pinterest Pin Analysis
1. Scheduled Trigger for Data Collection
At 8:00 AM (or your preferred time), an automated trigger starts the workflow.
Adjust the timing based on your marketing schedule to optimize trend tracking.
2. Fetch Data from Pinterest API
Retrieve recent Pinterest Pin performance data, including impressions, clicks, saves, and engagement rate.
Ensure both Organic and Paid Ads data are labeled correctly for clarity.
3. Store Data in Airtable
Pins are logged and categorized in an Airtable database for further analysis.
Sample Airtable Template for Pinterest Pins
Column Name Description
pin_id Unique identifier for each Pin
created_at Timestamp of when the Pin was created
title Title of the Pin
description Short description of the Pin
link URL linking to the Pin
type Type of Pin (e.g., organic, ad)
4. AI Agent Analyzes Pinterest Trends
The AI model reviews the latest Pinterest data and identifies:
Trending Topics & Keywords
Engagement Patterns
Audience Interests & Behavior Changes
Optimal Posting Times & Formats
5. Generate Content Suggestions with AI
The AI Agent recommends new Pin ideas and content calendar updates to maximize engagement.
Suggestions include creative formats, hashtags, and timing adjustments for better performance.
6. Summary & Insights Generated by AI
A concise report is created, summarizing Pinterest trends and actionable insights for content strategy.
7. Email Report Sent to the Marketing Manager
The summary is emailed to the Marketing Manager to assist with content planning and execution.
The report includes:
Performance Overview of Recent Pins
Trending Content Ideas
Best Performing Pin Formats
AI-Generated Recommendations
This workflow enables marketing teams to automate Pinterest analysis and optimize their content strategy through AI-driven insights. üöÄ"
Content Generator for WordPress v3,https://n8n.io/workflows/2849-content-generator-for-wordpress-v3/,"UPDATE
This is an updated version of the previous Ultimate Content Generator for WordPress v2.
https://n8n.io/workflows/2737-ultimate-content-generator-for-wordpress/
It includes an additional workflow to generate content using Anthropic Claude in addition to OpenAI 4o.
I also mofidied it to use the AI Agent Tool node with structured output for more consistent outputs.
Lastly, the workflow updates a new table in AirTable with the published blog post url and featured image url.
Overview-
This workflow automates the end-to-end process of creating, optimizing, and publishing content on WordPress.
It integrates AI-powered tools, Airtable, and WordPress plugins to generate high-quality, on-brand posts effortlessly.
Perfect for content creators, marketers, and business owners looking to save time and scale their content strategy.
Features
Content Creation:
AI-Powered Content: Generates SEO-friendly blog posts with structured headings, relevant keywords, and meta descriptions.
Custom Prompts: Tailor the AI-generated content to match your brand‚Äôs tone and voice.
SEO Optimization: RankMath Plugin Integration: Updates RankMath SEO with focus keywords and meta descriptions, ensuring your content is search-engine optimized.
Content Management:
Airtable Integration: Organizes content ideas, drafts, and publishing schedules in one place. Easily scalable for teams or solo creators.
Visuals:
Branded Featured Images: Automatically generates on-brand images for every post.
Publishing:
Effortless Formatting: Adapts content to fit your WordPress theme and schedules it for publication.
Workflow Steps
Trigger: Initiated manually or on a schedule.
Content Management: Retrieves and organizes ideas from Airtable.
Content Generation: Generates AI-driven blog content tailored to your audience.
SEO Optimization: Automatically updates RankMath with SEO details.
Featured Image Creation: Produces on-brand images for the post.
Publishing: Formats and schedules the post on WordPress.
Prerequisites
API Keys:
OpenAI
Claude
Airtable
WordPress REST API
Custom Code:
Add a small update to your WordPress theme‚Äôs functions.php file to enable seamless automation.
Customization
Replace Airtable with another content management system if preferred.
Adjust AI prompts to reflect different tones, styles, or industries.
Add integrations for additional plugins, analytics, or storage services.
Usage
Import the workflow into your n8n instance.
Configure API credentials for WordPress, Airtable, OpenAI, Claude and Slack.
Update your functions.php file with the provided code snippet.
Customize prompts and Airtable structure for your content needs.
Trigger the workflow manually or set it on a schedule.
Notes
Experiment with Airtable views or add filters for more granular control over your content pipeline.
Extend the workflow to include social media posting or analytics tracking.
Feel free to adapt and extend this workflow to meet your specific needs! üéâ"
Modular & Customizable AI-Powered Email Routing: Text Classifier for eCommerce,https://n8n.io/workflows/2851-modular-and-customizable-ai-powered-email-routing-text-classifier-for-ecommerce/,"How It Works
Form Submission:
The workflow starts with the On form submission node, which triggers when a user submits a contact form. The form collects the user's name, email, and message.
Text Classification:
The Text Classifier node uses an AI model (GPT-4) to classify the submitted message into one of the predefined categories:
Request Quote: For quote requests.
Product info: For general product inquiries.
General problem: For issues or problems related to products.
Order: For questions about placed orders.
Other: For any messages that don‚Äôt fit the above categories.
Email Routing:
Based on the classification, the workflow routes the message to the appropriate department via email:
Prod. Dep.: For product-related inquiries.
Quote Dep.: For quote requests.
Gen. Dep.: For general problems.
Order Dep.: For order-related questions.
Other Dep.: For all other inquiries.
Each email includes the user's name, email, message, and the classified category.
Data Logging:
The workflow logs the form submission and classification results into a Google Sheets document. Each department has its own sheet where the data is appended, including:
User‚Äôs name, email, and message.
Submission date and time.
Assigned category.
Email recipient details.
AI Model Integration:
The OpenAI node provides the AI model (GPT-4) used by the Text Classifier to classify the messages. The model is instructed to classify the text into one of the predefined categories without additional explanations.
Set Up Steps
Configure the Form Trigger:
Set up the On form submission node to collect user inputs (name, email, and message) and trigger the workflow.
Set Up the Text Classifier:
Configure the Text Classifier node to use the OpenAI model (GPT-4) for text classification.
Define the categories and their descriptions (e.g., ""Request Quote"", ""Product info"", etc.).
Set the fallback category to ""Other"" for unclassifiable messages.
Configure Email Sending:
Set up the Email Send nodes for each department (Prod. Dep., Quote Dep., Gen. Dep., Order Dep., Other Dep.).
Configure the email subject, body, and reply-to address using the form data and classification results.
Ensure SMTP credentials are correctly configured for sending emails.
Set Up Google Sheets Integration:
Configure the Google Sheets nodes to append data to the appropriate sheets for each department.
Map the form data (name, email, message, date, category, and recipient) to the corresponding columns in the Google Sheets document.
Test the Workflow:
Submit a test form to ensure the workflow correctly classifies the message, sends the email to the right department, and logs the data in Google Sheets.
Verify that the OpenAI model is classifying messages accurately.
Activate the Workflow:
Once tested, activate the workflow to automate the process of handling contact form submissions.
Key Features
Automated Classification: Uses AI to classify messages into relevant categories, reducing manual effort.
Email Routing: Sends emails to the appropriate department based on the classification.
Data Logging: Logs all form submissions and classification results in Google Sheets for tracking and analysis.
Scalability: Easily adaptable to additional categories or departments by modifying the workflow.
This workflow is ideal for eCommerce businesses or customer support teams looking to automate and streamline the handling of contact form submissions.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
BambooHR AI-Powered Company Policies and Benefits Chatbot,https://n8n.io/workflows/2850-bamboohr-ai-powered-company-policies-and-benefits-chatbot/,"How it works
This workflow enables companies to provide instant HR support by automating responses to employee queries about policies and benefits:
Retrieves company policies, benefits, and HR documents from BambooHR.
Uses AI to analyze and answer employee questions based on company records.
Identifies the most relevant contact person for escalations.
Seamlessly integrates with company systems to provide real-time HR assistance.
Set up steps:
Estimated time: ~20 minutes
Connect your BambooHR account to allow policy retrieval.
Configure AI parameters and access control settings.
(Optional) Set up the employee lookup tool for personalized responses.
Test the chatbot to ensure accurate responses and seamless integration.
Benefits
This workflow is perfect for HR teams looking to enhance employee support while reducing manual inquiries.
Outperform BambooHR's ""Ask BambooHR"" Chatbot
#1. Superior specificity of replies to general inquiries

#2. More appropriate escalations when responding to sensitive employee concerns"
Smart Email Outreach Sequence ‚Äì AI-Powered & Customizable,https://n8n.io/workflows/2833-smart-email-outreach-sequence-ai-powered-and-customizable/,"Automated Email Outreach Workflow
This intelligent email automation workflow helps you maximize engagement through domain-based outreach. It utilizes AI-powered personalization and strategic follow-ups to increase response rates. The workflow includes a customizable three-step email sequence (initial email + two follow-ups) sent at optimized 3-day intervals.
üî• Key Features
‚úÖ Smart Domain Extraction ‚Äì Automatically extracts domains from email addresses for precise targeting.
‚úÖ Website Intelligence ‚Äì Uses Jina.ai to analyze recipient websites and craft ultra-relevant messaging.
‚úÖ AI-Powered Personalization ‚Äì Leverages GPT-3.5 to generate compelling and highly customized email content.
‚úÖ Adjustable AI Prompts ‚Äì Easily customize AI-generated messages to align with your product, service, or campaign.
‚úÖ Automated Follow-Up Sequence ‚Äì Sends a structured series of three emails over time to increase engagement.
‚úÖ Human-Like Sending Patterns ‚Äì Randomized delays help bypass spam filters and improve deliverability.
‚úÖ Error-Resilient Execution ‚Äì Built-in error handling ensures a smooth, uninterrupted workflow.
üéØ Ideal Use Cases
‚úîÔ∏è Domain Sales & Lead Generation ‚Äì Engage potential buyers with hyper-personalized emails.
‚úîÔ∏è B2B Partnerships & Outreach ‚Äì Automate connections with potential business partners.
‚úîÔ∏è Cold Emailing at Scale ‚Äì Improve engagement with structured, AI-optimized emails.
‚öôÔ∏è How It Works
1Ô∏è‚É£ Setup & Configuration
Integrate Gmail via OAuth for secure email sending.
Connect your Google Sheets with leads for automated processing.
Customize AI-generated messages by adjusting prompt inputs to fit your strategy.
2Ô∏è‚É£ Workflow Execution
Domain Extraction ‚Äì Automatically extracts recipient domains from email addresses.
Website Analysis ‚Äì Uses Jina.ai to gather insights for ultra-personalized messaging.
AI-Powered Email Generation ‚Äì GPT-3.5 crafts highly engaging outreach emails.
Automated Email Sending ‚Äì Initial email is sent, followed by two optimized follow-ups if no response is received.
Engagement Tracking & Adaptation ‚Äì Ensures delivery success and prevents emails from being flagged as spam.
‚ö° Setup Instructions
‚úÖ Preconditions
You need a Gmail account with OAuth authentication enabled.
Your Google Sheets must contain lead emails formatted correctly.
Ensure your n8n instance is running and connected to your workflow.
üõ† Implementation Steps
Install and open n8n.
Import the workflow JSON file into your n8n instance.
Configure your Gmail credentials in the workflow settings.
Link your Google Sheets containing the leads' email addresses.
Customize AI prompts in the GPT-3.5 node to match your campaign strategy.
Test the workflow manually to ensure proper execution.
Activate the workflow and let automation take over!
üìà Expected Results
üöÄ Higher Response Rates ‚Äì Personalization improves recipient engagement.
üìä Increased Conversions ‚Äì Optimized email sequences boost lead-to-client conversions.
üí° Time-Saving Automation ‚Äì Hands-free execution streamlines your outreach process.
üîß Ready to supercharge your outreach? Set up the workflow and start automating today! üöÄüî•"
Spot Workplace Discrimination Patterns with AI,https://n8n.io/workflows/2825-spot-workplace-discrimination-patterns-with-ai/,"How It Works:
‚Ä¢ Scrapes company review data from Glassdoor using ScrapingBee.
‚Ä¢ Extracts demographic-based ratings using AI-powered text analysis.
‚Ä¢ Calculates workplace disparities with statistical measures like z-scores, effect sizes, and p-values.
‚Ä¢ Generates visualizations (scatter plots, bar charts) to highlight patterns of discrimination or bias.
Example Visualizations:
Set Up Steps:
Estimated time: ~20 minutes.
‚Ä¢ Replace ScrapingBee and OpenAI credentials with your own.
‚Ä¢ Input the company name you want to analyze (best results with large U.S.-based organizations).
‚Ä¢ Run the workflow and review the AI-generated insights and visual reports.
This workflow empowers users to identify potential workplace discrimination trends, helping advocate for greater equity and accountability.
Additional Credit: Wes Medford
For algorithms and inspiration"
AI Social Media Caption Creator creates social media post captions in Airtable,https://n8n.io/workflows/2817-ai-social-media-caption-creator-creates-social-media-post-captions-in-airtable/,"Welcome to my AI Social Media Caption Creator Workflow!
What this workflow does
This workflow automatically creates a social media post caption in an editorial plan in Airtable. It also uses background information on the target group, tonality, etc. stored in Airtable.
This workflow has the following sequence:
Airtable trigger (scan for new records every minute)
Wait 1 Minute so the Airtable record creator has time to write the Briefing field
retrieval of Airtable record data
AI Agent to write a caption for a social media post. The agent is instructed to use background information stored in Airtable (such as target group, tonality, etc.) to create the post.
Format the output and assign it to the correct field in Airtable.
Post the caption into Airtable record.
Requirements
Airtable Database: Documentation
AI API access (e.g. via OpenAI, Anthropic, Google or Ollama)
Example of an editorial plan in Airtable:
Editorial Plan example in Airtable
For this workflow you need the Airtable fields ""created_at"", ""Briefing"" and ""SoMe_Text_AI""
Feel free to contact me via LinkedIn, if you have any questions!"
"Domain Outbound : Automate Lead Extraction, and Targeted Outreach",https://n8n.io/workflows/2821-domain-outbound-automate-lead-extraction-and-targeted-outreach/,"Domain Outbound Machine: Automate Lead Generation, Email Extraction, and Targeted Outreach with n8n
Description
Domain Outbound Machine is an n8n workflow designed to fully automate the domain sales process: lead generation, email extraction, personalized outreach, and automated email sending. It also stores extracted email addresses and sent emails in an Excel file for tracking and follow-up.
How It Works?
1. Domain Input
You provide the domain name you want to sell. The workflow takes this domain as the starting point.
2. Google Maps Query Generation
The workflow automatically generates 100 Google Maps queries based on the domain's niche or industry. This ensures you target businesses or individuals who might be interested in acquiring the domain.
3. Web Scraping for Emails
Using the generated queries, the workflow scrapes relevant websites to extract email addresses. This step ensures you have a targeted list of potential buyers.
4. Email Enrichment
For each extracted email, the workflow gathers publicly available information (e.g., LinkedIn, company websites) to enhance personalization.
5. Personalized Email Creation
Based on the enriched data, the workflow crafts a custom email for each lead, emphasizing the domain‚Äôs value, its relevance to their business, and a compelling call-to-action.
6. Email Sending via Gmail
The workflow sends the personalized emails directly through your Gmail account, ensuring timely and professional outreach.
7. Email & Lead Tracking in Excel
All extracted email addresses and sent messages are automatically stored in an Excel file, allowing you to track responses and manage follow-ups efficiently.
Key Features
‚úÖ Automated Lead Generation ‚Äì Saves time by automatically finding and extracting potential leads.
‚úÖ Personalized Outreach ‚Äì Boosts engagement by tailoring each email to the recipient.
‚úÖ Seamless Integration ‚Äì Uses n8n‚Äôs automation to connect Google Maps, web scraping tools, and Gmail in one workflow.
‚úÖ Scalable & Trackable ‚Äì Adjust the number of queries or emails as needed, while keeping track of everything in Excel.
Domain Outbound Machine is perfect for domain investors, digital marketers, and anyone looking to sell domains efficiently. By automating the tedious tasks, you can focus on closing deals and growing your business."
Make OpenAI Citation for File Retrieval RAG,https://n8n.io/workflows/2693-make-openai-citation-for-file-retrieval-rag/,"Make OpenAI Citation for File Retrieval RAG
Use case
In this example, we will ensure that all texts from the OpenAI assistant search for citations and sources in the vector store files. We can also format the output for Markdown or HTML tags.
This is necessary because the assistant sometimes generates strange characters, and we can also use dynamic references such as citations 1, 2, 3, for example.
What this workflow does
In this workflow, we will use an OpenAI assistant created within their interface, equipped with a vector store containing some files for file retrieval.
The assistant will perform the file search within the OpenAI infrastructure and will return the content with citations.
We will make an HTTP request to retrieve all the details we need to format the text output.
Setup
Insert an OpenAI Key
How to adjust it to your needs
At the end of the workflow, we have a block of code that will format the output, and there we can add Markdown tags to create links. Optionally, we can transform the Markdown formatting into HTML."
Notion to Pinecone Vector Store Integration,https://n8n.io/workflows/2797-notion-to-pinecone-vector-store-integration/,"This n8n automation is designed to extract, process, and store content from Notion pages into a Pinecone vector store. Here's a breakdown of the workflow:
Notion - Page Added Trigger: The automation starts by monitoring for newly added pages in a specific Notion database. It triggers whenever a new page is created, capturing the page's metadata.
Notion - Retrieve Page Content: Once triggered, the automation fetches the full content of the newly added Notion page, including blocks like text, images, and videos.
Filter Non-Text Content: The next step filters out non-text content (such as images and videos), ensuring only textual content is processed.
Summarize - Concatenate Notion's blocks content: The remaining text content is concatenated into a single block of text for easier processing.
Token Splitter: The concatenated text is then split into manageable tokens, which are chunks of text that can be used for embedding.
Create metadata and load content: Metadata such as the page ID, creation time, and title are added to the content, making it easy to reference and track.
Embeddings Google Gemini: The processed text is passed through a Google Gemini model to generate embeddings, which are numerical representations of the text that capture its semantic meaning.
Pinecone Vector Store: Finally, the embeddings, along with the content and metadata, are stored in a Pinecone vector store, making it searchable and ready for use in applications like document retrieval or natural language processing tasks.
This workflow ensures that every new page added to the Notion database is processed into a format that can be easily searched and used in machine learning applications. The automation runs every minute to capture new data in real-time, providing an up-to-date and searchable vector database of Notion content.
Use Case:
This automation converts Notion pages into vector embeddings and stores them in Pinecone for enhanced search and AI-driven insights. It‚Äôs ideal for teams using Notion for knowledge management, enabling semantic search and context-based content retrieval. For example, employees can easily find relevant information across documents, and data scientists can use AI models to analyze and summarize the content stored in Notion."
AI News Research Team: 24/7 Newsletter Automation with Citations with Perplexity,https://n8n.io/workflows/2778-ai-news-research-team-247-newsletter-automation-with-citations-with-perplexity/,"Purpose of workflow:
This AI-powered workflow automatically generates detailed, well-researched newsletters by monitoring and analyzing specified news topics (like Bitcoin, Nvidia, etc.). It uses a team of AI research agents to gather, analyze, and compile information with automatic citations, saving significant time in newsletter creation.
How it works:
Multi-agent system:
Research Leader: Analyzes topics and creates content outline
Project Planner: Breaks down research into specific tasks
Research Assistants: Conduct detailed research on assigned subtopics
Editor: Combines research and polishes final output
Key features:
Automated daily monitoring of specified news topics
Real-time information gathering using Perplexity AI
Auto-citation functionality for source verification
Flexible time window filtering (day/week/month)
Options for detailed or simple reports
Direct email delivery of completed newsletters
Step-by-step setup:
Perplexity API Setup:
Create account at perplexity.ai
Navigate to API tab
Generate API key
Set up credentials with 'Bearer' authentication
Workflow Configuration:
Connect Google Sheets containing news monitoring topics
Configure schedule trigger for daily execution
Set up email delivery settings
Define report type preferences (detailed/simple)
Specify time window for news gathering
Integration:
Connect with newsletter tools like Kit
Import generated content as starting point
Edit and customize as needed before publishing"
Daily meetings summarization with Gemini AI,https://n8n.io/workflows/2789-daily-meetings-summarization-with-gemini-ai/,"This workflow implements the Gemini AI chat model to summarize your daily meetings and send the summary to a Slack channel daily at 9 AM (or any other time you choose).
It automatically retrieves your Google Calendar events and feeds them to the model.
The workflow uses Google‚Äôs Gemini AI for response generation.
How it works
The workflow uses a Scheduled Trigger Node as the main trigger.
The AI Agent Node uses the Google Calendar action to retrieve relevant meeting data.
The AI Agent sends the retrieved information to the Google Gemini Chat Model (gemini-flash).
The Google Gemini Chat Model generates a summary and informative response based on today‚Äôs meetings.
++Setup Steps++
Google Cloud Project and Vertex AI API:
Create a Google Cloud project.
Enable the Vertex AI API for your project.
Google AI API Key:
Obtain a Google AI API key from Google AI Studio.
Credentials in n8n:
Configure credentials in your n8n environment for:
Google Gemini (PaLM) API (using your Google AI API key).
Import the Workflow:
Import this workflow into your n8n instance.
Configure the Workflow:
Update both Slack and Gemini nodes with your credentials."
Automated End-to-End Fine-Tuning of OpenAI Models with Google Drive Integration,https://n8n.io/workflows/2781-automated-end-to-end-fine-tuning-of-openai-models-with-google-drive-integration/,"1. How it Works
This n8n workflow automates fine-tuning OpenAI models through these key steps:
Manual Trigger:
Starts with the ""When clicking ‚ÄòTest workflow‚Äô"" event to initiate the process.
Downloads a .jsonl file from Google Drive
Upload to OpenAI:
Uploads the .jsonl file to OpenAI via the ""Upload File"" node (with purpose ""fine-tune"").
Create Fine-tuning Job:
Sends a POST request to the endpoint https://api.openai.com/v1/fine_tuning/jobs with:
{  
  ""training_file"": ""{{ $json.id }}"",  
  ""model"": ""gpt-4o-mini-2024-07-18""  
}  
OpenAI automatically starts training the model based on the provided file.
Interaction with the Trained Model:
An ""AI Agent"" uses the custom model (e.g., ft:gpt-4o-mini-2024-07-18:n3w-italia::XXXX7B) to respond to chat messages.
2. Set up Steps
To configure the workflow:
Prepare the Training File:
Create a .jsonl file following the specified syntax (e.g., travel assistant Q/A examples).
Upload it to Google Drive and update the ID in the ""Google Drive"" node.
Configure Credentials:
Google Drive: Connect an account via OAuth2 (googleDriveOAuth2Api).
OpenAI: Add your API key in the ""OpenAI Chat Model"" and ""Upload File"" nodes.
Customize the Model:
In the ""OpenAI Chat Model"" node, specify the name of your fine-tuned model (e.g., ft:gpt-4o-mini-...).
Update the HTTP request body (Create Fine-tuning Job) if needed (e.g., a different base model).
Start the Workflow:
Use the manual trigger (""Test workflow"") to begin the upload and training process.
Test the model via the ""Chat Trigger"" (chat messages).
Integrated Documentation:
Follow the instructions in the Sticky Notes to:
Properly format the .jsonl (Step 1).
Monitor progress on OpenAI (Step 2, link: https://platform.openai.com/finetune/).
Note: Ensure the .jsonl file adheres to OpenAI‚Äôs required structure and that credentials are valid."
AI Data Extraction with Dynamic Prompts and Airtable,https://n8n.io/workflows/2771-ai-data-extraction-with-dynamic-prompts-and-airtable/,"This n8n template introduces the Dynamic Prompts Ai workflow pattern which are incredible for certain types of data extraction tasks where attributes are unknown or need to remain flexible.
The general idea behind this pattern is that the prompts for requested attributes to be extracted live outside the template and so can be changed at any time - without needing to edit the template. This seriously cuts down on maintainance requirements and is reusable for any number of tables at little cost.
Check out the video demo I did for n8n Studio here: https://www.youtube.com/watch?v=_fNAD1u8BZw
Check out the example Airtable here: https://airtable.com/appAyH3GCBJ56cfXl/shrXzR1Tj99kuQbyL
Looking for the Baserow Version? https://n8n.io/workflows/2780-ai-data-extraction-with-dynamic-prompts-and-baserow/
How it works
Given we have an ""input"" field for context and a number of fields for the data we want to extract, this template will run in the background to react to any changes to either the ""input"" or fields and automatically update the rows accordingly.
The key is that Airtable fields have a special property called the ""field description"". In this pattern, we use this property to allow the user to store a simple prompt describing the data that should exist in the column.
Our n8n template reads these column descriptions aka ""prompts"" to use as instructions to perform tasks on the ""input"".
In this template, the ""input"" is a PDF of a resume/CV and the columns are attributes a HR person would want to extract from it - such as full name, address, last position, years of experience etc.
How to use
First publish this template and ensure it's accessible via webhook URL.
You then have to run the ""create airtable webhooks"" mini-flow to configure your Airtable to send change events to the n8n template. This mini-flow exists in the template but you'll have to update the IDs.
Check the template for more instructions.
Requirements
Airtable for Tables/Database
OpenAI for LLM and extraction. Feel free to choose another LLM if preferred.
Customising this workflow
If you're not using files, you can replace the ""input"" field with anything you like. For example, the ""input"" could be single line text."
Extract personal data with self-hosted LLM Mistral NeMo,https://n8n.io/workflows/2766-extract-personal-data-with-self-hosted-llm-mistral-nemo/,"This workflow shows how to use a self-hosted Large Language Model (LLM) with n8n's LangChain integration to extract personal information from user input. This is particularly useful for enterprise environments where data privacy is crucial, as it allows sensitive information to be processed locally.
üìñ For a detailed explanation and more insights on using open-source LLMs with n8n, take a look at our comprehensive guide on open-source LLMs.
üîë Key Features
Local LLM
Connect Ollama to run Mistral NeMo LLM locally
Provide a foundation for compliant data processing, keeping sensitive information on-premises
Data extraction
Convert unstructured text to a consistent JSON format
Adjust the JSON schema to meet your specific data extraction needs.
Error handling
Implement auto-fixing for LLM outputs
Include error output for further processing
‚öôÔ∏è Setup and —Åonfiguration
Prerequisites
n8n AI Starter Kit installed
Configuration steps
Add the Basic LLM Chain node with system prompts.
Set up the Ollama Chat Model with optimized parameters.
Define the JSON schema in the Structured Output Parser node.
üîç Further resources
Run LLMs locally with n8n
Video tutorial on using local AI with n8n
Apply the power of self-hosted LLMs in your n8n workflows while maintaining control over your data processing pipeline!"
AI-Driven Lead Management and Inquiry Automation with ERPNext & n8n,https://n8n.io/workflows/2758-ai-driven-lead-management-and-inquiry-automation-with-erpnext-and-n8n/,"Overview
This workflow template automates lead management and customer inquiry processing by integrating ERPNext, AI agents, and email notifications. It streamlines the process of capturing leads, analyzing inquiries, and generating actionable responses. The workflow uses ERPNext to capture inquiries, analyzes them with AI, and notifies the appropriate team or individual, all while maintaining a professional approach.
What This Template Does
ERPNext Webhook Integration:
Captures leads and inquiries through ERPNext webhooks.
Triggers the workflow when a new lead is created.
AI-Powered Inquiry Analysis:
Uses AI to extract key details from lead notes (e.g., customer name, organization, inquiry summary).
Classifies inquiries as valid or invalid based on relevance to products, services, or solutions.
Contact Assignment:
Matches inquiries to the appropriate contact(s) using a Google Sheets database or ERPNext contact information.
Handles multiple contacts if required.
Email Notifications:
Generates professional email notifications for valid inquiries.
Sends emails to the appropriate contact(s) with inquiry details and action steps.
Invalid Lead Handling:
Identifies invalid inquiries (e.g., unrelated to products or services) and flags them for follow-up or dismissal.
Custom Email Formatting:
Converts plain text into professionally formatted HTML emails.
Ensures that communication is clear, concise, and visually appealing.
How It Works
Step 1: Capture Lead Data
Webhook in ERPNext:
Create a webhook in ERPNext for the ""Lead"" DocType.
Set the trigger to on_insert to capture new leads in real-time.
Lead Details:
The workflow fetches lead details, including notes, contact information, and the source of the lead.
Step 2: Validate and Analyze Inquiry
AI Agent for Analysis:
An AI agent analyzes the lead notes to extract key details and classify the inquiry as valid or invalid.
The analysis includes checking the relevance of the inquiry to products, services, or solutions offered by the company.
Invalid Leads:
If the inquiry is invalid, the workflow flags it and stops further processing.
Step 3: Assign Contact(s)
Google Sheets Integration:
Uses a Google Sheets database to map products, services, or solutions to responsible contacts.
Ensures that inquiries are directed to the right person or team.
Multiple Contacts:
Handles cases where multiple contacts are responsible for a particular product or service.
Step 4: Generate and Send Email Notifications
AI-Generated Emails:
The workflow generates a professional email summarizing the inquiry.
Emails include details like customer name, organization, inquiry summary, and action steps.
Custom HTML Formatting:
Emails are converted to HTML for a polished and professional appearance.
Send Notifications:
Sends email notifications through Microsoft Outlook or another configured email client.
Optionally, notifies via WhatsApp or SMS for urgent inquiries.
Step 5: Post-Inquiry Actions
ERPNext Record Updates:
Updates the lead record in ERPNext with relevant details, including inquiry status and contact information.
Setup Instructions
Prerequisites
ERPNext:
A configured ERPNext instance with lead data and a webhook for the ""Lead"" DocType.
Google Sheets:
A sheet mapping products, services, or solutions to responsible contacts.
AI Integration:
Credentials for OpenAI or other supported AI platforms.
Email Client:
Credentials for Microsoft Outlook or another email client.
Step-by-Step Setup
ERPNext Configuration:
Create a webhook for the ""Lead"" DocType in ERPNext.
Test the webhook with sample data to ensure proper integration.
Workflow Import:
Import the workflow template into n8n.
Configure nodes with your API credentials for ERPNext, Google Sheets, and AI tools.
Google Sheets Integration:
Prepare a Google Sheet with columns for product, service, or solution and the responsible contact(s).
Link the sheet to the workflow.
AI Agent Configuration:
Customize the AI agent‚Äôs prompts to align with your business‚Äôs products and services.
Adjust criteria for valid and invalid inquiries as needed.
Email Setup:
Configure the email client node with your email service credentials.
Customize the email template for your organization.
Testing:
Run the workflow with sample leads to validate the entire process.
Check email notifications, contact assignments, and record updates in ERPNext.
Dos and Don‚Äôts
Dos:
Test Thoroughly: Test the workflow with various scenarios before deploying in production.
Secure Credentials: Keep API and email credentials secure to avoid unauthorized access.
Customize Prompts: Tailor AI prompts to match your business needs and language style.
Use Professional Email Templates: Ensure emails are clear and well-formatted.
Don‚Äôts:
Skip Validation: Always validate inquiry data to avoid sending irrelevant notifications.
Overload the Workflow: Avoid adding unnecessary nodes that can slow down processing.
Ignore Errors: Monitor logs and address errors promptly for a smooth workflow.
Resources
GET n8n Now
N8N COURSE
n8n Book
YouTube Tutorial:
Watch the full step-by-step tutorial on setting up this workflow:
SyncBricks YouTube Channel
Courses and Training:
Learn more about ERPNext and AI automation through my comprehensive courses:
SyncBricks LMS
Support and Contact:
Email: amjid@amjidali.com
Website: SyncBricks
LinkedIn: Amjid Ali"
AI-Powered Candidate Shortlisting Automation for ERPNext,https://n8n.io/workflows/2757-ai-powered-candidate-shortlisting-automation-for-erpnext/,"Template Guide for Employee Shortlisting AI Agent Automation
Overview
This template automates the process of shortlisting job applicants using ERPNext, n8n, and AI-powered decision-making tools like Google Gemini and OpenAI. It reduces manual effort, ensures fast evaluations, and provides justifiable decisions about applicants. This is ideal for businesses aiming to streamline their recruitment process while maintaining accuracy and professionalism.
YouTube Tutorial: For a full walkthrough of this template, visit:
Integrate AI in ERPNext: Automate Recruitment Job Applicant Shortlisting in Seconds!
What Does This Template Do?
Webhook Integration with ERPNext: Automatically triggers the workflow when a job application is created in ERPNext.
Resume Validation: Ensures resumes are attached and correctly processes various file formats like PDF and DOC.
AI-Powered Evaluation: Uses AI to compare resumes against job descriptions and provides a:
Fit Level (Strong, Moderate, or Weak)
Score (0‚Äì100)
Justification for the decision.
Automated Decision Making: Based on AI-generated scores:
Candidates with a score of 80 or higher are Accepted.
Candidates below 80 are Rejected.
Applications missing required fields or attachments are put On Hold.
ERPNext Integration: Updates applicant records in ERPNext, including custom fields such as justification, fit level, and scores.
Notifications: Notifies candidates via email, WhatsApp, or SMS about their application status.
Step-by-Step Guide
Step 1: Set Up ERPNext Webhook
Go to Webhooks in ERPNext.
Create a webhook for the Job Applicant DocType.
Set the trigger to Insert.
Pin and test the webhook to ensure proper data flow.
Step 2: Import the Template into n8n
Open your n8n instance.
Import the provided workflow template.
Check all nodes for proper configuration.
Step 3: Configure Credentials
Add your ERPNext API credentials to the ERPNext nodes.
Add credentials for AI services like OpenAI or Google Gemini.
Configure additional services like WhatsApp or email if you plan to use them for notifications.
Step 4: Test Resume Validation
Test how the workflow handles different file types (e.g., PDF, DOC, JPG).
Ensure resumes without the proper format or attachment are flagged and rejected.
Step 5: AI Evaluation
The AI model (Google Gemini or OpenAI) will evaluate resumes against job descriptions.
Customize the AI prompt to suit your job evaluation needs.
The output will include a Fit Level, Score, Rating, and Justification.
Step 6: Decision Automation
The workflow automatically categorizes applicants:
Accepted for scores ‚â• 80.
Rejected for scores < 80.
On Hold if essential fields or attachments are missing.
Step 7: Update ERPNext Records
The workflow updates the Job Applicant record in ERPNext with:
Status (Accepted, Rejected, On Hold)
AI-generated Fit Level, Score, Rating, and Justification.
Step 8: Notify Candidates
Configure notification nodes (email, WhatsApp, or SMS).
Inform candidates about their application status and include feedback if required.
How It Works
Trigger: The workflow starts when a job application is submitted in ERPNext.
Validation: Checks if the resume is attached and in the correct format.
AI Evaluation: Compares the resume with the job description and generates a decision.
ERPNext Update: Updates the applicant's record with the decision and justification.
Notification: Sends a personalized notification to the candidate.
Dos and Don‚Äôts
Dos:
Customize Prompts: Tailor the AI prompt to match your specific job evaluation requirements.
Test the Workflow: Run sample data to ensure the process works as intended.
Secure Your Credentials: Keep your API credentials safe and do not share them publicly.
Optimize for Different Formats: Ensure the workflow can handle all types of resumes you expect.
Don‚Äôts:
Avoid Manual Intervention: Let the workflow handle most of the tasks to ensure efficiency.
Do Not Skip Testing: Always test the workflow with various scenarios to avoid errors.
Do Not Overlook Notifications: Ensure candidates are notified promptly to maintain professionalism.
Customization Options
Add logic for more file types (e.g., scanned images using OCR).
Enhance the AI prompts to analyze more complex resume data.
Integrate additional tools like Slack or Trello for recruitment tracking.
Resources
GET n8n Now
N8N COURSE
n8n Book
YouTube Tutorial: For a full walkthrough of this template, visit:
SyncBricks YouTube Channel
Detailed Guides and Courses: Learn more about ERPNext and AI-driven automation at:
SyncBricks LMS
Support
If you encounter issues or want to explore more possibilities with AI-driven automation, feel free to reach out:
Email: amjid@amjidali.com
Website: ERPNext and Other Courses
LinkedIn: Amjid Ali
Let me know if you'd like further details or modifications to the guide!"
Ultimate Content Generator for WordPress,https://n8n.io/workflows/2737-ultimate-content-generator-for-wordpress/,"Overview
This workflow automates the end-to-end process of creating, optimizing, and publishing content on WordPress.
It integrates AI-powered tools, Airtable, and WordPress plugins to generate high-quality, on-brand posts effortlessly.
Perfect for content creators, marketers, and business owners looking to save time and scale their content strategy.
Features
Content Creation:
AI-Powered Content: Generates SEO-friendly blog posts with structured headings, relevant keywords, and meta descriptions.
Custom Prompts: Tailor the AI-generated content to match your brand‚Äôs tone and voice.
SEO Optimization:
RankMath Plugin Integration: Updates RankMath SEO with focus keywords and meta descriptions, ensuring your content is search-engine optimized.
Content Management:
Airtable Integration: Organizes content ideas, drafts, and publishing schedules in one place. Easily scalable for teams or solo creators.
Visuals:
Branded Featured Images: Automatically generates on-brand images for every post.
Publishing:
Effortless Formatting: Adapts content to fit your WordPress theme and schedules it for publication.
Workflow Steps
Trigger:
Initiated manually or on a schedule.
Content Management:
Retrieves and organizes ideas from Airtable.
Content Generation:
Generates AI-driven blog content tailored to your audience.
SEO Optimization:
Automatically updates RankMath with SEO details.
Featured Image Creation:
Produces on-brand images for the post.
Publishing:
Formats and schedules the post on WordPress.
Prerequisites
API Keys:
OpenAI
Airtable
WordPress REST API
RankMath SEO Plugin
Custom Code:
Add a small update to your WordPress theme‚Äôs functions.php file to enable seamless automation.
Customization
Replace Airtable with another content management system if preferred.
Adjust AI prompts to reflect different tones, styles, or industries.
Add integrations for additional plugins, analytics, or storage services.
Usage
Import the workflow into your n8n instance.
Configure API credentials for WordPress, Airtable, OpenAI, and RankMath.
Update your functions.php file with the provided code snippet.
Customize prompts and Airtable structure for your content needs.
Trigger the workflow manually or set it on a schedule.
Notes
Experiment with Airtable views or add filters for more granular control over your content pipeline.
Extend the workflow to include social media posting or analytics tracking.
For questions, refer to n8n documentation or reach out to the creator.
Tools Used
Airtable
OpenAI GPT
WordPress REST API
RankMath SEO Plugin
Feel free to adapt and extend this workflow to meet your specific needs! üéâ"
Stock Technical Analysis with Google Gemini,https://n8n.io/workflows/2735-stock-technical-analysis-with-google-gemini/,"The purpose of this workflow, ""Sell: Stock Vision,"" is to create an AI-powered technical analysis agent capable of analyzing financial charts for equity stocks and cryptocurrencies. This workflow provides users with actionable insights into market trends, price movements, candlestick patterns, and technical indicators to support informed trading decisions.
How It Works
Integration with TradingView: The workflow uses the Chart-Img.com API to fetch detailed financial charts for the specified stock or cryptocurrency.
AI-Powered Analysis: The workflow employs advanced AI models, including Google's Gemini Chat Model, to analyze the retrieved charts for candlestick patterns, support/resistance levels, and technical indicators like MACD and RSI.
News and Sentiment Analysis: By integrating with SerpAPI, the workflow gathers relevant news about the stock or cryptocurrency to evaluate its potential impact on market movements.
Comprehensive Insights: It provides detailed trading strategies, including buy/sell recommendations, stop-loss levels, and risk-reward evaluations.
Continuous Memory: The AI agent uses buffer memory to retain context for enhanced analysis and continuity.
Use Case
This workflow is perfect for traders and analysts who need reliable and AI-powered market analysis to make informed trading decisions efficiently.
Tutorial
Obtain API keys for Chart-Img.com and SerpAPI.
Configure the workflow in your n8n instance by inputting the required API keys and connecting the nodes.
Trigger the workflow by providing the stock or cryptocurrency symbol, and let the agent do the rest!
https://youtu.be/9fR4qNMT5LM"
Auto-Categorize blog posts in wordpress using A.I.,https://n8n.io/workflows/2706-auto-categorize-blog-posts-in-wordpress-using-ai/,"WordPress Post Auto-Categorization Workflow
üí° Click here to read detailed case study
üì∫ Click here to watch youtube tutorial
üéØ Purpose
Automatically categorize WordPress blog posts using AI, saving hours of manual work. This workflow analyzes your post titles and assigns them to predefined categories using artificial intelligence.
üîÑ What This Workflow Does
‚Ä¢ Connects to your WordPress site
‚Ä¢ Retrieves all uncategorized posts
‚Ä¢ Uses AI to analyze post titles
‚Ä¢ Automatically assigns appropriate category IDs
‚Ä¢ Updates posts with new categories
‚Ä¢ Processes dozens of posts in minutes
‚öôÔ∏è Setup Requirements
WordPress site with admin access
Predefined categories in WordPress
OpenAI API credentials (or your preferred AI provider)
n8n with WordPress credentials
üõ†Ô∏è Configuration Steps
Add your WordPress categories (manually in WordPress)
Note down category IDs
Update the AI prompt with your category IDs
Configure WordPress credentials in n8n
Set up AI API connection
üîß Customization Options
‚Ä¢ Modify AI prompts for different categorization criteria
‚Ä¢ Adjust for multiple category assignments
‚Ä¢ Add tag generation functionality
‚Ä¢ Customize for different content types
‚Ä¢ Add additional metadata updates
‚ö†Ô∏è Important Notes
‚Ä¢ Backup your WordPress database before running
‚Ä¢ Test with a few posts first
‚Ä¢ Review AI categorization results initially
‚Ä¢ Categories must be created manually first
üéÅ Bonus Features
‚Ä¢ Can be modified for tag generation
‚Ä¢ Works with scheduled posts
‚Ä¢ Handles bulk processing
‚Ä¢ Maintains categorization consistency
Perfect for content managers, bloggers, and website administrators looking to organize their WordPress content efficiently.
#n8n #WordPress #ContentManagement #Automation #AI
Created by rumjahn"
Chat Assistant (OpenAI assistant) with Postgres Memory And API Calling Capabalities,https://n8n.io/workflows/2637-chat-assistant-openai-assistant-with-postgres-memory-and-api-calling-capabalities/,"Workflow Description
Your workflow is an intelligent chatbot, using ++OpenAI assistant++, integrated with a backend that supports WhatsApp Business, designed to handle various use cases such as sales and customer support. Below is a breakdown of its functionality and key components:
Workflow Structure and Functionality
Chat Input (Chat Trigger)
The flow starts by receiving messages from customers via WhatsApp Business.
Collects basic information, such as session_id, to organize interactions.
Condition Check (If Node)
Checks if additional customer data (e.g., name, age, dependents) is sent along with the message.
If additional data is present, a customized prompt is generated, which includes this information. The prompt specifies that this data is for the assistant's awareness and doesn‚Äôt require a response.
Data Preparation (Edit Fields Nodes)
Formats customer data and the interaction details to be processed by the AI assistant.
Compiles the customer data and their query into a single text block.
AI Responses (OpenAI Nodes)
The assistant‚Äôs prompt is carefully designed to guide the AI in providing accurate and relevant responses based on the customer‚Äôs query and data provided.
Prompts describe the available functionalities, including which APIs to call and their specific purposes, helping to prevent ‚Äúhallucinated‚Äù or irrelevant responses.
Memory and Context (Postgres Chat Memory)
Stores context and messages in continuous sessions using a database, ensuring the chatbot maintains conversation history.
API Calls
The workflow allows the use of APIs with any endpoints you choose, depending on your specific use case. This flexibility enables integration with various services tailored to your needs.
The OpenAI assistant understands JSON structures, and you can define in the prompt how the responses should be formatted. This allows you to structure responses neatly for the client, ensuring clarity and professionalism.
Make sure to describe the purpose of each endpoint in the assistant‚Äôs prompt to help guide the AI and prevent misinterpretation.
Customer Response Delivery
After processing and querying APIs, the generated response is sent to the backend and ultimately delivered to the customer through WhatsApp Business.
Best Practices Implemented
Preventing Hallucinations
Every API has a clear description in its prompt, ensuring the AI understands its intended use case.
Versatile Functionality
The chatbot is modular and flexible, capable of handling both sales and general customer inquiries.
Context Persistence
By utilizing persistent memory, the flow maintains continuous interaction context, which is crucial for longer conversations or follow-up queries.
Additional Recommendations
Include practical examples in the assistant‚Äôs prompt, such as frequently asked questions or decision-making flows based on API calls.
Ensure all responses align with the customer‚Äôs objectives (e.g., making a purchase or resolving technical queries).
Log interactions in detail for future analysis and workflow optimization.
This workflow provides a solid foundation for a robust and multifunctional virtual assistant üöÄ"
"Hacker News Throwback Machine - See What Was Hot on This Day, Every Year!",https://n8n.io/workflows/2688-hacker-news-throwback-machine-see-what-was-hot-on-this-day-every-year/,"This is a simple workflow that grabs HackerNews front-page headlines from today's date across every year since 2007 and uses a little AI magic (Google Gemini) to sort 'em into themes, sends a neat Markdown summary on Telegram.
How it works
Runs daily, grabs Hacker News front page for this day across every year since 2007.
Pulls headlines & dates.
Uses Google Gemini to sort headlines into topics & spot trends.
Sends a Markdown summary to Telegram.
Set up steps
Clone the workflow.
Add your Google Gemini API key.
Add your Telegram bot token and chat ID.
Built on Day-01 as part of the #100DaysOfAgenticAi
Fork it, tweak it, have fun!"
Analyze & Sort Suspicious Email Contents with ChatGPT,https://n8n.io/workflows/2666-analyze-and-sort-suspicious-email-contents-with-chatgpt/,"Analyze & Sort Suspicious Email Contents with ChatGPT and Jira
Who is this for?
This workflow is tailored for IT security teams, managed service providers (MSPs), and organizations aiming to streamline the detection and reporting of phishing emails. It's especially useful for teams handling high email volumes and requiring quick, automated analysis.
What problem is this workflow solving?
Phishing emails pose a significant cybersecurity threat, and manual review processes are time-consuming and prone to human error. This workflow automates the identification of malicious emails, provides AI-driven insights, and generates structured reports, enabling faster and more efficient responses to email-based threats.
What this workflow does
This workflow integrates Gmail or Microsoft Outlook to monitor and capture incoming emails. It processes the email content and headers, converts the email's body to a visual screenshot for clarity, and uses ChatGPT's advanced AI to analyze the email for phishing indicators. Based on the analysis, it categorizes emails as potentially malicious or benign, creating detailed Jira tickets for each case. Attachments, including the email body and screenshots, are automatically uploaded for comprehensive reporting.
Key steps include:
Email Integration: Captures emails from Gmail or Microsoft Outlook.
Content Processing: Extracts and organizes email content and metadata.
AI Analysis: Uses ChatGPT to evaluate email content and headers.
Classification: Categorizes emails as malicious or benign.
Automated Reporting: Creates Jira tickets with detailed analysis and attachments.
Setup
Authentication: Configure Gmail or Microsoft Outlook credentials in n8n.
API Keys: Add credentials for the HTML screenshot service (hcti.io) and OpenAI.
Jira Configuration: Set up project and issue types in the Jira nodes.
Customization: Update sticky notes and nodes to fit your organizational requirements, such as modifying the AI prompt or Jira ticket fields.
How to customize this workflow to your needs
Adjust email triggers to include or exclude specific senders or subjects.
Refine the AI prompt in the ChatGPT node to tailor phishing detection criteria.
Modify Jira ticket content to include additional fields or match specific workflows.
This workflow is ideal for automating email threat detection, reducing response times, and enhancing overall cybersecurity processes. By leveraging AI-powered insights, it helps organizations stay ahead of phishing attacks."
Generate 9:16 Images from Content and Brand Guidelines,https://n8n.io/workflows/2662-generate-916-images-from-content-and-brand-guidelines/,"Overview
This n8n workflow automates the creation of 9:16 aspect ratio images optimized for short-form video content and thumbnails. It integrates multiple tools to retrieve content, generate scripts, and create AI-generated imagery.
Key Features
Trigger Workflow Manually
The workflow starts when triggered manually in n8n.
Retrieve Brand Guidelines
Fetch brand elements like style, tone, and guidelines from Airtable.
SEO Keywords and Blog Post Retrieval
Retrieves blog posts and associated SEO keywords from Airtable to form the basis of image content.
Content Preparation
Uses GPT-4 to prepare a 4-scene script and thumbnail prompts for short-form videos.
AI Image Generation
Uses Leonardo.ai API to generate:
Thumbnail Images
Scene-specific Images (9:16 Aspect Ratio)
Airtable Asset Management
Generated assets (images) are saved back into Airtable with metadata like URLs and file sizes.
Tools and Integrations
n8n: Workflow automation platform.
OpenAI: Generates scripts and prompts (GPT-4O-MINI).
Leonardo.ai: AI tool for improving prompts and generating high-quality images.
Airtable: Used as a data source for brand guidelines, blog posts, and to store generated assets.
Workflow Steps
Manual Trigger
Initiate the workflow.
Retrieve Brand and SEO Guidelines
Fetch essential brand elements like tone, style, and keywords.
Filter and Fetch Blog Content
Search for blog posts relevant to selected SEO keywords.
Script Preparation
Use GPT-4 to generate a script with image prompts for scenes and thumbnails.
Image Generation
Call Leonardo.ai to create:
Scene Images in 9:16 Aspect Ratio.
A Thumbnail Image with an improved prompt.
Store Assets
Save generated assets (images) to Airtable for future use.
Workflow Structure
Nodes Breakdown:
Manual Trigger: Start the workflow.
Get Brand Guidelines: Pull brand-related information (style, tagline, tone, etc.) from Airtable.
Set Guidelines: Prepare fetched data.
Get SEO Keywords: Retrieve keywords to filter relevant content.
Keyword Filter: Filter results for specified keywords (e.g., ""AI Automation"").
Script Prep: Generate 4-scene scripts and prompts with GPT-4.
Leo - Improve Prompt: Improve image prompts for clarity and detail.
Leo - Generate Image: Create AI-generated images for scenes and thumbnails.
Wait Nodes: Ensures Leonardo image generation is complete.
Add Asset Info: Store the generated images back into Airtable with metadata.
API Credentials Required
Ensure the following credentials are configured in n8n:
OpenAI API Key
Leonardo.ai API Key
Airtable API Token
Output
Generated Images: High-quality AI-generated images with a 9:16 aspect ratio.
Saved Metadata: Asset details (URLs, sizes, types) stored in Airtable.
Usage
Import this workflow into n8n.
Set up your Airtable API, Leonardo.ai API, and OpenAI API credentials.
Run the workflow manually.
Monitor image generation and check the Airtable output for results.
Tags
OpenAI
RunwayML
Leonardo
Airtable
Video Automation
Author
AlexK1919
AI-Native Workflow Architect
More Workflow Templates
YouTube Channel
Connect with Alex"
Parse PDF with LlamaParse and save to Airtable,https://n8n.io/workflows/2661-parse-pdf-with-llamaparse-and-save-to-airtable/,"Video Guide
I prepared a comprehensive guide detailing how to automate the parsing of invoices using n8n and LlamaParse, seamlessly capturing and storing vital billing information.
Youtube Link
Who is this for?
This workflow is ideal for finance teams, accountants, and business operations managers who need to streamline invoice processing. It is particularly helpful for organizations seeking to reduce manual entry errors and improve efficiency in managing billing information.
What problem does this workflow solve?
Manually processing invoices can be time-consuming and error-prone. This automation eliminates the need for manual data entry by capturing invoice details directly from uploaded documents and storing structured data efficiently. This enhances productivity and accuracy across financial operations.
What this workflow does
The workflow leverages n8n and LlamaParse to automatically detect new invoices in a designated Google Drive folder, parse essential billing details, and store the extracted data in a structured format. The key functionalities include:
Real-time detection of new invoices via Google Drive triggers.
Automated HTTP requests to initiate parsing through Lama Cloud.
Structured storage of invoice details and line items in a database for future reference.
Google Drive Integration: Monitors a specific folder in Google Drive for new invoice uploads.
Parsing with LlamaParse: Automatically sends invoices for parsing and processes results through webhooks.
Data Storage in Airtable: Creates records for invoices and their associated line items, allowing for detailed tracking.
Setup
N8N Workflow
Google Drive Trigger:
Set up a trigger to detect new files in a specified folder dedicated to invoices.
File Upload to LlamaParse:
Create an HTTP request that sends the invoice file to LlamaParse for parsing, including relevant header settings and webhook URL.
Webhook Processing:
Establish a webhook node to handle parsed results from LlamaParse, extracting needed invoice details effectively.
Invoice Record Creation:
Create initial records for invoices in your database using the parsed details received from the webhook.
Line Item Processing:
Transform string data into structured line item arrays and create individual records for each item linked to the main invoice."
AI Data Analyst Agent and Visualization Agent for Large Spreadsheets,https://n8n.io/workflows/2653-ai-data-analyst-agent-and-visualization-agent-for-large-spreadsheets/,"Purpose of workflow:
This workflow transforms spreadsheet data into an interactive, AI-powered knowledge base that enables users to gain deep insights through natural language queries, searchability, and comparative analysis.
How it works:
Data Storage & Integration:
Spreadsheet data is imported into a no-code database (NocoDB)
System connects with an AI data analyst agent
Agent accesses table metadata and column information
Query Processing:
Users input natural language questions
AI agent interprets queries and converts them to database filters
System retrieves relevant data using filter formulas
AI synthesizes responses with analysis and insights
Advanced Capabilities:
Performs comparative analysis across multiple data points
Handles complex multi-part queries
Automatically creates visualizations:
Visualization AI Agent figures out the data and the chart type and generates professional visualization using Quickchart
Step by step setup:
Create account on nocodb.com
Create table by importing csv, copy table id
Create API token https://app.nocodb.com/#/account/tokens
In workflow, settings node, update with table id
In NocoDB tool node, setup authentication with API token created in step 3
Specify the workspace and base fields after connecting to NocoDB"
Flux Dev Image Generation (Fal.ai) to Google Drive,https://n8n.io/workflows/2644-flux-dev-image-generation-falai-to-google-drive/,"This workflow automates AI-based image generation using the Fal.ai Flux API. Define custom prompts, image parameters, and effortlessly generate, monitor, and save the output directly to Google Drive. Streamline your creative automation with ease and precision.
Who is this for?
This template is for content creators, developers, automation experts, and creative professionals looking to integrate AI-based image generation into their workflows. It‚Äôs ideal for generating custom visuals with the Fal.ai Flux API and automating storage in Google Drive.
What problem is this workflow solving?
Manually generating AI-based images, checking their status, and saving results can be tedious. This workflow automates the entire process ‚Äî from requesting image generation, monitoring its progress, downloading the result, and saving it directly to a Google Drive folder.
What this workflow does
1. Sets Custom Image Parameters: Allows you to define the prompt, resolution, guidance scale, and steps for AI image generation.
2. Sends a Request to Fal.ai: Initiates the image generation process using the Fal.ai Flux API.
3. Monitors Image Status: Checks for completion and waits if needed.
4. Downloads the Generated Image: Fetches the completed image once ready.
5. Saves to Google Drive: Automatically uploads the generated image to a specified Google Drive folder.
Setup
1. Prerequisites:
‚Ä¢ Fal.ai API Key: Obtain it from the Fal.ai platform and set it as the Authorization header in HTTP Header Auth credentials.
‚Ä¢ Google Drive OAuth Credentials: Connect your Google Drive account in n8n.
2. Configuration:
‚Ä¢ Update the ‚ÄúEdit Fields‚Äù node with your desired image parameters:
‚Ä¢ Prompt: Describe the image (e.g., ‚ÄúThai young woman net idol 25 yrs old, walking on the street‚Äù).
‚Ä¢ Width/Height: Define image resolution (default: 1024x768).
‚Ä¢ Steps: Number of inference steps (e.g., 30).
‚Ä¢ Guidance Scale: Controls image adherence to the prompt (e.g., 3.5).
‚Ä¢ Set your Google Drive folder ID in the ‚ÄúGoogle Drive‚Äù node to save the image.
3. Run the Workflow:
‚Ä¢ Trigger the workflow manually to generate the image.
‚Ä¢ The workflow waits, checks status, and saves the final output seamlessly.
Customization
‚Ä¢ Modify Image Parameters: Adjust the prompt, resolution, steps, and guidance scale in the ‚ÄúEdit Fields‚Äù node.
‚Ä¢ Change Storage Location: Update the Google Drive node with a different folder ID.
‚Ä¢ Add Notifications: Integrate an email or messaging node to alert you when the image is ready.
‚Ä¢ Additional Outputs: Expand the workflow to send the generated image to Slack, Dropbox, or other platforms.
This workflow streamlines AI-based image generation and storage, offering flexibility and customization for creative automation."
Automate Sales Meeting Prep with AI & APIFY Sent To WhatsApp,https://n8n.io/workflows/2582-automate-sales-meeting-prep-with-ai-and-apify-sent-to-whatsapp/,"This n8n template builds a meeting assistant that compiles timely reminders of upcoming meetings filled with email history and recent LinkedIn activity of other people on the invite. This is then discreetly sent via WhatsApp ensuring the user is always prepared, informed and ready to impress!
How it works
A scheduled trigger fires hourly to check for upcoming personal meetings.
When found, the invite is analysed by an AI agent to pull email and LinkedIn details of the other invitees.
2 subworkflows are then triggered for each invitee to (1) search for last email correspondence with them and (2) scrape their LinkedIn profile + recent activity for social updates.
Using both available sources, another AI agent is used to summarise this information and generate a short meeting prep message for the user.
The notification is finally sent to the user's WhatsApp, allowing them ample time to review.
How to use
There are a lot of moving parts in this template so in it's current form, it's best to use this for personal rather than team calendars.
The LinkedIn scraping method used in this workflow requires you to paste in your LinkedIn cookies from your browser which essentially let's n8n impersonate you. You can retrieve this from dev console or ask someone technical for help!
Note: It may be wise to switch to other LinkedIn scraping approaches which do not impersonate your own account for production.
Requirements
OpenAI for LLM
Gmail for Email
Google Calendar for upcoming events
WhatsApp Business account for notifications
Customising this workflow
Try adding information sources which are relevant to you and your invitees. Such as company search, other social media sites etc.
Create an on-demand version which doesn't rely on the scheduled trigger. Sometimes you want to know prepare for meetings hours or days in advance where this could help immensely."
Handling Job Application Submissions with AI and n8n Forms,https://n8n.io/workflows/2579-handling-job-application-submissions-with-ai-and-n8n-forms/,"This n8n template leverages n8n's multi-form feature to build a 2 part job application submission journey which aims to eliminate the need for applicants to re-enter data found on their CVs/Resumes.
How it works
The application submission process starts with an n8n form trigger to accept CV files in the form of PDFs.
The PDF is validated using the text classifier node to determine if it is a valid CV else the applicant is asked to reupload.
A basic LLM node is used to extract relevant information from the CV as data capture. A copy of the original job post is included to ensure relevancy.
Applicant's data is then sent to an ATS for processing. For our demo, we used airtable because we could attach PDFs to rows.
Finally, a second form trigger is used for the actual application form. However, it is prefilled to save the applicant's time and allow them to amend any of the generated application fields.
How to use
Ensure to change the redirect URL in the form ending node to use the host domain of your n8n instance.
Requirements
OpenAI for LLM
Airtable to capture applicant data
Customising the workflow
Application form is pretty basic for this demonstration but could be extended to ask more in-depth questions.
If it fits the job, why not ask applicants to upload portfolio works and have AI describe/caption them."
"AI Agent to chat with you Search Console Data, using OpenAI and Postgres",https://n8n.io/workflows/2541-ai-agent-to-chat-with-you-search-console-data-using-openai-and-postgres/,"Edit 19/11/2024: As explained on the workflow, the AI Agent with the original system prompt was not effective when using gpt4-o-mini.
To address this, I optimized the prompt to work better with this model. You can find the prompts I‚Äôve tested on this Notion Page. And yes, there is one that works well with gpt4-o-mini.
AI Agent to chat with you Search Console Data, using OpenAI and Postgres
This AI Agent enables you to interact with your Search Console data through a chat interface. Each node is documented within the template, providing sufficient information for setup and usage. You will also need to configure Search Console OAuth credentials.
Follow this n8n documentation to set up the OAuth credentials.
Important Notes
Correctly Configure Scopes for Search Console API Calls
It‚Äôs essential to configure the scopes correctly in your Google Search Console API OAuth2 credentials. Incorrect configuration can cause issues with the refresh token, requiring frequent reconnections. Below is the configuration I use to avoid constant re-authentication:

Of course, you'll need to add your client_id and client_secret from the Google Cloud Platform app you created to access your Search Console data.
Configure Authentication for the Webhook
Since the webhook will be publicly accessible, don‚Äôt forget to set up authentication. I‚Äôve used Basic Auth, but feel free to choose the method that best meets your security requirements.
ü§©üíñ Example of awesome things you can do with this AI Agent"
Create Content from Form Inputs and Save it to Google Drive using AI,https://n8n.io/workflows/2525-create-content-from-form-inputs-and-save-it-to-google-drive-using-ai/,"AI Content Generator Workflow
Introduction
This workflow automates the process of creating high-quality articles using AI, organizing them in Google Drive, and tracking their progress in Google Sheets. It's perfect for marketers, bloggers, and businesses looking to streamline content creation. With minimal setup, you can have a fully operational system to generate, save, and manage your articles in one cohesive workflow.
How It Works
Collect Inputs: Users fill out a form with details like article title, keywords, and instructions.
Generate Content: AI creates an outline and writes the article based on user inputs.
Organize Files: Saves the outline and final article in Google Drive for easy access.
Track Progress: Updates Google Sheets with links to the generated content for tracking.
Set Up Steps
Time Required: Approximately 15‚Äì20 minutes to connect all integrations and test the workflow.
Steps:
Connect Google Drive and Google Sheets: Authorize access to store files and update the spreadsheet.
Set Up OpenAI Integration: Add your OpenAI API key for generating the outline and article content.
Customize the Form: Modify the form fields to match the details you want to collect for each article.
Test the Workflow: Run the workflow with sample inputs to ensure everything works smoothly.
This workflow not only simplifies the process of article creation but also sets a foundation for expanding into additional automations, like posting to social media platforms."
üìà Receive Daily Market News from FT.com to your Microsoft outlook inbox,https://n8n.io/workflows/2516-receive-daily-market-news-from-ftcom-to-your-microsoft-outlook-inbox/,"üìà Daily Financial News - Description
This workflow automates the process of collecting, organizing, and delivering a daily summary of financial news by following these key steps:
Scheduled Activation
The workflow starts at 7:00 AM each day, triggered by the Schedule Trigger node.
News Retrieval
The HTTP Request node fetches the latest financial news from FT.com.
Content Extraction
The Extract Specific Content node scrapes targeted sections of the HTML (headlines, editor's picks, top stories, etc.) using CSS selectors to locate and capture relevant content.
News Aggregation
The Set Node gathers and organizes the extracted news data, preparing it for summarization. Categories like ""Headline #1,"" ""Editor's Picks,"" and ""Europe News"" are all structured into a single data block.
Summarization
An AI Agent (Google Gemini) takes the aggregated news data and creates a concise, HTML-formatted summary tailored to give investors an insightful market snapshot.
Email Delivery
Finally, the Microsoft Outlook node sends the summary via email to the designated recipient with the subject ""Financial news from today.""
This process ensures that financial news is efficiently curated, summarized, and delivered without manual intervention."
WordPress - AI Chatbot to enhance user experience - with Supabase and OpenAI,https://n8n.io/workflows/2504-wordpress-ai-chatbot-to-enhance-user-experience-with-supabase-and-openai/,"This is the first version of a template for a RAG/GenAI App using WordPress content.
As creating, sharing, and improving templates brings me joy üòÑ, feel free to reach out on LinkedIn if you have any ideas to enhance this template!
How It Works
This template includes three workflows:
Workflow 1: Generate embeddings for your WordPress posts and pages, then store them in the Supabase vector store.
Workflow 2: Handle upserts for WordPress content when edits are made.
Workflow 3: Enable chat functionality by performing Retrieval-Augmented Generation (RAG) on the embedded documents.
Why use this template?
This template can be applied to various use cases:
Build a GenAI application that requires embedded documents from your website's content.
Embed or create a chatbot page on your website to enhance user experience as visitors search for information.
Gain insights into the types of questions visitors are asking on your website.
Simplify content management by asking the AI for related content ideas or checking if similar content already exists. Useful for internal linking.
Prerequisites
Access to Supabase for storing embeddings.
Basic knowledge of Postgres and pgvector.
A WordPress website with content to be embedded.
An OpenAI API key
Ensure that your n8n workflow, Supabase instance, and WordPress website are set to the same timezone (or use GMT) for consistency.
Workflow 1 : Initial Embedding
This workflow retrieves your WordPress pages and posts, generates embeddings from the content, and stores them in Supabase using pgvector.
Step 0 : Create Supabase tables
Nodes :
Postgres - Create Documents Table: This table is structured to support OpenAI embedding models with 1536 dimensions
Postgres - Create Workflow Execution History Table
These two nodes create tables in Supabase:
The documents table, which stores embeddings of your website content.
The n8n_website_embedding_histories table, which logs workflow executions for efficient management of upserts. This table tracks the workflow execution ID and execution timestamp.
Step 1 : Retrieve and Merge WordPress Pages and Posts
Nodes :
WordPress - Get All Posts
WordPress - Get All Pages
Merge WordPress Posts and Pages
These three nodes retrieve all content and metadata from your posts and pages and merge them.
**Important: ** Apply filters to avoid generating embeddings for all site content.
Step 2 : Set Fields, Apply Filter, and Transform HTML to Markdown
Nodes :
Set Fields
Filter - Only Published & Unprotected Content
HTML to Markdown
These three nodes prepare the content for embedding by:
Setting up the necessary fields for content embeddings and document metadata.
Filtering to include only published and unprotected content (protected=false), ensuring private or unpublished content is excluded from your GenAI application.
Converting HTML to Markdown, which enhances performance and relevance in Retrieval-Augmented Generation (RAG) by optimizing document embeddings.
Step 3: Generate Embeddings, Store Documents in Supabase, and Log Workflow Execution
Nodes:
Supabase Vector Store
Sub-nodes:
Embeddings OpenAI
Default Data Loader
Token Splitter
Aggregate
Supabase - Store Workflow Execution
This step involves generating embeddings for the content and storing it in Supabase, followed by logging the workflow execution details.
Generate Embeddings: The Embeddings OpenAI node generates vector embeddings for the content.
Load Data: The Default Data Loader prepares the content for embedding storage. The metadata stored includes the content title, publication date, modification date, URL, and ID, which is essential for managing upserts.
‚ö†Ô∏è Important Note : Be cautious not to store any sensitive information in metadata fields, as this information will be accessible to the AI and may appear in user-facing answers.
Token Management: The Token Splitter ensures that content is segmented into manageable sizes to comply with token limits.
Aggregate: Ensure the last node is run only for 1 item.
Store Execution Details: The Supabase - Store Workflow Execution node saves the workflow execution ID and timestamp, enabling tracking of when each content update was processed.
This setup ensures that content embeddings are stored in Supabase for use in downstream applications, while workflow execution details are logged for consistency and version tracking.
This workflow should be executed only once for the initial embedding.
Workflow 2, described below, will handle all future upserts, ensuring that new or updated content is embedded as needed.
Workflow 2: Handle document upserts
Content on a website follows a lifecycle‚Äîit may be updated, new content might be added, or, at times, content may be deleted.
In this first version of the template, the upsert workflow manages:
Newly added content
Updated content
Step 1: Retrieve WordPress Content with Regular CRON
Nodes:
CRON - Every 30 Seconds
Postgres - Get Last Workflow Execution
WordPress - Get Posts Modified After Last Workflow Execution
WordPress - Get Pages Modified After Last Workflow Execution
Merge Retrieved WordPress Posts and Pages
A CRON job (set to run every 30 seconds in this template, but you can adjust it as needed) initiates the workflow. A Postgres SQL query on the n8n_website_embedding_histories table retrieves the timestamp of the latest workflow execution.
Next, the HTTP nodes use the WordPress API (update the example URL in the template with your own website‚Äôs URL and add your WordPress credentials) to request all posts and pages modified after the last workflow execution date. This process captures both newly added and recently updated content. The retrieved content is then merged for further processing.
Step 2 : Set fields, use filter
Nodes :
Set fields2
Filter - Only published and unprotected content
The same that Step 2 in Workflow 1, except that HTML To Makrdown is used in further Step.
Step 3: Loop Over Items to Identify and Route Updated vs. Newly Added Content
Here, I initially aimed to use 'update documents' instead of the delete + insert approach, but encountered challenges, especially with updating both content and metadata columns together. Any help or suggestions are welcome! :)
Nodes:
Loop Over Items
Postgres - Filter on Existing Documents
Switch
Route existing_documents (if documents with matching IDs are found in metadata):
Supabase - Delete Row if Document Exists: Removes any existing entry for the document, preparing for an update.
Aggregate2: Used to aggregate documents on Supabase with ID to ensure that Set Fields3 is executed only once for each WordPress content to avoid duplicate execution.
Set Fields3: Sets fields required for embedding updates.
Route new_documents (if no matching documents are found with IDs in metadata):
Set Fields4: Configures fields for embedding newly added content.
In this step, a loop processes each item, directing it based on whether the document already exists. The Aggregate2 node acts as a control to ensure Set Fields3 runs only once per WordPress content, effectively avoiding duplicate execution and optimizing the update process.
Step 4 : HTML to Markdown, Supabase Vector Store, Update Workflow Execution Table
The HTML to Markdown node mirrors Workflow 1 - Step 2. Refer to that section for a detailed explanation on how HTML content is converted to Markdown for improved embedding performance and relevance.
Following this, the content is stored in the Supabase vector store to manage embeddings efficiently. Lastly, the **workflow execution table is updated. These nodes mirros the Workflow 1 - Step 3 nodes.
Workflow 3 : An example of GenAI App with Wordpress Content : Chatbot to be embed on your website
Step 1: Retrieve Supabase Documents, Aggregate, and Set Fields After a Chat Input
Nodes:
When Chat Message Received
Supabase - Retrieve Documents from Chat Input
Embeddings OpenAI1
Aggregate Documents
Set Fields
When a user sends a message to the chat, the prompt (user question) is sent to the Supabase vector store retriever. The RPC function match_documents (created in Workflow 1 - Step 0) retrieves documents relevant to the user‚Äôs question, enabling a more accurate and relevant response.
In this step:
The Supabase vector store retriever fetches documents that match the user‚Äôs question, including metadata.
The Aggregate Documents node consolidates the retrieved data.
Finally, Set Fields organizes the data to create a more readable input for the AI agent.
Directly using the AI agent without these nodes would prevent metadata from being sent to the language model (LLM), but metadata is essential for enhancing the context and accuracy of the AI‚Äôs response. By including metadata, the AI‚Äôs answers can reference relevant document details, making the interaction more informative.
Step 2: Call AI Agent, Respond to User, and Store Chat Conversation History
Nodes:
AI Agent
Sub-nodes:
OpenAI Chat Model
Postgres Chat Memories
Respond to Webhook
This step involves calling the AI agent to generate an answer, responding to the user, and storing the conversation history. The model used is gpt4-o-mini, chosen for its cost-efficiency."
Narrating over a Video using Multimodal AI,https://n8n.io/workflows/2467-narrating-over-a-video-using-multimodal-ai/,"This n8n template takes a video and extracts frames from it which are used with a multimodal LLM to generate a script. The script is then passed to the same multimodal LLM to generate a voiceover clip.
This template was inspired by Processing and narrating a video with GPT's visual capabilities and the TTS API
How it works
Video is downloaded using the HTTP node.
Python code node is used to extract the frames using OpenCV.
Loop node is used o batch the frames for the LLM to generate partial scripts.
All partial scripts are combined to form the full script which is then sent to OpenAI to generate audio from it.
The finished voiceover clip is uploaded to Google Drive.
Sample the finished product here: https://drive.google.com/file/d/1-XCoii0leGB2MffBMPpCZoxboVyeyeIX/view?usp=sharing
Requirements
OpenAI for LLM
Ideally, a mid-range (16GB RAM) machine for acceptable performance!
Customising this workflow
For larger videos, consider splitting into smaller clips for better performance
Use a multimodal LLM which supports fully video such as Google's Gemini."
Multi-Agent PDF-to-Blog Content Generation,https://n8n.io/workflows/2457-multi-agent-pdf-to-blog-content-generation/,"Purpose of workflow:
The purpose of this workflow is to automatically transform complex, lengthy PDFs into concise, insightful blog posts. This process aims to make dense information more accessible and understandable to a broader audience.
How it works:
Multi-agent team:
Working together to transform PDF to insightful blog
Content Analyst: Extracts key information and structure from the PDF.
Project Planner: Organizes the workflow and ensures efficient processing.
Writers: Multiple agents that craft engaging content based on the analyzed information.
Editor: Polishes the final output for accuracy, reliability, and readability.
Step by step
User inputs parameters via a n8n form (tone, word count, sections, PDF file).
System extracts text from the PDF.
Content Analyst AI agent analyzes the extracted text, identifying main topics and key points.
Project Planner AI agent organizes the workflow, creating a structure for the blog post.
Multiple Writer AI agents (based on the number of sections specified) each write a portion of the blog post.
Editor AI agent combines all sections and polishes the final draft.
Generates a title for the blog post.
Final content is uploaded to a publishing Ghost platform"
Telegram Bot with Supabase memory and OpenAI assistant integration,https://n8n.io/workflows/2453-telegram-bot-with-supabase-memory-and-openai-assistant-integration/,"Video Guide
I prepared a detailed guide that showed the whole process of building an AI bot, from the simplest version to the most complex in a template.
Who is this for?
This workflow is ideal for developers, chatbot enthusiasts, and businesses looking to build a dynamic Telegram bot with memory capabilities. The bot leverages OpenAI's assistant to interact with users and stores user data in Supabase for personalized conversations.
What problem does this workflow solve?
Many simple chatbots lack context awareness and user memory. This workflow solves that by integrating Supabase to keep track of user sessions (via telegram_id and openai_thread_id), allowing the bot to maintain continuity and context in conversations, leading to a more human-like and engaging experience.
What this workflow does
This Telegram bot template connects with OpenAI to answer user queries while storing and retrieving user information from a Supabase database. The memory component ensures that the bot can reference past interactions, making it suitable for use cases such as customer support, virtual assistants, or any application where context retention is crucial.
1.Receive New Message: The bot listens for incoming messages from users in Telegram.
2. Check User in Database: The workflow checks if the user is already in the Supabase database using the telegram_id.
3. Create New User (if necessary): If the user does not exist, a new record is created in Supabase with the telegram_id and a unique openai_thread_id.
4. Start or Continue Conversation with OpenAI: Based on the user‚Äôs context, the bot either creates a new thread or continues an existing one using the stored openai_thread_id.
5. Merge Data: User-specific data and conversation context are merged.
6. Send and Receive Messages: The message is sent to OpenAI, and the response is received and processed.
7. Reply to User: The bot sends OpenAI‚Äôs response back to the user in Telegram.
Setup
Create a Telegram Bot using the Botfather and obtain the bot token.
Set up Supabase:
Create a new project and generate a SUPABASE_URL and SUPABASE_KEY.
Create a new table named telegram_users with the following SQL query:
create table
  public.telegram_users (
    id uuid not null default gen_random_uuid (),
    date_created timestamp with time zone not null default (now() at time zone 'utc'::text),
    telegram_id bigint null,
    openai_thread_id text null,
    constraint telegram_users_pkey primary key (id)
  ) tablespace pg_default;
OpenAI Setup:
Create an OpenAI assistant and obtain the OPENAI_API_KEY.
Customize your assistant‚Äôs personality or use cases according to your requirements.
Environment Configuration in n8n:
Configure the Telegram, Supabase, and OpenAI nodes with the appropriate credentials.
Set up triggers for receiving messages and handling conversation logic.
Set up OpenAI assistant ID in ""++OPENAI - Run assistant++"" node."
Voice Activated Multi-Agent Demo for Vagent.io using Notion and Google Calendar,https://n8n.io/workflows/2446-voice-activated-multi-agent-demo-for-vagentio-using-notion-and-google-calendar/,"Purpose
Use a lightweight Voice Interface, for you and your entire organization, to interact with an AI Supervisor, a personal AI Assistant, which has access to your custom workflows. You can also connect the supervisor to your already existing Agents.
Demo & Explanation
How it works
After recording a message in the Vagent App, it gets transcribed and sent in combination with a session ID to the registered webhook
The Main Agent acts as a router. I interprets the message while using the stored chat history (bound to the session ID) and chooses which tool to use to perform the required action and. Tools on this level are workflows, which contain subordinated Agents. Since the Main Agent interprets the original message, the raw input is passed to the Tools/Sub-Agents as a separate parameter
Within the Sub-Agents the actual processing takes place. Each of those has it‚Äôs separate chat memory (with a suffix to the main session ID), to achieve a clear separation of concerns
Depending on the required action an HTTP Request Tool is called. The result is being formatted in Markdown and returned to the Main Agent with an additional short prompt, so it does not get interpreted by the Main Agent.
Drafts are separated from a short message by added indentation (angle brackets). If some information is missing, no tool is called just yet, instead a message is returned back to the user
The Main Agent then outputs the result from the called Sub-Agent. If a draft is included, it gets separated from the spoken output
Finally the formatted output is returned as response to the webhook. The message is split into a spoken and a text version, which enables the App to read out loud unnecessary information like drafts in this example
See the full documentation of Vagent: https://vagent.io/docs
Setup
Import this workflow into your n8n instance
Follow the instructions given in the sticky notes on the canvas
Setup your credentials. OpenAI can be replaced by another LLM in the workflow, but is required for the App to work. Google Calendar and Notion are required for all scenarios to work
Copy the Webhook URL from the Webhook node of the main workflow
Download the Vagent App from https://vagent.io
In the settings paste your OpenAI API Token, the Webhook URL and the password defined for Header Auth
Now you can use the App to interact with the Multi-Agent using your Voice by tapping the Mic symbol in the App to record your message.
To use the chat trigger (for testing) properly, temporarily disable the nodes after the Tools Agent."
Daily Podcast Summary,https://n8n.io/workflows/2433-daily-podcast-summary/,"What this workflow does
Downloads the daily top podcasts of a selected genre
Summarizes the content of each podcast in a few paragraphs
Sends the summaries and the direct link to each podcast in a formatted email
Setup
Create a free API key on Taddy here: https://taddy.org/signup/developers
Input your user number and API key into the TaddyTopDaily node in the header parameters X-USER-ID and X-API-KEY respectively.
Create access credentials for your Gmail as described here: https://developers.google.com/workspace/guides/create-credentials. Use the credentials from your client_secret.json in the Gmail node.
In the Genre node, set the genre of podcasts you want a summary for. Valid values are: TECHNOLOGY, NEWS, ARTS, COMEDY, SPORTS, FICTION, etc. Look at api.taddy.org for the full list (they will be displayed in the help docs as PODCASTSERIES_TECHNOLOGY, PODCASTSERIES_NEWS, etc.)
Enter your email address in the Gmail node.
Change the schedule time for sending email from Schedule to whichever time you want to receive the email.
Test:
Hit Test Workflow.
Check your email for the results.
That's it! It should take less than 5 minutes total."
Transcribing Bank Statements To Markdown Using Gemini Vision AI,https://n8n.io/workflows/2421-transcribing-bank-statements-to-markdown-using-gemini-vision-ai/,"This n8n workflow demonstrates an approach to parsing bank statement PDFs with multimodal LLMs as an alternative to traditional OCR. This allows for much more accurate data extraction from the document especially when it comes to tables and complex layouts.
Multimodal Parsing is better than traditiona OCR because:
It reduces complexity and overhead by avoiding the need to preprocess the document into text format such as markdown before passing to the LLM.
It handles non-standard PDF formats which may produce garbled output via traditional OCR text conversion.
It's orders of magnitude cheaper than premium OCR models that still require post-processing cleanup and formatting. LLMs can format to any schema or language you desire!
How it works
You can use the example bank statement created specifically for this workflow here: https://drive.google.com/file/d/1wS9U7MQDthj57CvEcqG_Llkr-ek6RqGA/view?usp=sharing
A PDF bank statement is imported via Google Drive. For this demo, I've created a mock bank statement which includes complex table layouts of 5 columns. Typically, OCR will be unable to align the columns correctly and mistake some deposits for withdrawals.
Because multimodal LLMs do not accept PDFs directly, well have to convert the PDF to a series of images. We can achieve this by using a tool such as Stirling PDF. Stirling PDF is self-hostable which is handy for sensitive data such as bank statements.
Stirling PDF will return our PDF as a series of JPGs (one for each page) in a zipped file. We can use n8n's decompress node to extract the images and ensure they are ordered by using the Sort node.
Next, we'll resize each page using the Edit Image node to ensure the right balance between resolution limits and processing speed.
Each resized page image is then passed into the Basic LLM node which will use our multimodal LLM of choice - Gemini 1.5 Pro. In the LLM node's options, we'll add a ""user message"" of type binary (data) which is how we add our image data as an input.
Our prompt will instruct the multimodal LLM to transcribe each page to markdown. Note, you do not need to do this - you can just ask for data points to extract directly! Our goal for this template is to demonstrate the LLMs ability to accurately read the page.
Finally, with our markdown version of all pages, we can pass this to another LLM node to extract required data such as deposit line items.
Requirements
Google Gemini API for Multimodal LLM.
Google Drive access for document storage.
Stirling PDF instance for PDF to Image conversion
Customising the workflow
At time of writing, Gemini 1.5 Pro is the most accurate in text document parsing with a relatively low cost. If you are not using Google Gemini however you can switch to other multimodal LLMs such as OpenAI GPT or Antrophic Claude.
If you don't need the markdown, simply asking what to extract directly in the LLM's prompt is also acceptable and would save a few extra steps.
Not parsing any bank statements any time soon? This template also works for Invoices, inventory lists, contracts, legal documents etc."
Easy Image Captioning with Gemini 1.5 Pro,https://n8n.io/workflows/2418-easy-image-captioning-with-gemini-15-pro/,"This n8n workflow demonstrates how to automate image captioning tasks using Gemini 1.5 Pro - a multimodal LLM which can accept and analyse images. This is a really simple example of how easy it is to build and leverage powerful AI models in your repetitive tasks.
How it works
For this demo, we'll import a public image from a popular stock photography website, Pexel.com, into our workflow using the HTTP request node.
With multimodal LLMs, there is little do preprocess other than ensuring the image dimensions fit within the LLMs accepted limits. Though not essential, we'll resize the image using the Edit image node to achieve fast processing.
The image is used as an input to the basic LLM node by defining a ""user message"" entry with the binary (data) type.
The LLM node has the Gemini 1.5 Pro language model attached and we'll prompt it to generate a caption title and text appropriate for the image it sees.
Once generated, the generated caption text is positioning over the original image to complete the task. We can calculate the positioning relative to the amount of characters produced using the code node.
An example of the combined image and caption can be found here: https://res.cloudinary.com/daglih2g8/image/upload/f_auto,q_auto/v1/n8n-workflows/l5xbb4ze4wyxwwefqmnc
Requirements
Google Gemini API Key.
Access to Google Drive.
Customising the workflow
Not using Google Gemini? n8n's basic LLM node supports the standard syntax for image content for models that support it - try using GPT4o, Claude or LLava (via Ollama).
Google Drive is only used for demonstration purposes. Feel free to swap this out for other triggers such as webhooks to fit your use case."
CV Resume PDF Parsing with Multimodal Vision AI,https://n8n.io/workflows/2416-cv-resume-pdf-parsing-with-multimodal-vision-ai/,"This n8n workflow demonstrates how we can use Multimodal LLMs to parse and extract from PDF documents in n8n.
In this particular scenario, we're passing a candidate's CV/resume to an AI which filters out unqualified applications. However, this sneaky candidate has added in hidden prompt to bypass our bot! Whatever will we do? No fret, using AI Vision is one approach to solve this problem... read on!
How it works
Our candidate's CV/Resume is a PDF downloaded via Google Drive for this demonstration.
The PDF is then converted into an image PNG using a tool called Stirling PDF. Since the hidden prompt has a white font color, it is is invisible in the converted image.
The image is then forwarded to a Basic LLM node to process using our multimodal model - in this example, we'll use Google's Gemini 1.5 Pro.
In the Basic LLM node, we'll need to set a User Message with the type of Binary. This allows us to directly send the image file in our request.
The LLM is now immune to the hidden prompt and its response is has expected.
The example CV/Resume with hidden prompt can be found here: https://drive.google.com/file/d/1MORAdeev6cMcTJBV2EYALAwll8gCDRav/view?usp=sharing
Requirements
Google Gemini API Key. Alternatively, GPT4 will also work for this use-case.
Stirling PDF or another service which can convert PDFs into images. Note for data privacy, this example uses a public API and it is recommended that you self-host and use a private instance of Stirling PDF instead.
Customising the workflow
Swap out the manual trigger for another trigger such as a webhook to integrate into your existing services.
This example demonstrates a validation use-case ie. ""does the candidate look qualified?"". You can try additionally extracting data points instead such as years of experiences, previous companies etc."
AI Agent with charts capabilities using OpenAI Structured Output and Quickchart,https://n8n.io/workflows/2400-ai-agent-with-charts-capabilities-using-openai-structured-output-and-quickchart/,"This workflow is an experiment to integrate charts in AI Agents, using the new Structured Output from OpenAI and Quickchart.io.
How it works
Users chat with an AI Agent.
Anytime the AI Agent considers a chart is needed, it calls a tool to generate a chart
OpenAI generates a chart using the Quickchart definition
This object is added at the end of a Quickchart.io URL (see documentation)
The url is added in the conversation via the AI Agent as markdown.
Set up steps
Create an OpenAI API Key
Create the OpenAI credentials
Use the credentials for the HTTP Request node (as Predefined Credential type)
Activate your workflow
Start chatting
For example, you can ask the AI Agent to generate a chart about the top 5 movies at the box office
Start exploring the limits
Shout-out
Quickchart.io is an amazing open source project that provides a free API to test. Go check them out!
Example of chart"
Telegram chat with PDF,https://n8n.io/workflows/2392-telegram-chat-with-pdf/,"What this template does
This template serves as a Chatbot that enables you to ask questions about the content of a PDF directly in Telegream.
It checks incoming Telegram messages if they contain a document. If they do, it stores the PDF in a Pinecone Vector store. If there's no document, it will search the Vector Store for information and try to answer your question.
Setup
Open the Telegram app and search for the BotFather user (@BotFather)
Start a chat with the BotFather
Type /newbot to create a new bot
Follow the prompts to name your bot and get a unique API token
Save your access token and username
Once you set your bot, you can send the pdf, and then ask questions about the content.
How to adjust it to your needs
You can exchange the Groq chat model with any model that you like
Exchange Pinecone with any other vector store tool you like (e.g. Supabase, Postgres or QDrant)
#Telegram, #Pinecone, #Openai, #GroQ"
Advanced AI Demo (Presented at AI Developers #14 meetup),https://n8n.io/workflows/2358-advanced-ai-demo-presented-at-ai-developers-14-meetup/,"This workflow was presented at the AI Developers meet up in San Fransico on 24 July, 2024.
AI workflows
Categorize incoming Gmail emails and assign custom Gmail labels. This example uses the Text Classifier node, simplifying this usecase.
Ingest a PDF into a Pinecone vector store and chat with it (RAG example)
AI Agent example showcasing the HTTP Request tool. We teach the agent how to check availability on a Google Calendar and book an appointment."
"Automate Competitor Research with Exa.ai, Notion and AI Agents",https://n8n.io/workflows/2354-automate-competitor-research-with-exaai-notion-and-ai-agents/,"This n8n workflow demonstrates a simple multi-agent setup to perform the task of competitor research. It showcases how using the HTTP request tool could reduce the number of nodes needed to achieve a workflow like this.
How it works
For this template, a source company is defined by the user which is sent to Exa.ai to find competitors.
Each competitor is then funnelled through 3 AI agents that will go out onto the internet and retrieve specific datapoints about the competitor; company overview, product offering and customer reviews.
Once the agents are finished, the results are compiled into a report which is then inserted in a notion database.
Check out an example output here: https://jimleuk.notion.site/2d1c3c726e8e42f3aecec6338fd24333?v=de020fa196f34cdeb676daaeae44e110&pvs=4
Requirements
An OpenAI account for the LLM.
Exa.ai account for access to their AI search engine.
SerpAPI account for Google search.
Firecrawl.dev account for webscraping.
Notion.com account for database to save final reports.
Customising the workflow
Add additional agents to gather more datapoints such as SEO keywords and metrics.
Not using notion? Feel free to swap this out for your own database."
"Handling Appointment Leads and Follow-up With Twilio, Cal.com and AI",https://n8n.io/workflows/2342-handling-appointment-leads-and-follow-up-with-twilio-calcom-and-ai/,"This n8n workflow builds an appointment scheduling AI agent which can
Take enquiries from prospective customers and help them book an appointment by checking appointment availability
Where no appointment is booked, the Agent is able to send follow-up messages to re-engage leads.
After an appointment is booked, the agent is able reschedule or even cancel the booking for the user without human intervention.
For small outfits, this workflow could contribute the necessary ""man-power"" required to increase business sales.
The sample Airtable can be found here: https://airtable.com/appO2nHiT9XPuGrjN/shroSFT2yjf87XAox
2024-10-22 Updated to Cal.com API v2.
How it works
The customer sends an enquiry via SMS to trigger our workflow. For this trigger, we'll use a Twilio webhook.
The prospective or existing customer's number is logged in an Airtable Base which we'll be using to track all our enquries.
Next, the message is sent to our AI Agent who can reply to the user and decide if an appointment booking can be made. The reply is made via SMS using Twilio.
A scheduled trigger which runs every day, checks our chat logs for a list of prospective customers who have yet to book an appointment but still show interest.
This list is sent to our AI Agent to formulate a personalised follow-up message to each lead and ask them if they want to continue with the booking.
The follow-up interaction is logged so as to not to send too many messages to the customer.
Requirements
A Twilio account to receive customer messages.
An Airtable account and Base to use as our datastore for enquiries.
Cal.com account to use as our scheduling service.
OpenAI account for our AI model.
Customising this workflow
Not using Airtable? Swap this out for your CRM of choice such as hubspot or your own service.
Not using Cal.com? Swap this out for API-enabled services such as Acuity Scheduling or your own service."
Breakdown Documents into Study Notes using Templating MistralAI and Qdrant,https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/,"This n8n workflow takes in a document such as a research paper, marketing or sales deck or company filings, and breaks them down into 3 templates: study guide, briefing doc and timeline.
These templates are designed to help a student, associate or clerk quickly summarise, learn and understand the contents to be more productive.
Study guide - a short quiz of questions and answered generated by the AI Agent using the contents of the document.
Briefing Doc - key information and insights are extracted by the AI into a digestable form.
Timeline - key events, durations and people are identified and listed into a simple to understand timeline by the AI
How it works
A local file trigger watches a local network directory for new documents.
New documents are imported into the workflow, its contents extracted and vectorised into a Qdrant vector store to build a mini-knowledgebase.
The document then passes through a series of template generating prompts where the AI will perform ""research"" on the knowledgebase to generate the template contents.
Generated study guide, briefing and timeline documents are exported to a designated folder for the user.
Requirements
Self-hosted version of n8n.
Qdrant instance for knowledgebase.
Mistral.ai account for embeddings and AI model.
Customising your workflow
Try adding your own templates or adjusting the existing templates to suit your unique use-case. Anything is quite possible and limited only by your imagination!
Want to go fully local?
A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/1VV5R2nW-IhVcFP_k8uEks4LsLRZrHSNG/view?usp=sharing"
Build a Financial Documents Assistant using Qdrant and Mistral.ai,https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/,"This n8n workflow demonstrates how to manage your Qdrant vector store when there is a need to keep it in sync with local files. It covers creating, updating and deleting vector store records ensuring our chatbot assistant is never outdated or misleading.
Disclaimer
This workflow depends on local files accessed through the local filesystem and so will only work on a self-hosted version of n8n at this time. It is possible to amend this workflow to work on n8n cloud by replacing the local file trigger and read file nodes.
How it works
A local directory where bank statements are downloaded to is monitored via a local file trigger. The trigger watches for the file create, file changed and file deleted events.
When a file is created, its contents are uploaded to the vector store.
When a file is updated, its previous records are replaced.
When the file is deleted, the corresponding records are also removed from the vector store.
A simple Question and Answer Chatbot is setup to answer any questions about the bank statements in the system.
Requirements
A self-hosted version of n8n. Some of the nodes used in this workflow only work with the local filesystem.
Qdrant instance to store the records.
Customising the workflow
This workflow can also work with remote data. Try integrating accounting or CRM software to build a managed system for payroll, invoices and more.
Want to go fully local?
A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/189F1fNOiw6naNSlSwnyLVEm_Ho_IFfdM/view?usp=sharing"
Talk to your SQLite database with a LangChain AI Agent üß†üí¨,https://n8n.io/workflows/2292-talk-to-your-sqlite-database-with-a-langchain-ai-agent/,"This n8n workflow demonstrates how to create an agent using LangChain and SQLite. The agent can understand natural language queries and interact with a SQLite database to provide accurate answers. üí™
üöÄ Setup
Run the top part of the workflow once.
It downloads the example SQLite database, extracts from a ZIP file and saves locally (chinook.db).
üó£Ô∏è Chatting with Your Data
Send a message in a chat window.
Locally saved SQLite database loads automatically.
User's chat input is combined with the binary data.
The LangChain Agend node gets both data and begins to work.
The AI Agent will process the user's message, perform necessary SQL queries, and generate a response based on the database information. üóÑÔ∏è
üåü Example Queries
Try these sample queries to see the AI Agent in action:
""Please describe the database"" - Get a high-level overview of the database structure, only one or two queries are needed.
""What are the revenues by genre?"" - Retrieve revenue information grouped by genre, LangChain agent iterates several time before producing the answer.
The AI Agent will store the final answer in its memory, allowing for context-aware conversations. üí¨
Read the full article: üëâ https://blog.n8n.io/ai-agents/"
Generate Text-to-Speech Using Elevenlabs via API,https://n8n.io/workflows/2245-generate-text-to-speech-using-elevenlabs-via-api/,"üéâ Do you want to master AI automation, so you can save time and build cool stuff?
I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners.
üëâüèª Join the AI Atelier üëàüèª
This workflow provides an API endpoint to generate speech from text using Elevenlabs.io, a popular text-to-speech service.
Step 1: Configure Custom Credentials in n8n
To set up your credentials in n8n, create a new custom authentication entry with the following JSON structure:
{
  ""headers"": {
    ""xi-api-key"": ""your-elevenlabs-api-key""
  }
}
Replace ""your-elevenlabs-api-key"" with your actual Elevenlabs API key.
Step 2: Send a POST Request to the Webhook
Send a POST request to the workflow's webhook endpoint with these two parameters:
voice_id: The ID of the voice from Elevenlabs that you want to use.
text: The text you want to convert to speech.
This workflow has been a significant time-saver in my video production tasks. I hope it proves just as useful to you!
Happy automating!
The n8Ninja"
Chat with OpenAIs GPT via a simple Telegram Bot,https://n8n.io/workflows/2114-chat-with-openais-gpt-via-a-simple-telegram-bot/,"Use case
LLMs have provided a lot of value for several use cases. Especially some OpenAI models are proving to be quite valuable. However, it's sometimes not super accessible to chat with these models. This workflow enables you to chate directly with OpenAI's GPT-3.5 via Telegram.
How it works
A simple telegram bot that connects to your botfather bot to give AI responses, using OpenAI's GPT 3.5 model, to a user's messages with emojis.
What to do
Add your telegram API key and your OpenAI api key and have fun!"
Chat with OpenAI Assistant (by adding a memory),https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/,"OpenAI Assistant is a powerful tool, but at the time of writing it doesn't automatically remember past messages from a conversation.
This workflow demonstrates how to get around this, by managing the chat history in n8n and passing it to the assistant when required.
This makes it possible to use OpenAI Assistant for chatbot use cases.
Note that to use this template, you need to be on n8n version 1.28.0 or later."
Chat with a Google Sheet using AI,https://n8n.io/workflows/2085-chat-with-a-google-sheet-using-ai/,"This workflow allows you to ask questions about the data in a Google Sheet over a chat interface. It uses n8n's built-in chat, but could be modified to work with Slack, Teams, WhatsApp, etc.
Behind the scenes, the workflow uses GPT4, so you'll need to have an OpenAI API key that supports it.
How it works
The workflow uses an AI agent with custom tools that call a sub-workflow. That sub-workflow reads the Google Sheet and returns information from it.
Because models have a context window (and therefore a maximum number of characters they can accept), we can't pass the whole Google Sheet to GPT - at least not for big sheets. So we provide three ways of querying less data, that can be used in combination to answer questions. Those three functions are:
List all the columns in the sheet
Get all values of a single column
Get all values of a single row
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Telegram AI bot with LangChain nodes,https://n8n.io/workflows/2035-telegram-ai-bot-with-langchain-nodes/,"This workflow connects Telegram bots with LangChain nodes in n8n.
The main AI Agent Node is configured as a Conversation Agent. It has a custom System Prompt which explains the reply formatting and provides some additional instructions.
The AI Agent has several connections:
OpenAI GPT-4 model is called to generate the replies
Window Buffer Memory stores the history of conversation with each user separately
There is an additional Custom n8n Workflow tool (Dall-E 3 Tool). AI Agent uses this tool when the user requests an image generation.
In the lower part of the workflow, there is a series of nodes that call Dall-E 3 model with the user Telegram ID and a prompt for a new image. Once image is ready, it is sent back to the user.
Finally, there is an extra Telegram node that masks HTML syntax for improved stability in case the AI Agent replies using the unsupported format."
Personalize marketing emails using customer data and AI,https://n8n.io/workflows/1978-personalize-marketing-emails-using-customer-data-and-ai/,"This workflow uses AI to analyze customer sentiment from product feedback. If the sentiment is negative, AI will determine whether offering a coupon could improve the customer experience.
Upon completing the sentiment analysis, the workflow creates a personalized email templates. This solution streamlines the process of engaging with customers post-purchase, particularly when addressing dissatisfaction, and ensures that outreach is both personalized and automated.
This workflow won the 1st place in our last AI contest.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
AI chatbot that can search the web,https://n8n.io/workflows/1959-ai-chatbot-that-can-search-the-web/,"This workflow is designed for dynamic and intelligent conversational capabilities. It incorporates OpenAI's GPT-4o model for natural language understanding and generation. Additional tools include SerpAPI and Wikipedia for enriched, data-driven responses. The workflow is triggered manually, and utilizes a 'Window Buffer Memory' to maintain the context of the last 20 interactions for better conversational continuity. All these components are orchestrated through n8n nodes, ensuring seamless interconnectivity.
To use this template, you need to be on n8n version 1.50.0 or later."
Suggest meeting slots using AI,https://n8n.io/workflows/1953-suggest-meeting-slots-using-ai/,"The purpose of this n8n workflow is to automate the process of identifying incoming Gmail emails that are requesting an appointment, evaluating their content, checking calendar availability, and then composing and sending a response email.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Goodreads Quote Extraction with Bright Data and Gemini,https://n8n.io/workflows/4438-goodreads-quote-extraction-with-bright-data-and-gemini/,"Goodreads Quote Extraction with Bright Data and Gemini
This workflow demonstrates how to fetch data specifically from Goodreads web pages using Bright Data and then extract specific information (quotes) from that data using a Google Gemini AI model.
How it works
The workflow is triggered manually.
It sends a request to a Bright Data collector to scrape data from a predefined list of Goodreads URLs.
The collected text data from Goodreads is then passed to a Google Gemini AI node.
The AI node processes the text and extracts quotes based on a specified JSON schema output format.
Set up steps
Setting up this workflow should take only a few minutes.
You will need a Bright Data API key to configure the 'Header Auth' credential.
You will need a Google Gemini API key to configure the 'Google Gemini(PaLM) Api account' credential.
Ensure the correct Bright Data collector ID is set in the 'Perform Bright Data Web Request' node URL.
Make sure the full list of target Goodreads URLs is correctly added to the 'Perform Bright Data Web Request' node's body.
Link your created credentials to the respective nodes ('Perform Bright Data Web Request' and 'Quotes Extractor').
Keep detailed descriptions for specific node configurations in sticky notes inside your workflow canvas."
"AI-Powered GitHub Bot: Auto-Triage Issues with GPT-4o, Pinecone & Discord Alerts",https://n8n.io/workflows/4248-ai-powered-github-bot-auto-triage-issues-with-gpt-4o-pinecone-and-discord-alerts/,"üöÄ GitHub MCP Webhook Tool Sub-workflow: AI-Powered Repo Assistant
Transform your GitHub workflows with a customizable AI agent that integrates seamlessly via a single webhook. Think ""Copilot for repos,"" but you own the code, prompts, and data.
üëÄ Key Features
üîß Fully Customizable: Tweak system prompts, swap AI models (GPT-4o, 4o-mini), or add tools like spam detection.

üì¶ Single Webhook Integration: Add AI to any existing workflow with 1 HTTP node.

ü§ñ Auto-Triage & Action: Sort issues by severity, auto-comment, flag spam (e.g., NSFW content), and ping teams via Discord/Slack.

üîç Codebase-Aware: Queries your repo‚Äôs docs and code for context-aware responses (Pinecone vector stores optional).

üí∏ Cost-Friendly: Starts at $1 ‚Äì cheaper than a coffee, infinitely more useful.
üõ†Ô∏è Getting Started
üöÄ Deploy the Template: Clone the ""GitHub MCP Server"" workflow.

üîë Set Variables: Update repoOwner, repoName, and Discord/Slack IDs in the ""CHANGE THESE!!!"" node.

üå≤ Pinecone Setup: Pre-index your docs/code for lightning-fast queries using the vector nodes.

üåç Go Live: Trigger via webhook ‚Äì use it for issue replies, triage, or even automated PR drafts!
üí° Use Cases
üö® Auto-Alert Teams: Flag critical bugs to senior devs, route FAQs to interns.

üóëÔ∏è Spam Shield: Delete explicit issues instantly, quarantine self-promo for review.

üìù Docs Bot: Answer contributor questions using your repo‚Äôs documentation."
Automate GitHub PR Linting with Google Gemini AI and Auto-Fix PRs,https://n8n.io/workflows/4073-automate-github-pr-linting-with-google-gemini-ai-and-auto-fix-prs/,"LintGuardian: Automated PR Linting with n8n & AI
What It Does
LintGuardian is an n8n workflow template that automates code quality enforcement for GitHub repositories. When a pull request is created, the workflow automatically analyzes the changed files, identifies linting issues, fixes them, and submits a new PR with corrections. This eliminates manual code style reviews, reduces back-and-forth comments, and lets your team focus on functionality rather than formatting.
How It Works
The workflow is triggered by a GitHub webhook when a PR is created. It fetches all changed files from the PR using the GitHub API, processes them through an AI-powered linting service (Google Gemini), and automatically generates fixes. The AI agent then creates a new branch with the corrected files and submits a ""linting fixes"" PR against the original branch. Developers can review and merge these fixes with a single click, keeping code consistently formatted with minimal effort.
Prerequisites
To use this template, you'll need:
n8n instance: Either self-hosted or using n8n.cloud
GitHub repository: Where you want to enforce linting standards
GitHub Personal Access Token: With permissions for repo access (repo, workflow, admin:repo_hook)
Google AI API Key: For the Gemini language model that powers the linting analysis
GitHub webhook: Configured to send PR creation events to your n8n instance
Setup Instructions
Import the template into your n8n instance
Configure credentials:
Add your GitHub Personal Access Token under Credentials ‚Üí GitHub API
Add your Google AI API key under Credentials ‚Üí Google Gemini API
Update repository information:
Locate the ""Set Common Fields"" code node at the beginning of the workflow
Change the gitHubRepoName and gitHubOrgName values to match your repository
const commonFields = {
  'gitHubRepoName': 'your-repo-name',
  'gitHubOrgName': 'your-org-name'
}
Configure the webhook:
Create a file named .github/workflows/lint-guardian.yml in your repository replacing the Trigger n8n Workflow step with your webhook:
name: Lint Guardian

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  trigger-linting:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger n8n Workflow
        uses: fjogeleit/http-request-action@v1
        with:
          url: 'https://your-n8n-instance.com/webhook/1da5a6e1-9453-4a65-bbac-a1fed633f6ad'
          method: 'POST'
          contentType: 'application/json'
          data: |
            {
              ""pull_request_number"": ${{ github.event.pull_request.number }},
              ""repository"": ""${{ github.repository }}"",
              ""branch"": ""${{ github.event.pull_request.head.ref }}"",
              ""base_branch"": ""${{ github.event.pull_request.base.ref }}""
            }
          preventFailureOnNoResponse: true
Customize linting rules (optional):
Modify the AI Agent's system message to specify your team's linting preferences
Adjust file handling if you have specific file types to focus on or ignore
Security Considerations
When creating your GitHub Personal Access Token, remember to:
Choose the minimal permissions needed (repo, workflow, admin:repo_hook)
Set an appropriate expiration date
Treat your token like a password and store it securely
Consider using GitHub's fine-grained personal access tokens for more limited scope
As GitHub documentation notes: ""Personal access tokens are like passwords, and they share the same inherent security risks.""
Extending the Template
You can enhance this workflow by:
Adding Slack notifications when linting fixes are submitted
Creating custom linting rules specific to your team's needs
Expanding it to handle different types of code quality checks
Adding approval steps for more controlled environments
This template provides an excellent starting point that you can customize to fit your team's exact workflow and code style requirements."
"Taxi Service Provider (Production-Ready, Part 4)",https://n8n.io/workflows/4047-taxi-service-provider-production-ready-part-4/,"Workflow Name: ü§ñ Taxi Service Provider
Template was created in n8n v1.90.2
Skill Level: High
Categories: n8n, Chatbot
Stacks
Execute Sub-workflow Trigger node
Chat Trigger node
Redis node
Postgres node
AI Agent node
Calculator node
If node, Switch node, Code node, Edit Fields (Set)
Prerequisite
Execute Sub-workflow Trigger: Taxi Service Workflow (or your own node)
Sub-workflow: Demo Call Back (or your own node)
Production Features
Scaling Design for n8n Queue mode in production environment
Provider Data from external Database with Caching Mechanism
Optional Score design output
Optimize Taxi Provider Prompt from database
Error Management
What this workflow does?
This is a n8n Taxi Provider Workflow demo. It will receive message from the Taxi Service Workflow, process the estimation and return to the caller.
How it works
The Flow Trigger node will wait for the message from other Sub-workflow.
When message is received, it will first check for the matching Provider from the PostgreSQL database.
Then it will increase a Provider Number in Redis for selection later
Trigger a AI Agent to process the fare estimation and create a NEW booking
Finally, send the AI response to the Call Output
Note: if the Provider is set to inactive, it will do nothing
Set up instructions
Pull and Set up the required SQL from our Github repository.
Create you Redis credentials, refer to n8n integration documentation for more information.
Select your Credentials in Provider Cache, Save Provider Cache and Provider Number.
Create you Postgres credentials, refer to n8n integration documentation for more information.
Select your Credentials in Load Provider Data and Create Booking Data.
Modify the AI Agent prompt to fit your need
How to adjust it to your needs
By default, this template will use the sys_provider table provider information, you could change it for your own design.
You can use any AI Model for the AI Agent node
Include is our prompt for the taxi service provider. It is a flexible design which use the data from the Provider Cache to customize the prompt, so you could create more Taxi Service Providers by duplicate this workflow in the Taxi Service node."
"AI Chatbot Call Center: Taxi Service (Production-Ready, Part 3)",https://n8n.io/workflows/4046-ai-chatbot-call-center-taxi-service-production-ready-part-3/,"Workflow Name: üõéÔ∏è Taxi Service
Template was created in n8n v1.90.2
Skill Level: High
Categories: n8n, Chatbot
Stacks
Execute Sub-workflow Trigger node
Chat Trigger node
Redis node
Postgres node
AI Agent node
If node, Switch node, Code node, Edit Fields (Set)
Prerequisite
Execute Sub-workflow Trigger: Taxi Service Workflow (or your own node)
Sub-workflow: Taxi Service Provider (or your own node)
Sub-workflow: Demo Call Back (or your own node)
Production Features
Scaling Design for n8n Queue mode in production environment
Service Data from external Database with Caching Mechanism
Optional Long Terms Memory design
Find Route Distance using Google Map API
Optional Multi-Language Wait Output example
Error Management
What this workflow does?
This is a n8n Taxi Service Workflow demo. It is the core node for Taxi Service. It will receive message from the Call Center Workflow, handling the QA from the caller, and pass to each of the Taxi Service Provider Workflow to process the estimation.
How it works
The Flow Trigger node will wait for the message from Call Center or other Sub-workflow.
When message is received, it will first check for the matching Service from the PostgreSQL database.
If no service or service is inactive, output Error.
Next, always reset the Session Data in Cache, with channel_no set to taxi
Next, delete the previous Route Data in Cache
Trigger a AI Agent to process the fare estimation question to create the Route Data
Use the Google Map Route API to calculate the distance.
Repeat until created the route data, then pass to all the Taxi Service Provider for an estimation.
Set up instructions
Pull and Set up the required SQL from our Github repository.
Create you Redis credentials, refer to n8n integration documentation for more information.
Select your Credentials in Service Cache, Save Service Cache, Reset Session, Delete Route Data, Route Data, Update User Session and Create Route Data.
Create you Postgres credentials, refer to n8n integration documentation for more information.
Select your Credentials in Load Service Data, Postgres Chat Memory, Load User Memory and Save User Memory.
Modify the AI Agent prompt to fit your need
Set you Google Map API key in Find Route Distance
How to adjust it to your needs
By default, this template will use the sys_service table provider information, you could change it for your own design.
You can use any AI Model for the AI Agent node
Learn we use the prompt for the Load/Save User Memory on demand.
Include is our prompt for the taxi service. It is a flexible design which use the data from the Service node to customize the prompt, so you could duplicate this workflow as another service.
Create difference Taxi Providers to process the and feedback the estimate."
Wix Chat Auto-Responder with OpenAI GPT and Email Fallback,https://n8n.io/workflows/3925-wix-chat-auto-responder-with-openai-gpt-and-email-fallback/,"Wix Chat Auto-Responder with OpenAI GPT and Email Fallback
This template connects Wix Chat with OpenAI via n8n to automate intelligent customer responses on your website only when no human has responded recently. It uses smart throttling, checks for member vs. anonymous visitors, pulls chat history, and optionally alerts support staff via email if AI shouldn't reply.
Perfect for solopreneurs, agencies, or customer support teams looking to auto-handle conversations while maintaining a fallback system.
Key Features
üîå Connects Wix Chat API to n8n seamlessly
üß† Uses GPT-4 (via OpenAI) to auto-generate replies
üßç‚Äç‚ôÇÔ∏è Detects member vs. guest via webhook payload
üì¨ Optional email alert to support staff if AI should not reply
‚è±Ô∏è Only responds if a human hasn't answered in the past 12 hours
‚úÇÔ∏è Splits long AI messages into chunks to fit Wix API constraints
üß± Modular & customizable‚Äîadapt for CRM, Slack, SMS, etc.
Requirements
Published Wix site with Wix Inbox enabled
Wix Developer App credentials (Client ID, Secret, Instance ID)
OpenAI API Key
Active n8n instance (self-hosted or cloud)
Working SMTP credentials (for fallback email alert)
Setup Instructions
1. Import the Workflow
Download and import the .json file into your n8n instance. All nodes should appear in the visual editor.
2. Rename Key Nodes (Recommended for Clarity)
Rename the following nodes for easier maintenance:
If ‚Üí Check Member vs Visitor
Execute Workflow ‚Üí Generate OAuth Token
Send Email ‚Üí Alert Technician via Email
3. Configure OAuth for Wix API
In the node HTTP Request1, replace the placeholder values:
""client_id"": ""YOUR_WIX_APP_ID"",
""client_secret"": ""YOUR_WIX_APP_SECRET"",
""instance_id"": ""YOUR_WIX_INSTANCE_ID""
üí° You may move this to a separate workflow and call it using Execute Workflow.
4. Set Up the Webhook in Wix
Copy the Webhook URL from the Webhook node.
Go to your Wix dashboard:
Navigate to: Settings &gt; Automations &gt; Create New
Trigger: ""When someone sends a message via chat""
Action: ""Send a Webhook""
Paste the n8n Webhook URL in the configuration.
5. Add Your OpenAI API Key
Open both OpenAI Chat Model1 and OpenAI Chat Model2 nodes:
Add your OpenAI credentials.
Adjust the model (e.g., GPT-4 or GPT-4o) and temperature as needed.
6. Customize Response Logic (Optional)
In the Code node labeled Response Throttle, modify:
const allowChat = true;
const allowEmail = false;
const humanResponseTimeWindow = 43200000; // 12 hours in ms
Change values to:
Disable chat or email
Modify the delay window before AI is allowed to respond
7. Set Up Email Fallback (Optional)
If human support is required, the workflow sends an email:
Add SMTP credentials in the Send Email node
Customize the message, or replace the node with Slack, webhook, or SMS alert
8. Test
Open your Wix site in an incognito browser tab.
Use the chat and monitor the bot's response.
Check Executions in n8n to debug or verify the flow.
How to Customize
Trigger follow-up actions (e.g., Zapier, CRM sync) after AI response
Customize GPT prompt via AI Agent nodes
Prevent replies by channel (e.g., don‚Äôt reply via email)
Add filters for keywords, lead scoring, or VIP contacts with If or Code nodes
Includes
‚úÖ 1 Workflow JSON file
üîê OAuth2 logic (can be modularized into a separate workflow)
üìù Sticky notes and comments to guide usage
‚öôÔ∏è Production-ready, extensible logic for any support stack"
Extract Named Entities from Web Pages with Google Natural Language API,https://n8n.io/workflows/3950-extract-named-entities-from-web-pages-with-google-natural-language-api/,"Who is this for?
Content strategists analyzing web page semantic content
SEO professionals conducting entity-based analysis
Data analysts extracting structured data from web pages
Marketers researching competitor content strategies
Researchers organizing and categorizing web content
Anyone needing to automatically extract entities from web pages
What problem is this workflow solving?
Manually identifying and categorizing entities (people, organizations, locations, etc.) on web pages is time-consuming and error-prone. This workflow solves this challenge by:
Automating the extraction of named entities from any web page
Leveraging Google's powerful Natural Language API for accurate entity recognition
Processing web pages through a simple webhook interface
Providing structured entity data that can be used for analysis or further processing
Eliminating hours of manual content analysis and categorization
What this workflow does
This workflow creates an automated pipeline between a webhook and Google's Natural Language API to:
Receive a URL through a webhook endpoint
Fetch the HTML content from the specified URL
Clean and prepare the HTML for processing
Submit the HTML to Google's Natural Language API for entity analysis
Return the structured entity data through the webhook response
Extract entities including people, organizations, locations, and more with their salience scores
Setup
Prerequisites:
An n8n instance (cloud or self-hosted)
Google Cloud Platform account with Natural Language API enabled
Google API key with access to the Natural Language API
Google Cloud Setup:
Create a project in Google Cloud Platform
Enable the Natural Language API for your project
Create an API key with access to the Natural Language API
Copy your API key for use in the workflow
n8n Setup:
Import the workflow JSON into your n8n instance
Replace ""YOUR-GOOGLE-API-KEY"" in the ""Google Entities"" node with your actual API key
Activate the workflow to enable the webhook endpoint
Copy the webhook URL from the ""Webhook"" node for later use
Testing:
Use a tool like Postman or cURL to send a POST request to your webhook URL
Include a JSON body with the URL you want to analyze: {""url"": ""https://example.com""}
Verify that you receive a response containing the entity analysis data
How to customize this workflow to your needs
Analyzing Specific Entity
Modify the ""Google Entities"" node parameters to include entityType filters
Add a ""Function"" node after ""Google Entities"" to filter specific entity types
Create conditions to extract only entities of interest (people, organizations, etc.)
Processing Multiple URLs in Batch:
Replace the webhook with a different trigger (HTTP Request, Google Sheets, etc.)
Add a ""Split In Batches"" node to process multiple URLs
Use a ""Merge"" node to combine results before sending the response
Enhancing Entity Data:
Add additional API calls to enrich extracted entities with more information
Implement sentiment analysis alongside entity extraction
Create a data transformation node to format entities by type or relevance
Additional Notes
This workflow respects Google's API rate limits by processing one URL at a time
The Natural Language API may not identify all entities on a page, particularly for highly technical content
HTML content is trimmed to 100,000 characters if longer to avoid API limitations
Consider legal and privacy implications when analyzing and storing entity data from web pages
You may want to adjust the HTML cleaning process for specific website structures
‚ù§Ô∏è Hueston SEO Team"
"Automate Agile Refinement Prep with Gmail, OpenAI & Google Sheets",https://n8n.io/workflows/3909-automate-agile-refinement-prep-with-gmail-openai-and-google-sheets/,"üë§ Who is this for?
This workflow is designed for Scrum Masters, Agile Coaches, and Product Owners who want to automate backlog refinement preparation using Google Sheets, Gmail, and OpenAI. It‚Äôs ideal for teams seeking consistent, high-quality refinement sessions with minimal manual effort.
üß© What problem is this workflow solving?
Many Agile teams struggle with unprepared refinement sessions, unclear user stories, and inconsistent feedback. This workflow solves that by automating the selection, validation, and communication process‚Äîsaving time while improving backlog quality and team alignment.
‚öôÔ∏è What this workflow does
The workflow checks the Scrum Master‚Äôs Google Calendar for an upcoming refinement event. It then pulls potential user stories from a Google Sheets backlog, filters them by status, and validates them using OpenAI agents for Scrum, business, and technical feedback. Finally, it compiles everything into a structured HTML email, either creating a draft or sending it upon approval.
üöÄ Setup
Use consistent event naming in Google Calendar
Configure environment variables for your project (e.g., sheet names, statuses)
Connect your own backlog and Definition of Ready
Customize the AI prompts and email layout
üõ†Ô∏è How to customize this workflow to your needs
Replace Google Sheets with Jira or Airtable
Switch Gmail to Outlook, SMTP, or Mailgun
Extend error handling or approval logic
Tailor the email tone and AI prompts to match your team
üî• Unique Selling Points (USPs)
AI-Powered Multi-Perspective Feedback
Automated Definition of Ready (DoR) Checks
Calendar-Aware Triggering
Fully Formatted Gmail Emails, With Approval Flow
Plug-and-Play Customizability"
LinkedIn Company ICP Scoring Automation with Airtop & Google Sheets,https://n8n.io/workflows/3475-linkedin-company-icp-scoring-automation-with-airtop-and-google-sheets/,"About The ICP Company Scoring Automation
Sorting through lists of potential leads manually to determine who's truly worth your sales team's time isn't just tedious, it's incredibly inefficient. Without proper qualification, your team might spend hours pursuing prospects who aren't the right fit for your product, while ideal customers slip through the cracks.
How to Automate Identifying Your Ideal Customers
With this automation, you'll learn how to automatically score and prioritize leads using data extracted directly from LinkedIn profiles via Airtop's integration with n8n. By the end, you'll have a fully automated workflow that analyzes prospects and calculates an Ideal Customer Profile (ICP) score, helping your sales team focus on high-potential opportunities.
What You'll Need
A free Airtop API key
A copy of this Google Sheets
Understanding the Process
This automation transforms how you qualify and prioritize leads by extracting real-time, accurate information directly from LinkedIn profiles. Unlike static databases that quickly become outdated, this workflow taps into the most current professional information available.
The workflow in this template:
Uses Airtop to extract comprehensive LinkedIn profile data
Analyzes the data to calculate an ICP score based on AI interest, technical depth, and seniority
Updates your Google Sheet with the enriched data and the ICP Company score
Company ICP Scoring Workflow
Our company-focused workflow analyzes company LinkedIn profiles with a comprehensive set of criteria:
Company Identity Extraction
Company Scale Assessment
Business Classification
Technical Sophistication Assessment
Investment Profile
To then calculate the ICP Scoring, it will focus on:
AI Implementation Level: Low-5 pts, Medium-10 pts, High-25 pts
Technical Sophistication: Basic-5 pts, Intermediate-15 pts, Advanced-25 pts, Expert-35 pts
Employee Count: 0-9 employees-5 pts, 10-150 employees-25 pts, 150+ employees-30 pts
Automation Agency Status: True-20 pts, False-0 pts
Geography: US/Europe Based-10 pts, Other-0 pts
Setting Up Your Automation
We've created ready-to-use templates for both person and company ICP scoring. Here's how to get started:
Configure your connections
Connect your Google Sheets account
Add your Airtop API key (obtain from the Airtop dashboard)
Set up your Google Sheet
Ensure your Google Sheet has the necessary columns for input data and result fields
Ensure that columns Linkedin_URL_Company and ICP_Score_Company exist at least
Configure the Airtop module
Set up the Airtop module to use the appropriate LinkedIn extraction prompt
Use our provided prompt that extracts company profile data
Customization Options
While our templates work out of the box, you might want to customize them for your specific needs:
Modify the ICP scoring criteria: Adjust the point values or add additional criteria specific to your business
Add notification triggers: Set up Slack or email notifications for high-value leads that exceed a certain ICP threshold
Implement batch processing: Modify the workflow to process leads in batches to optimize performance
Add conditional logic: Create different scoring models for different industries or product lines
Integrate with your CRM: Integrate this automation with your preferred CRM to get the details added automatically for you
Real-World Applications
Here's how businesses are using this automation:
AI Sales Platform: A B2B AI company could implement this workflow to process their trade show lead list of contacts. Within hours, they can identify the top 50 prospects based on ICP score.
SaaS Analytics Tool: A SaaS company could implement LinkedIn enrichment to identify which companies fit best. The automation processes weekly leads and categorizes them into high, medium, and low priority tiers, allowing their sales team to focus on the most promising opportunities first.
Best Practices
To get the most out of this automation:
Review and refine your ICP criteria quarterly: What constitutes an ideal customer may evolve as your product and market develop
Create tiered follow-up processes: Develop different outreach strategies based on ICP score ranges
Perform regular data validation: Periodically check the accuracy of the automated scoring against your actual sales results
What's Next?
Now that you've automated your ICP scoring with LinkedIn data, you might be interested in:
Setting up automated outreach sequences based on ICP score thresholds
Creating custom reporting dashboards to track conversion rates by ICP segment
Expanding your scoring model to include additional data sources
Implementing lead assignment automation based on ICP scores
Happy automating!"
AI-Powered Language and Coding Tutor with GPT-4 and Timed Telegram Messages,https://n8n.io/workflows/3347-ai-powered-language-and-coding-tutor-with-gpt-4-and-timed-telegram-messages/,"üìö MOTION TUTOR - AI-Powered Language Learning System
Unlock the Power of Language with Personalized AI Learning!
MOTION TUTOR is a revolutionary AI-powered language learning platform that adapts to your progress and guides you from basic vocabulary to complex sentence structures. Whether you're a beginner or looking to refine your skills, MOTION TUTOR provides an engaging and structured learning experience.
üéØ Key Features:
‚úÖ AI-Powered Lessons:
Receive interactive, real-life conversation-based lessons generated by cutting-edge AI. MOTION TUTOR dynamically adapts content to match your learning pace.
‚úÖ Personalized Progress Tracking:
Track your growth effortlessly! The system connects with Airtable to monitor your vocabulary, grammar, and situational language skills, ensuring lessons align with your goals.
‚úÖ Memory Retention for Contextual Learning:
With built-in memory capabilities, MOTION TUTOR remembers where you left off and keeps the conversation flowing. No need to repeat or restart ‚Äî pick up right where you left off!
‚úÖ Real-Time Telegram Integration:
Learn on the go! Engage with MOTION TUTOR directly through Telegram and receive instant feedback and lesson suggestions based on your performance.
‚úÖ Structured Learning Path:
From mastering essential vocabulary to crafting complex sentences, MOTION TUTOR guides you step-by-step with a well-defined roadmap.
Basic Vocabulary & Phrases
Basic Grammar Structures
Situational Vocabulary
Intermediate Grammar Concepts
Advanced Vocabulary
Complex Sentence Structures
‚úÖ Customized Role-Playing & Practice Scenarios:
Practice vocabulary and grammar in context by engaging in situational conversations that simulate real-life interactions.
üí° How It Works:
Message on Telegram: Send a message to initiate your learning session.
AI Processes Input: MOTION TUTOR analyzes your request and customizes the next lesson.
Context Retention: Continuation from previous sessions to ensure seamless learning.
Track Progress: Updates and tracks progress in Airtable for personalized learning insights.
Receive Feedback: AI-generated lessons and tips sent directly through Telegram.
üì° Why Choose MOTION TUTOR?
üéâ Engaging & Interactive Learning: Learn in a fun and practical way with interactive scenarios.
üöÄ AI Adaptation for All Levels: Whether you're learning from scratch or advancing your fluency, MOTION TUTOR customizes content to suit your journey.
üìù Progress You Can Measure: Track your achievements and growth with structured Airtable data.
üîÅ Never Lose Track: Memory-powered learning ensures you continue smoothly where you last paused.
üéÅ Ideal For:
Language learners seeking a personalized, guided learning experience.
Professionals preparing for international business communication.
Travelers aiming to learn practical vocabulary before their next adventure.
Students looking to enhance their grammar and conversation skills.
üöÄ Get Started Today!
Embrace the future of language learning with MOTION TUTOR and gain the confidence to engage in everyday conversations and complex dialogues effortlessly.
Ready to Speak with Confidence?
üí¨ Connect with MOTION TUTOR on Telegram and Start Learning Today!"
Create an AI-Powered Discord Assistant with GPT-4o for Multi-Channel Messaging,https://n8n.io/workflows/3346-create-an-ai-powered-discord-assistant-with-gpt-4o-for-multi-channel-messaging/,"ü§ñ Discord AI Workflow: Your Automated Assistant! üöÄ
üåü Workflow Overview
Transforms your Discord server into an intelligent, responsive powerhouse of communication and automation!
üîß Core Components
üí¨ AI-Powered Messaging
ü§ù Multi-Channel Interaction
üß† Smart Response Generation
üîó Seamless Workflow Integration
üö¶ Trigger Modes
1Ô∏è‚É£ Workflow Trigger
üîì Activated by external workflows
üì® Processes incoming tasks
üåê Supports complex automation scenarios
2Ô∏è‚É£ Chat Message Trigger
üó£Ô∏è Responds to direct Discord messages
ü§î Contextual understanding
üîç Real-time interaction
üõ†Ô∏è Key Features
ü§ñ AI-Driven Conversations
üìä Dynamic Message Handling
üîí Secure Credential Management
üåà Flexible Configuration
üöÄ Use Cases
üì¢ Automated Announcements
üÜò Support Ticket Management
üìù Content Generation
ü§ù Community Engagement
üí° Smart Capabilities
üß© Modular Design
üîÑ Seamless Data Flow
üìù Character Limit Management
üåê Multi-Channel Support
üõ°Ô∏è Security & Performance
üîê OAuth Integration
üöß Error Handling
üìä Performance Optimization
üõ†Ô∏è Continuous Improvement
üéØ Workflow Magic
User Input ‚û°Ô∏è AI Processing ‚û°Ô∏è Smart Response ‚û°Ô∏è Discord Channel 
   üåü        ü§ñ             üí¨            üì®
üîç Customization Playground
üé® Personalize AI Responses
üîß Adjust Interaction Rules
üìê Fine-Tune Workflow Behavior
üöß Troubleshooting Toolkit
üïµÔ∏è Credential Verification
üî¨ Permissions Check
üìã Comprehensive Logging
üÜò Error Handling Strategies
üåà Future Possibilities
ü§ñ Advanced AI Integration
üöÄ Expanded Interaction Modes
üß† Machine Learning Enhancements
üåê Ecosystem Expansion"
Extract Specific Pages from PDFs with CustomJS API,https://n8n.io/workflows/3872-extract-specific-pages-from-pdfs-with-customjs-api/,"This n8n template shows how to extract selected pages from a generated PDF with the PDF Toolkit by www.customjs.space.
@custom-js/n8n-nodes-pdf-toolkit
Notice
Community nodes can only be installed on self-hosted instances of n8n.
What this workflow does
Downloads each PDF using an HTTP Request.
Extract pages from the PDF file as needed.
Requirements
Self-hosted n8n instance
CustomJS API key for extracting PDF files.
PDF files to be merged to be converted into a PDF
Workflow Steps:
Manual Trigger:
Runs with user interaction.
Download PDF File:
Pass urls for PDF files to merge.
Extract Pages from PDF:
Extract selected pages from a generated PDF
Usage
Get API key from customJS
Sign up to customJS platform.
Navigate to your profile page
Press ""Show"" button to get API key
Set Credentials for CustomJS API on n8n
Copy and paste your API key generated from CustomJS here.
Design workflow
A Manual Trigger for starting workflow.
HTTP Request Nodes for downloading PDF files.
Extract Pages from PDF.
You can replace logic for triggering and returning results.
For example, you can trigger this workflow by calling a webhook and get a result as a response from webhook. Simply replace Manual Trigger and Write to Disk nodes.
Perfect for
Taking a note of specific pages from PDF files.
Splitting PDF file into multiple parts."
Automate URL Shortening with Bitly Using Llama3 Chat Interface,https://n8n.io/workflows/3885-automate-url-shortening-with-bitly-using-llama3-chat-interface/,"Who is this for?
This workflow is intended for online users who want and need workflow automation that minimizes link creation time and shortens links. This has an impact on the speed of work time and energy. In reality, links created one by one and are very long are very time consuming and impractical, especially if we have to compile them first and send them. This is also a form of service to the community at n8n and the n8n company, and so that reality is no longer tiring and is able to answer existing reality problems. It can also be used to learn how automatic link building works.
How it works?
Easy explanation:
Firstly, open chat is intended to trigger commands that are sent to the AI model and Bitly Tools, you can do any command and just include the link. Later here we will also create a link.
The two, AI Agents will send AI models to work according to input commands and produce output via open chat.
Third, the Bitly tool that is set up to create links will do its job. Because it is tied to the AI Agent and AI model.
Finally, you can immediately see and open the link that has been created.
This is very simple and very simple. Makes it easy for everyone.
Set up instructions
Complete what is in the nodes as stated in the notes column.
First, you download Ollama, using the guide that has been provided. Then after you run the model, you can create and connect a ""Credential Account"".
Second, you need an open chat trigger node or you can use other triggers too.
Third, the AI Agent selects the Ollama model, and the Bitly tools that have been prepared are set up here.
Fourth, just run it and it works.
This is very simple and very practical.
Requirements
As a reminder:
It should be set in each node, like what your goal is, also according to the conditions of your goal.
There must be (if not, make sure it is registered) in each ""Account Credential"" by following the guide on how to do it n8n the guide is very complete
Don't forget to save, and make sure the workflow is active.
How to customize this workflow to your needs
You can immediately add other nodes or other triggers to be able to send this link, such as email and so on, which will provide knowledge for your needs in additional nodes, so that accuracy is also high when carrying out tasks, task goals and responsibilities.
Congratulations on this automation and Thank you to n8n and the n8n community"
Extract & Analyze Brand Content with Bright Data and Google Gemini,https://n8n.io/workflows/3846-extract-and-analyze-brand-content-with-bright-data-and-google-gemini/,"Who this is for?
The Brand Content Extract, Summarization & Sentiment Analysis workflow is designed for professionals and teams who need to monitor, understand, and act on public brand perception at scale.
It is ideal for:
Brand Managers - Looking to track how their brand is portrayed online.
Marketing Analysts - Seeking insights from competitor and industry content.
PR & Communications Teams - Evaluating media tone and potential reputation risks.
Data Scientists & AI Developers - Automating content intelligence pipelines.
Growth Hackers - Performing large-scale web listening for campaign optimization.
What problem is this workflow solving?
Manually tracking and interpreting how your brand is mentioned across blogs, news sites, or product reviews is labor-intensive and unscalable. Traditional scraping tools return raw data but lack insights like summarization, sentiment analysis etc.
This workflow addresses:
Scalable extraction of brand-related content using Bright Data's infrastructure.
Textual data extract for easy decision-making or alerting.
Automated summarization of verbose or multi-paragraph articles using Gemini.
Sentiment analysis of how a brand is being portrayed.
What this workflow does
Receives input: A brand URL for the data extraction and analysis.
Uses Bright Data's Web Unlocker to extract content from relevant sites.
Cleans and preprocesses the scraped content for readability.
Sends the content to Google Gemini for:
Enriched results including:
Cleaned content
Summary
Sentiment Analysis
Sends the response to a target system via Webhook notification
Perists the response to disk
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Set URL and Bright Data Zone for setting the brand content URL and the Bright Data Zone name.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Update Source : Update the workflow input to read from Google Sheet or Airbase for dynamically tracking multiple brands or topics.
AI Prompt Customization : Tailor Gemini prompts for:
Summary length (brief vs. detailed)
Detailed Sentiment with the custom structured data format.
Brand-specific tone detection (e.g., trust, excitement, dissatisfaction)
Output Destinations: Configure the output node to send the responses to various platforms, such as Slack, CRM systems, or databases."
Retrieve NASA Space Weather & Asteroid Data with GPT-4o-mini and Telegram,https://n8n.io/workflows/3834-retrieve-nasa-space-weather-and-asteroid-data-with-gpt-4o-mini-and-telegram/,"Who is this for?
This workflow is intended for researchers, educators, developers, media publications, scientists, engineers, science teachers, researchers & scientists in the fields of astronomy, astrophysics, and space weather, science journalists & content creators, educators & education platform managers, data analysts & data engineers in the space research sector, museum managers or public science institutions and professions that use space data who want and need to automate workflows involving NASA space data, facilitate access to NASA science data without manual coding, increase the efficiency of research/education teams until they can be combined with other tools. In reality, data if processed and work on data one by one is very time-consuming and tiring, especially if we have to compile it first and send it. This is also a form of devotion to the community at n8n and the n8n company, as well as devotion to scientists, teachers, developers, education and media publications so that the reality is no longer tiring and is able to answer the problems of existing reality.
How it works?
Easy explanation:
The First Nodes Telegram Trigger is used to receive data that is delivered to the next Nodes. It is like inputting information or instructions to the AI Agent and the AI Agent will process it.
After that, the AI Agent will accept to work together with the AI Model, in this case I chose OpenAI because of its capabilities.
After that, the AI Agent will carry out its task, namely taking data from each NASA Node that I have set up.
Then, after the NASA Nodes send and are received and then processed by the AI Agent, the AI Agent will send it to the last node, namely Telegram, which receives and sends the results of the AI Agent's task.
You can immediately see the data in a mature manner, because the data has been processed. If you want to process it again, just direct this AI System to process it again.
And all tasks and responsibilities are completed
It is very practical and easy, plus its automated nature. I'm sure you can understand it wisely.
Set up instructions
Complete what is in the nodes as stated in the notes column.
First you have to connect Telegram ""Credential Account"" to the nodes.
Second, Next I set up using ""Define below"" it's like customization, because the output that is issued later will be in accordance with the existing trigger. Here I have set it up so just use it.
Then, the AI Model that fits the purpose and has the capability is the OpenAI Model. Here you need to create or connect a ""Credential Account"" and just choose the model. Here I have set up with a wise and capable choice.
Next, for each NASA Nodes, you must create or connect a ""Credential Account"" (Using only one Credential Account). Here, just follow the instructions of the module that has been given n8n, it makes it very easy.
Finally, for the Telegram node action, you only need to connect the ""Credential Account"" that you have created on the first Telegram node.
Everything has been set up wisely, so you just have to follow the notes or setup instructions given, or you can add other things according to your specific purpose and congratulations you are ready to use it.
Requirements
As a reminder:
It must be set in each node, such as what your goals are, also according to the conditions of your goals.
There must be (If not, make sure it is registered) in each ""Credential Account"" by following the guide on how to do it n8n the guide is very complete
Don't forget to save, and make sure the workflow is active.
How to customize this workflow to your needs
You can directly add nodes to store them again in the 2nd data layer. Prepare a knowledge base for your purposes in additional nodes, so that the accuracy is also high when performing tasks and answering them, and in accordance with the objectives of the task and responsibilities."
Enhance Chat Responses with Real-Time Search via Bright Data MCP & Gemini AI,https://n8n.io/workflows/3779-enhance-chat-responses-with-real-time-search-via-bright-data-mcp-and-gemini-ai/,"Disclaimer
This template is only available on n8n self-hosted as it's making use of the community node for MCP Client.
Who this is for?
The Chat Conversations with Bright Data MCP Search Engines & Google Gemini workflow is designed for users who need real-time, AI-enhanced conversations powered by live search engine results.
This workflow is tailored for:
Data Analysts - Who want live, search-based data fused with AI reasoning.
Marketing Researchers - Seeking up-to-the-minute market or competitor insights via conversational AI.
Product Managers - Exploring user needs, market trends, and competitor analysis in real time.
AI Developers - Building dynamic applications that combine live search data with intelligent conversation agents.
Growth Hackers - Who need fast, conversational research tools for campaign ideation, outreach, or content creation.
What problem is this workflow solving?
Traditional chatbots and AI systems often rely on static, outdated data.
This workflow enables AI agents to fetch live search engine data and converse intelligently about it, making interactions dynamic, accurate, and highly contextual.
This workflow solves the major gaps of:
Outdated Knowledge: Regular chatbots lack up-to-date information from live web searches.
Manual Search Fatigue: Manually searching for information and interpreting it is time-consuming.
Context Bridging: Connecting search results into meaningful, conversational replies requires human-level reasoning.
What this workflow does?
Accepts a user's conversational query input.
Triggers a search request to Bright Data‚Äôs MCP Search Engines API (Google, Bing, etc.) based on the query.
Waits for the search task to complete.
Retrieves real-time search results.
Feeds the search results and original question into Google Gemini.
Generates a human-like, contextually accurate AI response combining live information and conversational flow.
Outputs the response back into a chat app.
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine. Also, do ""Account Setup"" as mentioned in the @brightdata/mcp URL.
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.

Make sure to copy the Bright Data Web Unlocker API Token within the Environments textbox above as API_TOKEN=<your-token>.
7. Update the HTTP Request for Webhook Notification node for sending the Webhook notification for chat responses.
How to customize this workflow to your needs
Change Search Engine:
Add or Remove the Search Engine MCP tools based upon the Bright Data MCP Server updates.
Expand Outputs:
Send AI chat responses to Slack, Discord, custom chat UIs, WhatsApp, or CRM systems.
Store conversation logs in a database (PostgreSQL, MongoDB, etc.) for future audits or training."
üîê Generate PIX Payment QR Codes for Any Brazilian Bank Key,https://n8n.io/workflows/3704-generate-pix-payment-qr-codes-for-any-brazilian-bank-key/,"üîê PIX QR Code Generator for Any Bank and Key ‚Äì Powered by n8n
Easily generate PIX QR Codes with this ready-to-use n8n workflow, supporting any valid PIX key (CPF, CNPJ, phone, email, or random key) and following the official BACEN standard ‚Äî the Central Bank of Brazil's regulation.
Whether you want to automate payments in chatbots, CRM systems, e-commerce checkouts, or custom apps, this solution provides fast, secure, and fully compliant QR Code generation.
üí° What is PIX?
PIX is a real-time payment system developed by the Central Bank of Brazil (BACEN).
It allows instant transfers between banks, available 24/7, without fees for individuals.
Businesses widely use PIX for quick, easy, and reliable payments ‚Äî making QR Codes an essential tool for sales, subscriptions, and customer service.
‚öôÔ∏è What this Workflow Does
Creates a fully compliant PIX payment code with automatic checksum (CRC)
Generates a dynamic QR Code that customers can scan with any banking app
Returns:
The full PIX EMV payment code (ready for integration)
A public URL linking to the QR Code image
Clean payment metadata for frontend or messaging bots
All generated dynamically from the user's input: key, amount, receiver name, city, and payment description.
üß† How it Works
The user provides basic payment information: key, amount, name, city, and description.
The workflow assembles a PIX payload following BACEN's EMV QR standards.
It automatically generates and attaches the CRC (checksum) for validation.
A QR Code is created based on the complete PIX code.
The output returns both the PIX code and a QR Code image URL, ready to use anywhere.
Each component works seamlessly to deliver a ready-to-scan QR Code for instant payments.
üõ†Ô∏è How to Set Up
Deploy the workflow on your n8n Cloud or self-hosted instance.
Configure the input variables manually or through an API/Webhook:
pixKey: Your PIX key (CPF, CNPJ, email, phone, or random)
receiverName: Name of the individual or company receiving the payment
city: City name associated with the receiver
amount: Payment value (in BRL, ex: 79.90)
description: Short description for the transaction (optional)
Trigger the workflow:
Manually (for testing)
Via webhook/API (ideal for integrations with chatbots, e-commerce, CRM)
Customize the flow (optional):
Set default values if user inputs are missing
Adjust QR Code size or styling (inside the QR Code generator node)
Connect to database/storage if you want to save issued PIX transactions
üéØ Who is This For?
E-commerce stores needing instant payment links
Chatbots or WhatsApp bots offering product sales or bookings
CRM systems wanting to automate billing processes
Subscription platforms requiring fast payment confirmation
Any business wanting to accept payments faster and smarter
‚úÖ Requirements
Active n8n Cloud or self-hosted instance
A valid PIX key (CPF, CNPJ, email, phone, or random key)
Basic payment information: amount, city, receiver name, description
No external paid APIs needed ‚Äî fully open and server-side
üõí Ready to Automate Your Payments?
‚ù§Ô∏è Buy ready-to-use workflows at: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud with my partner link: https://n8n.partnerlinks.io/amanda"
Extract & Summarize Indeed Company Info with Bright Data and Google Gemini,https://n8n.io/workflows/3702-extract-and-summarize-indeed-company-info-with-bright-data-and-google-gemini/,"Who this is for?
Extract & Summarize Indeed Company Info is an automated workflow that extracts the Indeed company profile information using Bright Data Web Unlocker, transform it using Google Gemini‚Äôs LLM, and forward the transformed response with the summary to a specified webhook for downstream use.
This workflow is tailored for:
Recruiters and HR teams looking to assess companies quickly during talent sourcing.
Job seekers researching potential employers and needing summarized company insights.
Market researchers and analysts monitoring competitor or industry players.
What problem is this workflow solving?
Searching and evaluating company profiles on Indeed manually can be time-consuming and inefficient, especially when dealing with large volumes of companies. Manually browsing, copying, and summarizing company descriptions, reviews, and ratings from Indeed hinders productivity and limits real-time insights.
This workflow solves this by:
Automating the extraction of company details from Indeed using Bright Data Web Unlocker.
Summarizing the raw data using Google Gemini's language model for a quick, human-readable overview.
Sending the transformed response with the summary to a chosen endpoint, like Slack, Notion, Airtable, or a custom webhook.
What this workflow does
This automated pipeline does the following:
Scrape Indeed company profile pages (e.g., ratings, description, reviews) using Bright Data‚Äôs Web Unlocker.
Transform the scraped content into structured JSON using n8n‚Äôs built-in tools.
Summarize and extract meaningful insights using Google Gemini's large language model.
Forward the summarized data to a specified webhook or app for real-time access, storage, or analysis.
Forward the formatted response to a specified webhook or app for real-time access, storage, or analysis.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the search query, Bright Data zone by navigating to the Set Indeed Search Query node.
Update the Webhook Notifier with the Webhook endpoint of your choice.
How to customize this workflow to your needs
This workflow is built to be flexible - whether you're a company or a market researcher, entrepreneur, or data analyst. Here‚Äôs how you can adapt it to fit your specific use case:
Changing the data source:
Replace the Indeed search input with other job or business listing platforms if needed (e.g., Glassdoor, Crunchbase)
Refining the LLM prompt:
Tailor the Gemini prompt to transform or summarize the Indeed company information in a specific format.
Routing the output to different destinations:
Send summaries or transformed response to Google Sheets, Airtable, or CRMs like HubSpot or Salesforce etc."
Analyze & Tag User Feedback in Notion with GPT-4 Sentiment Analysis,https://n8n.io/workflows/3689-analyze-and-tag-user-feedback-in-notion-with-gpt-4-sentiment-analysis/,"Overview
This n8n workflow processes user feedback automatically, tags it with sentiment, and links it to relevant insights in Notion.
It uses GPT-4 to analyze each feedback entry, determine whether it corresponds to an existing insight or a new one, and update the Notion databases accordingly.
It helps teams centralize and structure qualitative user feedback at scale.
Who It‚Äôs For
Product teams looking to organize and prioritize user feedback.
Founders or solo builders seeking actionable insights from qualitative data.
Anyone managing a Notion workspace where feedback is collected and needs to be tagged or linked to features and improvements.
Prerequisites
A Notion account with:
A Feedback database (must include fields for feedback content and status).
An Insights database with multi-select fields for Solution, User Persona, and a relation to Feedback.
The Notion template (linked below) helps you get started quickly ‚Äî just remove the mock data.
A configured Notion API integration in n8n.
üëâ Don‚Äôt forget to connect the n8n integration to the correct Notion page.
An OpenAI API key
Notion Template
This workflow is designed to work seamlessly with a pre-configured Notion template that includes the required feedback and insights structure.
üëâ User Feedback Analysis ‚Äì Notion Template
How It Works
The workflow is triggered when a feedback item is updated in Notion (e.g. new feedback is submitted).
Sentiment analysis (Positive, Neutral, or Negative) is run using OpenAI and stored in a select field in Notion.
The AI agent analyzes the feedback to:
Identify whether it matches an existing insight.
Or create a new insight in Notion with a concise name, solution, and user persona.
The feedback is then linked to the appropriate insight and marked as ""Processed.""
How to Use It
Connect your Notion databases in all Notion nodes (including those used by the AI agent) for both Feedback and Insights ‚Äî follow the node names provided.
Ensure your OpenAI and Notion credentials are correctly set.
Set up your product context:
Define a ‚ÄúProduct Overview‚Äù and list your ‚ÄúCore Features‚Äù.
This helps the AI agent categorize insights more accurately.
(The Basecamp product is used as an example in the template.)
(Optional) Modify the prompt to better fit your specific product context.
Once feedback is added or updated in Notion, the workflow triggers automatically.
Notes
Only feedback with the status Received is processed.
New insights are only created if no relevant match is found.
Feedback is linked to insights via Notion‚Äôs relation property.
A fallback parser is included to fix potential formatting issues in the AI output.
You can swap the default n8n memory for a more robust backend like Supabase.
üôè Please share your feedback with us. It helps us tremendously!"
Analyze Telegram Messages with OpenAI and Send Notifications via Gmail & Telegram,https://n8n.io/workflows/3306-analyze-telegram-messages-with-openai-and-send-notifications-via-gmail-and-telegram/,"AI-powered Telegram message analysis with multi-tool notifications (Gmail, Telegram)
This workflow triggers on Telegram updates, analyzes messages with an AI Agent using MCP tools, and sends notifications via Gmail and Telegram.
Detailed Description
Who is this for?
This template is for teams, businesses, or individuals using Telegram for communication who need automated, AI-driven insights and notifications. It‚Äôs ideal for customer support teams, project managers, or tech enthusiasts wanting to process Telegram messages intelligently and receive alerts via Gmail and Telegram.
What problem is this workflow solving? Use case
This workflow solves the challenge of manually monitoring Telegram messages by automating message analysis and notifications. For example, a support team can use it to analyze customer queries on Telegram with AI tools (OpenAI, Airbnb, Brave, FireCrawl) and get notified via Gmail and Telegram for quick responses.
What this workflow does
The workflow:
Triggers on a Telegram update (e.g., a new message) using the Listen for Telegram Updates node.
Processes the message with the Analyze Message with AI node, an AI Agent using MCP tools like OpenAI Chat, Airbnb search, Brave search, and FireCrawl.
Sends notifications via the Send Gmail Notification and Send Telegram Alert nodes, including AI-generated insights.
Setup
Prerequisites:
Telegram bot token for the trigger and notification nodes.
Gmail API credentials for sending emails.
API keys for OpenAI, Airbnb, Brave, and FireCrawl (used in the AI Agent).
Steps:
Configure the Listen for Telegram Updates node with your Telegram bot token.
Set up the Analyze Message with AI node with your OpenAI API key and other tool credentials.
Configure the Send Gmail Notification node with your Gmail credentials.
Set up the Send Telegram Alert node with your Telegram bot token.
Test by sending a Telegram message to trigger the workflow.
Setup takes ~15-30 minutes. Detailed instructions are in sticky notes within the workflow.
How to customize this workflow to your needs
Add more AI tools (e.g., sentiment analysis) in the Analyze Message with AI node.
Modify the notification message in the Send Gmail Notification and Send Telegram Alert nodes to include specific AI outputs.
Add nodes for other channels like Slack or SMS after the AI Agent.
Disclaimer
This workflow uses Community nodes (e.g., Airbnb, Brave, FireCrawl), which are available only in self-hosted n8n instances. Ensure your n8n setup supports Community nodes before using this template."
Extract & Summarize Yelp Business Review with Bright Data and Google Gemini,https://n8n.io/workflows/3682-extract-and-summarize-yelp-business-review-with-bright-data-and-google-gemini/,"Who this is for?
Extract & Summarize Yelp Business Review is an automated workflow that extracts the Yelp business reviews using Bright Data Web Unlocker, process and formats the raw data, summarizes using the Google Gemini's LLM, and forward the concise summary with the review respose to a specified webhook endpoint.
This workflow is tailored for:
Local SEO Specialists who need structured insights from Yelp reviews to optimize listings.
Business Owners wanting quick summaries of what customers love or complain about.
Reputation Managers who monitor brand sentiment and identify customer pain points.
Data Analysts & Researchers extracting Yelp review patterns at scale.
AI Product Builders needing clean Yelp review data as input for their LLMs or recommender systems.
What problem is this workflow solving?
Yelp reviews are rich in customer sentiment but messy to work with manually. This workflow solves:
The pain of scraping Yelp review content manually.
The challenge of building the structured data with the summary.
The need for structured outputs suitable for analysis, reports, or AI input.
What this workflow does
This automated pipeline does the following:
Bright Data Integration: Queries Yelp and scrapes business listing data using Bright Data's Web Unlocker.
Structured Data Formatting: Formats the Yelp review data to a structured response in JSON format.
Google Gemini Summarization: Sends the cleaned reviews to Google Gemini to:
Output Delivery: Returns the structured response with the concise summary over the webhook endpoint.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Yelp Business Review URL with the Bright Data zone by navigating to the Set Yelp URL with the Bright Data Zone node.
Update the Webhook Notifier for the merged response node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
This workflow is built to be flexible - whether you‚Äôre a market researcher, entrepreneur, or data analyst. Here's how you can adapt it to fit your specific use case:
Target Specific Business Categories
Update the Yelp Business Review input to scrape different businesses like gyms, salons etc.
Limit Reviews
Add filters by description, location, page range to get the top reviews.
Tweak the Data Extraction Node
Update the Structured Data Extractor node Output Parser for building the JSON response with the appropriate fields or attributes.
Tweak the Summarization Prompt
Modify the Gemini prompt to generate a comprehensive summary.
Send Output to Other Destinations
Replace the Webhook URL to forward output to:
Google Sheets
Airtable
Slack or Discord
Custom API endpoints"
Automate LinkedIn Requests & Icebreaker with Browserflow and Google sheets,https://n8n.io/workflows/3345-automate-linkedin-requests-and-icebreaker-with-browserflow-and-google-sheets/,"Disclaimer: As this workflow uses a Community node, it is available only to self-hosted installation of n8n
Who is this for?
This n8n template is designed for professionals, recruiters, and marketers who need to automate LinkedIn data population and contact request processes. It is particularly useful for those looking to streamline their outreach efforts and manage LinkedIn connections more efficiently.
What problem is this workflow solving? / Use Case
The workflow addresses the challenge of manually managing LinkedIn connections and sending contact requests followed by an AI generated ice-breaker. By automating these tasks, users can save time, reduce errors, and ensure consistent follow-ups with potential clients, partners, or candidates.
What this workflow does:
This template extract LinkedIn adresses from a google sheet, check if they already are connected to you, and send them a contact request if hey are not. Then it sends to them an AI generated Ice breaker messages, which take into account the personal and company posts, and your company description, to find common points of interest.
You can contact up to 50 persons for free.
Setup
Copy this google sheet to your Google account
Enter your data in ""Set your Data Here"":
Google sheet URL: Paste here the URL of your google sheet
Your activity: Define here what your company is about (used to match your activity with the users activity)
Your name: Enter your name (used to sign your mail)
Your company: Enter here your company name
Your email: Used to send your email
Maxitems: The number of maximum posts to download from LinkedIn
Set an account on Rapid API, you are allowed for free to 50 credits (5$ for 500) and most of the calls cost 1 credit.
Setup an account at Browserflow You will benefit from a 7 days free trial. And then you will have to pay $17.95. With this account you'll be able to send 1920 connection invites and 3204 messages.
Install n8n-nodes-browserflow from your n8n Settings > Community Nodes. (it means for now that you'll need a self-hosted installation of n8n). and paste the API key you copied earlier as a credential.
Execute it regularly to check if your connections have accepted your connection request.
How to customize this workflow to your needs
Customize Contact Messages: Personalize the AI agent prompt that generates the icebreaker
Integrate with Other Tools: Add additional nodes to integrate with CRM systems or other marketing tools for enhanced functionality.
Monitor and Optimize: Regularly review the workflow's performance and make adjustments to improve efficiency and effectiveness.
By following this setup and customization guide, users can leverage this n8n template to enhance their LinkedIn outreach and connection management processes."
Create a Paul Graham Essay Q&A System with OpenAI and Milvus Vector Database,https://n8n.io/workflows/3574-create-a-paul-graham-essay-qanda-system-with-openai-and-milvus-vector-database/,"Create a Paul Graham Essay Q&A System with OpenAI and Milvus Vector Database
How It Works
This workflow creates a question-answering system based on Paul Graham essays. It has two main steps:
Data Collection & Processing:
Scrapes Paul Graham essays
Extracts text content
Loads them into a Milvus vector store
Chat Interaction:
Provides a question-answering interface using the stored vector embeddings
Utilizes OpenAI embeddings for semantic search
Set Up Steps
Set up a Milvus server following the official guide
Create a collection named ""my_collection""
Run the workflow to scrape and load Paul Graham essays
Start chatting with the QA system
The workflow handles the entire process from fetching essays, extracting content, generating embeddings via OpenAI, storing vectors in Milvus, and providing retrieval for question answering."
Support Automation with GPT-4o Chatbot in Intercom & Discord Thread Reports,https://n8n.io/workflows/3558-support-automation-with-gpt-4o-chatbot-in-intercom-and-discord-thread-reports/,"If you have any question, or difficulty, feel free to come discuss about it on my Telegram (you might find something there üéÅ)
This workflow connects your Intercom chat system with your own AI Agent and sends a complete log of each conversation to Discord using threads. It allows you to run a fully automated support system while maintaining full visibility of the bot's behavior in real time.
For every new conversation in Intercom, a thread is created in a specified Discord channel. Each message from the user, the AI, and even manual human responses is logged to the thread, offering full traceability and transparency.
You also have fine-grained control over the AI agent. By simply clicking the ‚≠êÔ∏è star in Intercom‚Äôs UI, support agents can instantly pause or resume AI responses for a specific chat ‚Äî no coding or config changes needed.
Requirements
A working n8n instance
An Intercom account with permission to set up webhooks
A Discord bot with the following permissions:
Send Messages
Create Public/Private Threads
Manage Threads
API credentials for your preferred LLM (OpenAI is used by default)
Google Chrome or any browser to access Intercom‚Äôs UI
Setup
Intercom:
Go to Intercom‚Äôs webhook settings.
Add a webhook that listens to new incoming messages and points to the Webhook URL in this n8n workflow.
Make sure to send full conversation data.
Discord:
Create a Discord bot and invite it to your server with the required permissions.
In the Discord + Token node at the top of the workflow:
Add your bot token
Add the ID of the channel where threads should be created
LLM / AI Agent:
By default, the workflow uses OpenAI via HTTP Request.
You can substitute it with any LLM provider of your choice.
Make sure to set up your credentials in n8n and select them in the HTTP nodes.
HTTP Authentication Tips:
For both Intercom and Discord API calls, it's recommended to create credentials in n8n's Credential Manager.
Then, assign those credentials inside each HTTP Request node for a cleaner setup.
Usage
When a new conversation starts in Intercom, a Discord thread is created automatically.
Each message ‚Äî user input, AI response, and human reply ‚Äî is logged into the Discord thread in real time.
The AI replies automatically unless the ‚≠êÔ∏è star is checked in Intercom:
‚òÜ Unchecked = AI replies enabled
‚≠êÔ∏è Checked = AI replies disabled, human takeover enabled
This gives you on-the-fly control of each conversation‚Äôs automation level directly from the Intercom inbox.
Customization
You can replace OpenAI with any LLM that provides a compatible API.
Discord channel ID, thread naming, and message formatting can be customized to match your team‚Äôs preferences.
You can expand the workflow to handle events like conversation closure or satisfaction ratings for deeper analytics."
Track Daily PG&E Energy Costs with Airtop and Email Notifications,https://n8n.io/workflows/3474-track-daily-pgande-energy-costs-with-airtop-and-email-notifications/,"About The Airtop Automation
Are you tired of being shocked by unexpectedly high energy bills? With this automation using Airtop and n8n, you can take control of your daily energy costs and ensure you‚Äôre always informed.
How to monitor your daily energy consumption
With this automation, we‚Äôll walk you through setting up an automation that retrieves your PG&E (Pacific Gas and Electric) energy usage data, calculates costs, and emails you the details‚Äîall without manual effort.
What You‚Äôll Need
To get started, make sure you have the following:
A free Airtop API Key
PG&E Account Credentials - with minor adaptations, this will also work with other providers
An Email Address - To receive the energy cost updates
Estimated setup time: 5 minutes
Understanding the Process
This automation works by:
Logging into your PG&E account using your credentials
Navigating to your energy usage data
Extracting relevant details about energy consumption and costs
Emailing the daily summary directly to your inbox
The automation is straightforward and ensures you have real-time insights into your energy usage, empowering you to adjust your habits and save money.
Setting Up Your Automation
We‚Äôve created a step-by-step guide to help you set up this workflow. Here‚Äôs how:
Insert Your Credentials:
In the tools section, add your PG&E login details as variables
In Airtop, add your Airtop API Key
Configure your email address to receive the updates
Run the Automation:
Start the scenario, and watch as the automation retrieves your energy data and sends you a detailed email summary.
Customization Options
While the default setup works seamlessly, you can tweak it to suit your needs:
Data Storage: Store energy usage data in a database for long-term tracking and analysis
Visualization: Plot graphs of your energy usage trends over time for better insights
Notifications: Change the automation to only send alerts on high usage instead of a daily email
Real-World Applications
This automation isn‚Äôt just about monitoring energy usage and taking control. Here are some practical applications:
Daily Energy Management: Receive updates every morning and adjust your energy consumption based on costs
Smart Home Integration: Use the data to automate appliances during off-peak hours
Budgeting: Track energy expenses over weeks or months to plan your budget more effectively
Happy automating!"
Sync WordPress Content to Pinecone for AI Chatbots with OpenAI,https://n8n.io/workflows/3557-sync-wordpress-content-to-pinecone-for-ai-chatbots-with-openai/,"This workflow automatically syncs your WordPress content (Pages and Posts) into a vector database like Pinecone, making it searchable and usable by AI agents through embeddings. This allows your AI chatbot to stay continuously up to date with your latest site content ‚Äî without manual uploads or retraining steps.
It pulls all published and private content via WordPress's REST API, processes the data into clean JSON, and sends it to Pinecone using vector embeddings (OpenAI by default, but swappable). This enables a dynamic AI knowledge base that grows with your website.
Best of all, you don‚Äôt need to modify your workflow: keep managing your content in WordPress ‚Äî this automation takes care of the rest.
Requirements
A WordPress site with API access
Pinecone account credentials (API Key + Environment)
OpenAI account (or any LLM with embedding capabilities)
An n8n instance (cloud or self-hosted)
Setup
Configure WordPress Access:
Create HTTP credentials in n8n with your WordPress site's base URL and authentication.
This workflow uses the REST API to fetch pages and posts.
Connect Pinecone and OpenAI:
Set up credentials for both Pinecone and OpenAI in n8n's Credentials Manager.
The workflow automatically uses them in the relevant nodes.
Set Your Schedule:
The workflow starts with a Schedule node. Open it to adjust how often the sync runs (e.g., daily, weekly).
Check the Nodes:
Make sure your folder structure and API URLs match your WordPress setup.
Adjust filters if needed (only published and private content is fetched by default).
Usage
Once the workflow is active:
It runs on a defined schedule.
It fetches all WordPress Pages and Posts.
It converts them into structured JSON, including metadata (title, tags, URL, categories, etc.).
It generates embeddings (via OpenAI) and sends them to Pinecone.
All vectors are stored with rich metadata for easy use in search/chat-based AI tools.
You‚Äôll have a continuously refreshed, vectorized copy of your WordPress site content.
Customization
Replace Pinecone: You can swap Pinecone for any other vector database by updating the vector node.
Swap Embedding Provider: You‚Äôre not limited to OpenAI ‚Äî just replace the embedding step with your LLM of choice.
Selective Sync: If you want to sync only posts or only pages:
Delete the node that fetches the type you don‚Äôt want.
Remove the associated Merge node.
Schedule: Edit the first node to control how often the automation runs.
If you have any question, or difficulty, feel free to come discuss about it on my Telegram (you might find something there üéÅ)"
Populate Retell Dynamic Variables with Google Sheets Data for Call Handling,https://n8n.io/workflows/3385-populate-retell-dynamic-variables-with-google-sheets-data-for-call-handling/,"Overview
This workflow provides Retell agent builders with a simple way to populate dynamic variables using n8n.
The workflow fetches user information from a Google Sheet based on the phone number and sends it back to Retell.
It is based on Retell's Inbound Webhook Call.
Retell is a service that lets you create Voice Agents that handle voice calls simply, based on a prompt or using a conversational flow builder.
Who is it for
For builders of Retell's Voice Agents who want to make their agents more personalized.
Prerequisites
Have a Retell AI Account
Create a Retell agent
Purchase a phone number and associate it with your agent
Create a Google Sheets - for example, make a copy of this one.
Your Google Sheet must have at least one column with the phone number. The remaining columns will be used to populate your Retell agent‚Äôs dynamic variables.
All fields are returned as strings to Retell (variables are replaced as text)
How it works
The webhook call is received from Retell. We filter the call using their whitelisted IP address.
It extracts data from the webhook call and uses it to retrieve the user from Google Sheets.
It formats the data in the response to match Retell's expected format.
Retell uses this data to replace dynamic variables in the prompts.
How to use it
See the description for screenshots!
Set the webhook name (keep it as POST).
Copy the Webhook URL (e.g., https://your-instance.app.n8n.cloud/webhook/retell-dynamic-variables) and paste it into Retell's interface. Navigate to ""Phone Numbers"", click on the phone number, and enable ""Add an inbound webhook"".
In your prompt (e.g., ""welcome message""), use the variable with this syntax: {{variable_name}} (see Retell's documentation).
These variables will be dynamically replaced by the data in your Google Sheet.
Notes
In Google Sheets, the phone number must start with '+.
Phone numbers must be formatted like the example: with the +, extension, and no spaces.
You can use any database‚Äîjust replace Google Sheets with your own, making sure to keep the phone number formatting consistent.
üëâ Reach out to us if you're interested in analysing your Retell Agent conversations."
Extract & Summarize Wikipedia Data with Bright Data and Gemini AI,https://n8n.io/workflows/3539-extract-and-summarize-wikipedia-data-with-bright-data-and-gemini-ai/,"Who this is for?
This workflow automates the process of Wikipedia data extraction using the Bright Data Web Unlocker, parsing and cleaning the data, and then sending the results to a specified webhook URL for downstream processing, reporting, or integration.
What problem is this workflow solving?
Researchers who need structured information from Wikipedia pages regularly.
Data Engineers building knowledge bases or enriching datasets with factual data.
Digital Marketers or Content Writers automating fact-checking or content sourcing.
Automation Enthusiasts who want to trigger external systems with rich context from Wikipedia.
What this workflow does
This workflow addresses the challenges of manually retrieving, structuring, and using data from Wikipedia at scale.
Workflow Breakdown
Trigger
Type: Scheduled or Manual
Purpose: Starts the workflow either on a fixed schedule (e.g., daily) or on-demand via a manual trigger or incoming webhook.
Bright Data Wikipedia Scraping
Tool Used: Bright Data Web Unlocker
Action: Scrape the HTML content of one or multiple Wikipedia article URLs.
Parse & Extract Structured Data
The Basic LLM Chain node is responsible for producing a human readable content.
Summarization
Summarize the Wikipedia content by utilizing the Summarization Chain node.
Send to Webhook
Initiates a Webhook notification to the specified URL as part of the ""Summary Webhook Notifier"" node.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Set Wikipedia URL with Bright Data Zone node with the Wikipedia URL and Bright Data Zone.
Update the Summary Webhook Notifier node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Update Wikipedia URL
Replace with your own Wikipedia URL of your interest.
Make sure to set the Wikipedia URL as part of the ""Set Wikipedia URL with Bright Data Zone"" node.
Modify Data Extraction Logic
Extract entire article content or just specific sections by extending the ""LLM Data Extractor"" node prompt.
Extend AI Summarization
Extract key bullet points or entities.
Create short-form summaries by extending the ""Concise Summary Generator"" node.
Extend Summary Webhook Notifier
Send to Slack, Discord, Telegram, MS Teams via the Webhook notification mechanism.
Connect to your internal database/API via the Webhook notification mechanism."
Extract & Summarize Bing Copilot Search Results with Gemini AI and Bright Data,https://n8n.io/workflows/3536-extract-and-summarize-bing-copilot-search-results-with-gemini-ai-and-bright-data/,"Who is this for?
This workflow automates the process of querying Bing's Copilot Search, extracting structured data from the results, summarizing the information, and sending a notification via webhook. It leverages the Microsoft Copilot to retrieve search results and integrates AI-powered tools for data extraction and summarization.
What problem is this workflow solving?
Data Analysts and Researchers: Who need to gather and summarize information from Bing search results efficiently.
Developers and Engineers: Looking to integrate Bing search data into applications or services.
Digital Marketers and SEO Specialists: Interested in monitoring search engine results for specific keywords or topics.
What this workflow does
Manually extracting and summarizing information from search engine results can be time-consuming and error-prone. This workflow automates the process by:
Performing Bing searches using Bright Data's Bing Search API.
Extracting structured data from the search results.
Summarizing the extracted information using AI tools.
Sending the summarized data to a specified endpoint via webhook.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Perform a Bing Copilot Request node with the prompt you wish to perform the search.
Update the Structured Data Webhook Notifier node with the Webhook endpoint of your choice.
Update the Summary Webhook Notifier node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Modify Search Queries: Adjust the search terms to target different topics or keywords.
Change Data Extraction Logic: Customize the extraction process to capture specific data points from the search results.
Alter Summarization Techniques: Integrate different AI models or adjust parameters to change how summaries are generated.
Update Webhook Endpoints: Direct the summarized data to different endpoints as required.
Schedule Workflow Runs: Set up automated triggers to run the workflow at desired intervals."
Process Multiple Prompts in Parallel with Azure OpenAI Batch API,https://n8n.io/workflows/3537-process-multiple-prompts-in-parallel-with-azure-openai-batch-api/,"Process Multiple Prompts in Parallel with Azure OpenAI Batch API
Who is this for?
This workflow is designed for developers and data scientists who want to efficiently send multiple prompts to the Azure OpenAI Batch API and retrieve responses in a single batch process. It is particularly useful for applications that require processing large volumes of text data, such as chatbots, content generation, or data analysis.
What problem is this workflow solving?
Sending multiple prompts to the Azure OpenAI API can be time-consuming and inefficient if done sequentially. This workflow automates the process of batching requests, allowing users to submit multiple prompts at once and retrieve the results in a streamlined manner. This not only saves time but also optimizes resource usage.
What this workflow does
This workflow:
Accepts an array of requests, each containing a prompt and associated parameters.
Converts the requests into a JSONL format suitable for batch processing.
Uploads the batch file to the Azure OpenAI API.
Creates a batch job to process the prompts.
Polls for the job status and retrieves the output once processing is complete.
Parses the output and returns the results.
Key Features of Azure OpenAI Batch API
The Azure OpenAI Batch API is designed to handle large-scale and high-volume processing tasks efficiently. Key features include:
Asynchronous Processing: Handle groups of requests with separate quotas, targeting a 24-hour turnaround at 50% less cost than global standards.
Batch Requests: Send a large number of requests in a single file, avoiding disruption to online workloads.
Key Use Cases
Large-Scale Data Processing: Quickly analyze extensive datasets in parallel.
Content Generation: Create large volumes of text, such as product descriptions or articles.
Document Review and Summarization: Automate the review and summarization of lengthy documents.
Customer Support Automation: Handle numerous queries simultaneously for faster responses.
Data Extraction and Analysis: Extract and analyze information from vast amounts of unstructured data.
Natural Language Processing (NLP) Tasks: Perform tasks like sentiment analysis or translation on large datasets.
Marketing and Personalization: Generate personalized content and recommendations at scale.
Setup
Azure OpenAI Credentials: Ensure you have your Azure OpenAI API credentials set up in n8n.
Configure the Workflow:
Set the az_openai_endpoint in the ""Setup defaults"" node to your Azure OpenAI endpoint.
Adjust the api-version in the ""Set desired 'api-version'"" node if necessary.
Run the Workflow: Trigger the workflow using the ""Run example"" node to see it in action.
How to customize this workflow to your needs
Modify Prompts: Change the prompts in the ""One query example"" node to suit your application.
Adjust Parameters: Update the parameters in the requests to customize the behavior of the OpenAI model.
Add More Requests: You can add more requests in the input array to process additional prompts.
Example Input
[
  {
    ""api-version"": ""2025-03-01-preview"",
    ""requests"": [
      {
        ""custom_id"": ""first-prompt-in-my-batch"",
        ""params"": {
          ""messages"": [
            {
              ""content"": ""Hey ChatGPT, tell me a short fun fact about cats!"",
              ""role"": ""user""
            }
          ]
        }
      },
      {
        ""custom_id"": ""second-prompt-in-my-batch"",
        ""params"": {
          ""messages"": [
            {
              ""content"": ""Hey ChatGPT, tell me a short fun fact about bees!"",
              ""role"": ""user""
            }
          ]
        }
      }
    ]
  }
]
Example Output
[
  {
    ""custom_id"": ""first-prompt-in-my-batch"",
    ""response"": {
      ""body"": {
        ""choices"": [
          {
            ""message"": {
              ""content"": ""Did you know that cats can make over 100 different sounds?""
            }
          }
        ]
      }
    }
  },
  {
    ""custom_id"": ""second-prompt-in-my-batch"",
    ""response"": {
      ""body"": {
        ""choices"": [
          {
            ""message"": {
              ""content"": ""Bees communicate through a unique dance called the 'waggle dance'.""
            }
          }
        ]
      }
    }
  }
]
Additional Notes
Job Management: You can cancel a job at any time, and any remaining work will be canceled while already completed work is returned. You will be charged for any completed work.
Data Residency: Data stored at rest remains in the designated Azure geography, while data may be processed for inferencing in any Azure OpenAI location.
Exponential Backoff: If your batch jobs are large and hitting the enqueued token limit, certain regions support queuing multiple batch jobs with exponential backoff.
This template provides a comprehensive solution for efficiently processing multiple prompts using the Azure OpenAI Batch API, making it a valuable tool for developers and data scientists alike."
Extract University Term Dates from Excel using CloudFlare Markdown Conversion,https://n8n.io/workflows/3505-extract-university-term-dates-from-excel-using-cloudflare-markdown-conversion/,"This n8n template imports an XLSX containing terms dates for a university, extracts the relevant events using AI and converts the events to an ICS file which can be imported into iCal, Google Calendar or Outlook.
Manually adding important term dates to your calendar by hand? Stop! Automate it with this simple AI/LLM-powered document understanding and extraction template. This cool use-case can be applied to many scenarios where Excel files are predominantly used.
How it works
The term dates excel file (xlsx) are imported into the workflow from the university's website using the http request node.
To parse the excel file, we use an external service - Cloudflare's Markdown Conversion Service. This converts the excel's sheets into markdown tables which our LLM can read.
To extract the events and their dates from the markdown, we can use the Information Extractor node for structured output. LLMs are great for this use-case because they can understand the layout; one row may have many data points.
With our data, there are endless possibilities to use it! But for this demonstration, we'll generate an ICS file so that we can import the extracted events into our calendar. We use the Python code node to combine the events into the ICS spec and the ""Convert to File"" node to create the ICS binary.
Finally, let's distribute the ICS file by email to other students or instructors who may also find this incredibly helpful for the upcoming semester!
How to use
Ensure you're downloading the correct excel file and amend the URL parameter of the ""Get Term Dates Excel"" as necessary.
Update the gmail node with your email or other emails as required. Alternatively, send the ICS file to Google Drive or a student portal.
Requirements
Cloudflare Account is required to use the Markdown Conversion Service.
Gemini for LLM document understanding and extraction.
Gmail for email sending.
Customising the workflow
This template should work for other Excel files which - for a university - there are many. Some will be more complicated than others so experiment with different parsers and extraction tools and strategies."
Post on X using Airtop and automate content pipelines,https://n8n.io/workflows/3482-post-on-x-using-airtop-and-automate-content-pipelines/,"About The Post to X Automation
Seamlessly automate posting to X using Airtop and Make.
How to Automate Posting to X with Airtop
Consistently engaging your audience on X (formerly Twitter) can be a challenge, particularly when done manually. Developers and automation engineers often struggle with repetitive tasks like scheduling tweets, maintaining consistent posting cycles, and integrating content from various sources or AI-generated feeds. Manually managing content updates increases fatigue, human error, and decreases scalability.
This n8n automation, powered by Airtop, simplifies automated content publishing onto X. Whether you're sharing daily updates, integrating dynamically generated AI content, or streamlining your marketing content pipeline, Airtop‚Äôs automation helps eliminate manual labor and reduces potential execution errors.
Who is this Automation for?
Social Media Managers scheduling recurring or automated posts on X
Content Marketers integrating AI-generated content into their publishing process
Developers implementing automated social media pipelines
Automation Engineers minimizing errors and manual posting efforts
Key Benefits
Real-time, authenticated API postings via X
Reliable structured workflows minimize manual errors
Seamless integration with AI content pipelines
Use Cases
Automatically publish scheduled daily content updates
Seamlessly post AI-generated insights, news summaries or industry updates
Distribute alerts and event announcements reliably at set intervals
Maintain active audience engagement by automating regular, high-frequency posts
How the Post to X Automation Works
This Airtop automation works by using your Airtop Profile signed-in into X via Airtop. Once authenticated securely with your X credentials, n8n handles the structured data flow, which can come from manual inputs, AI-generated sources, databases, or RSS feeds. Airtop then securely publishes the posts, providing reliable scheduled updates directly on X, removing manual oversight and streamlining your social media workflows.
What You‚Äôll Need
An Airtop API key
Your X (Twitter) account
An Airtop Profile signed into X
Setting Up the Automation
Connect your Airtop account using your free Airtop API key
Create an Airtop Profile and connect it to your X account
Activate and schedule your scenario to automate regular posting
Customize the Automation
Customize your posting workflow extensively using Airtop's built-in node in n8n:
Integrate diverse sources like RSS feeds and AI tools to dynamically customize automated posts
Schedule precise posting intervals or diversify times for maximum audience engagement
Set conditional logic to automate content posting based on predefined triggers and events
Utilize Airtop‚Äôs structured data flows to manage categories, hashtags, or mentions in your posts
Automation Best Practices
Consistently update security credentials for uninterrupted access
Clearly structure your workflow to simplify troubleshooting and logic updates
Monitor posting frequency to ensure optimal audience reach and engagement
Regularly review content sources to maintain quality control of automated postings
Happy Automating!"
üîä Browser Recording Audio Transcribing and AI Analysis with Deepgram and GPT-4o,https://n8n.io/workflows/3451-browser-recording-audio-transcribing-and-ai-analysis-with-deepgram-and-gpt-4o/,"Overview
Transcript Evalu8r V2 is a robust browser-based transcript analysis tool powered by Deepgram‚Äôs speech-to-text API and built into an n8n workflow template. This release introduces full in-browser audio recording, device selection, and playback functionality, allowing users to capture and analyze conversations without leaving their browser.
Designed for researchers, support teams, podcasters, legal professionals, and analysts, Transcript Evalu8r simplifies how you extract value from voice data using intuitive tools, AI-powered insights, and seamless automation.
‚úÖ What‚Äôs New in V2
### üî¥ Record in Browser <br>Users can now record audio directly in the browser with one click. Record meetings, calls, or voice notes instantly‚Äîno external software required. ### üéß Select Audio Input Devices <br>Users can choose from connected input devices (e.g., internal mic, USB mic, headset) before recording, ensuring maximum control and audio quality.
‚ñ∂Ô∏è Listen to Recording in Browser
Once a recording is complete, users can immediately play back the audio in-browser to review before submitting for transcription‚Äîensuring content is clear and relevant.
How It Works
Record or upload an audio file
**Audio recording is uploaded to Google Drive
Audio file is sent to Deepgram for transcribing and AI analysis
Audio File is moved to completed folder
JSON transcript is saved to Google Drive
Google Doc of AI analysis, summary, key points, action items, and full transctip is created
Use the Web UI to Explore:
Speaker Names
Transcript
Key points & tasks
Sentiment chart
Topic and intent lists with time stamps
Download or export transcript and insights
Add Additional automations or continue workflow (e.g., send to Notion, Slack, Google Sheets, todoist)
Key Features
üîπ Transcript Processing & Uploads
Upload or record audio directly in your browser
Track progress of file uploads in real-time
View, select, or download transcripts in Google Docs, JSON, and other formats
üîπ Interactive Transcript Viewer
Full transcript display with timestamps, speaker IDs, and auto-scroll
Clickable word-level navigation for pinpointing dialogue
Real speaker labeling (not just ""Speaker 1"")
üîπ AI-Powered Analysis
Sentiment visualization over time, per speaker
Topic and intent extraction with confidence scores
Speaker contribution insights, interruptions, and common word frequency
üîπ Insight Summaries Google Doc
AI-generated summary, key points, and action items
Speaker-specific task suggestions and idea highlights
üîπ UI Enhancements
Collapsible sections for focused analysis
Light and dark mode support
Responsive design across desktop, tablet, and mobile
Use Cases
‚úÖ Interview & podcast summarization
‚úÖ Customer support QA monitoring
‚úÖ Legal or compliance transcription logging
‚úÖ Market research and trend analysis
‚úÖ Internal meeting follow-up automation
Requirements
n8n instance (self-hosted or cloud)
Deepgram API Key (for audio transcription)
Supported browser with mic permissions enabled
Why Choose Transcript Evalu8r V2?
üöÄ Instantly record and transcribe within your browser
üß† Extract sentiment, insights, and intent automatically
üìà Visualize speaker tone and engagement
üéß Choose audio input for cleaner recordings
üîó Connect with your favorite tools using n8n‚Äôs integrations"
OAuth2 Settings Finder with OpenRouter Chat Model and Llama 3.3,https://n8n.io/workflows/3279-oauth2-settings-finder-with-openrouter-chat-model-and-llama-33/,"Find OAuth URIs with AI Llama
Overview:
The AI agent identifies:
Authorization URI
Token URI
Audience
Methodology:
Confidence scoring is utilized to assess the trustworthiness of extracted data:
Score Range: 0 < x ‚â§ 1
Score Granularity: 0.01 increments
Model Details:
Leveraging the Wayfarer Large 70b Llama 3.3 model.
How it works:
This template is designed to assist users in obtaining OAuth2 settings using AI-powered insights. It is ideal for developers, IT professionals, or anyone working with APIs that require OAuth2 authentication. By leveraging the AI agent, users can simplify the process of extracting and validating key details such as the authorization_url, token_url, and audience.
Set up instructions:
1. Configuration Nodes
Structured Output Node: Parses the AI model's output using a predefined JSON schema. This ensures the data is structured for downstream processing.
Code Node: If the AI model‚Äôs output does not match the required format, use the Code node to re-arrange and transform the data. Example code snippets are provided below for common scenarios.
2. AI Model Prompt
The prompt for the AI model includes:
A detailed structure and objectives of the query.
Flexibility for the model to improvise when accurate results cannot be determined.
3. Confidence Scoring
The AI model assigns a confidence score (0 < x ‚â§ 1) to indicate the reliability of the extracted data. Scores are provided in increments of 0.01 for granularity.
Adaptability
Customize this template:
Update the AI model prompt with details specific to your API or OAuth2 setup.
Adjust the JSON schema in the Structured Output node to match the data format.
Modify the Code logic to suit the application's requirements."
Get Real-time NFT Marketplace Insights with OpenSea Marketplace Agent Tool,https://n8n.io/workflows/3239-get-real-time-nft-marketplace-insights-with-opensea-marketplace-agent-tool/,"Track NFT listings, offers, orders, and trait-based pricing in real time! This workflow integrates OpenSea API, AI-powered analytics (GPT-4o-mini), and n8n automation to provide instant insights into NFT trading activity. Ideal for NFT traders, collectors, and investors looking to monitor the market and identify profitable opportunities.
How It Works
A user submits a query about NFT listings, offers, or order history.
The OpenSea Marketplace Agent determines the correct API tool:
Retrieve active NFT listings for a collection.
Fetch valid offers for individual NFTs or entire collections.
Identify the cheapest NFT listings by collection or token ID.
Track the highest offer made for a single NFT.
Access detailed order history for a transaction.
The OpenSea API (requires API key) is queried to fetch real-time data.
The AI engine processes and structures the response, making it easy to interpret.
The NFT marketplace insights are delivered via Telegram, Slack, or stored in a database.
What You Can Do with This Agent
üîπ Find the Best NFT Listings ‚Üí Retrieve the cheapest available listings in any collection.
üîπ Track Offers on NFTs ‚Üí See all active offers, including highest bids.
üîπ Analyze Collection-Wide Market Data ‚Üí Compare listings, offers, and sales activity.
üîπ Retrieve Order Details ‚Üí Search by order hash to check buyer, seller, and transaction status.
üîπ Fetch NFT Trait-Based Offers ‚Üí Identify rare traits that receive premium bids.
üîπ Monitor Multi-Chain Listings ‚Üí Works across Ethereum, Polygon (Matic), Arbitrum, Optimism, and more.
Example Queries You Can Use
‚úÖ ""Show me the 10 cheapest listings for Bored Ape Yacht Club.""
‚úÖ ""Find the highest bid for CryptoPunk #1234.""
‚úÖ ""Track all open offers for Azuki NFTs.""
‚úÖ ""Retrieve details for this OpenSea order: 0x123abc... on Ethereum.""
‚úÖ ""List all NFTs for sale in the 'CloneX' collection.""
Available API Tools & Endpoints
1Ô∏è‚É£ Get All Listings by Collection ‚Üí /api/v2/listings/collection/{collection_slug}/all (Fetches active listings for a collection)
2Ô∏è‚É£ Get All Offers by Collection ‚Üí /api/v2/offers/collection/{collection_slug}/all (Retrieves all offers for a collection)
3Ô∏è‚É£ Get Best Listing by NFT ‚Üí /api/v2/listings/collection/{collection_slug}/nfts/{identifier}/best (Finds the lowest-priced NFT listing)
4Ô∏è‚É£ Get Best Listings by Collection ‚Üí /api/v2/listings/collection/{collection_slug}/best (Fetches the cheapest listings per collection)
5Ô∏è‚É£ Get Best Offer by NFT ‚Üí /api/v2/offers/collection/{collection_slug}/nfts/{identifier}/best (Retrieves the highest offer for an NFT)
6Ô∏è‚É£ Get Collection Offers ‚Üí /api/v2/offers/collection/{collection_slug} (Shows collection-wide offers)
7Ô∏è‚É£ Get Item Offers ‚Üí /api/v2/orders/{chain}/{protocol}/offers (Fetches active item-specific offers)
8Ô∏è‚É£ Get Listings by Chain & Protocol ‚Üí /api/v2/orders/{chain}/{protocol}/listings (Retrieves active listings across blockchains)
9Ô∏è‚É£ Get Order Details by Hash ‚Üí /api/v2/orders/chain/{chain}/protocol/{protocol_address}/{order_hash} (Checks order status using an order hash)
üîü Get Trait-Based Offers ‚Üí /api/v2/offers/collection/{collection_slug}/traits (Fetches offers for specific NFT traits)
Set Up Steps
Get an OpenSea API Key
Sign up at OpenSea API and request an API key.
Configure API Credentials in n8n
Add your OpenSea API key under HTTP Header Authentication.
Connect the Workflow to Telegram, Slack, or Database (Optional)
Use n8n integrations to send alerts to Telegram, Slack, or save results to Google Sheets, Notion, etc.
Deploy and Test
Send a query (e.g., ""Get the best listing for BAYC #5678"") and receive instant insights!
Stay ahead of the NFT market‚Äîgain powerful insights with AI-powered OpenSea analytics!"
Analyze NFT Market Trends with AI-Powered OpenSea Analytics Agent Tool,https://n8n.io/workflows/3237-analyze-nft-market-trends-with-ai-powered-opensea-analytics-agent-tool/,"Get deep insights into NFT market trends, sales data, and collection statistics‚Äîall powered by AI and OpenSea! This workflow connects GPT-4o-mini, OpenSea API, and n8n automation to provide real-time analytics on NFT collections, wallet transactions, and market trends. It is ideal for NFT traders, collectors, and investors looking to make informed decisions based on structured data.
How It Works
Receives user queries via Telegram, webhooks, or another connected interface.
Determines the correct API tool based on the request (e.g., collection stats, wallet transactions, event tracking).
Retrieves data from OpenSea API (requires API key).
Processes the information using an AI-powered analytics agent.
Returns structured insights in an easy-to-read format for quick decision-making.
What You Can Do with This Agent
üîπ Retrieve NFT Collection Stats ‚Üí Get floor price, volume, sales data, and market cap.
üîπ Track Wallet Activity ‚Üí Analyze transactions for a given wallet address.
üîπ Monitor NFT Market Trends ‚Üí Track historical sales, listings, bids, and transfers.
üîπ Compare Collection Performance ‚Üí View side-by-side market data for different NFT projects.
üîπ Analyze NFT Transaction History ‚Üí Check real-time ownership changes for any NFT.
üîπ Identify Market Shifts ‚Üí Detect sudden spikes in demand, price changes, and whale movements.
Example Queries You Can Use
‚úÖ ""Get stats for the Bored Ape Yacht Club collection.""
‚úÖ ""Show me all NFT sales from the last 24 hours.""
‚úÖ ""Fetch all NFT transfers for wallet 0x123...abc on Ethereum.""
‚úÖ ""Compare the last 3 months of sales volume for Azuki and CloneX.""
‚úÖ ""Track the top 10 wallets making the most NFT purchases this week.""
Available API Tools & Endpoints
1Ô∏è‚É£ Get Collection Stats ‚Üí /api/v2/collections/{collection_slug}/stats (Retrieve NFT collection-wide market data)
2Ô∏è‚É£ Get Events ‚Üí /api/v2/events (Fetch global NFT sales, transfers, listings, bids, redemptions)
3Ô∏è‚É£ Get Events by Account ‚Üí /api/v2/events/accounts/{address} (Track transactions by wallet)
4Ô∏è‚É£ Get Events by Collection ‚Üí /api/v2/events/collection/{collection_slug} (Get sales activity for a collection)
5Ô∏è‚É£ Get Events by NFT ‚Üí /api/v2/events/chain/{chain}/contract/{address}/nfts/{identifier} (Retrieve historical transactions for a specific NFT)
Set Up Steps
Get an OpenSea API Key
Sign up at OpenSea API and request an API key.
Configure API Credentials in n8n
Add your OpenSea API key under HTTP Header Authentication.
Connect the Workflow to Telegram, Slack, or Database (Optional)
Use n8n integrations to send alerts to Telegram, Slack, or save results to Google Sheets, Notion, etc.
Deploy and Test
Send a query (e.g., ""Azuki latest sales"") and receive instant NFT market insights!
Stay ahead in the NFT market‚Äîget real-time analytics with OpenSea‚Äôs AI-powered analytics agent!"
Get Real-time NFT Insights with OpenSea AI-Powered NFT Agent Tool,https://n8n.io/workflows/3238-get-real-time-nft-insights-with-opensea-ai-powered-nft-agent-tool/,"Instantly access NFT metadata, collections, traits, contracts, and ownership details from OpenSea! This workflow integrates GPT-4o-mini AI, OpenSea API, and n8n automation to provide structured NFT data for traders, collectors, and investors.
How It Works
Receives user queries via Telegram, webhooks, or another connected interface.
Determines the correct API tool based on the request (e.g., user profile, NFT metadata, contract details).
Retrieves data from OpenSea API (requires API key).
Processes the information using an AI-powered NFT insights engine.
Returns structured insights in an easy-to-read format for quick decision-making.
What You Can Do with This Agent
üîπ Retrieve OpenSea User Profiles ‚Üí Get user bio, links, and profile info.
üîπ Fetch NFT Collection Details ‚Üí Get collection metadata, traits, fees, and contract info.
üîπ Analyze NFT Metadata ‚Üí Retrieve ownership, rarity, and trait-based pricing.
üîπ Monitor NFTs Owned by a Wallet ‚Üí Track all NFTs under a specific account.
üîπ Retrieve Smart Contract Data ‚Üí Get blockchain contract details for an NFT collection.
üîπ Identify Valuable Traits ‚Üí Fetch NFT trait insights and rarity scores.
Example Queries You Can Use
‚úÖ ""Get OpenSea profile for 0xA5f49655E6814d9262fb656d92f17D7874d5Ac7E.""
‚úÖ ""Retrieve details for the 'Azuki' NFT collection.""
‚úÖ ""Fetch metadata for NFT #5678 from 'Bored Ape Yacht Club'.""
‚úÖ ""Show all NFTs owned by 0x123... on Ethereum.""
‚úÖ ""Get contract details for NFT collection 'CloneX'.""
Available API Tools & Endpoints
1Ô∏è‚É£ Get OpenSea Account Profile ‚Üí /api/v2/accounts/{address_or_username} (Retrieve user bio, links, and image)
2Ô∏è‚É£ Get NFT Collection Details ‚Üí /api/v2/collections/{collection_slug} (Get collection-wide metadata)
3Ô∏è‚É£ Get NFT Metadata ‚Üí /api/v2/chain/{chain}/contract/{address}/nfts/{identifier} (Retrieve individual NFT details)
4Ô∏è‚É£ Get NFTs Owned by Account ‚Üí /api/v2/chain/{chain}/account/{address}/nfts (List all NFTs owned by a wallet)
5Ô∏è‚É£ Get NFTs by Collection ‚Üí /api/v2/collection/{collection_slug}/nfts (Retrieve all NFTs from a specific collection)
6Ô∏è‚É£ Get NFTs by Contract ‚Üí /api/v2/chain/{chain}/contract/{address}/nfts (Retrieve all NFTs under a contract)
7Ô∏è‚É£ Get Payment Token Details ‚Üí /api/v2/chain/{chain}/payment_token/{address} (Fetch info on payment tokens used in NFT transactions)
8Ô∏è‚É£ Get NFT Traits ‚Üí /api/v2/traits/{collection_slug} (Retrieve collection-wide trait data)
Set Up Steps
Get an OpenSea API Key
Sign up at OpenSea API and request an API key.
Configure API Credentials in n8n
Add your OpenSea API key under HTTP Header Authentication.
Connect the Workflow to Telegram, Slack, or Database (Optional)
Use n8n integrations to send alerts to Telegram, Slack, or save results to Google Sheets, Notion, etc.
Deploy and Test
Send a query (e.g., ""Azuki latest sales"") and receive instant NFT market insights!
Unlock powerful NFT analytics with AI-powered OpenSea insights‚Äîstart now!"
Chinese Translator via Line x OpenRouter (Text & Image),https://n8n.io/workflows/3211-chinese-translator-via-line-x-openrouter-text-and-image/,"This workflow template, ""Chinese Translator via Line x OpenRouter (Text & Image)"" is designed to provide seamless Chinese translation services directly within the LINE messaging platform. By integrating with OpenRouter.ai and advanced language models like Qwen, this workflow translates text or images containing Chinese characters into pinyin and English translations, making it an invaluable tool for language learners, travelers, and businesses operating in multilingual environments.
This template is ideal for:
Language Learners: Who want to practice Chinese by receiving instant translations of text or images.
Travelers: Looking for quick translations of Chinese signs, menus, or documents while abroad.
Educators: Teaching Chinese language courses and needing tools to assist students with translations.
Businesses: Operating in multilingual markets and requiring efficient communication tools.
Automation Enthusiasts: Seeking to build intelligent chatbots that can handle language translation tasks.
What Problem Does This Workflow Solve?
Translating Chinese text or images into English and pinyin can be challenging, especially for beginners or those without access to reliable translation tools. This workflow solves that problem by:
Automatically detecting and translating text or images containing Chinese characters.
Providing accurate translations in both pinyin and English for better comprehension.
Supporting multiple input formats (text, images) to cater to diverse user needs.
Sending replies directly to users via the LINE messaging platform , ensuring accessibility and ease of use.
What This Workflow Does
Receive Messages via LINE Webhook
The workflow is triggered when a user sends a message (text, image, or other types) to the LINE bot.
Display Loading Animation
A loading animation is displayed to reassure the user that their request is being processed.
Route Input Types
The workflow uses a Switch node to determine the type of input (text, image, or unsupported formats).
If the input is text , it is sent to the OpenRouter.ai API for translation.
If the input is an image , the workflow extracts the image content, converts it to base64, and sends it to the API for translation.
Unsupported formats trigger a polite response indicating the limitation.
Translate Content Using OpenRouter.ai
The workflow leverages Qwen models from OpenRouter.ai to generate translations:
For text inputs, it provides Chinese characters , pinyin , and English translations .
For images, it extracts and translates using the qwen-VL model which can take images
Reply with Translations
The translated content is formatted and sent back to the user via the LINE Reply API.
Setup Guide
Pre-Requisites
Access to the LINE Developers Console to configure your webhook and channel access token.
An OpenRouter.ai account with credentials to access Qwen models.
Basic knowledge of APIs, webhooks, and JSON formatting.
Step-by-Step Setup
Configure the LINE Webhook:
Go to the LINE Developers Console and set up a webhook to receive incoming messages.
Copy the Webhook URL from the Line Webhook node and paste it into the LINE Console.
Remove any ""test"" configurations when moving to production.
Set Up OpenRouter.ai:
Create an account on OpenRouter.ai and obtain your API credentials.
Connect your credentials to the OpenRouter nodes in the workflow.
Test the Workflow:
Simulate sending text or images to the LINE bot to verify that translations are processed and replied correctly.
How to Customize This Workflow to Your Needs
Add More Languages: Extend the workflow to support additional languages by modifying the API calls.
Enhance Image Processing: Integrate more advanced OCR tools to improve text extraction from complex images.
Customize Responses: Modify the reply format to include additional details, such as grammar explanations or cultural context.
Expand Use Cases: Adapt the workflow for specific industries, such as tourism or e-commerce, by tailoring the translations to relevant vocabulary.
Why Use This Template?
Real-Time Translation: Provides instant translations of text and images, improving user experience and accessibility.
Multimodal Support: Handles both text and image inputs, catering to diverse user needs.
Scalable: Easily integrate into existing systems or scale to support multiple users and workflows.
Customizable: Tailor the workflow to suit your specific audience or industry requirements."
Chinese Translator via Line x OpenRouter,https://n8n.io/workflows/3207-chinese-translator-via-line-x-openrouter/,"The Chinese Translator workflow automates the translation of text into Chinese characters, pinyin, and English translations via Line Messaging API. This workflow leverages OpenRouter.ai to call advanced language models such as Qwen for accurate translations and ensures smooth user interaction by providing loading animations and timely replies.
Purpose
This workflow aims to
Provide users with real-time translations of input text into Chinese characters, pinyin, and English
Deliver seamless user experience through interactive features like loading animations and quick reply messages
Enable easy integration with Line Messaging API for scalable deployment
Key Features
Real-Time Translation : Translates user-inputted text instantly using OpenRouter.ai's standardized API.
Comprehensive Output : Delivers Chinese characters, pinyin, and English translations for each word or phrase.
Interactive User Experience : Incorporates loading animations to inform users that the workflow is processing their request.
Line Integration : Utilizes Line Webhooks and Reply APIs to facilitate communication between users and the translation service.
Data Flow
Receiving Input
Node: Line Webhook
Captures incoming messages from Line users.
Extracts the text content and reply token from the webhook payload.
Loading Animation
Node: Line Loading Animation
Sends a loading animation back to the user, indicating that the workflow is processing the request.
Enhances user experience by providing immediate feedback.
Translation Processing
Node: Use OpenRouter
Sends the extracted text to OpenRouter.ai's API, utilizing the Qwen model for translation.
Requests Chinese characters, pinyin, and English translations for the input text.
Sending Response
Node: Line Reply
Formats the translation results into a readable text message.
Sends the translated text back to the user via Line's Reply API.
Setup Instructions
Prerequisites
Line Developer Account : Create a Line channel to obtain necessary credentials for webhooks and messaging.
OpenRouter.ai Account : Set up an account and configure access to utilize their language models.
Steps to Configure
Set Up Line Webhook :
Navigate to the Line Developers Console and create a new webhook URL.
Copy the generated webhook URL and paste it into the Line Webhook node in n8n.
Configure OpenRouter.ai :
Obtain API credentials from OpenRouter.ai and integrate them into the Use OpenRouter node within the workflow.
Adjust Workflow Settings :
Ensure the timezone is set to Asia/Bangkok .
Verify that all nodes are correctly connected and configured with appropriate credentials.
Intended Audience
This workflow is ideal for:
Language Learners : Seeking quick translations and pronunciation guides for Chinese language studies.
Travelers : Looking to communicate effectively while traveling in Chinese-speaking regions.
Businesses : Aiming to provide multilingual support to customers and clients.
Benefits
Enhanced Learning : Provides comprehensive translations, including pinyin, aiding in language acquisition.
User-Friendly Interface : Real-time loading animations and prompt replies ensure a smooth user experience.
Scalable Deployment : Easily integrates with Line's extensive user base for widespread accessibility."
‚ú® ideoGener8r ‚Äì Complete Ideogram AI Image Generator UI with Google Integration,https://n8n.io/workflows/3148-ideogener8r-complete-ideogram-ai-image-generator-ui-with-google-integration/,"ideoGener8r ‚Äì Self-Hosted Ideogram AI Interface in n8n
üî• March Sale ‚Äì n8n Community Members Get ideoGener8r for Just $27! (Reg. $47)
Use Coupon Code: ILoven8n
(Valid until 3/31/2025 for n8n community members)

ideoGener8r is an n8n template that sets up a self-contained, front-end interface for Ideogram AI image generation. It offers a complete workflow to generate, upscale, remix, and store images‚Äîentirely on your self-hosted n8n instance.
Key Benefits & Limited-Time Offer
Fully Self-Hosted: Avoid monthly fees and keep your data private.
All-in-One: Generate, remix, and upscale images without third-party tools.
Streamlined Automation: Integrates directly with Google Sheets & Drive.
Sale: Save $20 ‚Äì Get ideoGener8r for just $27 with coupon code ILoven8n (valid until 3/31/2025).
Requirements & Overview
Ideogram AI API Key
Sign up at Ideogram AI and create an API key.
Google Sheets & Drive
A Google Sheet for storing metadata.
A Google Drive folder for saving generated images.
n8n Auth Credentials
Basic Auth for the login webhook.
Header Auth (Bearer token) for the generation/remix webhooks.
With ideoGener8r, you can instantly create a private user interface‚Äîwithin n8n‚Äîto produce AI images on demand, trigger image generation through webhooks, and automatically log data to Google Sheets.
Required Google Sheet Columns
Your Google Sheet must contain (at minimum) the following columns for proper logging:
Cheeck Setup note in workflow for csv template
Step-by-Step Setup Instructions
Import the Template
Download the JSON file you received upon purchase.
In your n8n instance, go to Workflows ‚Üí Import, then upload the JSON file.
Configure Ideogram API
In n8n, create an HTTP Header Auth credential.
For the ‚ÄúAuthorization‚Äù header, use Bearer YOUR_IDEOGRAM_API_KEY.
Attach this credential to the Ideogram-related nodes (e.g., IDEOGRAM Image generator, ideogram Upscale).
Google Sheets Setup
Copy or create a Google Sheet with the columns listed above.
In the template, locate the Google Sheets ‚ÄúAppend‚Äù nodes.
Select the correct Sheet, tab, and map each column accordingly.
Google Drive Folder
Create a Drive folder to store images (sharing permissions are up to you).
In the Google Drive nodes, enter the folder ID to save new images there.
Basic Auth for Login
Open the Login Webhook-ideoGener8r node.
Set it to use Basic Auth and create credentials for a username/password.
This will secure the login route (e.g., /ideogener8r-login).
Bearer Token for Generation & History
Open the ‚ÄúGeneration Webhook‚Äù (e.g., /ideogen) and ‚ÄúHistory Webhook‚Äù (e.g., /ideogener8r-history).
Set them to Header Auth and use Authorization: Bearer &lt;YOUR_SECRET_TOKEN&gt;.
Any requests to these endpoints must include that token in the header.
Publish & Test
Activate or publish your workflow.
Go to /ideogener8r-login, enter your Basic Auth credentials, and test by generating images.
Verify that images appear in your Google Drive folder, and the metadata is logged in the Google Sheet.
Ideogram API Details
Endpoint: The template uses Ideogram‚Äôs /generate, /upscale, and /remix endpoints.
Headers: You must provide your API key as a Bearer token in the request header.
Rate Limits: Check your Ideogram AI account for any usage or rate-limiting policies.
More info at ideoGener8r.com"
Generate Complete Stories with GPT-4o and Save Them in Google Drive,https://n8n.io/workflows/3153-generate-complete-stories-with-gpt-4o-and-save-them-in-google-drive/,"AI Story Generator with GPT-4o and Google Drive Integration
Automatically generate complete stories with GPT-4o and seamlessly save them to Google Drive.
Who is this for?
Creative writers and authors
Marketing and sales professionals
Educators and content creators
Fan fiction enthusiasts
Anyone interested in automating storytelling with AI
What problem is this workflow solving?
Manually creating engaging, structured narratives can be time-consuming. Writers and content creators often struggle to maintain consistency, depth, and engaging storytelling structure. This workflow solves these challenges by automating story creation using advanced AI (GPT-4o) and proven storytelling techniques.
What this workflow does
This n8n automation generates comprehensive stories through an iterative AI-driven process:
Step 1: Provide Your Story Idea
Users input a clear description and specify their desired story format (short story, fan fiction, sales email, etc.).
Step 2: AI-Driven Analysis
GPT-4o analyzes the provided idea, categorizes the story, selects relevant storytelling frameworks inspired by PipDeck Storyteller Tactics, and determines narrative tone and direction.
Step 3: Story and Character
FoundationEstablishes core themes, emotional hooks, and detailed character backgrounds.
Step 4: Initial Story Development
Creates a structured plot outline including engaging elements such as hooks, twists, and resolutions.
Step 5: Iterative Enhancement
Refines the story through multiple automated prompts, improving narrative depth, character development, dialogue, and realism.
Step 6: Editorial Feedback
Generates automated critiques highlighting clich√©s, weak dialogues, and areas for improvement.
Step 7: Final Polished Version
Incorporates editorial feedback to produce a complete, polished, ready-to-use narrative.
Step 8: Instant Google Drive
OrganizationAutomatically saves the final story directly to your specified Google Drive folder for easy access and management.
Setup Instructions
Prerequisites:
n8n account (cloud or self-hosted)
GPT-4o API access via OpenAI
Google Drive account
Configure OpenAI Node:
Add your GPT-4o API key in the OpenAI node settings.
Configure Google Drive Node:
Connect your Google Drive account by authenticating with n8n.
Specify the folder where generated stories should be saved.
Test Workflow:
Run the workflow with a simple story prompt to ensure proper setup.
How to Customize this Workflow
Adjust Prompt Details: Modify AI prompt instructions to suit your specific story style and audience.
Expand or Narrow Iterations: Change the number of iterations to balance between speed and story complexity.
Customize Feedback Level: Adjust the level of editorial feedback provided.
Dependencies and Requirements
GPT-4o API from OpenAI
Google Drive integration enabled in n8n
Get Started
Download and deploy this template today to streamline your storytelling process and produce consistently engaging, high-quality content effortlessly."
Real Estate Cold Call Scripts for Price Reduced FSBO Properties (Zillow Data),https://n8n.io/workflows/3143-real-estate-cold-call-scripts-for-price-reduced-fsbo-properties-zillow-data/,"Real Estate Price-Reduced Property Opportunity Guide
Overview
This comprehensive automation solution targets FSBO properties listed on Zillow that have recently had their price reduced, providing both investment analysis and tailored outreach scripts. The workflow gathers comprehensive market data to analyze local trends for each specific city and incorporates this intelligence into personalized communication strategies (more aggressive in buyers' markets, more value-focused in sellers' markets).
Key benefits:
Identify motivated homeowners who have recently reduced their property prices
Prepare data-driven offer strategies based on detailed market conditions
Develop effective, personalized communication approaches
Create a consistent, automated workflow to find and capitalize on opportunities
Requirements
Airtable account (free tier works)
n8n automation platform
OpenAI API key
Zillow Rapid API access
Basic understanding of automation workflows
Step 1: Set Up Airtable Database
Create a new Airtable base
Set up a table with the following fields:
Property ID (primary field)
Address
Price
Status (single select field with options: ""Contacted"" and ""Uncontacted"")
Phone Number
Communication Template
Market Analysis
Offer Price
Investment Metrics (Long text field for detailed financial analysis)
Expected Cash Flow (Currency field)
Expected ROI (Percentage field)
Follow-up Date
Set the default Status to ""Uncontacted""
Generate a Personal Access Token from Airtable settings for API connections
Step 2: Configure Zillow API Connection
Set up an HTTP request node in n8n to connect to Zillow's Rapid API
Configure the search parameters:
Listing type: ""For Sale By Owner""
Location: Your target market
Price reduction: true
Auction properties: false
Add filters to:
Exclude properties without contact information
Only include listings with recent price reductions
Filter by minimum square footage or bedrooms if desired
Set maximum days since price reduction (e.g., last 14 days)
Step 3: Market Analysis Workflow
Our market analysis engine processes Zillow market data to provide strategic insights for each location:
Comprehensive Market Indicators:
Current inventory levels and new listing rates
Median list price and sale price comparison
Days-to-pending metrics (current and historical)
Sale-to-list price ratios
Percentage of properties selling above/below list price
Market Trend Analysis:
6-month and year-over-year comparative trends
Market trend scoring system (-10 to +10 scale)
Market cycle position identification (Early, Mid, Late)
Seasonal market pattern recognition
Strategic Investment Guidance:
Market type classification (Buyer's, Balanced, Seller's)
Negotiation power assessment
Market timing recommendations
Risk evaluation (Low to High scale)
Actionable investment strategies based on current conditions
This analysis is transformed into actionable advice that directs your negotiation approach and helps you calibrate offers appropriately for market conditions.
Step 4: Calculate Investment Metrics
Our investment calculator analyzes each Zillow property and its RentZestimate to provide comprehensive financial projections:
Purchase & Financing Analysis:
Purchase price and standard acquisition costs (20% down payment, 3% closing costs)
Loan amount and monthly mortgage payment (30-year term)
Total initial investment calculation
Monthly Expense Projections:
Property tax (2% of property value annually)
Insurance (0.5% of property value annually)
Maintenance reserves (1% of property value annually)
Vacancy allowance (8% of rental income)
Total monthly expense calculation
Rental Income & Returns:
RentZestimate integration (with 1% rule fallback when unavailable)
Monthly and annual cash flow projections
Cash-on-cash ROI calculation
Break-even timeline analysis
Investment Quality Filtering:
Automatic filtering to show only positive cash flow properties
Property identification with complete details (address, beds, baths, sqft)
Full investment metrics breakdown for investor decision-making
This calculator ensures you focus only on properties with profit potential, saving you countless hours of manual analysis.
Step 5: Generate Communication Templates
Create templates that combine:
Property details (address, price, features)
Market intelligence from your analysis
Investment metrics showing the property's potential
Configures your templates to include:
A professional introduction
Reference to the specific property address
Mention of the price reduction
A strategic cash offer (typically 5-15% below asking)
Local market statistics to build credibility
A clear call to action (request to visit the property)
Step 6: Automated Workflow
Set up the complete n8n workflow to:
Check for new listings with price reductions daily
Run market analysis on each new property
Calculate potential investment returns
Generate personalized communication templates
Upload data to Airtable
Flag properties ready for contact
Update status to ""Contacted"" after outreach
Troubleshooting
If outreach isn't being tracked: Check API connections to Airtable
If market data seems incorrect: Verify Zillow API parameters and location settings
If offers are too low/high: Adjust your calculation parameters
Step by Step tutorial:
https://youtu.be/IqVw9CIL254?si=MKE5UY4rD0TOMLPg
By following this guide, you'll have a functional system that can identify opportunities, analyze market conditions, and help you create personalized offers to potential sellers‚Äîall while minimizing the manual effort required so you can focus on closing deals."
Audio Conversation Analysis & Visualization with DeepGram and GPT-4o,https://n8n.io/workflows/3140-audio-conversation-analysis-and-visualization-with-deepgram-and-gpt-4o/,"Transcript Evalu8r ‚Äì AI-Powered Transcribing and Transcript Analysis Workflow
Overview
Transcript Evalu8r is an AI-powered transcript analysis workflow that automates the processing, visualization, and evaluation of transcribed conversations. This n8n workflow template is designed to help users quickly analyze sentiment, extract topics and intents, generate key insights, and enhance transcript navigation.
It is ideal for customer service teams, legal professionals, researchers, content creators, and businesses seeking actionable intelligence from recorded conversations.
Key Features
üîπ Automated Transcript Processing
Audio-to-Text Conversion ‚Äì Upload an audio file and generate transcripts.
File Upload Management ‚Äì Track file uploads with real-time progress indicators.
Transcript Selection ‚Äì Browse and select previously analyzed transcripts from a dropdown.
Download Transcripts ‚Äì Export transcripts in various formats (Google Docs and JSON).
Error Handling ‚Äì Detects and alerts users about upload or processing failures.
üîπ Advanced UI Components
Interactive Transcript Viewer ‚Äì Displays full transcripts with:
Timestamps and speaker identification.
Clickable words for easy navigation.
Auto-scrolling to highlight the active portion of the transcript.
Key Points & Action Items Summary ‚Äì AI extracts:
Critical insights from conversations.
Actionable follow-ups detected in the discussion.
Speaker Labeling ‚Äì Dynamically assigns real speaker names instead of generic placeholders (e.g., ""Speaker 1"").
Collapsible Sections ‚Äì Enables users to expand/collapse transcript details, key insights, and analysis sections.
üîπ AI-Powered Conversation Analysis
Sentiment Analysis & Visualization
Dynamic sentiment graph tracks emotional shifts.
Color-coded speaker sentiment tracking:
üîµ Positive
‚ö´ Neutral
üî¥ Negative
Time-based sentiment tracking ‚Äì Shows sentiment fluctuations across the conversation.
Topic & Intent Recognition
Automated Topic Extraction ‚Äì Identifies key themes discussed.
Hierarchical Topic Organization ‚Äì Groups related topics for structured insights.
Intent Classification ‚Äì Recognizes whether statements reflect:
Questioning
Commands
Decision-making
Information seeking
Confidence Scoring ‚Äì Displays confidence levels for topic and intent detection.
Speaker Contribution Metrics
Talk-time percentages per speaker.
Interruption detection ‚Äì Highlights moments where speakers cut each other off.
Word Frequency Analysis ‚Äì Identifies commonly used terms and phrases.
üîπ Seamless Workflow Automation
API Integration ‚Äì Connects with transcription engines like OpenAI Whisper, AWS Transcribe, and Deepgram.
Webhook Support ‚Äì Automates workflows when new transcripts are available.
Batch Processing ‚Äì Handles multiple transcript analyses simultaneously.
Export & Reporting ‚Äì Sends insights to Google Sheets, Notion, Airtable, or Slack.
How It Works
Upload an audio file or select a pre-existing transcript.
Transcript Processing:
Speech-to-text conversion (if required).
AI-driven sentiment, topic, and intent analysis.
Visualizations & Insights:
Review sentiment charts, speaker contributions, and key topics.
Extract action items and critical takeaways.
Download results or trigger automated follow-ups.
Use Cases
‚úÖ Customer Support Review ‚Äì Detect customer concerns and analyze sentiment trends.
‚úÖ Podcast & Video Content Analysis ‚Äì Summarize episodes and extract key themes.
‚úÖ Sales Call Insights ‚Äì Identify common objections and customer pain points.
‚úÖ Market Research ‚Äì Extract trends from focus groups and discussions.
‚úÖ Legal Compliance Monitoring ‚Äì Track policy violations in recorded conversations.
Setup Instructions
Import the Transcript Evalu8r workflow into your n8n instance.
Connect DeepGram Speech to Text API.
Configure workflow parameters (sentiment thresholds, topic extraction settings).
Run the workflow to analyze and visualize transcript data.
Review insights and export as needed.
Why Use Transcript Evalu8r?
üöÄ Automate tedious transcript analysis
üí° Unlock hidden insights in conversations
üìä Visualize conversations with interactive sentiment tracking
‚ö° Boost efficiency with AI-powered automation
üîó Seamlessly integrate with your existing workflows
Requirements
n8n instance (cloud or self-hosted)
**API key for DeegGram API"
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage,https://n8n.io/workflows/3031-callforge-01-filter-gong-calls-synced-to-salesforce-by-opportunity-stage/,"Workflow Description
Who is this for?
This workflow is designed for sales and revenue teams using Gong and Salesforce to track and analyze sales calls. It helps automate the extraction, filtering, and preprocessing of Gong call data for further AI analysis.
What problem is this solving?
Sales teams often generate large amounts of call data, but not all calls are relevant for deeper analysis. This workflow filters calls based on predefined criteria, extracts relevant metadata, and formats the data before passing it to an AI processing pipeline.
What this workflow does
Triggers on new Gong calls synced to Salesforce every hour.
Filters calls based on opportunity stage (Discovery or Meeting Booked).
Retrieves Gong call details via API.
Formats call data into a structured JSON object for AI processing.
Passes the structured data to a Gong Call Preprocessor workflow for further insights.
Setup
Ensure that you have connected Salesforce and Gong APIs with valid credentials.
Modify the Salesforce query in Get all custom Salesforce Gong Objects to match your organization‚Äôs requirements.
Set the schedule trigger interval in the Run Hourly node if needed.
Connect this workflow to an AI processing workflow to analyze call transcripts.
Workflow Templates:
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
How to customize
Change filtering logic: Adjust the opportunity stage filter (Check if Opportunity Stage is Meeting Booked or Discovery) to match your sales process.
Modify data formatting: Add or remove fields in the Format call into correct JSON Object node to customize the output.
Adjust trigger frequency: Change the Run Hourly node to run at a different interval if required."
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization,https://n8n.io/workflows/3032-callforge-02-prep-gong-calls-with-sheets-and-notion-for-ai-summarization/,"CallForge - AI Gong Sales Call Processor
Streamline your sales call analysis with CallForge, an automated workflow that extracts, enriches, and refines Gong.io call data for AI-driven insights.
Who is This For?
This workflow is designed for:
‚úÖ Sales teams looking to automate sales call insights.
‚úÖ Revenue operations (RevOps) professionals optimizing call data processing.
‚úÖ Businesses using Gong.io to analyze and enhance sales call transcripts.
What Problem Does This Workflow Solve?
Manually analyzing sales calls is time-consuming and prone to inconsistencies. While Gong provides raw call data, interpreting these conversations and improving AI-generated summaries can be challenging.
With CallForge, you can:
‚úîÔ∏è Automate transcript extraction from Gong.io.
‚úîÔ∏è Enhance AI insights by adding product and competitor data.
‚úîÔ∏è Reduce errors from AI-generated summaries by correcting mispronunciations.
‚úîÔ∏è Eliminate duplicate calls to prevent redundant processing.
What This Workflow Does
1. Extracts Gong Call Data
Retrieves call recordings, metadata, meeting links, and duration from Gong.
2. Removes Duplicate Entries
Queries Notion to ensure that already processed calls are not duplicated.
3. Enriches Call Data
Fetches integration details from Google Sheets.
Retrieves competitor insights from Notion.
Merges data to provide AI with a more comprehensive context.
4. Prepares AI-Friendly Transcripts
Cleans up transcripts for structured AI processing.
Reduces prompt complexity for more accurate OpenAI outputs.
5. Sends Processed Data to an AI Call Processor
Delivers the cleaned and enriched transcript to an AI-powered workflow for generating structured call summaries.
How to Set Up This Workflow
1. Connect Your APIs
üîπ Gong API Access ‚Äì Set up your Gong API credentials in n8n.
üîπ Google Sheets Credentials ‚Äì Provide API access for retrieving integration data.
üîπ Notion API Setup ‚Äì Connect Notion to fetch competitor insights and store processed data.
üîπ AI Processing Workflow ‚Äì Ensure an OpenAI-powered workflow is in place for structured summaries.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
2. Customize to Fit Your Needs
üí° Modify Data Sources ‚Äì Update connections if using a different CRM, database, or analytics tool.
üí° Adjust AI Processing Logic ‚Äì Optimize transcript formatting based on your preferred AI model.
üí° Expand Data Enrichment ‚Äì Integrate CRM data, industry benchmarks, or other insights.
Why Use CallForge?
By automating Gong call processing, CallForge empowers sales teams to:
üìà Gain valuable AI-driven insights from calls.
‚ö° Speed up decision-making with cleaner, structured data.
üõ† Improve sales strategies based on enriched, accurate transcripts.
üöÄ Start automating your Gong call analysis today!"
CallForge - 07 - AI Marketing Data Processing with Gong & Notion,https://n8n.io/workflows/3037-callforge-07-ai-marketing-data-processing-with-gong-and-notion/,"CallForge - AI-Powered Marketing Insights Extraction from Sales Calls
Automate marketing intelligence gathering from AI-analyzed sales calls and store insights in Notion.
üéØ Who is This For?
This workflow is designed for:
‚úÖ Marketing teams looking to extract trends and insights from sales conversations.
‚úÖ Product managers who need direct customer feedback from sales calls.
‚úÖ Revenue operations (RevOps) teams optimizing AI-driven call analysis.
It streamlines AI-powered marketing intelligence, identifying customer pain points, competitor mentions, and recurring trends‚Äîall automatically stored in Notion.
üîç What Problem Does This Workflow Solve?
Manually reviewing sales call transcripts for marketing insights is time-consuming and inconsistent.
With CallForge, you can:
‚úî Extract key marketing insights from AI-analyzed sales calls.
‚úî Track recurring discussion topics across multiple conversations.
‚úî Generate actionable marketing recommendations for strategy and content.
‚úî Store structured insights in Notion for seamless access.
This automation eliminates manual work and ensures marketing teams get data-driven insights from real customer conversations.
üìå Key Features & Workflow Steps
üéôÔ∏è AI-Driven Marketing Insights Processing
This workflow processes AI-generated sales call insights and organizes them in Notion databases:
Triggers when AI sales call data is received.
Identifies marketing-related data (trends, customer pain points, competitor mentions).
Extracts key marketing insights, categorizing product discussions and recurring topics.
Logs trends across multiple calls, ensuring marketing teams spot recurring themes.
Processes actionable insights, capturing marketing strategy recommendations.
Stores all findings in Notion, enabling structured, searchable insights.
üìä Notion Database Integration
Marketing Insights ‚Üí Logs key trends and product mentions from sales calls.
Recurring Topics ‚Üí Tracks frequently discussed themes across calls.
Actionable Recommendations ‚Üí Stores AI-generated recommendations for marketing teams.
üõ† How to Set Up This Workflow
1. Prepare Your AI Call Analysis Data
Ensure AI-generated sales call insights are available.
Compatible with Gong, Fireflies.ai, Otter.ai, and other AI transcription tools.
2. Connect Your Notion Database
Set up Notion databases for:
üîπ Marketing Insights (logs trends and product mentions)
üîπ Recurring Topics (tracks frequently discussed customer concerns)
üîπ Actionable Recommendations (stores marketing strategy insights)
3. Configure n8n API Integrations
Connect your Notion API key in n8n under ‚ÄúNotion API Credentials.‚Äù
Set up webhook triggers to receive AI-generated sales insights.
Test the workflow using a sample AI sales call analysis.
üîß How to Customize This Workflow
üí° Modify Notion Data Structure ‚Äì Adjust fields to match marketing strategy needs.
üí° Refine AI Data Processing Rules ‚Äì Customize what insights are extracted and logged.
üí° Integrate with Slack or Email ‚Äì Notify teams when key marketing trends emerge.
üí° Expand CRM Integration ‚Äì Sync insights with HubSpot, Salesforce, or Pipedrive.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
‚öôÔ∏è Key Nodes Used in This Workflow
üîπ If Nodes ‚Äì Detect if marketing insights, recurring topics, or recommendations exist in AI data.
üîπ Notion Nodes ‚Äì Create and update entries in Notion databases.
üîπ Split Out & Aggregate Nodes ‚Äì Process multiple insights and consolidate AI-generated data.
üîπ Wait Nodes ‚Äì Ensure smooth sequencing of API calls and database updates.
üöÄ Why Use This Workflow?
‚úî Eliminates manual sales call review for marketing teams.
‚úî Provides structured, AI-driven insights for marketing and product strategy.
‚úî Tracks competitor mentions and customer pain points automatically.
‚úî Improves content marketing and campaign planning with real customer insights.
‚úî Scalable for teams using n8n Cloud or self-hosted deployments.
This workflow empowers marketing teams by transforming sales call data into actionable intelligence, streamlining strategy, content planning, and competitor analysis. üöÄ"
CallForge - 08 - AI Product Insights from Sales Calls with Notion,https://n8n.io/workflows/3039-callforge-08-ai-product-insights-from-sales-calls-with-notion/,"CallForge - AI-Powered Product Insights Processor from Sales Calls
Automate product feedback extraction from AI-analyzed sales calls and store structured insights in Notion for data-driven product decisions.
üéØ Who is This For?
This workflow is designed for:
‚úÖ Product managers tracking customer feedback and feature requests.
‚úÖ Engineering teams identifying usability issues and AI/ML-related mentions.
‚úÖ Customer success teams monitoring product pain points from real sales conversations.
It streamlines product intelligence gathering, ensuring customer insights are structured, categorized, and easily accessible in Notion for better decision-making.
üîç What Problem Does This Workflow Solve?
Product teams often struggle to capture, categorize, and act on valuable feedback from sales calls.
With CallForge, you can:
‚úî Automatically extract and categorize product feedback from AI-analyzed sales calls.
‚úî Track AI/ML-related mentions to gauge customer demand for AI-driven features.
‚úî Identify feature requests and pain points for product development prioritization.
‚úî Store structured feedback in Notion, reducing manual tracking and increasing visibility across teams.
This workflow eliminates manual feedback tracking, allowing product teams to focus on innovation and customer needs.
üìå Key Features & Workflow Steps
üéôÔ∏è AI-Powered Product Feedback Processing
This workflow processes AI-generated sales call insights and organizes them in Notion databases:
Triggers when AI sales call data is received.
Detects product-related feedback (feature requests, bug reports, usability issues).
Extracts key product insights, categorizing feedback based on customer needs.
Identifies AI/ML-related mentions, tracking customer interest in AI-driven solutions.
Aggregates feedback and categorizes it by sentiment (positive, neutral, negative).
Logs insights in Notion, making them accessible for product planning discussions.
üìä Notion Database Integration
Product Feedback ‚Üí Logs feature requests, usability issues, and bug reports.
AI Use Cases ‚Üí Tracks AI-related discussions and customer interest in machine learning solutions.
üõ† How to Set Up This Workflow
1. Prepare Your AI Call Analysis Data
Ensure AI-generated sales call insights are available.
Compatible with Gong, Fireflies.ai, Otter.ai, and other AI transcription tools.
2. Connect Your Notion Database
Set up Notion databases for:
üîπ Product Feedback (logs feature requests and bug reports).
üîπ AI Use Cases (tracks AI/ML mentions and customer demand).
3. Configure n8n API Integrations
Connect your Notion API key in n8n under ‚ÄúNotion API Credentials.‚Äù
Set up webhook triggers to receive AI-generated sales insights.
Test the workflow using a sample AI sales call analysis.
üîß How to Customize This Workflow
üí° Modify Notion Data Structure ‚Äì Adjust fields to align with your product team's workflow.
üí° Refine AI Data Processing Rules ‚Äì Customize how feature requests and pain points are categorized.
üí° Integrate with Slack or Email ‚Äì Notify teams when recurring product issues emerge.
üí° Expand with Project Management Tools ‚Äì Sync insights with Jira, Trello, or Asana to create product tickets automatically.
‚öôÔ∏è Key Nodes Used in This Workflow
üîπ If Nodes ‚Äì Detect if product feedback, AI mentions, or feature requests exist in AI data.
üîπ Notion Nodes ‚Äì Create and update structured feedback entries in Notion.
üîπ Split Out & Aggregate Nodes ‚Äì Process multiple insights and consolidate AI-generated data.
üîπ Wait Nodes ‚Äì Ensure smooth sequencing of API calls and database updates.
üöÄ Why Use This Workflow?
‚úî Eliminates manual sales call review for product teams.
‚úî Provides structured, AI-driven insights for feature planning and prioritization.
‚úî Tracks AI/ML mentions to assess demand for AI-powered solutions.
‚úî Improves product development strategies by leveraging real customer insights.
‚úî Scalable for teams using n8n Cloud or self-hosted deployments.
This workflow empowers product teams by transforming sales call data into actionable intelligence, optimizing feature planning, bug tracking, and AI/ML strategy. üöÄ"
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher,https://n8n.io/workflows/3033-callforge-03-gong-transcript-processor-and-salesforce-enricher/,"CallForge - AI Gong Transcript PreProcessor
Transform your Gong.io call transcripts into structured, enriched, and AI-ready data for better sales insights and analytics.
Who is This For?
This workflow is designed for:
‚úÖ Sales teams looking to automate call transcript formatting.
‚úÖ Revenue operations (RevOps) professionals optimizing AI-driven insights.
‚úÖ Businesses using Gong.io that need structured, enriched call transcripts for better decision-making.
What Problem Does This Workflow Solve?
Manually processing raw Gong call transcripts is inefficient and often lacks essential context for AI-driven insights.
With CallForge, you can:
‚úî Extract and format Gong call transcripts for structured AI processing.
‚úî Enhance metadata using sales data from Salesforce.
‚úî Classify speakers as internal (sales team) or external (customers).
‚úî Identify external companies by filtering out free email domains (e.g., Gmail, Yahoo).
‚úî Enrich customer profiles using PeopleDataLabs to identify company details and locations.
‚úî Prepare transcripts for AI models by structuring conversations and removing unnecessary noise.
What This Workflow Does
1. Retrieves Gong Call Data
Calls the Gong API to extract call metadata, speaker interactions, and collaboration details.
Fetches call transcripts for AI processing.
2. Processes and Cleans Transcripts
Converts call transcripts into structured, speaker-based dialogues.
Assigns each speaker as either Internal (Sales Team) or External (Customer).
3. Extracts Company Information
Retrieves Salesforce data to match customers with existing sales opportunities.
Filters out free email domains to determine the customer‚Äôs actual company domain.
Calls the PeopleDataLabs API to retrieve additional company data and location details.
4. Merges and Enriches Data
Combines Gong metadata, Salesforce customer details and insights.
Ensures all necessary data is available for AI-driven sales insights.
5. Final Formatting for AI Processing
Merges all call transcript data into a single structured format for AI analysis.
Extracts the final cleaned, enriched dataset for further AI-powered insights.
How to Set Up This Workflow
1. Connect Your APIs
üîπ Gong API Access ‚Äì Set up your Gong API credentials in n8n.
üîπ Salesforce Setup ‚Äì Ensure API access if you want customer enrichment.
üîπ PeopleDataLabs API ‚Äì Required to retrieve company and location details based on email domains.
üîπ Webhook Integration ‚Äì Modify the webhook call to push enriched call data to an internal system.
CallForge - 01 - Filter Gong Calls Synced to Salesforce by Opportunity Stage
CallForge - 02 - Prep Gong Calls with Sheets & Notion for AI Summarization
CallForge - 03 - Gong Transcript Processor and Salesforce Enricher
CallForge - 04 - AI Workflow for Gong.io Sales Calls
CallForge - 05 - Gong.io Call Analysis with Azure AI & CRM Sync
CallForge - 06 - Automate Sales Insights with Gong.io, Notion & AI
CallForge - 07 - AI Marketing Data Processing with Gong & Notion
CallForge - 08 - AI Product Insights from Sales Calls with Notion
How to Customize This Workflow
üí° Modify Data Sources ‚Äì Connect different CRMs (e.g., HubSpot, Zoho) instead of Salesforce.
üí° Expand AI Analysis ‚Äì Add another AI model (e.g., OpenAI GPT, Claude) for advanced conversation insights.
üí° Change Speaker Classification Rules ‚Äì Adjust internal vs. external speaker logic to match your team‚Äôs structure.
üí° Filter Specific Customers ‚Äì Modify the free email filtering logic to better fit your company‚Äôs needs.
Why Use CallForge?
üöÄ Automate Gong call transcript processing to save time.
üìä Improve AI accuracy with enriched, structured data.
üõ† Enhance sales strategy by extracting actionable insights from calls.
Start optimizing your Gong transcript analysis today!"
‚öõÔ∏èüêãü§ñ Extract Data from YAPE Receipts via Telegram OCR and Store in Google Sheets,https://n8n.io/workflows/3073-extract-data-from-yape-receipts-via-telegram-ocr-and-store-in-google-sheets/,"Detailed Technical Description
This n8n workflow automates Yape payment receipt processing, integrating Telegram bot, AI-powered OCR, and Google Sheets automation. By leveraging ChatGPT Vision Computing, it extracts and structures transaction details, eliminating the need for manual entry. Ideal for freelancers, businesses, and finance teams, this workflow ensures error-free, real-time financial tracking. The AI agent powered by DeepSeek refines and formats the extracted text, storing it in Google Sheets for easy accessibility and reporting. Users can track payments, monitor cash flow, and generate financial reports without any manual work. This seamless integration boosts efficiency, reduces errors, and automates financial record-keeping with 100% accuracy.
üõ†Ô∏è Technologies Used:
‚úÖn8n ‚Äì Workflow orchestrator.
‚úÖTelegram ‚Äì Handles image reception and notifications.
‚úÖGoogle Drive ‚Äì Manages file creation and storage.
‚úÖGoogle Sheets ‚Äì Automatically logs extracted data into spreadsheets.
ü§ñArtificial Intelligence:
‚úÖChatGPT Vision Computing ‚Äì Performs OCR on payment receipts.
‚úÖDeepSeek AI ‚Äì Organizes and converts extracted data into a structured format.
üìå pre-conditions:
üì© A Telegram Bot ‚Äì Must be created to receive images. Setup Guide
üîë Google Sheets API Key ‚Äì Required to store extracted data. Setup Guide
‚öõÔ∏è ChatGPT API Key ‚Äì Used for OCR and AI text extraction. Get it here
üêã DeepSeek API Key ‚Äì Processes extracted text into structured data. Get it here
1Ô∏è‚É£ Image Reception & OCR Processing üì©
The user attaches a Yape payment receipt image to the Telegram bot conversation. ü§≥üèª
2Ô∏è‚É£ Analyze Image (OCR) üëÅÔ∏è‚Äçüó®Ô∏è
A ‚öõÔ∏èChatGPT Vision Computing model processes the image and extracts all visible text, ensuring high-accuracy OCR for structured data extraction.
3Ô∏è‚É£ Analyze and format text
Intelligent Data Processing with AI ü§ñ
The extracted text is sent to a üêãDeepSeek-based AI agent that:
Identifies and structures key transaction details (amount, date, sender, transaction ID, etc.).
Converts the data into a structured JSON format.
4Ô∏è‚É£ Data Storage in Google Sheets üìä
Google Drive integration is established.
If the Google Sheets file does not exist, it is automatically created.
Extracted data is automatically recorded in the corresponding spreadsheet, enabling effortless tracking and streamlined financial organization.
üöÄ Benefits:
‚úÖ Time-saving ‚Äì Eliminates manual payment processing.
‚úÖ Error-free data entry ‚Äì Reduces human mistakes in record-keeping.
‚úÖ 100% automation ‚Äì No manual intervention required.
‚úÖ Seamless integration ‚Äì Easily connects with other workflows.
üí°NOTE: The extracted transaction data will be stored in a Google Sheets file with the following columns:
Column Name Description
id Unique identifier for each transaction.
beneficiaryName Name of the recipient of the payment.
amount Payment amount in the specified currency.
currency Currency used for the transaction (e.g., PEN, USD).
company The entity or service handling the transaction.
date Date of the transaction in a human-readable format.
hour Time of the transaction.
originalDate The exact date extracted from the receipt.
dateToISO Standardized ISO 8601 date format.
operation Type of financial operation (e.g., deposit, transfer).
operationNumber Unique operation number provided by the payment system.
beneficiaryNumber Account or phone number of the recipient.
commission Any commission or fee charged for the transaction.
account Account number or reference used for the payment.
channel Payment method used (e.g., app, POS, bank transfer).
agentCode Identifier of the agent or entity processing the payment.
This workflow is perfect for businesses, freelancers, and individuals who need a fully automated solution to process Yape payment receipts efficiently. By leveraging AI-powered OCR and structured data extraction, it eliminates manual effort, reduces errors, and ensures that all transactions are accurately logged in Google Sheets. Whether you're managing personal finances, freelance payments, or business transactions, this system provides a seamless, hands-free approach to financial tracking, allowing you to focus on what truly matters."
AI-Powered Gratitude Reminder Workflow for LINE,https://n8n.io/workflows/3040-ai-powered-gratitude-reminder-workflow-for-line/,"This workflow template, ""Daily Gratitude Reminder Bot for LINE,"" is designed to help users cultivate a habit of gratitude by sending personalized, AI-generated reminders every evening at 9:00 PM. Using Azure OpenAI , the bot generates varied and engaging messages to prompt users to reflect on the positive aspects of their day. The reminders are then sent directly to users via the LINE messaging platform , ensuring a seamless and impactful experience.
Whether you're a developer, counselor, or business owner, this template offers a customizable and scalable solution for promoting mental wellness and fostering a culture of gratitude.
profile
Qwen2.5-Max
2:56 pm
Here‚Äôs a polished and detailed description for your ""Daily Gratitude Reminder Bot for LINE"" template, following the submission guidelines provided. This description is structured to meet all the requirements, including SEO optimization, clarity, and user convenience.
Daily Gratitude Reminder Bot for LINE
This workflow template, ""Daily Gratitude Reminder Bot for LINE,"" is designed to help users cultivate a habit of gratitude by sending personalized, AI-generated reminders every evening at 9:00 PM. Using Azure OpenAI , the bot generates varied and engaging messages to prompt users to reflect on the positive aspects of their day. The reminders are then sent directly to users via the LINE messaging platform , ensuring a seamless and impactful experience.
Whether you're a developer, counselor, or business owner, this template offers a customizable and scalable solution for promoting mental wellness and fostering a culture of gratitude.
Who Is This Template For?
Developers who want to integrate AI-powered workflows into messaging platforms like LINE.
Counselors & Therapists looking to encourage mindfulness and emotional well-being among their clients.
Businesses & Organizations focused on employee wellness or customer engagement through positive reinforcement.
Educators & Nonprofits seeking tools to promote mental health awareness and self-care practices.
What Problem Does This Workflow Solve?
Gratitude journaling has been proven to improve mental health, reduce stress, and increase overall happiness. However, many people struggle to maintain the habit due to busy schedules or forgetfulness. This workflow solves that problem by automating daily reminders to reflect on positive experiences, making it easier for users to build and sustain a gratitude practice.
What This Workflow Does
Scheduled Trigger:
The workflow is triggered every evening at 9:00 PM using a schedule node.
AI-Powered Message Generation:
An Azure OpenAI Chat Model generates a unique and engaging reminder message with a temperature setting of 0.9 to ensure variety and creativity.
Message Formatting:
The generated message is reformatted to comply with the LINE Push API requirements, ensuring smooth delivery.
Push Notification via LINE:
The formatted message is sent to the user via the LINE Push API , delivering the reminder directly to their chat.
Setup Guide
Pre-Requisites
Access to an Azure OpenAI account with credentials.
A LINE Developers Console account with access to the Push API.
Basic knowledge of n8n workflows and JSON formatting.
How to Customize This Workflow to Your Needs
Change the Time: Adjust the schedule trigger to send reminders at a different time.
Modify the Prompt: Edit the AI model's input prompt to generate messages tailored to your audience (e.g., focus on work achievements or personal growth).
Expand Recipients: Update the LINE Push API node to send reminders to multiple users or groups.
Integrate Additional Features: Add nodes to log user responses or track engagement metrics.
Why Use This Template?
Promotes Mental Wellness: Encourages users to reflect on positive experiences, improving emotional well-being.
Highly Customizable: Easily adapt the workflow to suit different audiences and use cases.
Scalable: Send reminders to one user or thousands, making it suitable for both personal and organizational use.
AI-Powered Creativity: Avoid repetitive messages by leveraging AI to generate fresh and engaging content."
Line Chatbot Handling AI Responses with Groq and Llama3,https://n8n.io/workflows/2977-line-chatbot-handling-ai-responses-with-groq-and-llama3/,"Workflow overview:
This workflow is designed for dynamic and intelligent conversational capabilities. It incorporates Meta's llama3.3-versatile model for personal assistant. There are no issues when sending simple text to the LINE reply API, so in this workflow you can see how to handle large and complex text sending from AI chat without any errors.
Workflow description:
User uses Line Messaging API to send message to the chatbot, create line business ID from here: Line Business
Set the message from Step 1 to the proper value
Send the message to process at Groq using API key that we have created from Groq
Send the reply message from AI Agent back to Line Messaging API account
Key Features:
Utilizes Meta's llama 3.3 model for robust conversational capabilities
Handles large and complex text interactions with ease, ensuring reliable connections to LINE Messaging API
Demonstrates effective strategies for processing and responding to large and complex text inputs from AI chat
To use this template, you need to be on n8n version 1.79.0 or later."
üß† *NEW* Claude 3.7 Extended Thinking AI Agent ü§ñ ‚Äì Unlock Ultimate Intelligence,https://n8n.io/workflows/3009-new-claude-37-extended-thinking-ai-agent-unlock-ultimate-intelligence/,"üß† NEW Claude 3.7 Extended Thinking AI Agent ü§ñ ‚Äì Unlock Ultimate Intelligence
Supercharge Claude 3.7 with full customization & deeper reasoning!
Who is this for? üéØ
‚úÖ AI Enthusiasts & Researchers: Push Claude‚Äôs intelligence to new heights.
‚úÖ Professionals & Analysts: Get in-depth, high-quality insights.
‚úÖ Developers & Power Users: Unlock settings not available on the official Claude site.
What makes this different? üöÄ
Even Claude Pro limits your control‚Äîthis tool breaks those barriers! Adjust thinking tokens to supercharge extended reasoning, fine-tune every setting, and get smarter, more strategic responses that outperform the default experience.
How it works üåü
1Ô∏è‚É£ Customize Claude 3.7‚Äôs settings (beyond what‚Äôs possible on the official site).
2Ô∏è‚É£ Increase thinking tokens to drastically improve deep reasoning.
3Ô∏è‚É£ Chat with Claude at peak performance for more insightful answers.
Quick Setup ‚öôÔ∏è (1-3 min)
üîπ Connect the Claude API.
üîπ Adjust settings for ultimate AI performance.
üîπ Start getting deeper, smarter responses instantly!
Required API üîó
Claude 3.7 API (Paid)
Why use this tool? üí°
Unlock full customization & go beyond Claude‚Äôs default limits.
Enhance extended thinking for more powerful reasoning.
Optimize response depth, quality & accuracy.
üëâ Take Claude 3.7 to the next level‚Äîstart customizing now!"
Automated AI image tagging and writing keywords into image (via community node),https://n8n.io/workflows/2997-automated-ai-image-tagging-and-writing-keywords-into-image-via-community-node/,"Welcome to my Automated Image Metadata Tagging Workflow!
DISCLAIMER: This workflow only works with self-hosted n8n instances! You have to install the n8n-nodes-exif-data Community Node!
This workflow automatically analyzes the image content with the help of AI and writes it directly back into the image file as keywords.
(https://n8n.io/workflows/2995).**
This workflow has the following steps:
Google Drive trigger (scan for new files added in a specific folder)
Download the added image file
Analyse the content of the image
Merge Metadata and image file
Write the Keywords into the Metadata (dc:subject/keywords) and create new image file
Update the original file in the Google Drive folder
The following accesses are required for the workflow:
You have to install the n8n-nodes-exif-data Community Node
Google Drive: Documentation
AI API access (e.g. via OpenAI, Anthropic, Google or Ollama)
You can contact me via LinkedIn, if you have any questions: https://www.linkedin.com/in/friedemann-schuetz"
"AI-Powered Financial Chart Analyzer | OpenRouter, MarketStack, macOS Shortcuts",https://n8n.io/workflows/2970-ai-powered-financial-chart-analyzer-or-openrouter-marketstack-macos-shortcuts/,"The AI-Powered Financial Chart Analyzer is a cutting-edge automation tool that streamlines financial analysis using n8n workflows, AI agents, and MacOS Shortcuts. It enables users to capture, process, and analyze candlestick charts for both stocks and cryptocurrencies. By integrating powerful tools like ChatGPT-4o-mini (via OpenRouter), MarketStack, and SerpAPI, this automation provides real-time market insights, technical analysis, and AI-driven stock trend predictions.
Workflow
The Webhook node will receive image data from a macOS shortcut in Base64 format.
The Convert to File node will convert the Base64 image into a binary file.
The AI Agent node will process the binary image. It utilizes OpenRouter, Windows buffer memory, MarketStack, and the SerpAPI tool.
Remember to use a model capable of processing images; otherwise, the workflow will throw an error.
We use the MarketStack tool to fetch the latest stock data; however, it is rarely used.
SerpAPI is used for market research, such as news and insights about stocks.
Finally, the Markdown node converts Markdown to HTML.
The response is then sent to the Webhook.
Use Case
Traders & Investors: Quickly analyze candlestick charts and identify trading opportunities.
Financial Analysts: Automate chart interpretation and data aggregation for in-depth reports.
AI & Automation Enthusiasts: Experiment with AI-driven financial workflows using n8n.
Business Owners: Gain real-time stock insights to make informed investment decisions.
Setup Instructions
Install MacOS Shortcut
Download the MacOS Shortcut provided with this product and double-click on it to install.
If you don‚Äôt have the Shortcuts app (parent app) installed, first download it from the App Store, then follow Step 1.
Set Up Workflow
Import the n8n workflow provided with this product.
Set Up Credentials
üîπWebhook Authentication
Create an API key (you can use a key generation tool or simply use a custom string).
Add the API key to your n8n Webhook and MacOS Shortcut.
If you prefer not to use authentication, remove it from both the n8n Webhook and the MacOS Shortcut.
üîπOpenRouter API Setup
Get a free API key from OpenRouter and add it to your workflow.
Free API keys have usage limits.
OpenRouter provides multiple models‚Äîensure that the selected model supports image processing.
üîπMarketStack API Setup
Get a free API key from MarketStack and use it in your workflow.
Free API keys have usage limits.
üîπSerpAPI Setup
Get a free API key from SerpAPI and use it in your workflow.
Free API keys have usage limits.
Disclaimer
This tool is designed for educational and informational purposes only. The AI-generated insights should not be considered as financial advice. Users should conduct their own research before making investment decisions. AgentsOps is not responsible for any financial losses incurred from using this automation."
#Ô∏è‚É£Nostr #damus AI Powered Reporting + Gmail + Telegram,https://n8n.io/workflows/2946-nostr-damus-ai-powered-reporting-gmail-telegram/,"The n8n Nostr Community Node is a tool that integrates Nostr functionality into n8n workflows, allowing users to interact with the Nostr protocol seamlessly. It provides both read and write capabilities and can be used for various automation tasks.
Disclaimer
This node is ideal for self-hosted n8n setups, as ++community nodes are not supported on n8n cloud++. It opens up exciting possibilities for integrating workflows with the decentralized Nostr protocol.
n8n Community Node for Nostr
n8n-nodes-nostrobots
Features
Write Operations: Send notes and events (kind1) to the Nostr network.
Read Operations: Fetch events based on criteria such as event ID, public key, hashtags, mentions, or search terms.
Utility Functions: Convert events into different formats like naddr or nevent and handle key transformations between bech32 and hex formats.
Trigger Events: Monitor the Nostr network for specific mentions or events and trigger workflows automatically.
Use Cases
Automating note posting without exposing private keys.
Setting up notifications for mentions or specific events.
Creating bots or AI assistants that respond to mentions on Nostr.
Installation
Install n8n on your system.
Add the Nostr Community Node to your instance.
Configure your credentials using a Nostr secret key (supports bech32 or hex formats)."
Convert RSS to tweet with text and image using free Twitter API,https://n8n.io/workflows/2933-convert-rss-to-tweet-with-text-and-image-using-free-twitter-api/,"üéØ Who is this automation for?
Perfect for content creators, marketers, and any X (Twitter) user looking to grow their account quickly without manual effort.
‚öôÔ∏è How does this workflow work?
‚úîÔ∏è Auto-publishing: Whenever a new article appears in your RSS feed, a tweet is generated and posted on X.
‚úîÔ∏è Optimized content: A unique, engaging tweet with text and an image is created in under 280 characters. (Verified users can bypass this limit for longer tweets).
‚úîÔ∏è Free X API: No extra subscription costs for automation.
‚úîÔ∏è AI-powered: Use models like OpenAI, gemini and Perplexity, but you can configure your preferred model, whether free or paid.
üõ†Ô∏è Easy Setup
üìÇ The downloaded file includes a step-by-step video tutorial. If you need help, I‚Äôm here for you!
üí° Save time and grow your X audience efficiently with this n8n automation."
Generate multispeaker podcast üéôÔ∏è with AI natural-sounding ü§ñüß† & Google Sheets,https://n8n.io/workflows/2927-generate-multispeaker-podcast-with-ai-natural-sounding-and-google-sheets/,"This workflow automates the generation of multi-speaker podcasts using AI-powered text-to-speech technology. It starts by retrieving a podcast script from a Google Sheets document, where each speaker‚Äôs lines are clearly defined. The workflow then processes the script, generates a natural-sounding audio file with different voices for each speaker, and stores the final audio file in Google Drive.
The workflow is designed to save time and resources by automating the podcast production process, making it ideal for content creators, marketers, and businesses that need to produce high-quality audio content regularly.
How It Works
Triggering the Workflow:
The workflow starts with the When clicking ‚ÄòTest workflow‚Äô node, which can be triggered manually to begin the process.
Data Retrieval:
The Get Podcast text node retrieves data from a Google Sheets document containing the podcast script. The document includes columns for the speaker's name and the corresponding text.
Data Aggregation:
The Get all rows node aggregates the data from the Google Sheets document, combining the speaker names and their respective text into a single dataset.
Text Formatting:
The Full Podcast Text node processes the aggregated data, formatting it into a single string where each speaker's text is prefixed with their name.
Audio Generation:
The Create Audio node sends a request to the API to generate a multi-speaker podcast audio file. The request includes the formatted text and specifies the voices for each speaker.
When you register for the API service you will get 1$ for free. For continuous work add API credits to your account.
Status Check:
The Get status node checks the status of the audio generation request. If the status is ""COMPLETED"", the workflow proceeds; otherwise, it waits again.
Audio Retrieval:
The Get Url Audio node retrieves the URL of the generated audio file.
The Get File Audio node downloads the audio file from the provided URL.
Audio Storage:
The Upload Audio node uploads the generated audio file to a specified Google Drive folder for storage.
Key Features
Multi-Speaker Support: Generates podcasts with different voices for each speaker, creating a more dynamic and engaging listening experience.
Google Sheets Integration: Retrieves podcast scripts from a Google Sheets document, making it easy to manage and update content.
AI-Powered Text-to-Speech: Uses advanced AI models to generate natural-sounding audio from text.
Automated Audio Generation: Streamlines the process of creating podcast audio files, reducing the need for manual recording and editing.
Google Drive Storage: Automatically uploads the generated audio files to Google Drive for easy access and sharing.
This workflow is ideal for businesses and content creators looking to automate the production of multi-speaker podcasts. It leverages AI to handle the complex task of generating natural-sounding audio, allowing users to focus on creating compelling content."
UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports,https://n8n.io/workflows/2921-utm-link-creator-and-qr-code-generator-with-scheduled-google-analytics-reports/,"UTM Link Creator & QR Code Generator with Scheduled Google Analytics Reports
This workflow enables marketers to generate UTM-tagged links, convert them into QR codes, and automate performance tracking in Google Analytics with scheduled reports every 7 days. This solution helps monitor traffic sources from different marketing channels and optimize campaign performance based on analytics data.
Prerequisites
Before implementing this workflow, ensure you have the following:
Google Analytics 4 (GA4) Account & Access
Ensure you have a GA4 property set up.
Access to the GA4 Data API to schedule performance tracking. Refer to the Google Analytics Data API Overview for more information.
Airtable Account & API Key
Create an Airtable base to store UTM links, QR codes, and analytics data.
Obtain an Airtable API key from your Account Settings. Detailed instructions are available in the Airtable API Authentication Guide.
Step-by-Step Guide to Setting Up the Workflow
1. Generate UTM Links
Create a form or interface to input:
Base URL (e.g., https://example.com)
Campaign Name (utm_campaign)
Source (utm_source)
Medium (utm_medium)
Term (Optional: utm_term)
Content (Optional: utm_content)
Append UTM parameters to generate a trackable URL.
2. Store UTM Links & QR Codes in Airtable
Set up an Airtable base with the following columns:
UTM Link
QR Code
Campaign Name
Source
Medium
Date Created
Adjust as needed based on your tracking requirements. For guidance on setting up your Airtable base and using the API, refer to the Airtable Web API Documentation.
3. Convert UTM Links to QR Codes
Use a QR code generator API (e.g., goqr.me, qrserver.com) to generate QR codes for each UTM link and store them in Airtable.
4. Schedule Google Analytics Performance Reports (Every 7 Days)
Use the Google Analytics Data API to pull weekly performance reports based on UTM parameters.
Extract key metrics such as:
Sessions
Users
Bounce Rate
Conversions
Revenue (if applicable)
Store the data in Airtable for tracking and analysis.
Adjust timeframe as needed
For more details on accessing and using the Google Analytics Data API, consult the Google Analytics Data API Overview.
Benefits of This Workflow
‚úÖ Track Marketing Campaigns: Easily monitor which channels drive traffic.
‚úÖ Automate QR Code Creation: Seamless integration of UTM links with QR codes.
‚úÖ Scheduled Google Analytics Reports: No manual reporting‚Äîeverything runs automatically.
‚úÖ Improve Data-Driven Decisions: Optimize ad spend and marketing strategies based on performance insights.
This version ensures proper Markdown structure, includes relevant documentation links, and improves readability. Let me know if you need any further refinements! üöÄ"
Extract license plate number from image uploaded via an n8n form,https://n8n.io/workflows/2911-extract-license-plate-number-from-image-uploaded-via-an-n8n-form/,"What it does
This is a simplistic demo workflow showing how to extract a license plate number from an image of a car submitted via a form ‚Äì or in more general terms showcasing how you can:
use a form trigger to upload files and feed it into an LLM
use a changeable LLM model for image-to-text analysis
Set up steps
Import the workflow
Ensure you have registered and account, purchased some credits and created and API key for OpenRouter.ai
Create/adapt the OpenRouter credential with your indivial API key for OpenRouter
""Test workflow"" and submit an image of a car with license plate to extract its number
How to adapt
By changing the ""prompt"" in th ""Settings"" node you can quickly adapt this exemplatory workflow to other image-to-text use cases, such as:
summarization: ""summarize what's seen in the image""
location finding: ""identify the location where the image was taken""
text extraction: ""extract all text from the image and return it as markdown""
Thanks to using OpenRouter, you also can quickly experiment with finding good model choices by simply changing the ""model"" in the ""Settings"" node. The following models gave good results for this demo use-case:
google/gemini-2.0-flash-001
meta-llama/llama-3.2-90b-vision-instruct
openai/gpt-4o
The llama-3.2-11b and even claude-3.5-sonnet didn't recognize all characters in all test images.
Using a generic LLM-model offers a quick way of prototyping an image-to-text application. For specific use cases in serious and scalable production deployments, consider using an API based service specifically made to that purpose, such as:
Google Cloud Vision API
Microsoft Azure Computer Vision
Azure AI Document Intelligence
Amazon Textract"
AI Virtual TryOn automated generation ü§ñüß† for WooCommerce clothing catalog üëî,https://n8n.io/workflows/2892-ai-virtual-tryon-automated-generation-for-woocommerce-clothing-catalog/,"This AI Agent is designed to streamline the process of creating realistic images of clothing products worn by models, eliminating the need for expensive photoshoots. The primary goal is to automate the generation of virtual try-on images for an eCommerce store selling clothing, using advanced image processing technologies.
Example of results
How It Works
Triggering the Workflow:
The workflow can be triggered manually using the When clicking ‚ÄòTest workflow‚Äô node or automatically via the Schedule Trigger node, which runs the workflow at regular intervals (e.g., every 5 minutes).
Data Retrieval:
The Get new product node retrieves data from a Google Sheets document containing the URLs of the model image, the clothing product image, and the WooCommerce product ID. The document also includes a column for the resulting virtual try-on image URL, which is initially empty.
Setting Up the Request:
The Set data node prepares the data for the AI request by assigning the model image URL and the product image URL to variables.
AI Image Generation:
The Create Image node sends a request to API to generate a virtual try-on image. The request includes the URLs of the model and the clothing product.
When you register for the API service you will get 1$ for free. For continuous work add API credits to your account.
Image Retrieval:
The Get Url image node retrieves the URL of the generated virtual try-on image.
The Get File image node downloads the generated image from the provided URL.
Image Storage:
The Upload Image node uploads the generated image to a specified Google Drive folder for storage.
Updating Google Sheets:
The Update result node updates the Google Sheets document with the URL of the generated virtual try-on image.
Updating WooCommerce:
The Update product node updates the corresponding product in WooCommerce by adding the generated virtual try-on image to the product's image gallery.
Functionality
This AI Agent is designed to streamline the process of creating realistic images of clothing products worn by models, eliminating the need for expensive photoshoots. The primary goal is to automate the generation of virtual try-on images for an eCommerce store selling clothing, using advanced image processing technologies.
By offering realistic images of clothing items worn by models, this automation process saves time and resources, making product catalog management more efficient. This approach not only enhances the competitiveness of the eCommerce store but also provides greater flexibility in creating high-quality visual content adaptable to various digital marketing campaigns.
Key Features
Automated Virtual Try-On: Generates realistic images of models wearing clothing items using AI.
Google Sheets Integration: Retrieves and updates data in Google Sheets for seamless data management.
Image Storage: Uploads generated images to Google Drive for easy access and archiving.
WooCommerce Integration: Updates product pages with virtual try-on images, enhancing the shopping experience.
Scheduled Automation: Can be triggered manually or run at regular intervals for continuous processing.
This workflow is ideal for eCommerce businesses looking to enhance their product catalogs with high-quality, realistic images without the need for costly photoshoots. It leverages AI to provide a cost-effective and efficient solution for virtual try-on image generation."
Connect AI to any chats in Kommo,https://n8n.io/workflows/2841-connect-ai-to-any-chats-in-kommo/,"Entrust customer service to AI using n8n and Kommo!
Using this workflow, you can make the AI agent answer customer questions for your managers.
See how it works in the video.
Advantages of integration
Works with any message channel that is connected to Kommo (telegram, whatsapp, facebook)
Understands voice and text messages
You can stop for a specific transaction or contact if you need a person's help.
It is possible to supplement the AI agent with additional tools to suit your needs
Where it can be useful
In customer support
In the qualification of clients
When invoicing
How it works
Any incoming message to the Kommo chats is sent by the webhook to n8n
n8n processes the webhook according to the specified logic
n8n sends a reply message to the Kommo chat
Installation Steps
Install workflow
Follow the instructions to connect the kommo to the n8n
Set up Credentials for OpenAI
Fill in the Credentials as shown in the workflow notes
Activate Workflow
Write your first message as client"
AI-Generated Summary Block for WordPress Posts,https://n8n.io/workflows/2822-ai-generated-summary-block-for-wordpress-posts/,"What is this workflow?
This n8n template automates the process of adding an AI-generated summary at the top of your WordPress posts.
It retrieves, processes, and updates your posts dynamically, ensuring efficiency and flexibility without relying on a heavy WordPress plugin.
Example of AI Summary Section
How It Works
Triggers ‚Üí Runs on a scheduled interval or via a webhook when a new post is published.
Retrieves posts ‚Üí Fetches content from WordPress and converts HTML to Markdown for AI processing.
AI Summary Generation ‚Üí Uses OpenAI to create a concise summary.
Post Update ‚Üí Inserts the summary at the top of the post while keeping the original excerpt intact.
Data Logging & Notifications ‚Üí Saves processed posts to Google Sheets and notifies a Slack channel.
Why use this workflow?
‚úÖ No need for a WordPress plugin ‚Üí Keeps your site lightweight.
‚úÖ Highly flexible ‚Üí Easily connect with Google Sheets, Slack, or other services.
‚úÖ Customizable ‚Üí Adapt AI prompts, formatting, and integrations to your needs.
‚úÖ Smart filtering ‚Üí Ensures posts are not reprocessed unnecessarily.
üí° Check the detailed sticky notes for setup instructions and customization options!"
Analyse papers from Hugging Face with AI and store them in Notion,https://n8n.io/workflows/2804-analyse-papers-from-hugging-face-with-ai-and-store-them-in-notion/,"This workflow automates the process of retrieving Hugging Face paper summaries, analyzing them with OpenAI, and storing the results in Notion. Here‚Äôs a breakdown of how it works:
‚è∞ Scheduled Trigger:
The flow is set to run automatically at 8 AM on weekdays.
üìÑ Fetching Paper Data:
It fetches Hugging Face paper summaries using their API.
üîç Data Check:
Before processing, the workflow checks if the paper already exists in Notion to avoid duplicates.
ü§ñ Content Analysis with OpenAI:
If the paper is new, it extracts the summary and uses OpenAI to analyze the content.
üì• Store Results in Notion:
After analysis, the summarized data is saved in Notion for easy reference.
‚öôÔ∏è Set Up Steps for Automation
Follow these steps to set up this automated workflow with Hugging Face, OpenAI, and Notion integration:
üîë Obtain API Tokens:
You‚Äôll need the Notion and OpenAI API tokens to authenticate and connect these services with n8n.
üîó Integration in n8n:
Link Hugging Face, OpenAI, and Notion by configuring the appropriate nodes in n8n.
üîß Configure Workflow Logic:
Set up a cron trigger for automatic execution at 8 AM on weekdays.
Use an HTTP request node to fetch Hugging Face paper data.
Add logic to check if the data already exists in Notion.
Set up the OpenAI integration to analyze the paper‚Äôs summary.
Store the results in Notion for easy access and reference.
Result:"
Transform Image to Lego Style Using Line and Dall-E,https://n8n.io/workflows/2738-transform-image-to-lego-style-using-line-and-dall-e/,"Who is this for?
This workflow is designed for:
Content creators, artists, or hobbyists looking to experiment with AI-generated art.
Small business owners or marketers using LEGO-style designs for branding or promotions.
Developers or AI enthusiasts wanting to automate image transformations through messaging platforms like LINE.
What problem is this workflow solving?
Simplifies the process of creating custom AI-generated LEGO-style images.
Automates the manual effort of transforming user-uploaded images into AI-generated artwork.
Bridges the gap between messaging platforms (LINE) and advanced AI tools (DALL¬∑E).
Provides a seamless system for users to upload an image and receive an AI-transformed output without technical expertise.
What this workflow does
Image Upload via LINE:
Users send an image to the LINE chatbot.
AI-Powered Prompt Creation:
GPT generates a prompt to describe the uploaded image for LEGO-style conversion.
AI Image Generation:
DALL¬∑E 3 processes the prompt and creates a LEGO-style isometric image.
Image Delivery:
The generated image is returned to the user in LINE.
Setup
Prerequisites
LINE Developer Account with API credentials.
Access to OpenAI API with DALL¬∑E and GPT-4 capabilities.
A configured n8n instance to run this workflow.
Steps
Environment Setup:
Add your LINE API Token and OpenAI credentials as environment variables (LINE_API_TOKEN, OPENAI_API_KEY) in n8n.
Configure LINE Webhook:
Point the LINE webhook to your n8n instance.
Connect OpenAI:
Set up OpenAI API credentials in the workflow nodes for GPT-4 and DALL¬∑E.
Test Workflow:
Upload a sample image in LINE and ensure it returns the LEGO-style AI image.
How to customize this workflow to your needs
Localization:
Modify response messages in LINE to fit your audience's language and tone.
Integration:
Add nodes to send notifications through other platforms like Slack or email.
Image Style:
Replace the LEGO-style image prompt with other artistic styles or themes.
Advanced Use Cases
Art Contests:
Users upload images and receive AI-enhanced outputs for community voting or branding.
Marketing Campaigns:
Quickly generate creative visual content for ads and promotions using customer-submitted photos.
Education:
Use the workflow to teach students about AI-generated art and automation through a hands-on approach.
Tips for Optimization
Error Handling:
Add fallback nodes to handle invalid images or API errors gracefully.
Logging:
Implement a logging mechanism to track requests and outputs for debugging and analytics.
Scalability:
Use queue-based systems or cloud scaling to handle large volumes of image requests.
Enhancements
Add sticky notes in n8n to provide inline instructions for configuring each node.
Create a tutorial video or documentation for first-time users to set up and customize the workflow.
Include advanced filters to allow users to select from multiple styles beyond LEGO (e.g., pixel art, watercolor).
This workflow enables seamless interaction between messaging platforms and advanced AI capabilities, making it highly versatile for various creative and business applications."
Remove Personally Identifiable Information (PII) from CSV Files with OpenAI,https://n8n.io/workflows/2779-remove-personally-identifiable-information-pii-from-csv-files-with-openai/,"What this workflow does
Monitors Google Drive: The workflow triggers whenever a new CSV file is uploaded.
Uses AI to Identify PII Columns: The OpenAI node analyzes the data and identifies PII-containing columns (e.g., name, email, phone).
Removes PII: The workflow filters out these columns from the dataset.
Uploads Cleaned File: The sanitized file is renamed and re-uploaded to Google Drive, ensuring the original data remains intact.
How to customize this workflow to your needs
Adjust PII Identification: Modify the prompt in the OpenAI node to align with your specific data compliance requirements.
Include/Exclude File Types: Adjust the Google Drive Trigger settings to monitor specific file types (e.g., CSV only).
Output Destination: Change the folder in Google Drive where the sanitized file is uploaded.
Setup
Prerequisites:
A Google Drive account.
An OpenAI API key.
Workflow Configuration:
Configure the Google Drive Trigger to monitor a folder for new files.
Configure the OpenAI Node to connect with your API
Set the Google Drive Upload folder to a different location than the Trigger folder to prevent workflow loops."
AI Data Extraction with Dynamic Prompts and Baserow,https://n8n.io/workflows/2780-ai-data-extraction-with-dynamic-prompts-and-baserow/,"This n8n template introduces the Dynamic Prompts AI workflow pattern which are incredible for certain types of data extraction tasks where attributes are unknown or need to remain flexible.
The general idea behind this pattern is that the prompts for requested attributes to be extracted live outside the template and so can be changed at any time - without needing to edit the template. This seriously cuts down on maintainance requirements and is reusable for any number of tables at little cost.
Check out the n8n Studio Episode here: https://www.youtube.com/watch?v=_fNAD1u8BZw
Community post here: https://community.n8n.io/t/dynamic-prompts-with-n8n-baserow-and-airtable/72052
Looking for the Airtable Version? https://n8n.io/workflows/2771-ai-data-extraction-with-dynamic-prompts-and-airtable/
How it works
Given we have an ""input"" field for context and a number of fields for the data we want to extract, this template will run in the background to react to any changes to either the ""input"" or fields and automatically update the rows accordingly.
The key is that Baserow fields have a special property called the ""field description"". In this pattern, we use this property to allow the user to store a simple prompt describing the data that should exist in the column.
Our n8n template reads these column descriptions aka ""prompts"" to use as instructions to perform tasks on the ""input"".
In this template, the ""input"" is a PDF of a resume/CV and the columns are attributes a HR person would want to extract from it - such as full name, address, last position, years of experience etc.
How to use
First publish this template and ensure it's accessible via webhook URL.
You then have to complete the ""create Baserow webhooks"" steps to configure your baserow to send change events to the n8n template. Baserow webhooks are created in the Baserow web interface.
Check the template for more instructions.
Requirements
Baserow for Tables/Database
OpenAI for LLM and extraction. Feel free to choose another LLM if preferred.
Customising this workflow
If you're not using files, you can replace the ""input"" field with anything you like. For example, the ""input"" could be single line text."
Integrating AI with Open-Meteo API for Enhanced Weather Forecasting,https://n8n.io/workflows/2692-integrating-ai-with-open-meteo-api-for-enhanced-weather-forecasting/,"Use case
Workshop
We are using this workflow in our workshops to teach how to use Tools a.k.a functions with artificial intelligence. In this specific case, we will use a generic ""AI Agent"" node to illustrate that it could use other models from different data providers.
Enhanced Weather Forecasting
In this small example, it's easy to demonstrate how to obtain weather forecast results from the Open-Meteo site to accurately display the upcoming days.
This can be used to plan travel decisions, for example.
What this workflow does
We will make an HTTP request to find out the geographic coordinates of a city.
Then, we will make other HTTP requests to discover the weather for the upcoming days.
In this workshop, we demonstrate that the AI will be able to determine which tool to call first‚Äîit will first call the geolocation tool and then the weather forecast tool. All of this within a single client conversation call.
Setup
Insert an OpenAI Key and activate the workflow.
by Davi Saranszky Mesquita
https://www.linkedin.com/in/mesquitadavi/"
LINE Assistant with Google Calendar and Gmail Integration,https://n8n.io/workflows/2671-line-assistant-with-google-calendar-and-gmail-integration/,"Who is this for?
This workflow is for small business owners, personal assistants, or project managers who rely on multiple platforms for communication and scheduling.
Ideal for users managing customer support, personal scheduling, or group event coordination via LINE, Google Calendar, and Gmail.
What problem is this workflow solving?
Reduces the manual effort needed to manage conversations, schedule events, and handle email communications.
Provides an intelligent system for replying to user messages and fetching relevant calendar or email information in real time.
Bridges the gap between messaging platforms and productivity tools, improving efficiency.
What this workflow does
LINE Chatbot Automation: Automatically processes and responds to messages received via LINE.
Google Calendar Management: Retrieves upcoming events or schedules new events dynamically based on user queries.
Email Retrieval: Fetches recent emails using Gmail and filters them based on user instructions.
AI-Powered Replies: Uses OpenAI GPT to interpret user queries and provide tailored responses.
Setup
Prerequisites:
LINE Developer account and API access.
Google Calendar and Gmail accounts with OAuth credentials.
An n8n instance with access to environment variables.
Steps:
Set up environment variables (LINE_API_TOKEN and DYNAMIC_EMAIL).
Configure API credentials for Google Calendar and Gmail in n8n.
Test the workflow by sending a sample message via LINE.
Enhancements:
Use sticky notes to provide inline instructions for each node.
Include a video walkthrough or a step-by-step document for first-time users.
How to customize this workflow to your needs
Localization: Modify responses in the AI Agent node to match the language and tone of your audience.
Integration: Add more integrations like Slack or Microsoft Teams for additional notifications.
Advanced Filters: Add specific conditions to Gmail or Google Calendar nodes to fetch only relevant data, such as events with specific keywords or emails from certain senders.
Advanced Use Cases
Customer Support: Automatically schedule meetings with clients based on their messages in LINE.
Event Management: Handle RSVP confirmations, event reminders, and email follow-ups for planned events.
Personalized Assistant: Use the workflow to act as a personal virtual assistant that syncs your schedule, replies to messages, and summarizes emails.
Tips for Optimization
Edit Fields Node: Add a centralized node to configure dynamic inputs (e.g., tokens, emails, or thresholds) for easy updates.
Fallback Responses: Use a switch node to handle unrecognized input gracefully and provide clear feedback to users.
Logs and Monitoring: Add nodes to log interactions and track message flows for debugging or analytics.
Let me know if you'd like me to expand on any specific section or add more customization ideas!"
Using External Workflows as Tools in n8n,https://n8n.io/workflows/2713-using-external-workflows-as-tools-in-n8n/,"This guide will show you how to use a workflow as a reusable tool in n8n, such as integrating an AI Agent or other specialized processes into your workflows.
By the end of this example, you'll have a simple, reusable workflow that can be easily plugged into larger projects, making your automations more efficient and scalable.
With this approach, you can create reusable workflows like ""Scrape a Page,"" ""Search Brave,"" or ""Generate an Image,"" which you can then call whenever needed.
While n8n makes it easy to build these workflows from scratch, setting them up as reusable components saves time as your automations grow in complexity.
Setup
Add the ""Execute Workflow Trigger"" node
Add the node(s) to perform the desired tasks in the workflow
Add a final ""Set"" or ""Edit Fields"" node at the end to ensure all external workflows return a consistent output format
Details
In this example, the ""Execute Workflow Trigger"" expects input in the following JSON format:
[
  {
    ""query"": {
      ""url"": ""https://en.wikipedia.org/wiki/some_info""
    }
  }
]
Once your external workflow is ready, you can instruct the AI Agent to use this tool by connecting it to the external workflow. Set up the schema type to ""Generate from JSON Example"" using this structure:
{
  ""url"": ""URL_TO_GET""
}
Finally, ensure your external workflow includes a ""Set"" or ""Edit Fields"" node at the end to define the response format. This helps keep the outputs of your reusable workflows consistent and predictable."
Learn Anything from HN - Get Top Resource Recommendations from Hacker News,https://n8n.io/workflows/2697-learn-anything-from-hn-get-top-resource-recommendations-from-hacker-news/,"Learning something new? Endlessly searching to find the best resources? This workflow finds top community-recommended learning resources on any topic from Hacker News, delivered to your inbox.
How it works
User submits a topic they want to learn via a simple form.
The workflow searches for relevant ""Ask HN"" posts on Hacker News and extracts top-level comments.
An LLM analyzes the comments and identifies the best learning resources.
A personalized email is sent to the user with a Markdown formatted list of top recommendations, categorized by resource type (e.g., book, course, article) and difficulty level.
Set up steps
Add your Google Gemini API credentials. You'll need to create a project and enable the Generative Language API.
Add your SMTP credentials for sending emails.
Customize the Form and email subject (optional)
Activate the workflow
Screenshots for Workflow, Form and Email
Built on Day-03 as part of the #100DaysOfAgenticAi
Fork it, tweak it, have fun!"
Obsidian Notes Read Aloud using AI: Available as a Podcast Feed,https://n8n.io/workflows/2699-obsidian-notes-read-aloud-using-ai-available-as-a-podcast-feed/,"How it works:
Send notes from Obsidian via Webhook to start the audio conversion
OpenAI converts your text to natural-sounding audio and generates episode descriptions
Audio files are stored in Cloudinary and automatically attached to your notes in Obsidian
A professional podcast feed is generated, compatible with all major podcast platforms (Apple, Spotify, Google)
Set up steps:
Install and configure the Post Webhook Plugin in Obsidian
Set up Custom Auth credentials in n8n for Cloudinary using the following JSON:
{
  ""name"": ""Cloudinary API"",
  ""type"": ""httpHeaderAuth"",
  ""authParameter"": {
    ""type"": ""header"",
    ""key"": ""Authorization"",
    ""value"": ""Basic {{Buffer.from('your_api_key:your_api_secret').toString('base64')}}""
  }
}
Configure podcast feed metadata (title, author, cover image, etc.)
Note: The second flow is a generic Podcast Feed module that can be reused in any '[...]-to-Podcast' workflow. It generates a standard RSS feed from Google Sheets data and podcast metadata, making it compatible with all major podcast platforms."
Analyze Suspicious Email Contents with ChatGPT Vision,https://n8n.io/workflows/2665-analyze-suspicious-email-contents-with-chatgpt-vision/,"Phishing Email Detection and Reporting with n8n
Who is this for?
This workflow is designed for IT teams, security professionals, and managed service providers (MSPs) looking to automate the process of detecting, analyzing, and reporting phishing emails.
What problem is this workflow solving?
Phishing emails are a significant cybersecurity threat, and manually detecting and reporting them is time-consuming and prone to errors. This workflow streamlines the process by automating email analysis, generating detailed reports, and logging incidents in a centralized system like Jira.
What this workflow does
This workflow automates phishing email detection and reporting by integrating Gmail and Microsoft Outlook email triggers, analyzing the content and headers of incoming emails, and generating Jira tickets for flagged phishing emails. Here‚Äôs what happens:
Email Triggers: Captures incoming emails from Gmail or Microsoft Outlook.
Email Analysis: Extracts email content, headers, and metadata for analysis.
HTML Screenshot: Converts the email‚Äôs HTML body into a visual screenshot.
AI Phishing Detection: Leverages ChatGPT to analyze the email and detect potential phishing indicators.
Jira Integration: Automatically creates a Jira ticket with detailed analysis and attaches the email screenshot for review by the security team.
Customizable Reports: Includes options to customize ticket descriptions and adapt the workflow to organizational needs.
Setup
Authentication: Set up Gmail and Microsoft Outlook OAuth credentials in n8n to access your email accounts securely.
API Keys: Add API credentials for the HTML screenshot service (hcti.io) and ChatGPT.
Jira Integration: Configure your Jira project and issue types in the workflow.
Workflow Configuration: Update sticky notes and nodes to include any additional setup or configuration details unique to your system.
How to customize this workflow to your needs
Email Filters: Modify email triggers to filter specific subjects or sender addresses.
Analysis Scope: Adjust the ChatGPT prompt to refine phishing detection logic.
Integration: Replace Jira with your preferred ticketing system or modify the ticket fields to include additional information.
This workflow provides an end-to-end automated solution for phishing email management, enhancing efficiency and reducing security risks. It‚Äôs perfect for teams looking to minimize manual effort and improve incident response times."
Vector Database as a Big Data Analysis Tool for AI Agents [3/3 - anomaly],https://n8n.io/workflows/2656-vector-database-as-a-big-data-analysis-tool-for-ai-agents-33-anomaly/,"Vector Database as a Big Data Analysis Tool for AI Agents
Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"".
This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases.
Uploading (image) datasets to Qdrant
Set up meta-variables for anomaly detection in Qdrant
Anomaly detection tool
KNN classifier tool
For anomaly detection
The first pipeline to upload an image dataset to Qdrant.
The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection.
3. This is the third pipeline --- the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset.
For KNN (k nearest neighbours) classification
The first pipeline to upload an image dataset to Qdrant.
The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset.
To recreate both
You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage.
[This workflow] Anomaly Detection Tool
This is the tool that can be used directly for anomalous images (crops) detection.
It takes as input (any) image URL and returns a text message telling if whatever this image depicts is anomalous to the crop dataset stored in Qdrant.
An Image URL is received via the Execute Workflow Trigger, which is used to generate embedding vectors using the Voyage AI Embeddings API.
The returned vectors are used to query the Qdrant collection to determine if the given crop is known by comparing it to threshold scores of each image class (crop type).
If the image scores lower than all thresholds, then the image is considered an anomaly for the dataset."
Vector Database as a Big Data Analysis Tool for AI Agents [1/3 anomaly][1/2 KNN],https://n8n.io/workflows/2654-vector-database-as-a-big-data-analysis-tool-for-ai-agents-13-anomaly12-knn/,"Vector Database as a Big Data Analysis Tool for AI Agents
Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"".
This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases.
Uploading (image) datasets to Qdrant
Set up meta-variables for anomaly detection in Qdrant
Anomaly detection tool
KNN classifier tool
For anomaly detection
1. This is the first pipeline to upload an image dataset to Qdrant.
2. The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection.
3. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset.
For KNN (k nearest neighbours) classification
1. This is the first pipeline to upload an image dataset to Qdrant.
2. The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset.
To recreate both
You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage.
[This workflow] Batch Uploading Images Dataset to Qdrant
This template imports dataset images from Google Could Storage, creates Voyage AI embeddings for them in batches, and uploads them to Qdrant, also in batches. In this particular template, we work with crops dataset. However, it's analogous to uploading lands dataset, and in general, it's adaptable to any dataset consisting of image URLs (as the following pipelines are).
First, check for an existing Qdrant collection to use; otherwise, create it here. Additionally, when creating the collection, we'll create a payload index, which is required for a particular type of Qdrant requests we will use later.
Next, import all (dataset) images from Google Cloud Storage but keep only non-tomato-related ones (for anomaly detection testing).
Create (per batch) embeddings for all imported images using the Voyage AI multimodal embeddings API.
Finally, upload the resulting embeddings and image descriptors to Qdrant via batch upload."
Prompt-based Object Detection with Gemini 2.0,https://n8n.io/workflows/2649-prompt-based-object-detection-with-gemini-20/,"This n8n template demonstrates how to get started with Gemini 2.0's new Bounding Box detection capabilities in your workflows.
The key difference being this enables prompt-based object detection for images which is pretty powerful for things like contextual search over an image. eg. ""Put a bounding box around all adults with children in this image"" or ""Put a bounding box around cars parked out of bounds of a parking space"".
How it works
An image is downloaded via the HTTP node and an ""Edit Image"" node is used to extract the file's width and height.
The image is then given to the Gemini 2.0 API to parse and return coordinates of the bounding box of the requested subjects. In this demo, we've asked for the AI to identify all bunnies.
The coordinates are then rescaled with the original image's width and height to correctl align them.
Finally to measure the accuracy of the object detection, we use the ""Edit Image"" node to draw the bounding boxes onto the original image.
How to use
Really up to the imagination! Perhaps a form of grounding for evidence based workflows or a higher form of image search can be built.
Requirements
Google Gemini for LLM
Customising the workflow
This template is just a demonstration of an experimental version of Gemini 2.0. It is recommended to wait for Gemini 2.0 to come out of this stage before using in production."
Sentiment Analysis Tracking on Support Issues with Linear and Slack,https://n8n.io/workflows/2647-sentiment-analysis-tracking-on-support-issues-with-linear-and-slack/,"This n8n template monitors active support issues in Linear.app to track the mood of their ongoing conversation between reporter and assignee using Sentiment Analysis. When sentiment dips into the negative, a notification is sent via Slack to alert the team.
How it works
A scheduled trigger is used to fetch recently updated issues in Linear using the GraphQL node.
Each issue's comments thread is passed into a simple Information Extractor node to identify the overall sentiment.
The resulting sentiment analysis combined with the some issue details are uploaded to Airtable for review.
When the template is re-run at a later date, each issue is re-analysed for sentiment
Each issue's new sentiment state is saved to the airtable whilst its previous state is moved to the ""previous sentiment"" column.
An Airtable trigger is used to watch for recently updated rows
Each matching Airtable row is filtered to check if it has a previous non-negative state but now has a negative state in its current sentiment.
The results are sent via notification to a team slack channel for priority.
Check out the sample Airtable here: https://airtable.com/appViDaeaFw4qv9La/shrq6HgeYzpW6uwXL
How to use
Modify the GraphQL filter to fetch issues to a relevant issue type, team or person.
Update the Slack channel to ensure messages are sent to the correct location or persons.
The Airtable also serves to give a snapshot of Sentiment across support tickets for a given period. It's possible to use this to assess the daily operations.
Requirements
Linear for issue tracking (but feel free to use another system if preferred)
Airtable for Database
OpenAI for LLM and Sentiment Analysis
Customising the workflow
Add more granular levels of sentiment to reduce the number of alerts.
Explore different types of sentiment based on issue types and customer types. This may help prioritise alerts and response.
Run across teams or categories of issues to get an overview of sentiment across the support organisation."
Intelligent Web Query and Semantic Re-Ranking Flow using Brave and Google Gemini,https://n8n.io/workflows/2643-intelligent-web-query-and-semantic-re-ranking-flow-using-brave-and-google-gemini/,"Workflow Description
This workflow is a powerful, fully automated web query and semantic reranking system that allows users to perform precise, detailed searches, intelligently rank search results and provide high-quality, structured output. Built with AI-powered components, the workflow leverages semantic query generation, result re-ranking, and real-time reporting to deliver actionable insights.
It is particularly well-suited for real-time data retrieval, market research, and any domain requiring automated yet customizable search result processing.
How It Works
Webhook Integration for Input:
The workflow begins with a Webhook Node that captures the user's search query as input, enabling seamless integration with other systems.
Step 1: Semantic Query Generation (Powered by ""Semantic Search - Query Maker""):
Using AI (Google Gemini), the initial query is refined and transformed into a context-aware, expert-level search query.
The process ensures that the search engine retrieves the most relevant and precise results.
Step 2: Web Search Execution:
A free Brave Search API processes the refined query to fetch search results, ensuring speed and cost efficiency.
Step 3: Semantic Re-Ranking of Results (Powered by ""Semantic Search - Result Re-Ranker""):
The workflow reranks the search results based on relevance to the original question, prioritizing the most relevant URLs dynamically.
Results are passed through AI-powered intelligent reranking to ensure the final output reflects optimal relevance and quality.
Step 4: Structured Output Generation:
Results are converted into a well-structured, organized JSON format, ranking the top 10 search results with their titles, links, and descriptions.
Missing ranks (if fewer than 10 results) are handled gracefully with placeholders, ensuring consistency.
Step 5: Real-Time Reporting:
The reranked search results are sent back to the user or integrated system via the Webhook Node in a JSON-formatted response.
Reports are highly structured and ready for downstream processing or consumption.
Key Features
AI-Powered Query Refinement:
Transforms basic queries into detailed, expert-level search terms for optimal results.
Dual-Stage Semantic Search:
Combines query generation and result reranking for precise, high-relevance outputs.
Top 10 Result Reranking:
Dynamically ranks and organizes the top 10 results based on semantic relevance to the query.
Customizable Integration:
Fully modifiable for alternative APIs or integrations, such as other search engines or custom ranking logic.
JSON-Formatted Structured Results:
Outputs reranked results in a standardized format, ideal for integration into systems requiring machine-readable data.
Webhook-Based Flexibility:
Works seamlessly with Webhook inputs for easy deployment in diverse workflows.
Cost-Effective API Usage:
Pre-integrated with the free Brave Search API, minimizing operational costs while delivering accurate search results.
Instructions for API Setup
Brave Search API:
Visit api.search.brave.com to obtain a free-tier API key for web search.
AI Integration (Google Gemini):
Visit Google AI Studio and generate an API key for semantic query generation and reranking.
Webhook Configuration:
Set up the input Webhook to capture search queries and the output Webhook to deliver reranked results.
Why Choose This Workflow?
Precision and Relevance: Combines AI-based query generation with advanced reranking for accurate results.
Fully Customizable: Easily adapt the workflow to alternative APIs, search engines, or ranking logic.
Real-Time Insights: Provides structured, real-time output ready for immediate use.
Scalable and Modular: Ideal for businesses, researchers, and data analysts needing a robust, repeatable solution.
Tags
AI Workflow, Semantic Search, Query Refinement, Search Result Reranking, Real-Time Search, Web Search Automation, Google Search, Brave Search, News Search, API Integration, Market Research, Competitive Intelligence, Business Intelligence,Google Gemini, Anthropic Claude, OpenAI, GPT, LLM"
Extract insights & analyse YouTube comments via AI Agent chat,https://n8n.io/workflows/2636-extract-insights-and-analyse-youtube-comments-via-ai-agent-chat/,"Video Guide
I prepared a detailed guide to help you set up your workflow effectively, enabling you to extract insights from YouTube for content generation using an AI agent.
Youtube Link
Who is this for?
This workflow is ideal for content creators, marketers, and analysts looking to enhance their YouTube strategies through data-driven insights. It‚Äôs particularly beneficial for individuals wanting to understand audience preferences and improve their video content.
What problem does this workflow solve?
Navigating the content generation and optimization process can be complex, especially without significant audience insight. This workflow automates insights extraction from YouTube videos and comments, empowering users to create more engaging and relevant content effectively.
What this workflow does
The workflow integrates various APIs to gather insights from YouTube videos, enabling automated commentary analysis, video transcription, and thumbnail evaluation. The main functionalities include:
Extracting user preferences from comments.
Transcribing video content for enhanced understanding.
Analyzing thumbnails via AI for maximum viewer engagement insights.
AI Insights Extraction: Automatically pulls comments and metrics from selected YouTube creators to evaluate trends and gaps.
Dynamic Video Planning: Uses transcriptions to help creators outline video scripts and topics based on audience interest.
Thumbnail Assessment: Provides analysis on thumbnail designs to improve click-through rates and viewer attraction.
Setup
N8N Workflow
API Setup:
Create a Google Cloud project and enable the YouTube Data API.
Generate an API key to be included in your workflow requests.
YouTube Creator and Video Selection:
Start by defining a request to identify top creators based on their video views.
Capture the YouTube video IDs for further analysis of comments and other video metrics.
Comment Analysis:
Gather comments associated with the selected videos and analyze them for user insights.
Video Transcription:
Utilize the insights from transcriptions to formulate content plans.
Thumbnail Analysis:
Evaluate your video thumbnails by submitting the URL through the OpenAI API to gain insights into their effectiveness."
"Screen Applicants With AI, notify HR and save them in a Google Sheet",https://n8n.io/workflows/2632-screen-applicants-with-ai-notify-hr-and-save-them-in-a-google-sheet/,"What this workflow does
This workflow helps HR teams screen CVs with AI, store compatibility ratings in Google Sheets, and send email notifications to candidates and HR. It simplifies the recruitment process.
CV Submission Form:
Candidates submit their details and CV (PDF) through a web form, triggering the workflow in n8n.
PDF Extraction & AI Rating:
The submitted CV is processed to extract text, and AI analyzes it to generate a compatibility rating.
Results Storage & Notifications:
Ratings are stored in a Google Sheet for easy access and organization.
Confirmation emails are automatically sent to both HR and the candidate.
Setup
Use the provided template to configure your form and connect it to n8n.
Ensure your Google Sheets and email service integrations are active.
Customization Instructions:
Modify the email template to match your organization‚Äôs branding.
Adjust the AI compatibility rating thresholds based on your requirements.
Ensure you have updated the prompt for cv screening."
Deduplicate Scraping AI Grants for Eligibility using AI,https://n8n.io/workflows/2619-deduplicate-scraping-ai-grants-for-eligibility-using-ai/,"This n8n template scrapes a list of AI grants from grants.gov and qualifies them using AI; determining interest and eligibility for the business. It then sends an email alert of interesting items to team members in an email.
The template also shows how you can use the ""Remove Duplicates"" node to simplify deduplication of external listings without the need to manage this yourself.
Not particularly interested in AI Grants? This template works for other tender websites as long as you're able to scrape them.
How it works
A scheduled trigger is set to fetch a list of AI grants listed on the grants.gov website in the past day.
A Remove Duplicates node is used to track Grant IDs to filter out those already processed by the workflow.
New grants are summarized and analysed by AI nodes to determine eligibility and interest which is then saved to an Airtable database.
Another scheduled trigger starts a little later than the first to collect and summarize the new grants
The results are then compiled into an email template using the HTML node, in the form of a newsletter designed to alert and brief team members of new AI grants.
This email is then sent to a list of subscribers using the gmail node.
How to use
Make a copy of sample Airtable here: https://airtable.com/appiNoPRvhJxz9crl/shrRdP6zstgsxjDKL
The filters for fetching the grants is currently set to the ""AI"" category. Feel free to change this to include more categories.
Not interested in grants, this template can works for other sources of leads just change the endpoint and how you're defining the item ID to track.
Requirements
Airtable for database
OpenAI for LLM
Note: These are not hard requirements and can be exchanged for services available to you.
customising the workflow
""Eligibility"" criteria at this stage may be better served by identifying hard blockers instead ie. certifications, geographical considerations or certain legal checks. Be sure to mention any hard blockers into the Eligibility prompt.
Not particularly interested in AI prompts? This template works for other tender websites as long as you're able to scrape them."
"Email Subscription Service with n8n Forms, Airtable and AI",https://n8n.io/workflows/2618-email-subscription-service-with-n8n-forms-airtable-and-ai/,"This n8n template shows how anyone can build a simple newsletter-like subscription service where users can enrol themselves to receive messages/content on a regular basis. It uses n8n forms for data capture, Airtable for database, AI for content generation and Gmail for email sending.
How it works
An n8n form is setup up to allow users to subscribe with a desired topic and interval of which to recieve messages via n8n forms which is then added to the Airtable.
A scheduled trigger is executed every morning and searches for subscribers to send messages for based on their desired intervals.
Once found, Subscribers are sent to a subworkflow which performs the text content generation via an AI agent and also uses a vision model to generate an image.
Both are attached to an email which is sent to the subscriber. This email also includes an unsubscribe link.
The unsubscribe flow works similarly via n8n form interface which when submitted disables further scheduled emails to the user.
How to use
Make a copy of sample Airtable here: https://airtable.com/appL3dptT6ZTSzY9v/shrLukHafy5bwDRfD
Make sure the workflow is ""activated"" and the forms are available and reachable by your audience.
Requirements
Airtable for Database
OpenAI for LLM (but compatible with others)
Gmail for Email (but can be replaced with others)
Customising this workflow
This simple use can be extended to deliver any types of content such as your company newsletter, promotions, social media posts etc.
Doesn't have to be limited to just email - try social messaging, Whatsapp, Telegram and others."
Get Airtable data via AI and Obsidian Notes,https://n8n.io/workflows/2615-get-airtable-data-via-ai-and-obsidian-notes/,"I am submitting this workflow for the Obsidian community to showcase the potential of integrating Obsidian with n8n. While straightforward, it serves as a compelling demonstration of the potential unlocked by integrating Obsidian with n8n.
How it works
This workflow lets you retrieve specific Airtable data you need in seconds, directly within your Obsidian note, using n8n. By highlighting a question in Obsidian and sending it to a webhook via the Post Webhook Plugin, you can fetch specific data from your Airtable base and instantly insert the response back into your note. The workflow leverages OpenAI‚Äôs GPT model to interpret your query, extract relevant data from Airtable, and format the result for seamless integration into your note.
Set up steps
Install the Post Webhook Plugin: Add this plugin to your Obsidian vault from the plugin store or GitHub.
Set up the n8n Webhook: Copy the webhook URL generated in this workflow and insert it into the Post Webhook Plugin's settings in Obsidian.
Configure Airtable Access: Link your Airtable account and specify the desired base and table to pull data from.
Test the Workflow: Highlight a question in your Obsidian note, use the ‚ÄúSend Selection to Webhook‚Äù command, and verify that data is returned as expected."
Research AI Agent Team with auto citations using OpenRouter and Perplexity,https://n8n.io/workflows/2607-research-ai-agent-team-with-auto-citations-using-openrouter-and-perplexity/,"Purpose of workflow:
This AI-powered workflow is designed to automatically generate comprehensive, well-researched articles on any given topic. It utilizes a team of AI agents to streamline the research and writing process, producing high-quality content with proper citations and credible sources.
How it works
Multi-agent team:
Research Leader: Plans and conducts initial research, creating a table of contents.
Project Planner: Breaks down the table of contents into manageable sections.
Research Assistants: Multiple agents that conduct in-depth research on assigned sections.
Editor: Compiles and refines the final article, ensuring coherence and proper citations.
Key features:
Utilizes Perplexity AI for internet search and citation capabilities
Produces well-structured articles with proper citations
Customizable parameters (topic, tone, word count, number of sections)
Step by step setup:
Get account from OpenRouter.ai to access Perplexity API
Set API key in the Perplexity API node
Credential key name : Authorization and key value Bearer <api-key value>"
Time logging on Clockify using Slack,https://n8n.io/workflows/2604-time-logging-on-clockify-using-slack/,"Time Logging on Clockify Using Slack
How it works
This workflow simplifies time tracking for teams and agencies by integrating Slack with Clockify. It enables users to log, update, or delete time entries directly within Slack, leveraging an AI-powered assistant for seamless and conversational interactions. Key features include:
Effortless Time Logging: Create and manage time entries in Clockify without leaving Slack.
AI-Powered Assistant: Get step-by-step guidance to ensure accurate and efficient time logging.
Project and Client Management: Retrieve project and client information from Clockify effortlessly.
Overlap Prevention: Avoid overlapping entries with built-in time validation.
Automated Descriptions: Generate ethical, grammatically correct descriptions for time logs.
Set up steps
1. Prepare your integrations
Ensure you have active accounts for both Slack and Clockify.
Generate your Clockify API credentials for integration.
2. Import the workflow
Download and import the workflow template into your n8n instance.
Configure the workflow to connect with your Slack and Clockify accounts.
3. Configure the workflow
Add your Clockify API credentials in the workflow settings.
Set up the Slack Trigger to listen for app mentions or specific commands.
4. Test the workflow
Use Slack to create a time entry and verify it in Clockify.
Test updating and deleting existing entries to ensure smooth functionality.
Check for any overlapping time logs or incorrect data entries.
Why use this workflow?
Efficiency: Eliminate the need to switch between tools for time tracking.
Accuracy: AI-driven validation ensures error-free entries.
Automation: Simplify repetitive tasks like updating or deleting time logs.
Proactive Guidance: Conversational assistant ensures smooth operations."
Optimize & Update Printify Title and Description Workflow,https://n8n.io/workflows/2583-optimize-and-update-printify-title-and-description-workflow/,"Printify Automation - Update Title and Description Workflow
This n8n workflow automates the process of retrieving products from Printify, generating optimized product titles and descriptions, and updating them back to the platform. It leverages OpenAI for content generation and integrates with Google Sheets for tracking and managing updates.
Features
Integration with Printify: Fetch shops and products through Printify's API.
AI-Powered Optimization: Generate engaging product titles and descriptions using OpenAI's GPT model.
Google Sheets Tracking: Log and manage updates in Google Sheets.
Custom Brand Guidelines: Ensure consistent tone by incorporating brand-specific instructions.
Loop Processing: Iteratively process each product in batches.
Workflow Structure
Nodes Overview
Manual Trigger: Manually start the workflow for testing purposes.
Printify - Get Shops: Retrieves the list of shops from Printify.
Printify - Get Products: Fetches product details for each shop.
Split Out: Breaks down the product list into individual items for processing.
Loop Over Items: Iteratively processes products in manageable batches.
Generate Title and Desc: Uses OpenAI GPT to create optimized product titles and descriptions.
Google Sheets Integration:
Trigger: Monitors Google Sheets for changes.
Log Updates: Records product updates, including old and new titles/descriptions.
Conditional Logic:
If Nodes: Ensure products are ready for updates and stop processing once completed.
Printify - Update Product: Sends updated titles and descriptions back to Printify.
Brand Guidelines + Custom Instructions: Sets brand tone and seasonal instructions.
Setup Instructions
Prerequisites
n8n Instance: Ensure n8n is installed and configured.
Printify API Key:
Obtain an API key from your Printify account.
Add it to n8n under HTTP Header Auth.
OpenAI API Key:
Obtain an API key from OpenAI.
Add it to n8n under OpenAI API.
Google Sheets Integration:
Share your Google Sheets with the Google API service account.
Configure Google Sheets credentials in n8n.
Workflow Configuration
Set Brand Guidelines:
Update the Brand Guidelines + Custom Instructions node with your brand name, tone, and seasonal instructions.
Batch Size:
Configure the Loop Over Items node for optimal batch sizes.
Google Sheets Configuration:
Set the correct Google Sheets document and sheet names in the integration nodes.
Run the Workflow:
Start manually or configure the workflow to trigger automatically.
Key Notes
Customization:
Modify API calls to support other platforms like Printful or Vistaprint.
Scalability:
Use batch processing for efficient handling of large product catalogs.
Error Handling:
Configure retries or logging for any failed nodes.
Output Examples
Optimized Content Example
Input Title: ""Classic White T-Shirt""
Generated Title: ""Stylish Classic White Tee for Everyday Wear""
Input Description: ""Plain white T-shirt made of cotton.""
Generated Description: ""Discover comfort and style with our classic white tee, crafted from premium cotton for all-day wear. Perfect for casual outings or layering.""
Next Steps
Monitor Updates:
Use Google Sheets to review logs of updated products.
Expand Integration:
Add support for more Printify shops or integrate with other platforms.
Enhance AI Prompts:
Customize prompts for different product categories or seasonal needs.
Feel free to reach out for additional guidance or troubleshooting!"
Call analyzer with AssemblyAI transcription and OpenAI assistant integration,https://n8n.io/workflows/2547-call-analyzer-with-assemblyai-transcription-and-openai-assistant-integration/,"Video Guide
I prepared a detailed guide that showed the whole process of building a call analyzer.
Who is this for?
This workflow is ideal for sales teams, customer support managers, and online education services that conduct follow-up calls with clients. It‚Äôs designed for those who want to leverage AI to gain deeper insights into client needs and upsell opportunities from recorded calls.
What problem does this workflow solve?
Many follow-up sales calls lack structured analysis, making it challenging to identify client needs, gauge interest levels, or uncover upsell opportunities. This workflow enables automated call transcription and AI-driven analysis to generate actionable insights, helping teams improve sales performance, refine client communication, and streamline upselling strategies.
What this workflow does
This workflow transcribes and analyzes sales calls using AssemblyAI, OpenAI, and Supabase to store structured data. The workflow processes recorded calls as follows:
Transcribe Call with AssemblyAI: Converts audio into text with speaker labels for clarity.
Analyze Transcription with OpenAI: Using a predefined JSON schema, OpenAI analyzes the transcription to extract metrics like client intent, interest score, upsell opportunities, and more.
Store and Access Results in Supabase: Stores both transcription and analysis data in a Supabase database for further use and display in interfaces.
Setup
Preparation
Create Accounts: Set up accounts for N8N, Supabase, AssemblyAI, and OpenAI.
Get Call Link: Upload audio files to public Supabase storage or Dropbox to generate a direct link for transcription.
Prepare Artifacts for OpenAI:
Define Metrics: Identify business metrics you want to track from call analysis, such as client needs, interest score, and upsell potential.
Generate JSON Schema: Use GPT to design a JSON schema for structuring OpenAI‚Äôs responses, enabling efficient storage, analysis, and display.
Create Analysis Prompt: Write a detailed prompt for GPT to analyze calls based on your metrics and JSON schema.
Scenario 1: Transcribe Call with AssemblyAI
Set Up Request:
Header Authentication: Set Authorization with AssemblyAI API key.
URL: POST to https://api.assemblyai.com/v2/transcript/.
Parameters:
audio_url: Direct URL of the audio file.
webhook_url: URL for an N8N webhook to receive the transcription result.
Additional Settings:
speaker_labels (true/false): Enables speaker diarization.
speakers_expected: Specify expected number of speakers.
language_code: Set language (default: en_us).
Scenario 2: Process Transcription with OpenAI
Webhook Configuration: Set up a POST webhook to receive AssemblyAI‚Äôs transcription data.
Get Transcription:
Header Authentication: Set Authorization with AssemblyAI API key.
URL: GET https://api.assemblyai.com/v2/transcript/&lt;transcript_id&gt;.
Send to OpenAI:
URL: POST to https://api.openai.com/v1/chat/completions.
Header Authentication: Set Authorization with OpenAI API key.
Body Parameters:
Model: Use gpt-4o-2024-08-06 for JSON Schema support, or gpt-4o-mini for a less costly option.
Messages:
system: Contains the main analysis prompt.
user: Combined speakers‚Äô utterances to analyze in text format.
Response Format:
type: json_schema.
json_schema: JSON schema for structured responses.
Save Results in Supabase:
Operation: Create a new record.
Table Name: demo_calls.
Fields:
Input: Transcription text, audio URL, and transcription ID.
Output: Parsed JSON response from OpenAI‚Äôs analysis."
üìÑüåêPDF2Blog - Create Blog Post on Ghost CRM from PDF Document,https://n8n.io/workflows/2522-pdf2blog-create-blog-post-on-ghost-crm-from-pdf-document/,"From PDF to Powerful Blog Posts: AI-Powered Content Transformation
Turn complex documents into engaging digital content that drives results. This n8n Workflow uses AI to transforms lengthy PDFs into compelling blog posts that attract and retain readers while you focus on strategic initiatives.
Time-Saving Innovation
üöÖLightning-Fast Processing
Transform lengthy documents into polished blog content in under 1 minute, eliminating hours of manual work. Our system handles the heavy lifting, delivering up to a 95% reduction in content production time.
üì±Intelligent Analysis
The AI engine identifies and extracts key insights, organizing information for maximum impact. Each document undergoes comprehensive analysis to ensure no valuable content is overlooked.
Advanced Content Optimization
‚úçÔ∏èDynamic Writing Styles Possible
Adjust the prompt for multiple tone options:
Professional for corporate communications
Conversational for engaging blogs
Thought leadership for industry authority
üìäSEO-Ready Content Potential
Adjust the prompt to automatically optimized for search engines, incorporating relevant keywords and semantic structure to improve visibility and drive organic traffic.
Ideal Applications
ü§ºContent Marketing Teams
Scale content production without sacrificing quality or consistency. Perfect for teams looking to maintain a robust publishing schedule while maximizing resource efficiency.
üè´Academic Communication
Help researchers and institutions share complex findings with broader audiences through accessible, engaging content that maintains academic integrity.
üßë‚ÄçüíªDigital Publishers
Streamline the content transformation process while ensuring each piece meets modern digital standards and reader expectations.
Transform your content strategy with an intelligent system that delivers consistent, high-quality results while dramatically reducing production time."
Send Google analytics data to A.I. to analyze then save results in Baserow,https://n8n.io/workflows/2517-send-google-analytics-data-to-ai-to-analyze-then-save-results-in-baserow/,"Who's this for?
If you own a website and need to analyze your Google analytics data
If you need to create an SEO report on which pages are getting most traffic or how your google search terms are performing
If you want to grow your site based on suggestions from data

Use case
Instead of hiring an SEO expert, I run this report weekly. It checks compares the data from this week to the week before:
Views based on countries
The top performing pages
Google search console performance
Watch youtube tutorial here
Get my SEO A.I. agent system here
Read my detailed case study here
How it works
The workflow gathers google analytics for the past 7 days then it gathers the data for the week before for comparison.
It does this 3 times to get: views per country, engagement per page and google search console results for organic search results.
The google analytics nodes has already chosen the correct dimensions and metrics.
At the end, it passes the data to openrouter.ai for A.I. analyse.
Finally it saves to baserow.
How to use this
Input your Google analytics credentials
Input your property ID
Input your Openrouter.ai credentials
Input your baserow credentials
You will need to create a baserow database with columns: Name, Country Views, Page Views, Search Report, Blog (name of your blog).
Created by Rumjahn"
Summarize your emails with A.I. (via Openrouter) and send to Line messenger,https://n8n.io/workflows/2515-summarize-your-emails-with-ai-via-openrouter-and-send-to-line-messenger/,"Who is this template for?
Anyone who is drowning in emails
Busy parents who has alot of school emails
Busy executives with too many emails
Case Study
I get too many emails from my kid's school about soccer practice, lunch orders and parent events. I use this workflow to read all the emails and tell me what is important and what requires actioning.
Read more -> How I used A.I. to read all my emails
What this workflow does
It uses IMAP to read the emails from your email account (i.e. Gmail).
It then passes the email to Openrouter.ai and uses a free A.I. model to read and summarize the email.
It then sends the summary as a message to your messenger (i.e. Line).
Setup
You need to find your email server IMAP credentials.
Input your openrouter.ai API credentials or replace the HTTP request node with an A.I. node such as OpenAI.
Input your messenger credentials. I use Line but you can change the node to another messenger line Telegram.
You need to change the message ID to your ID inside the http request. You can find your user ID inside the https://developers.line.biz/console/. Change the ""to"": {insert your user ID}.
How to adjust it to your needs
You can change the A.I. prompt to fit your needs by telling it to mark emails from a certain address as important.
You can change the A.I. model from the current meta-llama/llama-3.1-70b-instruct:free to a paid model or other free models.
You can change the messenger node to telegram or any other messenger app you like."
Create LinkedIn Contributions with AI and Notify Users On Slack,https://n8n.io/workflows/2491-create-linkedin-contributions-with-ai-and-notify-users-on-slack/,"This workflow automates the process of gathering LinkedIn advice articles, extracting their content, and generating unique contributions for each article using an AI model. The contributions are then posted to a Slack channel and a NocoDB database for record-keeping. The workflow is triggered weekly to ensure new articles are continuously collected and responded to.
Who is this for?
This workflow is designed for professionals, marketers, and content creators looking to boost their LinkedIn presence by regularly engaging with LinkedIn advice articles. It‚Äôs especially useful for those who want to be seen as a ""thought leader"" or ""top voice"" in their niche by contributing relevant and unique advice to trending topics.
What problem is this workflow solving?
Manually searching for relevant LinkedIn articles, reading through them, and crafting thoughtful contributions can be time-consuming. This workflow solves that by automating the process of finding new articles, extracting key content, and generating AI-powered contributions. It helps users stay consistently active on LinkedIn, contributing value to trending discussions.
What this workflow does
Triggers Weekly: The workflow is set to run every Monday at 8:00 AM.
Search Google for LinkedIn Advice Articles: Uses a predefined Google search URL to find the latest LinkedIn advice articles based on the user's area of expertise.
Extract LinkedIn Article Links: A code node extracts all LinkedIn advice article links from the search results.
Retrieve Article Content: For each article link, the workflow retrieves the HTML content and extracts the article title, topics, and existing contributions.
Generate AI-Powered Contributions: The workflow sends the extracted article content to an AI model, which generates unique, helpful advice for each topic within the article.
Post to Slack & NocoDB: The AI-generated contributions, along with the article links, are posted to a designated Slack channel and stored in a NocoDB database for future reference.
Setup
Google Search URL: Update the Google search URL with the relevant LinkedIn advice query for your field (e.g., ""site:linkedin.com/advice 'marketing automation'"").
Slack Integration: Connect your Slack account and specify the Slack channel where you want the contributions to be posted.
NocoDB Integration: Set up your NocoDB project to store the generated contributions along with the article titles and links.
How to customize this workflow
Change Search Terms: Modify the Google search URL to focus on a different LinkedIn topic or expertise area.
Adjust Trigger Frequency: The workflow is set to run weekly, but you can adjust the frequency by changing the schedule trigger.
Enhance Contribution Quality: Customize the AI model's prompt to generate contributions that align with your brand voice or content strategy.
Workflow Summary
This workflow helps users maintain a consistent presence on LinkedIn by automating the discovery of new advice articles and generating unique contributions using AI. It is ideal for professionals who want to engage with LinkedIn content regularly without spending too much time manually searching and drafting responses."
Convert URL HTML to Markdown Format and Get Page Links,https://n8n.io/workflows/2476-convert-url-html-to-markdown-format-and-get-page-links/,"Use Case
Transform web pages into AI-friendly markdown format:
You need to process webpage content for LLM analysis
You want to extract both content and links from web pages
You need clean, formatted text without HTML markup
You want to respect API rate limits while crawling pages
What this Workflow Does
The workflow uses Firecrawl.dev API to process webpages:
Converts HTML content to markdown format
Extracts all links from each webpage
Handles API rate limiting automatically
Processes URLs in batches from your database
Setup
Create a Firecrawl.dev account and get your API key
Add your Firecrawl API key to the HTTP Request node's Authorization header
Connect your URL database to the input node (column name must be ""Page"") or edit the array in Example fields from data source
Configure your preferred output database connection
How to Adjust it to Your Needs
Modify input source to pull URLs from different databases
Adjust rate limiting parameters if needed
Customize output format for your specific use case
More templates and n8n workflows >>> @simonscrapes"
"Scale Deal Flow with a Pitch Deck AI Vision, Chatbot and QDrant Vector Store",https://n8n.io/workflows/2464-scale-deal-flow-with-a-pitch-deck-ai-vision-chatbot-and-qdrant-vector-store/,"Are you a popular tech startup accelerator (named after a particular higher order function) overwhelmed with 1000s of pitch decks on a daily basis? Wish you could filter through them quickly using AI but the decks are unparseable through conventional means? Then you're in luck!
This n8n template uses Multimodal LLMs to parse and extract valuable data from even the most overly designed pitch decks in quick fashion. Not only that, it'll also create the foundations of a RAG chatbot at the end so you or your colleagues can drill down into the details if needed. With this template, you'll scale your capacity to find interesting companies you'd otherwise miss!
Requires n8n v1.62.1+
How It Works
Airtable is used as the pitch deck database and PDF decks are downloaded from it.
An AI Vision model is used to transcribe each page of the pitch deck into markdown.
An Information Extractor is used to generate a report from the transcribed markdown and update required information back into pitch deck database.
The transcribed markdown is also uploaded to a vector store to build an AI chatbot which can be used to ask questions on the pitch deck.
Check out the sample Airtable here: https://airtable.com/appCkqc2jc3MoVqDO/shrS21vGqlnqzzNUc
How To Use
This template depends on the availability of the Airtable - make a duplicate of the airtable (link) and its columns before running the workflow.
When a new pitchdeck is received, enter the company name into the Name column and upload the pdf into the File column. Leave all other columns blank.
If you have the Airtable trigger active, the execution should start immediately once the file is uploaded. Otherwise, click the manual test trigger to start the workflow.
When manually triggered, all ""new"" pitch decks will be handled by the workflow as separate executions.
Requirements
OpenAI for LLM
Airtable For Database and Interface
Qdrant for Vector Store
Customising This Workflow
Extend this starter template by adding more AI agents to validate claims made in the pitch deck eg. Linkedin Profiles, Page visits, Reviews etc."
Enrich Company Data from Google Sheet with OpenAI Agent and ScrapingBee,https://n8n.io/workflows/2463-enrich-company-data-from-google-sheet-with-openai-agent-and-scrapingbee/,"This workflow demonstrates how to enrich data from a list of companies in a spreadsheet. While this workflow is production-ready if all steps are followed, adding error handling would enhance its robustness.
Important notes
Check legal regulations: This workflow involves scraping, so make sure to check the legal regulations around scraping in your country before getting started. Better safe than sorry!
Mind those tokens: OpenAI tokens can add up fast, so keep an eye on usage unless you want a surprising bill that could knock your socks off! üí∏
Main Workflow
Node 1 - Webhook
This node triggers the workflow via a webhook call. You can replace it with any other trigger of your choice, such as form submission, a new row added in Google Sheets, or a manual trigger.
Node 2 - Get Rows from Google Sheet
This node retrieves the list of companies from your spreadsheet.
here is the Google Sheet Template you can use.
The columns in this Google Sheet are:
Company: The name of the company
Website: The website URL of the company
These two fields are required at this step.
Business Area: The business area deduced by OpenAI from the scraped data
Offer: The offer deduced by OpenAI from the scraped data
Value Proposition: The value proposition deduced by OpenAI from the scraped data
Business Model: The business model deduced by OpenAI from the scraped data
ICP: The Ideal Customer Profile deduced by OpenAI from the scraped data
Additional Information: Information related to the scraped data, including:
Information Sufficiency:
Description: Indicates if the information was sufficient to provide a full analysis.
Options: ""Sufficient"" or ""Insufficient""
Insufficient Details:
Description: If labeled ""Insufficient,"" specifies what information was missing or needed to complete the analysis.
Mismatched Content:
Description: Indicates whether the page content aligns with that of a typical company page.
Suggested Actions:
Description: Provides recommendations if the page content is insufficient or mismatched, such as verifying the URL or searching for alternative sources.
Node 3 - Loop Over Items
This node ensures that, in subsequent steps, the website in ""extra workflow input"" corresponds to the row being processed. You can delete this node, but you'll need to ensure that the ""query"" sent to the scraping workflow corresponds to the website of the specific company being scraped (rather than just the first row).
Node 4 - AI Agent
This AI agent is configured with a prompt to extract data from the content it receives. The node has three sub-nodes:
OpenAI Chat Model: The model used is currently gpt4-o-mini.
Call n8n Workflow: This sub-node calls the workflow to use ScrapingBee and retrieves the scraped data.
Structured Output Parser: This parser structures the output for clarity and ease of use, and then adds rows to the Google Sheet.
Node 5 - Update Company Row in Google Sheet
This node updates the specific company's row in Google Sheets with the enriched data.
Scraper Agent Workflow
Node 1 - Tool Called from Agent
This is the trigger for when the AI Agent calls the Scraper. A query is sent with:
Company name
Website (the URL of the website)
Node 2 - Set Company URL
This node renames a field, which may seem trivial but is useful for performing transformations on data received from the AI Agent.
Node 3 - ScrapingBee: Scrape Company's Website
This node scrapes data from the URL provided using ScrapingBee. You can use any scraper of your choice, but ScrapingBee is recommended, as it allows you to configure scraper behavior directly. Once configured, copy the provided ""curl"" command and import it into n8n.
Node 4 - HTML to Markdown
This node converts the scraped HTML data to Markdown, which is then sent to OpenAI. The Markdown format generally uses fewer tokens than HTML.
Improving the Workflow
It's always a pleasure to share workflows, but creators sometimes want to keep some magic to themselves ‚ú®. Here are some ways you can enhance this workflow:
Handle potential errors
Configure the scraper tool to scrape other pages on the website. Although this will cost more tokens, it can be useful (e.g., scraping ""Pricing"" or ""About Us"" pages in addition to the homepage).
Instead of Google Sheets, connect directly to your CRM to enrich company data.
Trigger the workflow from form submissions on your website and send the scraped data about the lead to a Slack or Teams channel."
Automated Agentic News Event Monitoring with perplexity.ai,https://n8n.io/workflows/2458-automated-agentic-news-event-monitoring-with-perplexityai/,"Purpose of workflow:
The purpose of this workflow is to create a news reporter AI agent that automatically monitors specific news events and sends summary emails to the user.
This tool aims to keep users up-to-date with the latest happenings on topics of interest without the need to constantly check multiple news sources manually. It's particularly useful for investors or researchers who must stay informed about specific events that may impact their work or investments.
How it works:
It leverages the power of large language models, specifically Perplexity's (perplexity.ai) online models accessed through Open Router (openrouter.ai), to understand and summarize up-to-date news.
The workflow is scheduled to run at predetermined intervals (e.g., daily at 7 AM), automatically scanning for news on the specified topic, generating a summary, and sending it via email to the user.
Setup:
Sign up and generate an API key from Openrouter.ai This provides access to Perplexity's online language models
Update Perplexity node with Openrouter.ai API key
Specify the event/topic to monitor in the News Reporter node
Activate workflow to turn on scheduling feature"
Text automations using Apple Shortcuts,https://n8n.io/workflows/2456-text-automations-using-apple-shortcuts/,"Overview
This workflow answers user requests sent via Mac Shortcuts
Several Shortcuts call the same webhook, with a query and a type of query
Types of query are:
translate to english
translate to spanish
correct grammar (without changing the actual content)
make content shorter
make content longer
How it works
Select a text you are writing
Launch the shortcut
The text is sent to the webhook
Depending on the type of request, a different prompt is used
Each request is sent to an OpenAI node
The workflow responds to the request with the response from GPT
Shortcut replace the selected text with the new one
For a demo and setup instructions:
How to use it
Activate the workflow
Download this Shortcut template
Install the shortcut
In step 2 of the shortcut, change the url of the Webhook
In Shortcut details, ""add Keyboard Shortcut"" with the key you want to use to launch the shortcut
Go to settings, advanced, check ""Allow running scripts""
You are ready to use the shortcut. Select a text and hit the keyboard shortcut you just defined"
Automated Email Marketing Campaign Workflow,https://n8n.io/workflows/2452-automated-email-marketing-campaign-workflow/,"This n8n workflow demonstrates how to automate a large-scale personalized promotional email campaign, leveraging artificial intelligence to generate unique content for each recipient.
Save time and increase the effectiveness of your marketing campaigns by allowing AI to handle the creation of personalized content and email sending.
How it works:
The workflow fetches recipient data from a Google Sheets document.
It validates email addresses and checks if recipients haven't been contacted before.
For each valid recipient, the AI agent (using GPT-3.5) generates a personalized email including the recipient's name, product introduction, exclusive offer, and a unique promotional link.
The personalized email is sent via SMTP to the recipient.
The workflow updates the Google Sheet to mark the recipient as contacted.
A random delay is introduced between emails to mimic natural sending patterns and avoid triggering spam filters.
Requirements:
Google Sheets account for storing and managing recipient data.
OpenAI account for access to the GPT-3.5 model.
SMTP server for sending emails.
Customizing the workflow:
This example focuses on sending personalized promotional emails, but it could be extended to include follow-up sequences, tracking email opens and clicks, or integrating with a CRM system for more comprehensive customer management."
üöÄ Local Multi-LLM Testing & Performance Tracker,https://n8n.io/workflows/2442-local-multi-llm-testing-and-performance-tracker/,"üöÄ Local Multi-LLM Testing & Performance Tracker
This workflow is perfect for developers, researchers, and data scientists benchmarking multiple LLMs with LM Studio. It dynamically fetches active models, tests prompts, and tracks metrics like word count, readability, and response time, logging results into Google Sheets. Easily adjust temperature üî• and top P üéØ for flexible model testing.
Level of Effort:
üü¢ Easy ‚Äì Minimal setup with customizable options.
Setup Steps:
Install LM Studio and configure models.
Update IP to connect to LM Studio.
Create a Google Sheet for result tracking.
Key Outcomes:
Benchmark LLM performance.
Automate results in Google Sheets for easy comparison.
Version 1.0"
Convert image to text using GROQ LLaVA V1.5 7B,https://n8n.io/workflows/2437-convert-image-to-text-using-groq-llava-v15-7b/,"What this template does
This template uses GROQ LLAVA V1.5 7B API that offers fast inference for multimodal models with vision capabilities for understanding and interpreting visual data from images. .
The users send a image and get a description of the image from the model.
Setup
Open the Telegram app and search for the BotFather user (@BotFather)
Start a chat with the BotFather
Type /newbot to create a new bot
Follow the prompts to name your bot and get a unique API token
Save your access token and username
Once you set your bot, you can send the image, and get the descriptions."
Enrich FAQ sections on your website pages at scale with AI,https://n8n.io/workflows/2434-enrich-faq-sections-on-your-website-pages-at-scale-with-ai/,"This n8n workflow template lets you easily generate comprehensive FAQ (Frequently Asked Questions) content for multiple services (or any items or pages you need to add the FAQs to). Simply provide the Google Sheets document containing the items to scrape, and the workflow automatically creates detailed, AI-enhanced FAQ documents.
How it works
The workflow reads data from a Google Sheets document containing information about different services and categories (again, in your case - whatever objects you need).
For each service and category, it generates a set of standard questions and answers covering setup, permissions, integrations, use cases, and pricing benefits.
An AI model (OpenAI's GPT) is used to enhance or complete some of the answers, making the content more comprehensive and natural-sounding.
The workflow formats the Q&A pairs, combining AI-generated content with predefined answers where applicable.
It creates a text file (JSON) for each service or category, containing the formatted Q&A pairs.
The generated files are saved to specific folders in Google Drive, organized by the type of integration (native, credential-only, non-native) or category.
After processing each service or category, it updates the status in the original Google Sheets document to mark it as completed.
Ideal for:
Marketing teams: Rapidly create comprehensive FAQ documents for multiple products or services.
Customer support: Generate consistent and detailed answers for common customer queries.
Product managers: Easily maintain up-to-date documentation as products evolve.
Content creators: Streamline the process of creating informative content about various offerings.
Accounts required
Google account (for Google Sheets and Google Drive)
OpenAI API account (for AI-enhanced content generation)
n8n.io account (for workflow execution)
Set up instructions
Set up the required credentials for Google Sheets, Google Drive, and OpenAI when you first open the workflow.
Prepare your Google Sheets document with the service/category information. Here's an example of Google Sheet.
Fill the ""Define Sheets"" node with your sheets
Adjust the folder IDs in the ""Prepare Job"" node to match your Google Drive structure.
Configure the OpenAI model settings in the ""OpenAI Chat Model"" node if needed.
Test the workflow with a small subset of data before running it on your entire dataset.
Adjust the questions asked in the ""Create your Q&A templates"" section
After testing, activate your workflow for automated FAQ generation.
üôè Big, big kudos to Jim Le for his ideas, input and support when building this workflow. Your approach to AI workflows is always super helpful!"
Visual Regression Testing with Apify and AI Vision Model,https://n8n.io/workflows/2419-visual-regression-testing-with-apify-and-ai-vision-model/,"This n8n workflow is a proof-of-concept template exploring how we might work with multimodal LLMs and their multi-image analysis capabilities. In this demo, we compare 2 screenshots of a webpage taken at different timestamps and pass both to our multimodal LLM for a visual comparison of differences. Handling multiple binary inputs (ie. images) in an AI request is supported by n8n's basic LLM node.
How it works
This template is intended to run as 2 parts: first to generate the base screenshots and next to run the visual regression test which captures fresh screenshots.
Starting with a list of webpages captured in a Google sheet, base screenshots are captured for each using a external web scraping service called Apify.com (I prefer Apify but feel free to use whichever web scraping service available to you)
These base screenshots are uploaded to Google Drive and will be referenced later when we run our testing.
Phase 2 of the workflow, we'll use a scheduled trigger to fire sometime in the future which will reuse our web scraping service to generate fresh screenshots of our desired webpages.
Next, re-download our base screenshots in parallel and with both old and new captures, we'll pass these to our LLM node. In the LLM node's options, we'll define 2 ""user message"" inputs with the type of binary (data) for our images.
Finally, we'll prompt our LLM with our testing criteria and capture the regressions detected. Note, results will vary depending on which LLM you use.
A final report can be generated using the LLM's output and is uploaded to Linear.
Requirements
Apify.com API key for web screenshotting service
Google Drive and Sheets access to store list of webpages and captures
Customising this workflow
Have your own preferred web screenshotting service? Feel free to swap out Apify with your service of choice.
If the web screenshot is too large, it may prove difficult for the LLM to spot differences with precision. Try splitting up captures into smaller images instead."
KB Tool - Confluence Knowledge Base,https://n8n.io/workflows/2398-kb-tool-confluence-knowledge-base/,"Enhance Query Resolution with the Knowledge Base Tool!
Our KB Tool - Confluence KB is crafted to seamlessly integrate into the IT Ops AI SlackBot Workflow, enhancing the IT support process by enabling sophisticated search and response capabilities via Slack.
Workflow Functionality:
Receive Queries: Directly accepts user queries from the main workflow, initiating a dynamic search process.
AI-Powered Query Transformation: Utilizes OpenAI's models or local ai to refine user queries into searchable keywords that are most likely to retrieve relevant information from the Knowledge Base.
Confluence Integration: Executes searches within Confluence using the refined keywords to find the most applicable articles and information.
Deliver Accurate Responses: Gathers essential details from the Confluence results, including article titles, links, and summaries, preparing them to be sent back to the parent workflow for final user response.
To view a demo video of this workflow in action, click here.
Quick Setup Guide:
Ensure correct configurations are set for OpenAI and Confluence API integrations.
Customize query transformation logic as per your specific Knowledge Base structure to improve search accuracy.
Need Help?
Dive into our Documentation or get support from the Community Forum!
Deploy this tool to provide precise and informative responses, significantly boosting the efficiency and reliability of your IT support workflow."
Dynamically generate a webpage from user request using OpenAI Structured Output,https://n8n.io/workflows/2388-dynamically-generate-a-webpage-from-user-request-using-openai-structured-output/,"This workflow is a experiment to build HTML pages from a user input using the new Structured Output from OpenAI.
How it works:
Users add what they want to build as a query parameter
The OpenAI node generate an interface following a structured output defined in the body
The JSON output is then converted to HTML along with a title
The HTML is encapsulated in an HTML node (where the Tailwind css script is added)
The HTML is rendered to the user via the Webhook response.
Set up steps
Create an OpenAI API Key
Create the OpenAI credentials
Use the credentials for both nodes HTTP Request (as Predefined Credential type) and OpenAI
Activate your workflow
Once active, go to the production URL and add what you'd like to build as the parameter ""query""
Example: https://production_url.com?query=a%20signup%20form
Example of generated page"
Generate audio from text using OpenAI and Webhook | Text to Speech Workflow,https://n8n.io/workflows/2386-generate-audio-from-text-using-openai-and-webhook-or-text-to-speech-workflow/,"Who is this for?
This workflow is ideal for developers, content creators, or customer support teams looking to automate text-to-speech conversion using OpenAI.
What problem does this solve?
It automates the process of converting text inputs into speech, reducing manual effort and enhancing productivity.
What this workflow does:
This workflow triggers when a text input is received via a webhook, converts it into audio using the OpenAI API, and sends the generated speech back through a webhook response.
Setup:
Ensure you have an OpenAI API key (you can get it from OpenAI website).
Set up the webhook URL and parameters.
Configure the OpenAI node with your API key (Create New Credentials).
set up the responde to webhook node."
"Turn Emails into AI-Enhanced Tasks in Notion (Multi-User Support) with Gmail, Airtable and Softr",https://n8n.io/workflows/2377-turn-emails-into-ai-enhanced-tasks-in-notion-multi-user-support-with-gmail-airtable-and-softr/,"Purpose
This workflow automatically creates Tasks from forwarded Emails, similar to Asana, but better. Emails are processed by AI and converted to rather actionable task.
In addition this workflow is build in a way, that multiple users can share this single process by setting up their individual configuration through a user friendly portal (internal tool) instead of the need to manage their own workflows.
Demo
How it works
One Gmail account is used to process inbound mails from different users.
A custom web portal enables users to define ‚Äúroutes‚Äù. Thats where the mapping between an automatically generated Gmail Alias and a Notion Database URL, including the personal API Token, happens.
Using a Gmail Trigger, new entries are split by the Email Alias, so the corresponding route can be retrieved from the Database connected to the portal.
Every Email then gets processed by AI to get generate an actionable task and get a short summary of the original Email as well as some metadata.
Based on a predefined structure a new Page is created in the corresponding Notion Database.
Finally the Email is marked as ‚Äúprocessed‚Äù in Gmail.
If an error happens, the route gets paused for a possible overflow and the user gets notified by Email.
Setup
Create a new Google account (alternatively you can use an existing one and set up rules to keep your inbox organized)
Create two Labels in Gmail: ‚ÄúProcessed‚Äù and ‚ÄúError‚Äù
Clone this Softr template including the Airtable dataset and publish the application
Clone this workflow and choose credentials (Gmail, Airtable)
Follow the additional instructions provided within the workflow notes
Enable the workflow, so it runs automatically in the background
How to use
Open published Softr application
Register as a new user
Create a new route containing the Notion API key and the Notion Database URL
Expand the new entry to copy the Email address
Save the address as a new contact in your Email provider of choice
Forward an Email to it and watch how it gets converted to an actionable task
Disclamer
Airtable was chosen, so you can setup this template fairly quickly. It is advised to replace the persistence by something you own, like a self hosted SQL server, since we are dealing with sensitive information of multiple users
This solution is only meant for building internal tools, unless you own an embed license for n8n."
"Customer Insights with Qdrant, Python and Information Extractor",https://n8n.io/workflows/2373-customer-insights-with-qdrant-python-and-information-extractor/,"This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings:
Survey Insights
Customer Insights
Community Insights
This template demonstrates the Customer Insights scenario where Trustpilot reviews can be quickly grouped by similarity and an AI agent can generate insights on those groupings.
With this workflow, marketers can save days and even weeks of work breaking down their own or competitor reviews and identify frequently mentioned positives and negatives.
Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vQ6ipJnXWXgr5wlUJnhioNpeYrxaIpsRYZCwN3C-fFXumkbh9TAsA_JzE0kbv7DcGAVIP7az0L46_2P/pubhtml
How it works
Trustpilot reviews are scraped for a particular company using the HTTP request node.
Reviews are then inserted into a Qdrant collection carefully tagged with the question and Trustpilot metadata.
Reviews are fetched and put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups.
Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for.
The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the marketer.
Requirements
Qdrant Vectorstore for storing embeddings.
OpenAI account for embeddings and LLM.
Customising the Template
Adjust clustering parameters which make sense for your data.
Consider expanding date range of reviews for insights over common intervals: 3mth, 6mth and YTD."
Creating a AI Slack Bot with Google Gemini,https://n8n.io/workflows/2370-creating-a-ai-slack-bot-with-google-gemini/,"This is an example of how we can build a slack bot in a few easy steps
Before you can start, you need to o a few things
Create a copy of this workflow
Create a slack bot
Create a slash command on slack and paste the webhook url to the slack command
Note
Make sure to configure this webhook using a https:// wrapper and don't use the default http://localhost:5678 as that will not be recognized by your slack webhook.
Once the data has been sent to your webhook, the next step will be passing it via an AI Agent to process data based on the queries we pass to our agent.
To have some sort of a memory, be sure to set the slack token to the memory node. This way you can refer to other chats from the history.
The final message is relayed back to slack as a new message. Since we can not wait longer than 3000 ms for slack response, we will create a new message with reference to the input we passed.
We can advance this using the tools or data sources for it to be more custom tailored for your company.
Usage
To use the slackbot, go to slack and click on your set slash command eg /Bob and send your desired message.
This will send the message to your endpoint and get return the processed results as the message.
If you would like help setting this up, feel free to reach out to zacharia@effibotics.com"
Enhance Customer Chat by Buffering Messages with Twilio and Redis,https://n8n.io/workflows/2346-enhance-customer-chat-by-buffering-messages-with-twilio-and-redis/,"This n8n workflow demonstrates a simple approach to improve chat UX by staggering an AI Agent's reply for users who send in a sequence of partial messages and in short bursts.
How it works
Twilio webhook receives user's messages which are recorded in a message stack powered by Redis.
The execution is immediately paused for 5 seconds and then another check is done against the message stack for the latest message.
The purpose of this check lets use know if the user is sending more messages or if they are waiting for a reply.
The execution is aborted if the latest message on the stack differs from the incoming message and continues if they are the same.
For the latter, the agent receives the buffered messages up to that point and is able to respond to them in a single reply.
Requirements
A Twilio account and SMS-enabled phone number to receive messages.
Redis instance for the messages stack.
OpenAI account for the language model.
Customising the workflow
This workflow should work for other common messaging platforms such as Whatsapp and Telegram.
5 seconds too long or too short? Adjust the wait threshold to suit your customers."
Generating Image Embeddings via Textual Summarisation,https://n8n.io/workflows/2344-generating-image-embeddings-via-textual-summarisation/,"This n8n template demonstrates an approach to image embeddings for purpose of building a quick image contextual search. Use-cases could for a personal photo library, product recommendations or searching through video footage.
How it works
A photo is imported into the workflow via Google Drive.
The photo is processed by the edit image node to extract colour information. This information forms part of our semantic metadata used to identify the image.
The photo is also processed by a vision-capable model which analyses the image and returns a short description with semantic keywords.
Both pieces of information about the image are combined with the metadata of the image to form a document describing the image.
This document is then inserted into our vector store as a text embedding which is associated with our image.
From here, the user can query the vector store as they would any document and the relevant image references and/or links should be returned.
Requirements
Google account to download image files from Google Drive.
OpenAI account for the Vision-capable AI and Embedding models.
Customise this workflow
Text summarisation is just one of many techniques to generate image embeddings. If the results are unsatisfactory, there are dedicated image embedding models such as Google's vertex AI multimodal embeddings."
Introduction to the HTTP Tool,https://n8n.io/workflows/2343-introduction-to-the-http-tool/,"This n8n template showcases the new HTTP tool released in version 1.47.0.
Overall, the tool helps simplify AI Agent workflows where custom sub-workflows were performing the same simple http requests.
Comparisons
1. AI agent that can scrape webpages
Remake of https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/
Changes:
Replaces Execute Workflow Tool and Subworkflow
Replaces Response Formatting
2. Allow your AI to call an API to fetch data
Remake of https://n8n.io/workflows/2094-allow-your-ai-to-call-an-api-to-fetch-data/
Changes:
Replaces Execute Workflow Tool and Subworkflow
Replaces Manual Query Params Definitions
Replaces Response Formatting"
"Build a Tax Code Assistant with Qdrant, Mistral.ai and OpenAI",https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/,"This n8n workflows builds another example of creating a knowledgebase assistant but demonstrates how a more deliberate and targeted approach to ingesting the data can produce much better results for your chatbot.
In this example, a government tax code policy document is used. Whilst we could split the document into chunks by content length, we often lose the context of chapters and sections which may be required by the user.
Our approach then is to first split the document into chapters and sections before importing into our vector store. Additionally, using metadata correctly is key to allow filtering and scoped queries.
Example
Human: ""Tell me about what the tax code says about cargo for intentional commerce?""
AI: ""Section 11.25 of the Texas Property Tax Code pertains to ""MARINE CARGO CONTAINERS USED EXCLUSIVELY IN INTERNATIONAL COMMERCE."" In this section, a person who is a citizen of a foreign country or an en...""
How it works
The tax code policy document is downloaded as a zip file from the government website and its pages are extracted as separate chapters.
Each chapter is then parsed and split into its sections using data manipulation expressions.
Each section is then inserted into our Qdrant vector store tagged with its source, chapter and section numbers as metadata.
When our AI Agent needs to retrieve data from our vector store, we use a custom workflow tool to perform the query to Qdrant.
Because we're relying on Qdrant's advanced filtering capabilities, we perform the search using the Qdrant API rather than the Qdrant node.
When the AI Agent, needs to pull full wording or extracts, we can use Qdrant's scroll API and metadata filtering to do so. This makes Qdrant behave like a key-value store for our document.
Requirements
A Qdrant instance is required for the vector store and specifically for it's filtering functionality.
Mistral.ai account for Embeddings and AI models.
Customising this workflow
Depending on your use-case, consider returning actual PDF pages (or links) to the user for the extra confirmation and to build trust.
Not using Mistral? You are able to replace but note to match the distance and dimension size of Qdrant collection to your chosen embedding model."
Reconcile Rent Payments with Local Excel Spreadsheet and OpenAI,https://n8n.io/workflows/2338-reconcile-rent-payments-with-local-excel-spreadsheet-and-openai/,"This n8n workflow is designed to work on the local network and assists with reconciling downloaded bank statements with internal tenant records to quickly highlight any issues with payments such as missed or late payments or those of incorrect amounts.
This assistant can then generate a report to quick flag attention to ensure remedial action is taken.
How it works
The workflow monitors a local network drive to watch for new bank statements that are added.
This bank statement is then imported into the n8n workflow, its contents extracted and sent to the AI Agent.
The AI Agent analyses the line items to identify the dates and any incoming payments from tenants.
The AI agent then uses an locally-hosted Excel (""XLSX"") spreadsheet to get both tenant records and property records. From this data, it can determine for each active tenant when payment is due, the amount and the tenancy duration.
Comparing to the bank statement, the AI Agent can now report on where tenants have missed their payments, made late payments or are paying the incorrect amounts.
The final report is generated and logged in the same XLSX for a human to check and action.
Requirements
A self-hosted version of n8n is required.
OpenAI account for the AI model
Customising this workflow
If you organisation has a Slack or Teams account, consider sending reports to a channel for increased productivity. Email may be a good choice too.
Want to go fully local?
A version of this workflow is available which uses Ollama instead. You can download this template here: https://drive.google.com/file/d/1YRKjfakpInm23F_g8AHupKPBN-fphWgK/view?usp=sharing"
Organise Your Local File Directories With AI,https://n8n.io/workflows/2334-organise-your-local-file-directories-with-ai/,"If you have a shared or personal drive location with a high frequency of files created by humans, it can become difficult to organise. This may not matter... until you need to search for something!
This n8n workflow works with the local filesystem to target the messy folder and categorise as well as organise its files into sub directories automatically.
Disclaimer
Unfortunately due to the intended use-case, this workflow will not work on n8n Cloud and a self-hosted version of n8n is required.
How it works
Uses the local file trigger to activate once a new file is introduced to the directory
The new file's filename and filetype are analysed using AI to determine the best location to move this file.
The AI assess the current subdirectories as to not create duplicates. If a relevant subdirectory is not found, a new subdirectory is suggested.
Finally, an Execute Command node uses the AI's suggestions to move the new file into the correct location.
Requirements
Self-hosted version of n8n. The nodes used in this workflow only work in the self-hosted version.
If you are using docker, you must create a bind mount to a host directory.
Mistral.ai account for LLM model
Customise this workflow
If the frequency of files created is high enough, you may not want the trigger to active on every new file created event. Switch to a timer to avoid concurrency issues.
Want to go fully local?
A version of this workflow is available which uses Ollama instead. You can download this template here:
https://drive.google.com/file/d/1iqJ_zCGussXpfaUBYGrN5opziEFAEQMu/view?usp=sharing"
Recipe Recommendations with Qdrant and Mistral,https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/,"This n8n workflow demonstrates creating a recipe recommendation chatbot using the Qdrant vector store recommendation API.
Use this example to build recommendation features in your AI Agents for your users.
How it works
For our recipes, we'll use HelloFresh's weekly course and recipes for data. We'll scrape the website for this data.
Each recipe is split, vectorised and inserted into a Qdrant Collection using Mistral Embeddings
Additionally the whole recipe is stored in a SQLite database for later retrieval.
Our AI Agent is setup to recommend recipes from our Qdrant vector store. However, instead of the default similarity search, we'll use the Recommendation API instead.
Qdrant's Recommendation API allows you to provide a negative prompt; in our case, the user can specify recipes or ingredients to avoid.
The AI Agent is now able to suggest a recipe recommendation better suited for the user and increase customer satisfaction.
Requirements
Qdrant vector store instance to save the recipes
Mistral.ai account for embeddings and LLM agent
Customising the workflow
This workflow can work for a variety of different audiences. Try different sets of data such as clothes, sports shoes, vehicles or even holidays."
Customer Support Channel and Ticketing System with Slack and Linear,https://n8n.io/workflows/2323-customer-support-channel-and-ticketing-system-with-slack-and-linear/,"This n8n workflow demonstrates how to create a really simple yet effective customer support channel and pipeline by combining Slack, Linear and AI tools.
Built on n8n's ability to integrate anything, this workflow is intended for small support teams who want to maximise re-use of the tools they already have with an interface which is doesn't require any onboarding.
Read the blog post here: https://blog.n8n.io/automated-customer-support-tickets-with-n8n-slack-linear-and-ai/
How it works
The workflow is connected to a slack channel setup with the customer to capture support issues.
Only messages which are tagged with a ""‚úÖ"" reaction are captured by the workflow. Messages are tagged by the support team in the channel.
Each captured support issue is sent to the AI model to classify, prioritise and rewrite into a support ticket.
The generated support ticket is uploaded to Linear for the support team to investigate and track.
Support team is able to report back to the user via the channel when issue is fixed.
Requirements
Slack channel to be monitored
Linear account and project
Customising this workflow
Don't have Linear? This workflow can work just as well with traditional ticketing systems like JIRA."
Automate Your RFP Process with OpenAI Assistants,https://n8n.io/workflows/2321-automate-your-rfp-process-with-openai-assistants/,"This n8n workflow demonstrates how to automate oftern time-consuming form filling tasks in the early stages of the tendering process; the Request for Proposal document or ""RFP"".
It does this by utilising a company's knowledgebase to generating question-and-answer pairs using Large Language Models.
How it works
A buyer's RFP is submitted to the workflow as a digital document that can be parsed.
Our first AI agent scans and extracts all questions from the document into list form.
The supplier sets up an OpenAI assistant prior loaded with company brand, marketing and technical documents.
The workflow loops through each of the buyer's questions and poses these to the OpenAI assistant.
The assistant's answers are captured until all questions are satisified and are then exported into a new document for review.
A sales team member is then able to use this document to respond quickly to the RFP before their competitors.
Example Webhook Request
curl --location 'https://&lt;n8n_webhook_url&gt;' \
--form 'id=""RFP001""' \
--form 'title=""BlueChip Travel and StarBus Web Services""' \
--form 'reply_to=""jim@example.com""' \
--form 'data=@""k9pnbALxX/RFP Questionnaire.pdf""'
Requirements
An OpenAI account to use AI services.
Customising the workflow
OpenAI assistants is only one approach to hosting a company knowledgebase for AI to use. Exploring different solutions such as building your own RAG-powered database can sometimes yield better results in terms of control of how the data is managed and cost."
Automate LinkedIn Outreach with Notion and OpenAI,https://n8n.io/workflows/2288-automate-linkedin-outreach-with-notion-and-openai/,"This template is based on the following template. Thank you for the groundwork, Matheus.
How it works:
Store your snippets of text in a Notion table. Each snippet should have an image associated with it (copy + pasted into the text)
Connect to your table via a Notion ""integration"", from which N8N can then query your pre-meditated posts
The text is fed through an OpenAI assistant to boost engagement via formatting
The re-formatted text along with the image pulled from the Notion snippet are combined into a post for your LinkedIn
The row in the original Notion table from step 1 containing this post is set to a status of ""Done""
Set up steps:
You will need to create a Notion ""integration"", which will yield a ""secret key"" which you enter into your N8N as a ""Credential"".
You will need to create a LinkedIn ""app"" in order to post on your behalf. When creating your LinkedIn ""app"", you will be required to link this ""app"" to a company page on LinkedIn. If you are doing this for yourself, seach for the ""Default Company Payge (for API testing)"", and select this page as it is provided by LinkedIn for individuals. You can find your LinkedIn apps here, and if you get stuck, further instructions on setting up this workflow (including this LinkedIn OAuth piece) can be found in this YouTube Video Aide to these instructions.
Lastly, you will need to create an OpenAI API key, found on your OpenAI Playground Dashboard. Once you created an API key, make sure you have an assistant created from the ""Assistants"" tab on the OpenAI dashboard. This assistant and its instructions will be needed for carrying out the re-formatting of your post."
Share YouTube Videos with AI Summaries on Discord,https://n8n.io/workflows/2249-share-youtube-videos-with-ai-summaries-on-discord/,"Boost engagement on your Discord server by automatically sharing new YouTube videos along with AI generated summaries of their content. This workflow is ideal for content creators and community managers looking to provide value and spark interest through summarized content, making it easier for community members to decide if a video is of interest to them. Watch this video tutorial to learn more about the template.
How it works
RSS Feed Trigger: Monitors your YouTube channel for new uploads using the RSS feed.
Video Captions Retrieval: Fetches video captions using the YouTube API to get detailed content data.
AI Summary Generation: Uses an AI model to generate concise summaries from the video captions, highlighting key points.
Discord Notification: Posts video announcements along with their AI generated summaries to a specified Discord channel using a webhook.
Set up steps
Configure YouTube RSS Feed: Set up the RSS feed node to detect new video uploads. Add your YouTube channel ID to the URL in the first node: https://www.youtube.com/feeds/videos.xml?channel_id=YOUR_CHANNEL_ID.
Connect OpenAI Account: To enable AI summary generation, connect your OpenAI account in n8n.
Set Up Discord Webhook: Create a webhook in your Discord server and configure it in the Discord node.
Design the Message: Format the Discord message as you like to include the video title, link, and the AI generated summary.
Example
This template empowers you to maintain a highly engaging Discord community, ensuring members receive not only regular updates but also valuable insights into each video's content without needing to watch immediately."
Post New YouTube Videos to X,https://n8n.io/workflows/2242-post-new-youtube-videos-to-x/,"Automated YouTube Video Promotion Workflow
Automate the promotion of new YouTube videos on X (formerly Twitter) with minimal effort. This workflow is perfect for content creators, marketers, and social media managers who want to keep their audience updated with fresh content consistently.
How it works
This workflow triggers every 30 minutes to check for new YouTube videos from a specified channel. If a new video is found, it utilizes OpenAI's ChatGPT to craft an engaging, promotional message for X. Finally, the workflow posts the generated message to Twitter, ensuring your latest content is shared with your audience promptly.
Set up steps
Schedule the workflow to run at your desired frequency.
Connect to your YouTube account and set up the node to fetch new videos based on your Channel ID.
Integrate with OpenAI to generate promotional messages using GPT-3.5 turbo.
Link to your X account and set up the node to post the generated content.
Please note, you'll need API keys and credentials for YouTube, OpenAI, and X. Check out this quick video tutorial to make the setup process a breeze.
Additional Tips
Customize the workflow to match your branding and messaging tone.
Test each step to ensure your workflow runs smoothly before going live."
Automated AI image analysis and response via Telegram,https://n8n.io/workflows/2235-automated-ai-image-analysis-and-response-via-telegram/,"Example: @SubAlertMe_Bot
Summary:
The automated image analysis and response workflow using n8n is a sophisticated solution designed to streamline the process of analyzing images sent via Telegram and delivering insightful responses based on the analysis outcomes. This cutting-edge workflow employs a series of meticulously orchestrated nodes to ensure seamless automation and efficiency in image processing tasks.
Use Cases:
This advanced workflow caters to a myriad of scenarios where real-time image analysis and response mechanisms are paramount. The use cases include:
Providing immediate feedback on images shared within Telegram groups.
Enabling automated content moderation based on the analysis of image content.
Facilitating rapid categorization and tagging of images based on the results of the analysis.
Detailed Workflow Setup:
To effectively implement this workflow, users must adhere to a meticulous setup process, which includes:
Access to the versatile n8n platform, ensuring seamless workflow orchestration.
Integration of a Telegram account to facilitate image reception and communication.
Utilization of an OpenAI account for sophisticated image analysis capabilities.
Configuration of Telegram and OpenAI credentials within the n8n environment for seamless integration.
Proficiency in creating and interconnecting nodes within the n8n workflow for optimal functionality.
Detailed Node Description:
Get the Image (Telegram Trigger):
Actively triggers upon receipt of an image via Telegram, ensuring prompt processing.
Extracts essential information from the received image message to initiate further actions.
Merge all fields To get data from trigger:
Seamlessly amalgamates all relevant data fields extracted from the trigger node for comprehensive data consolidation.
Analyze Image (OpenAI):
Harnesses the powerful capabilities of OpenAI services to conduct in-depth analysis of the received image.
Processes the image data in base64 format to derive meaningful insights from the visual content.
Aggregate all fields:
Compiles and consolidates all data items for subsequent processing and analysis, ensuring comprehensive data aggregation.
Send Content for the Analyzed Image (Telegram):
Transmits the analyzed content back to the Telegram chat interface for seamless communication.
Delivers the analyzed information in textual format, enhancing user understanding and interaction.
Switch Node:
The Switch node is pivotal for decision-making based on predefined conditions within the workflow.
It evaluates incoming data to determine the existence or absence of specific elements, such as images in this context.
Utilizes a set of rules to assess the presence of image data in the message payload and distinguishes between cases where images are detected and when they are not.
This crucial node plays a pivotal role in directing the flow of the workflow based on the outcomes of its evaluations.
Conclusion:
The automation of image analysis processes through this sophisticated workflow not only enhances operational efficiency but also revolutionizes communication dynamics within Telegram interactions. By incorporating this advanced workflow solution, users can optimize their image analysis workflows, bolster communication efficacy, and unlock new levels of automation in image processing tasks."
Image Creation with OpenAI and Telegram,https://n8n.io/workflows/2216-image-creation-with-openai-and-telegram/,"Image Creation with OpenAI and Telegram
Check this channel: AutoTechAi_bot
Description:
In the realm of automation and artificial intelligence, n8n offers a sophisticated platform for seamlessly integrating AI algorithms to enhance image creation and communication processes. This innovative workflow leverages the capabilities of OpenAI and Telegram to facilitate creative image generation and streamline communication channels, ultimately enhancing user engagement and interaction.
How to Use:
Set Up Credentials:
Configure credentials for the Telegram account and OpenAI API to enable seamless integration.
Configure Nodes:
Telegram Trigger Node: Set up the node to initiate the workflow based on incoming messages from users on Telegram.
OpenAI Node: Utilize advanced AI algorithms to analyze text content from messages and generate intelligent responses.
Telegram Node: Send processed data, including images and responses, back to users on Telegram for seamless communication.
Merge Node: Organize and combine processed data for efficient handling and integration within the workflow.
Aggregate Node: Aggregate all item data, including binaries if specified, for comprehensive reporting and analysis purposes.
Run Workflow: Initiate the workflow to leverage AI-enhanced image processing and communication capabilities for enhanced user interactions.
Monitor Execution: Keep an eye on the workflow execution for any errors or issues that may occur during processing.
Customize Workflow:
Tailor the workflow nodes, parameters, or AI models to align with specific business objectives and user engagement strategies.
Experience Benefits:
Embrace the power of AI-driven image processing and interactive communication on Telegram to elevate user engagement and satisfaction levels.
By following these steps, businesses can unlock the transformative potential of AI integration in image creation and communication workflows using n8n. Elevate your user engagement strategies and deliver exceptional experiences to your audience through innovative AI-driven solutions.
Embark on a journey of innovation and efficiency with AI integration in image creation and communication workflows using n8n!"
AI-powered automated stock analysis,https://n8n.io/workflows/2209-ai-powered-automated-stock-analysis/,"Introduction:
Streamline your fundamental stock analysis process with AI-powered automation.
By harnessing the power of SEC 10K reports - comprehensive documents required by the SEC containing vital company information - this template automates the analysis workflow.
From planning by a Senior Research Analyst to execution by five Research Analysts and final review by a Senior Editor, this template takes an AI persona approach to compose the report that includes an overview of the business, strategy, SWOT (Strengths, Weaknesses, Opportunities, and Threads) analysis, near term catalysts, and top risks.
Additionally, this template allows you to control the length and detail of the report generated.
How it works
There are three personas in the workflow:
Senior Research Analyst is the first part of the workflow. They are responsible for planning the work for the rest of the team.
Squad of Research Analysts is the second part of the workflow. They execute the plan created.
Senior Editor is the third part of the workflow. They polish the draft report and send it to publish
This template uses a custom tool that is able to answer the SEC 10K questions from the Research team.
Setup steps
Setup the Stock Q&A Workflow for the company you want to analyze using this template
Customize the setup node by specifying the company to analyze
Customize the publish step by specifying the file name in the Google docs node
Credit
Inspired by Guilio's template"
Translate Telegram audio messages with AI (55 supported languages),https://n8n.io/workflows/2206-translate-telegram-audio-messages-with-ai-55-supported-languages/,"Use case
This workflow enables a Telegram bot that can:
Accept speech input in one of 55 supported languages
Automatically detect the language spoken and translate the speech to another language
Responds back with the translated speech output.
This allows users to communicate across language barriers by simply speaking to the bot, which will handle the translation seamlessly.
How does it work?
Translation
In the translation step the workflow converts the user's speech input to text and detects the language of the input text.
If it's English, it will translate to French. If it's French, it will translate to English.
To change the default translation languages, you can update the prompt in the AI node.
Output
In the output step, we provide the translated text output back to the user and speech output is generated in the translated language.
Setup steps
Obtain Telegram API Token
Start a chat with the BotFather.
Enter /newbot and reply with your new bot's display name and username.
Copy the bot token and use it in the Telegram node credentials in n8n.
Update the Settings node to customize the desired languages
Activate the flow
Full list of supported languages
All supported languages:"
"OpenAI Assistant workflow: upload file, create an Assistant, chat with it!",https://n8n.io/workflows/2201-openai-assistant-workflow-upload-file-create-an-assistant-chat-with-it/,"This is an end-to-end workflow for creating a simple OpenAI Assistant. The whole process is done with n8n nodes and do not require any programming experience.
The workflow is divided into three main steps:
Step 1: Get a Google Drive File and Upload to OpenAI
The workflow starts by retrieving a file from Google Drive using the ""Get File"" node.
The example file used is a Music Festival document.
The retrieved file is then uploaded to OpenAI using the ""Upload File to OpenAI"" node.
Run this section only once. The file is stored persistently on the OpenAI side.
Step 2: Set Up a New Assistant
In this step, a new assistant is created using the ""Create new Assistant"" node.
The assistant is given a name, description, and system prompt.
The uploaded file from Step 1 is attached as a knowledge source for the assistant.
Same as for Step 1, run this section only once.
Step 3: Chat with the Assistant
The ""Chat Trigger"" node initiates the conversation with the assistant.
The ""OpenAI Assistant"" node handles the conversation, using the assistant created in Step 2.
Step 4: Expand the Assistant
This step provides resources for ideas on how to expand the Assistant's capabilities:
Create a WhatsApp bot
Create a simple Telegram bot
Create a Telegram AI bot (YouTube video)
By following this workflow, users can create their own AI-powered assistants using OpenAI's API and integrate them with various platforms like WhatsApp and Telegram."
Compose reply draft in Gmail with OpenAI Assistant,https://n8n.io/workflows/2198-compose-reply-draft-in-gmail-with-openai-assistant/,"This workflow uses OpenAI Assistant to compose draft replies for labeled email messages. It automatically connects the drafts to Gmail threads.
üí° You can add knowledge base to your OpenAI Assistant and make your reply drafts very customized (e.g. compose response with product information in response to inquiry from customer).
üé¨ See this workflow in action in my YouTube video about automating Gmail.
How it works?
The workflow is triggered at regular intervals (default: every 1 minute ‚Äì you can change this value) to check for messages with a specific label (e.g., ""AI"").
The content of the retrieved email message is then forwarded to the OpenAI Assistant node, and a reply draft is generated. Next, the response from the Assistant is converted to HTML, and a raw message in RFC standard is composed.
üí° You can learn more about composing drafts with the Gmail API in the official Google documentation.
The raw email message (reply draft) is encoded and attached to the original thread ID. Finally, the trigger label (in this case: ""AI"") is removed to prevent the workflow from looping.
Set up steps
Set credentials for Gmail and OpenAI.
Add new label in Gmail account for messages that should be handled by the workflow (e.g. name it ""AI"").
Select this label in the first and last Gmail nodes in workflow.
Create and configure your OpenAI Assistant. Select your assistant in ""OpenAI Assistant"" node.
Optionally: change trigger interval (by default interval is 1 minute).
If you like this workflow, please subscribe to my YouTube channel and/or my newsletter."
Auto-label incoming Gmail messages with AI nodes,https://n8n.io/workflows/2197-auto-label-incoming-gmail-messages-with-ai-nodes/,"This workflow uses AI to analyze the content of every new message in Gmail and then assigns specific labels, according to the context of the email.
Default configuration of the workflow includes 3 labels:
‚ÄûPartnership‚Äù - email about sponsored content or cooperation,
‚ÄûInquiry‚Äù - email about products, services,
‚ÄûNotification‚Äù - email that doesn't require response.
You can add or edit labels and descriptions according to your use case.
üé¨ See this workflow in action in my YouTube video about automating Gmail.
How it works?
Gmail trigger performs polling every minute for new messages (you can change the trigger interval according to your needs). The email content is then downloaded and forwarded to an AI chain.
üí° The prompt in the AI chain node includes instructions for applying labels according to the email content - change label names and instructions to fit your use case.
Next, the workflow retrieves all labels from the Gmail account and compares them with the label names returned from the AI chain. Label IDs are aggregated and applied to processed email messages.
‚ö†Ô∏è Label names in the Gmail account and workflow (prompt, JSON schema) must be the same.
Set up steps
Set credentials for Gmail and OpenAI.
Add labels to your Gmail account (e.g. ‚ÄûPartnership‚Äù, ‚ÄûInquiry‚Äù and ‚ÄûNotification‚Äù).
Change prompt in AI chain node (update list of label names and instructions).
Change list of available labels in JSON schema in parser node.
Optionally: change polling interval in Gmail trigger (by default interval is 1 minute).
If you like this workflow, please subscribe to my YouTube channel and/or my newsletter."
AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow,https://n8n.io/workflows/2183-ai-crew-to-automate-fundamental-stock-analysis-qanda-workflow/,"How it works:
Using a Crew of AI agents (Senior Researcher, Visionary, and Senior Editor), this crew will automatically determine the right questions to ask to produce a detailed fundamental stock analysis.
This application has two components: a front-end and a Stock Q&A engine.
The front end is the team of agents automatically figuring out the questions to ask, and the back-end part is the ability to answer those questions with the SEC 10K data.
This template implements the Stock Q&A engine.
For the front-end of the application, you can choose one of two options:
using CrewAI with the Replit environment (code approach)
fully visual approach with n8n template (AI-powered automated stock analysis)
Setup steps:
Use first workflow in template to upsert a company annual report PDF (such as from SEC 10K filling)
Get URL for Webhook in second workflow template
CrewAI front-end: Youtube overview video
Fork this AI Agent environment Crew Agent Environment
Set the webhook URL into N8N_WEBHOOK_URL variable
Set OpenAI_API_KEY variable"
Scrape and summarize posts of a news site without RSS feed using AI and save them to a NocoDB,https://n8n.io/workflows/2180-scrape-and-summarize-posts-of-a-news-site-without-rss-feed-using-ai-and-save-them-to-a-nocodb/,"The News Site from Colt, a telecom company, does not offer an RSS feed, therefore web scraping is the choice to extract and process the news.
The goal is to get only the newest posts, a summary of each post and their respective (technical) keywords.
Note that the news site offers the links to each news post, but not the individual news. We collect first the links and dates of each post before extracting the newest ones.
The result is sent to a SQL database, in this case a NocoDB database.
This process happens each week thru a cron job.
Requirements:
Basic understanding of CSS selectors and how to get them via browser (usually: right click ‚Üí inspect)
ChatGPT API account - normal account is not sufficient
A NocoDB database - of course you may choose any type of output target
Assumptions:
CSS selectors work on the news site
The post has a date with own CSS selector - meaning date is not part of the news content
""Warnings""
Not every site likes to be scraped, especially not in high frequency
Each website is structured in different ways, the workflow may then need several adaptations."
Extract data from resume and create PDF with Gotenberg,https://n8n.io/workflows/2170-extract-data-from-resume-and-create-pdf-with-gotenberg/,"With this workflow you can extract data from resume documents uploaded via a Telegram bot. Workflow transform readable content of PDF resume into structured data, using AI nodes and returns PDF with formatted, plain HTML.
You can modify this workflow to perform other actions with structured data (e.g. insert it into database or create other, well-formatted documents).
Functionality of this workflow was presented during the n8n community call on March 7, 2024 - recording of presentation available here.
‚ö†Ô∏è Workflow made for demo purposes. If you want to use it in real life, please make sure necessary measures for personal data protection are set.
How it works?
User uploads readable PDF resume document into Telegram bot. After authentication based on chat ID parameter, workflow extracts text from the PDF and transfers it into AI chain with connected sub-nodes: OpenAI Chat Model and Structured Output (JSON) Parser.
Then, each extracted section (employment history, projects etc.) is formatted into desired HTML structure. Finally, the document is converted into new, structured PDF using Gotenberg.
üí° This workflow requires installed Gotenberg. If you are not familiar with this software, please have a look on my YouTube tutorial. You can also replace call to Gotenberg with other PDF generation service (such as PDFMonkey or ApiTemplate).
Set up steps
Create Telegram bot and add its credentials in n8n.
Set your chat ID parameter in Auth node.
Adjust JSON schema in Structured Output Parser according to your needs.
Optionally: replace HTTP call to Gotenberg with PDF generation service of your choice.
If you like this workflow, please subscribe to my YouTube channel and/or my newsletter."
ChatGPT Automatic Code Review in Gitlab MR,https://n8n.io/workflows/2167-chatgpt-automatic-code-review-in-gitlab-mr/,"Who this template is for
This template is for every engineer who wants to automate their code reviews or just get a 2nd opinion on their PR.
How it works
This workflow will automatically review your changes in a Gitlab PR using the power of AI. It will trigger whenever you comment with +0 to a Gitlab PR, get the code changes, analyze them with GPT, and reply to the PR discussion.
Set up Steps
Set up webhook of note_events in Gitlab repository (see here on how to do it)
Configure ChatGPT credentials
Note ""+0"" in MergeRequest to trigger automatic review by ChatGPT"
Qualify new leads in Google Sheets via OpenAI's GPT-4,https://n8n.io/workflows/2163-qualify-new-leads-in-google-sheets-via-openais-gpt-4/,"This n8n workflow was developed to evaluate and categorize incoming leads based on certain criteria. The workflow is triggered by adding a new row in a Google Sheets document.
The workflow uses the OpenAI node to process the lead information. The system query contains detailed qualification rules and the response format. The user message contains the data for the individual lead.
The JSON response from the OpenAI node is then processed by the Edit Fields node to extract the response. This response is merged together with the original lead data by the Merge node.
Finally, the Google Sheets node updates the original lead entry in the Google Sheets document with the qualification result (""qualified"" or ""not qualified"") in a separate column. This allows for easy tracking and sorting of the qualified leads."
AI-powered WooCommerce Support-Agent,https://n8n.io/workflows/2161-ai-powered-woocommerce-support-agent/,"With this workflow you get a fully automated AI powered Support-Agent for your WooCommerce webshop. It allows customers to request information about things like:
the status of their order
the ordered products
shipping and billing address
current DHL shipping status
How it works
The workflow receives chat messages from an in a website integrated chat. For security and data-privacy reasons, does the website transmit the email address of the user encrypted with the requests. That ensures that user can just request the information about their own orders.
An AI agent with a custom tool supplies the needed information. The tool calls a sub-workflow (in this case, in the same workflow for convenience) to retrieve the required information. This includes the full information of past orders plus the shipping information from DHL.
If otherr shipping providers are used it should be simple to adjust the workflow to query information from other APIs like UPS, Fedex or others."
Twitter Virtual AI Influencer,https://n8n.io/workflows/2139-twitter-virtual-ai-influencer/,"Twitter Virtual AI Influencer Workflow Template
This n8n workflow template empowers creators to launch a virtual AI influencer that tweets regularly, engaging audiences with a unique niche, writing style, and inspiration. By automating content creation and posting, it ensures a consistent and natural online presence, tailored to your specific influencer profile.
Features
Scheduled Posting: Automates tweet posting every 6 hours, with randomized posting minutes to mimic natural activity.
On-Demand Posting: Offers flexibility with manual trigger options for immediate content sharing.
Influencer Profile Configuration: Customize your virtual influencer by defining a target niche, personal writing style, and sources of inspiration.
Content Generation: Leverages advanced AI to craft tweets that resonate with your audience, aiming for viral engagement.
Tweet Validation: Ensures all generated content adheres to Twitter's character limit, maintaining quality and relevance.
Workflow Steps
Schedule Posting: Configured to post every 6 hours, this step introduces randomness in posting time to simulate human behavior.
Trigger Posting Manually: Provides an option to manually initiate a tweet, offering control over the timing of your content.
Configure Influencer Profile: Set up your influencer's niche, style, and inspiration to guide the AI in generating targeted content.
Generate Tweet Content: Utilizes a sophisticated AI model to produce engaging tweets based on the configured profile.
Validate Tweet: Checks if the generated tweet meets Twitter's length constraints, ensuring all content is ready for posting.
Post Tweet: Finalizes the process by sharing the AI-generated tweet to your designated Twitter account.
Configuration Notes
Niche: Define a specific area of interest, such as ""Modern Stoicism,"" to focus your influencer's content.
Writing Style: Customize the tone and style of the tweets to reflect a personal touch, enhancing relatability.
Inspiration: Input sources of inspiration, including books and philosophies, to steer the content generation process.
Getting Started
To deploy this template:
Import the workflow into your n8n workspace.
Customize the influencer profile settings to match your desired niche, style, and inspiration.
Connect your Twitter account through the provided OAuth2 credentials setup.
Activate the workflow to start building your virtual influencer's presence on Twitter.
Embrace the power of AI to create a distinctive and engaging virtual influencer, captivating your audience with minimal effort."
Allow your AI to call an API to fetch data,https://n8n.io/workflows/2094-allow-your-ai-to-call-an-api-to-fetch-data/,"Use n8n to bring data from any API to your AI. This workflow uses the Chat Trigger to provide the chat interface, and the Custom n8n Workflow Tool to call a second workflow that calls the API.
The second workflow uses AI functionality to refine the API request based on the user's query. It then makes an API call, and returns the response to the main workflow.
This workflow is used in Advanced AI examples | Call an API to fetch data in the documentation.
To use this workflow:
Load it into your n8n instance.
Add your credentials as prompted by the notes.
Requires n8n 1.28.0 or above"
Chat with a database using AI,https://n8n.io/workflows/2090-chat-with-a-database-using-ai/,"This workflow allows you to ask questions about data stored in a database using AI.
To use it, you'll need an OpenAI API key (although you could also swap in a model from another service).
Supported databases:
Postgres
MySQL
SQLite
The workflow uses n8n's embedded chat, but you could also modify it to work with a chat service such as Slack, MS Teams or WhatsApp.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
AI chat with any data source (using the n8n workflow tool),https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/,"This AI agent can access data provided by another n8n workflow. Since that workflow can be used to retrieve any data from any service, this template can be used give an agent access to any data.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Use an open-source LLM (via HuggingFace),https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/,"This workflow demonstrates how to connect an open-source model to a Basic LLM node.
The workflow is triggered when a new manual chat message appears. The message is then run through a Language Model Chain that is set up to process text with a specific prompt to guide the model's responses.
Note that open-source LLMs with a small number of parameters require slightly different prompting with more guidance to the model.
You can change the default Mistral-7B-Instruct-v0.1 model to any other LLM supported by HuggingFace. You can also connect other nodes, such as Ollama.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
AI: Conversational agent with custom tool written in JavaScript,https://n8n.io/workflows/1963-ai-conversational-agent-with-custom-tool-written-in-javascript/,"This workflow implements a custom tool via JavaScript code which returns a random color to users and excludes the given colors.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Load and summarize Google Drive files with AI,https://n8n.io/workflows/1962-load-and-summarize-google-drive-files-with-ai/,"This workflow includes advanced features like text summarization and tokenization, it's ideal for automating document processing tasks that require parsing and summarizing text data from Google Drive.
To use this template, you need to be on n8n version 1.19.4 or later."
Slack chatbot powered by AI,https://n8n.io/workflows/1961-slack-chatbot-powered-by-ai/,"This workflow offers an effective way to handle a chatbot's functionality, making use of multiple tools for information retrieval, conversation context storage, and message sending. It's a setup tailored for a Slack environment, aiming to offer an interactive, AI-driven chatbot experience.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
OpenAI GPT-3: Company Enrichment from website content,https://n8n.io/workflows/1862-openai-gpt-3-company-enrichment-from-website-content/,"Enrich your company lists with OpenAI GPT-3 ‚Üì
You‚Äôll get valuable information such as:
Market (B2B or B2C)
Industry
Target Audience
Value Proposition
This will help you to:
add more personalization to your outreach
make informed decisions about which accounts to target
I've made the process easy with an n8n workflow.
Here is what it does:
Retrieve website URLs from Google Sheets
Extract the content for each website
Analyze it with GPT-3
Update Google Sheets with GPT-3 data"
Googleform submission to create a Github issue bug report,https://n8n.io/workflows/2938-googleform-submission-to-create-a-github-issue-bug-report/,"Automated Workflow for Google Form Submissions, GitHub Issues, and Discord Notifications
This workflow streamlines how new Google Form submissions are processed by automatically creating GitHub issues and sending real-time notifications to a Discord channel through a webhook.
Who Is This Template For?
Developers looking to centralize bug reporting and issue tracking.
Project Managers seeking an efficient way to log tasks and updates.
QA Teams that need a fast, automated process for reporting and resolving issues.
Workflow Overview
Google Form Trigger
The workflow begins with a Google Sheets Trigger node that checks for new form submissions every minute.
Add New Form Submissions to Google Sheets
Each new submission is recorded in a dedicated Google Sheet for easy reference and archiving.
Filter Out Already Posted Issues
An If node checks if the issue is already posted on GitHub by looking for an existing GitHub link in the sheet.
Format Message / Output Parsing
If the issue is new, an OpenAI Chat Model node generates a structured output, including a title, description, and suggested fix.
Add Issue to GitHub
The structured output is used to create a new issue in the specified GitHub repository.
Send Notification to Discord
A Discord webhook is triggered to send a notification to your chosen channel, including a link to the newly created GitHub issue.
Add GitHub Link to the Sheet
The GitHub issue link is added back to the Google Sheet for easy cross-referencing.
Setup Steps
1. Google Sheets Setup
Create/Designate a Google Sheet: Ensure it‚Äôs set to receive form submissions.
Configure the Trigger Node: Provide the correct document ID and sheet name in the Google Sheets Trigger node.
2. OpenAI Configuration
API Credentials: Set up the OpenAI Chat Model node with valid OpenAI API credentials.
Prompt Definition: Create a prompt that formats the form submission data into a structured output (e.g., title, description, suggested fix).
3. GitHub Configuration
OAuth Credentials: Configure the GitHub node with your GitHub OAuth credentials.
Target Repository: Specify the repository where new issues should be created.
4. Discord Webhook
Webhook URL: Obtain a webhook URL from your Discord server.
Notification Setup: Use the HTTP Request node to send notifications to your chosen Discord channel.
5. Google Sheets Update
Document & Sheet Name: Provide the same Google Sheet details used in the first step.
Add GitHub Link: Update the corresponding row with the newly created GitHub issue link.
Recommended Google Sheet Columns
Timestamp: Automatically recorded when the form is submitted.
Issue Title: Generated by the OpenAI Chat Model.
Issue Description: Detailed breakdown of the issue.
GitHub Link: Automatically populated once the issue is created.
Discord Notification Status (optional): Indicates whether the notification was sent successfully.
Additional Notes
Duplicate Prevention: The conditional logic ensures existing issues aren‚Äôt recreated on GitHub.
AI-Powered Formatting: OpenAI helps structure the issue details, providing clarity for developers.
Real-Time Alerts: Discord notifications keep your entire team updated on new issues as they arise."
AppSheet Intelligent Query Orchestrator- Query any data!,https://n8n.io/workflows/2932-appsheet-intelligent-query-orchestrator-query-any-data/,"AppSheet Intelligent Query Orchestrator
A friendly, practical tool that makes working with AppSheet data simpler and more efficient. This workflow is your go-to helper for building precise queries without getting lost in a sea of different tables.
Background
Previously, I built a community node to enable this functionality: Appsheet n8n Community node
How It Works
This workflow fetches the most up-to-date schema and taxonomy from your Google Sheet mirror and constructs a custom query using key components:
TableName: Specifies exactly which table to query.
Selector: Uses powerful functions like SELECT(), FILTER(), and CONTAINS() to filter data with precision.
Columns Required: Extracts only the essential fields, keeping the payload lean and focused.
Natural Language Search Query: Provides a clear, descriptive context that helps refine and re-rank results.
Real-World Use Cases
This orchestrator is designed for various industries, making data retrieval effortless:
üì¶ Supply Chain & Manufacturing
Find the right product based on specific attributes.
Locate suppliers that meet certain quality or pricing criteria.
Obtain details about the lowest-priced raw materials.
üõç Retail & E-commerce
Match customer queries to the most relevant product listings.
Identify inventory levels and stock variations.
Compare pricing and product features across vendors.
üè• Healthcare
Retrieve patient records based on specific attributes.
Track inventory of medical supplies.
Schedule and manage appointments dynamically.
üéì Education
Monitor student attendance or performance metrics.
Allocate resources and track equipment usage.
Manage events and class schedules efficiently.
üîß Field Services & Maintenance
Schedule maintenance tasks by matching service requirements.
Track asset conditions and inventory for field equipment.
Monitor work orders and dispatch field teams based on real-time data.
Examples:


Iterative Refinement
This workflow operates iteratively, refining the query until it finds the best match‚Äîeven if it takes multiple rounds. This makes it incredibly versatile for complex inventory management, procurement, and precise data retrieval.
In a Nutshell
The AppSheet Intelligent Query Orchestrator is like having a smart assistant that:
‚úÖ Understands your data structure
‚úÖ Builds the perfect query every time
‚úÖ Handles a variety of real-world scenarios with ease
üöÄ Practical, adaptable, and ready to tackle your toughest data challenges!"
Automated Hugging Face Paper Summary Fetching & Categorization Workflow,https://n8n.io/workflows/2765-automated-hugging-face-paper-summary-fetching-and-categorization-workflow/,"How the Automated Workflow Works
Scheduled Fetching from Hugging Face ‚è∞
The workflow triggers every weekday at 8 AM, automatically fetching the latest papers from Hugging Face for easy access.
Duplication Check to Avoid Redundant Entries üîç
It ensures the paper's summary is not already stored in your Notion workspace, preventing duplicate records and keeping your database organized.
Content Analysis with OpenAI üß†
Using OpenAI's powerful capabilities, the workflow analyzes the fetched paper summary, extracts key insights, and categorizes the content for easier understanding.
Data Storage and Notification Integration üì•üîî
Once the summary is processed, it's automatically stored in your Notion workspace, and a notification containing the paper details is sent to your designated Slack channel for quick reference.
Set Up Your Automated Workflow
Create Your n8n Account üìù
Start by registering for an n8n account and logging into the n8n cloud service.
Connect OpenAI, Notion, and Slack üîó
Link your OpenAI, Notion, and Slack accounts by entering the appropriate tokens. This step will take approximately 10‚Äì15 minutes to complete.
Import the Workflow Template üì•
Import the provided workflow template into your n8n instance to streamline the setup process.
Activate the Workflow for Daily Summaries üöÄ
After importing, simply enable the workflow, and you‚Äôre all set to receive daily paper summaries automatically.
Setup Time ‚è≥: Approximately 15‚Äì20 minutes.
Why Use This Automated Workflow?
This automated workflow not only saves you time by fetching and categorizing the latest research papers but also helps streamline your Notion workspace and Slack notifications, allowing you to stay organized and efficient without manual intervention.
Results Presentation"
Scrape ProductHunt using Google Gemini,https://n8n.io/workflows/2698-scrape-producthunt-using-google-gemini/,"Workflow Description: Product Data Extractor
This workflow automates the extraction of product data from Product Hunt by combining webhook interactions, HTML processing, AI-based data analysis, and structured output formatting. It is designed to handle incoming requests dynamically and return detailed JSON responses for further usage.
Overview
The workflow processes a product name submitted through a webhook. It fetches the corresponding Product Hunt page, extracts and analyzes inline scripts, and structures the data into a well-defined JSON format using AI tools. The final JSON response is returned to the client through the webhook.
Workflow Steps
1. Webhook Listener
Node: Receive Product Request
Function: Captures incoming requests containing the product name to process.
Details: Accepts HTTP requests and extracts the product parameter from the query string, such as &lt;custom_webhook_url&gt;/?product=epigram.
2. Fetch Product HTML
Node: Fetch Product HTML
Function: Sends an HTTP request to retrieve the HTML content of the specified Product Hunt page.
Details: Constructs a dynamic URL using the product name and fetches the page data.
3. Extract Inline Scripts
Node: Extract Inline Scripts
Function: Parses the HTML content to extract inline scripts located within the &lt;head&gt; section.
Details: Excludes scripts containing src attributes and validates the presence of inline scripts.
4. Process Data with LLM
Node: Process Script with LLM
Function: Analyzes the extracted scripts using a language model to identify key product data.
Details: Processes the script to derive structured and meaningful insights.
5. Refine Data with Google Gemini
Node: Analyze Script with Google Gemini
Function: Leverages Google Gemini AI for enhanced analysis of script data.
Details: Ensures the extracted data is precise and enriched.
6. Format Product Data to JSON
Node: Format Product Data to JSON
Function: Structures the processed data into a clean JSON format.
Details: Defines a schema to ensure all relevant fields are included in the output.
7. Send JSON Response to Client
Node: Send JSON Response to Client
Function: Returns the final structured JSON response to the client.
Details: Sends the response back via the same webhook that initiated the request. For example, &lt;custom_webhook_url&gt;.
Key Features
Versatile Use Cases: This workflow can be used to gather Product Hunt data for creating blog posts or as a tool for AI agents to research products efficiently.
Dynamic Processing: Adapts to various product names through dynamic URL construction.
AI Integration: Utilizes the Gemini 1.5 8B AI model, offering reduced latency and minimal or no cost depending on the use case.
Selector Independence: Functions even if Product Hunt's DOM structure changes, as it does not rely on direct DOM selectors.
Reliable Data Output: A low temperature setting (0) and a precisely defined JSON schema ensure accurate and real data extraction.
Dynamic Processing: Adapts to various product names through dynamic URL construction.
AI Integration: Utilizes advanced language models for data extraction and refinement.
Structured Output: Ensures the output JSON adheres to a predefined schema for consistency.
Error Handling: Includes validations to handle missing or malformed data gracefully.
Customization Options
Limitations
Dependency on Product Hunt: Significant changes to the way Product Hunt loads data on its pages might require modifications to the workflow.
Adaptability: Even if changes occur, the workflow can be updated to maintain functionality due to its reliance on AI and not direct DOM selectors.
Modify the webhook path to suit your application.
Adjust the prompt for the language model to include additional fields.
Extend the JSON schema to capture more data fields as needed.
Expected Output
Performance Metrics
Response Time: Typically ~6 seconds per product.
Accuracy: Data extracted with >95% precision due to the pre-defined JSON schema.
A JSON object containing detailed information about the specified product. Below is an example of a complete response for the product Epigram:
{
  ""id"": ""861675"",
  ""slug"": ""epigram"",
  ""followersCount"": 181,
  ""name"": ""Epigram"",
  ""tagline"": ""Open-Source, Free, and AI-Powered News in Short"",
  ""reviewsRating"": 0,
  ""logoUuid"": ""735c2528-554c-467c-9dcf-745ee4b8bbdd.png"",
  ""postsCount"": 1,
  ""websiteUrl"": ""https://epigram.news"",
  ""websiteDomain"": ""epigram.news"",
  ""metaTitle"": ""Epigram - Open-source, free, and ai-powered news in short"",
  ""postName"": ""Epigram"",
  ""postTagline"": ""Open-source, free, and ai-powered news in short"",
  ""dailyRank"": ""3"",
  ""description"": ""An open-source, AI-powered news app for busy people. Stay updated with bite-sized news, real-time updates, and in-depth analysis. Experience balanced, trustworthy reporting tailored for fast-paced lifestyles in a sleek, user-friendly interface."",
  ""pricingType"": ""free"",
  ""userName"": ""Fazle Rahman"",
  ""userHeadline"": ""Co-founder & CEO, Hashnode"",
  ""userUsername"": ""fazlerocks"",
  ""userAvatarUrl"": ""https://ph-avatars.imgix.net/129147/f84e1796-548b-4d6f-9dcf-745ee4b8bbdd.jpeg"",
  ""makerName1"": ""Fazle Rahman"",
  ""makerHeadline1"": ""Co-founder & CEO, Hashnode"",
  ""makerUsername1"": ""fazlerocks"",
  ""makerAvatarUrl1"": ""https://ph-avatars.imgix.net/129147/f84e1796-548b-4d6f-9dcf-745ee4b8bbdd.jpeg"",
  ""makerName2"": ""Sandeep Panda"",
  ""makerHeadline2"": ""Co-Founder @ Hashnode"",
  ""makerUsername2"": ""sandeepg33k"",
  ""makerAvatarUrl2"": ""https://ph-avatars.imgix.net/101872/80b0b618-a540-4110-a6d1-74df39675ad0.jpeg"",
  ""primaryLinkUrl"": ""https://epigram.news/"",
  ""media1OriginalHeight"": 1080,
  ""media1OriginalWidth"": 1440,
  ""media1ImageUuid"": ""ac426fd1-3854-4734-b43d-34a5e06347ea.gif"",
  ""media1MediaType"": ""video"",
  ""media1MetadataUrl"": ""https://www.loom.com/share/b1a48a9b3cac4ba89ce772a3fbcc2847?sid=75efc771-25fa-4ac0-bb1b-5e38fc447deb"",
  ""media1VideoId"": ""b1a48a9b3cac4ba89ce772a3fbcc2847"",
  ""media2OriginalHeight"": 630,
  ""media2OriginalWidth"": 1200,
  ""media2ImageUuid"": ""8521a6bd-7640-487b-abd6-29b9f65fee32"",
  ""media2MediaType"": ""image"",
  ""media2MetadataUrl"": null,
  ""launchState"": ""featured"",
  ""thumbnailImageUuid"": ""735c2528-554c-467c-9dcf-745ee4b8bbdd.png"",
  ""link1StoreName"": ""Website"",
  ""link1WebsiteName"": ""epigram.news"",
  ""link2StoreName"": ""Github"",
  ""link2WebsiteName"": ""github.com"",
  ""latestScore"": 233,
  ""launchDayScore"": 233,
  ""userId"": ""129147"",
  ""topic1"": ""News"",
  ""topic2"": ""Open Source"",
  ""topic3"": ""Artificial Intelligence"",
  ""weeklyRank"": ""24"",
  ""commentsCount"": 20,
  ""postUrl"": ""https://www.producthunt.com/posts/epigram""
}
Target Audience
This workflow is ideal for developers, marketers, and data analysts seeking to automate the extraction and structuring of product data from Product Hunt for analytics, reporting, or integration with other tools."
Vector Database as a Big Data Analysis Tool for AI Agents [2/2 KNN],https://n8n.io/workflows/2657-vector-database-as-a-big-data-analysis-tool-for-ai-agents-22-knn/,"Vector Database as a Big Data Analysis Tool for AI Agents
Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"".
This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases.
Uploading (image) datasets to Qdrant
Set up meta-variables for anomaly detection in Qdrant
Anomaly detection tool
KNN classifier tool
For anomaly detection
The first pipeline to upload an image dataset to Qdrant.
The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection.
The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset.
For KNN (k nearest neighbours) classification
The first pipeline to upload an image dataset to Qdrant.
This pipeline is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset.
To recreate both
You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage.
[This workflow] KNN classification tool
This tool takes any image URL, and as output, it returns a class of the object on the image based on the image uploaded to the Qdrant dataset (lands).
An image URL is received via the Execute Workflow Trigger, which is then sent to the Voyage AI Multimodal Embeddings API to fetch its embedding.
The image's embedding vector is then used to query Qdrant, returning a set of X similar images with pre-labeled classes.
Majority voting is done for classes of neighbouring images.
A loop is used to resolve scenarios where there is a tie in Majority Voting, and we increase the number of neighbours to retrieve.
When the loop finally resolves, the identified class is returned to the calling workflow."
Vector Database as a Big Data Analysis Tool for AI Agents [2/3 - anomaly],https://n8n.io/workflows/2655-vector-database-as-a-big-data-analysis-tool-for-ai-agents-23-anomaly/,"Vector Database as a Big Data Analysis Tool for AI Agents
Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"".
This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases.
Uploading (image) datasets to Qdrant
Set up meta-variables for anomaly detection in Qdrant
Anomaly detection tool
KNN classifier tool
For anomaly detection
The first pipeline to upload an image dataset to Qdrant.
2. This is the second pipeline to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection.
The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset.
For KNN (k nearest neighbours) classification
The first pipeline to upload an image dataset to Qdrant.
The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset.
To recreate both
You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage.
[This workflow] Setting Up Cluster (Class) Centres & Cluster (Class) Threshold Scores for Anomaly Detection
Preparatory workflow to set cluster centres and cluster threshold scores so anomalies can be detected based on these thresholds.
Here, we're using two approaches to set up these centres: the ""distance matrix approach"" and the ""multimodal embedding model approach""."
"Extract Information from a Logo Sheet using forms, AI, Google Sheet and Airtable",https://n8n.io/workflows/2650-extract-information-from-a-logo-sheet-using-forms-ai-google-sheet-and-airtable/,"Instructions
This automation enables you to just upload any Image (via Form) of a Logo Sheet, containing multiple Images of Product Logos (most likely) which brings them in some context to one another.
After submitting an AI-Agent eats that Logo Sheet, turning it into an List of ""Productname"" and ""Attributes"", also checks if Tools are kind of similar to another, given the Context of the Image.
We utilize AI Vision capabilities for that. NOTE: It might not be able to extract all informations. For a ""upload and forget it"" Workflow it works for me. You can even run it multiple times, to be sure.
But if you need to make sure it extracts everything you might need to think about an Multi-Agent Setup with Validation-Agent Steps.
Once the Agent finishes the extraction, it will traditionally and deterministicly add those Attributes to Airtable (Creates those, if not already existing.) and also Upserts the Tool Informations.
It uses MD5 Hashes for turning Product Names into.. something fancy really, you could also use it without that, but I wanted to have something that looks atleast like an ID.
Setup
Set Up the Airtable like shown below.
Update and set Credentials for all Airtable Nodes.
Check or Adjust the Prompt of the Agent matching your use-case.
Activate the Workflow.
Open the Form (default: https://your-n8n.io/form/logo-sheet-feeder)
Enjoy growing your Airtable.

Enjoy the workflow! ‚ù§Ô∏è
let the work flow ‚Äî Workflow Automation & Development"
Telegram AI Bot: NeurochainAI Text & Image - NeurochainAI Basic API Integration,https://n8n.io/workflows/2584-telegram-ai-bot-neurochainai-text-and-image-neurochainai-basic-api-integration/,"This template provides a workflow to integrate a Telegram bot with NeurochainAI's inference capabilities, supporting both text processing and image generation. Follow these steps to get started:
Purpose: Enables seamless integration between your Telegram bot and NeurochainAI for advanced AI-driven text and image tasks.
Requirements
Telegram Bot Token.
NeurochainAI API Key.
Sufficient credits to utilize NeurochainAI services.
Features
Text processing through NeurochainAI's inference engine.
AI-powered image generation (Flux).
Easy customization and scalability for your use case.
Setup
Import the template into N8N.
Add your Telegram Bot Token and NeurochainAI API Key where prompted.
Follow the step-by-step instructions embedded in the template for configuration.
NeurochainAI Website
NeurochainAI Guides"
Summarize SERPBear data with AI (via Openrouter) and save it to Baserow,https://n8n.io/workflows/2565-summarize-serpbear-data-with-ai-via-openrouter-and-save-it-to-baserow/,"Who's this for?
If you own a website and need to analyze your keyword rankings
If you need to create a keyword report on your rankings
If you want to grow your keyword positions
SerpBear is an opensourced SEO tool specifically for keyword analytics.
Click here to read details of how I use it
Example output of A.I.
**Key Observations about Ranking Performance:**

- The top-performing keyword is ‚ÄúOpenrouter N8N‚Äù with a current position of 7 and an improving trend.
- Two keywords, ‚ÄúBest Docker Synology‚Äù and ‚ÄúBitwarden Synology‚Äù, are not ranking in the top 100 and have a stable trend.
- Three keywords, ‚ÄúObsidian Second Brain‚Äù, ‚ÄúAI Generated Reference Letter‚Äù, and ‚ÄúActual Budget Synology‚Äù, and ‚ÄúN8N Workflow Generator‚Äù are not ranking well and have a declining trend.

**Keywords showing the most improvement:**

- ‚ÄúOpenrouter N8N‚Äù has an improving trend and a relatively high ranking of 7.

**Keywords needing attention:**

- ‚ÄúObsidian Second Brain‚Äù has a declining trend and a low ranking of 69.
- ‚ÄúAI Generated Reference Letter‚Äù has a declining trend and a low ranking of 84.
- ‚ÄúActual Budget Synology‚Äù, ‚ÄúN8N Workflow Generator‚Äù, ‚ÄúBest Docker Synology‚Äù, and ‚ÄúBitwarden Synology‚Äù are not ranking in the top 100.
Use case
Instead of hiring an SEO expert, I run this report weekly. It checks the keyword rankings of the past week and gives me recommendations on what to improve.
How it works
The workflow gathers SerpBear analytics for the past 7 days.
It passes the data to openrouter.ai for A.I. analysis.
Finally it saves to baserow.
How to use this
Input your SerpBearcredentials
Enter your domain name
Input your Openrouter.ai credentials
Input your baserow credentials
You will need to create a baserow database with columns: Date, Note, Blog
Created by Rumjahn"
Send daily translated Calvin and Hobbes Comics to Discord,https://n8n.io/workflows/2560-send-daily-translated-calvin-and-hobbes-comics-to-discord/,"How it works
Automates the retrieval of Calvin and Hobbes daily comics.
Extracts the comic image URL from the website.
Translates comic dialogues to English and Korean.
Posts the comic and translations to Discord daily.
Set up steps
Estimated setup time: ~10-15 minutes.
Use a Schedule Trigger to automate the workflow at 9 AM daily.
Add nodes for parameter setup, HTTP request, data extraction, and integration with Discord.
Add detailed notes to each node in the workflow for easy understanding."
MongoDB AI Agent - Intelligent Movie Recommendations,https://n8n.io/workflows/2554-mongodb-ai-agent-intelligent-movie-recommendations/,"Who is this for?
This workflow is designed for:
Database administrators and developers working with MongoDB
Content managers handling movie databases
Organizations looking to implement AI-powered search and recommendation systems
Developers interested in combining LangChain, OpenAI, and MongoDB capabilities
What problem does this workflow solve?
Traditional database queries can be complex and require specific MongoDB syntax knowledge. This workflow addresses:
The complexity of writing MongoDB aggregation pipelines
The need for natural language interaction with movie databases
The challenge of maintaining user preferences and favorites
The gap between AI language models and database operations
What this workflow does
This workflow creates an intelligent agent that:
Accepts natural language queries about movies
Translates user requests into MongoDB aggregation pipelines
Queries a movie database containing detailed information including:
Plot summaries
Genre classifications
Cast and director information
Runtime and release dates
Ratings and awards
Provides contextual responses using OpenAI's language model
Allows users to save favorite movies to the database
Maintains conversation context using a window buffer memory
Setup
Required Credentials:
OpenAI API credentials
MongoDB connection details
Node Configuration:
Configure the MongoDB connection in the MongoDBAggregate node
Set up the OpenAI Chat Model with your API key
Ensure the webhook trigger is properly configured for receiving chat messages
Database Requirements:
A MongoDB collection named ""movies"" with the specified document structure
Proper indexes for efficient querying
Appropriate user permissions for read/write operations
How to customize this workflow
Modify the Document Structure:
Update the tool description in the MongoDBAggregate node to match your collection schema
Adjust the aggregation pipeline templates for your specific use case
Enhance the AI Agent:
Customize the prompt in the ""AI Agent - Movie Recommendation"" node
Modify the window buffer memory size based on your context needs
Add additional tools for more functionality
Extend Functionality:
Add more MongoDB operations beyond aggregation
Implement additional workflows for different types of queries
Create custom error handling and validation
Add user authentication and rate limiting
Integration Options:
Connect to external APIs for additional movie data
Add webhook endpoints for different platforms
Implement caching mechanisms for frequent queries
Add data transformation nodes for specific output formats
This workflow serves as a foundation that can be adapted to various use cases beyond movie recommendations, such as e-commerce product search, content management systems, or any scenario requiring intelligent database interaction."
Telegram to Spotify with OpenAI,https://n8n.io/workflows/2526-telegram-to-spotify-with-openai/,"Search music and play to Spotify from Telegram
This workflow is a simple demonstration on accessing a message model from Telegram and it makes searching for songs an easy task even if you can't remember the artist or song name.
An OpenAI message model tries to figure out the song and sends it to an active Spotify device**.
Use case
Imagine an office where you play music in the background and the employees can control the music without having to login to the playing account.
How it works
You describe the song in Telegram.
Telegram bot sends the text to n8n.
An OpenAI message model tries to find the song.
Spotify gets the search query string.
First match is then added to queue.
-- If there is no match a message is sent to Telegram and the process ends.
We change to the next track in the list.
We make sure the song starts playing by trying to resume.
We fetch the currently playing track.
We return ""now playing"" information to Telegram: Song Name - Artist Name - Album Name.
Error handling
Every Spotify step has it's on error handler under settings where we output the error.
Message parser receives the error and sends it to Telegram.
Requirements
Active workflow*
OpenAI API key
Telegram bot
Spotify account and Oauth2 API
Spotify active on a device**
.* The Telegram trigger is activated only if this workflow is active. You can however TEST the workflow in the editor by clicking ""Test step"" and then it waits for the Telegram event. When event is received, just step through all steps or just clicking ""Test step"" on the ""Fetch Now Playing"" node.
.** You must have a Spotify device active when trying to communicate with a device. Open Spotify and play something - not it is active."
Summarize Umami data with AI (via Openrouter) and save it to Baserow,https://n8n.io/workflows/2520-summarize-umami-data-with-ai-via-openrouter-and-save-it-to-baserow/,"Who's this for?
Anyone who wants to improve the SEO of their website
Umami users who want insights on how to improve their site
SEO managers who need to generate reports weekly
Case study
Watch youtube tutorial here
Get my SEO A.I. agent system here
You can read more about how this works here.
How it works
This workflow calls the Umami API to get data
Then it sends the data to A.I. for analysis
It saves the data and analysis to Baserow
How to use this
Input your Umami credentials
Input your website property ID
Input your Openrouter.ai credentials
Input your baserow credentials
You will need to create a baserow database with columns: Date, Summary, Top Pages, Blog (name of your blog).
Future development
Use this as a template. There's alot more Umami stats you can pull from the API.
Change the A.I. prompt to give even more detailed analysis.
Created by Rumjahn"
Enhance Security Operations with the Qualys Slack Shortcut Bot!,https://n8n.io/workflows/2510-enhance-security-operations-with-the-qualys-slack-shortcut-bot/,"Enhance Security Operations with the Qualys Slack Shortcut Bot!
Our Qualys Slack Shortcut Bot is strategically designed to facilitate immediate security operations directly from Slack. This powerful tool allows users to initiate vulnerability scans and generate detailed reports through simple Slack interactions, streamlining the process of managing security assessments.
Workflow Highlights:
Interactive Modals: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations.
Dynamic Workflow Execution: Integrates seamlessly with Qualys to execute vulnerability scans and create reports based on user-specified parameters.
Real-Time Feedback: Offers instant feedback within Slack, updating users about the status of their requests and delivering reports directly through Slack channels.
Operational Flow:
Parse Webhook Data: Captures and parses incoming data from Slack to understand user commands accurately.
Execute Actions: Depending on the user's selection, the workflow triggers other sub-workflows like 'Qualys Start Vulnerability Scan' or 'Qualys Create Report' for detailed processing.
Respond to Slack: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses.
Setup Instructions:
Verify that Slack and Qualys API integrations are correctly configured for seamless interaction.
Customize the modal interfaces to align with your organization's operational protocols and security policies.
Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected.
Need Assistance?
Explore our Documentation or get help from the n8n Community for more detailed guidance on setup and customization.
Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of vulnerabilities and streamlined reporting.
To handle the actual processing of requests, you will also need to deploy these two subworkflows:
Qualys Start Vulnerability Scan
Qualys Create Report
To simplify deployment, use this Slack App manifest to quickly create an app with the correct permissions:
{
    ""display_information"": {
        ""name"": ""Qualys n8n Bot"",
        ""description"": ""n8n Integration for Qualys"",
        ""background_color"": ""#2a2b2e""
    },
    ""features"": {
        ""bot_user"": {
            ""display_name"": ""Qualys n8n Bot"",
            ""always_online"": false
        },
        ""shortcuts"": [
            {
                ""name"": ""Scan Report Generator"",
                ""type"": ""global"",
                ""callback_id"": ""qualys-scan-report"",
                ""description"": ""Generate a report from the latest scan to review vulnerabilities and compliance.""
            },
            {
                ""name"": ""Launch Qualsys VM Scan"",
                ""type"": ""global"",
                ""callback_id"": ""trigger-qualys-vmscan"",
                ""description"": ""Start a Qualys Vulnerability scan from the comfort of your Slack Workspace""
            }
        ]
    },
    ""oauth_config"": {
        ""scopes"": {
            ""bot"": [
                ""commands"",
                ""channels:join"",
                ""channels:history"",
                ""channels:read"",
                ""chat:write"",
                ""chat:write.customize"",
                ""files:read"",
                ""files:write""
            ]
        }
    },
    ""settings"": {
        ""interactivity"": {
            ""is_enabled"": true,
            ""request_url"": ""Replace everything inside the double quotes with your workflow webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad"""",
            ""message_menu_options_url"": ""Replace everything inside the double quotes with your workflow message options webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad""""
        },
        ""org_deploy_enabled"": false,
        ""socket_mode_enabled"": false,
        ""token_rotation_enabled"": false
    }
}"
Monthly Spotify Track Archiving and Playlist Classification,https://n8n.io/workflows/2502-monthly-spotify-track-archiving-and-playlist-classification/,"Monthly Spotify Track Archiving and Playlist Classification
This n8n workflow allows you to automatically archive your monthly Spotify liked tracks in a Google Sheet, along with playlist details and descriptions. Based on this data, Claude 3.5 is used to classify each track into multiple playlists and add them in bulk.
Who is this template for?
This workflow template is perfect for Spotify users who want to systematically archive their listening history and organize their tracks into custom playlists.
What problem does this workflow solve?
It automates the monthly process of tracking, storing, and categorizing Spotify tracks into relevant playlists, helping users maintain well-organized music collections and keep a historical record of their listening habits.
Workflow Overview
Trigger Options: Can be initiated manually or on a set schedule.
Spotify Playlists Retrieval: Fetches the current playlists and filters them by owner.
Track Details Collection: Retrieves information such as track ID and popularity from the user‚Äôs library.
Audio Features Fetching: Uses Spotify's API to get audio features for each track.
Data Merging: Combines track information with their audio features.
Duplicate Checking: Filters out tracks that have already been logged in Google Sheets.
Data Logging: Archives new tracks into a Google Sheet.
AI Classification: Uses an AI model to classify tracks into suitable playlists.
Playlist Updates: Adds classified tracks to the corresponding playlists.
Setup Instructions
Credentials Setup:
Make sure you have valid Spotify OAuth2 and Google Sheets access credentials.
Trigger Configuration:
Choose between manual or scheduled triggers to start the workflow.
Google Sheets Preparation:
Set up a Google Sheet with the necessary structure for logging track details.
Spotify Playlists Setup:
Have a diverse range of playlists and exhaustive description (see example) ready to accommodate different music genres and moods.
Customization Options
Adjust Playlist Conditions:
Modify the AI model‚Äôs classification criteria to align with your personal music preferences.
Enhance Track Analysis:
Incorporate additional audio features or external data sources for more refined track categorization.
Personalize Data Logging:
Customize which track attributes to log in Google Sheets based on your archival preferences.
Configure Scheduling:
Set a preferred schedule for periodic track archiving, e.g., monthly or weekly.
Cost Estimate
For 300 tracks, the token usage amounts to approximately 60,000 tokens (58,000 for input and 2,000 for completion), costing around 20 cents with Claude 3.5 Sonnet (as of October 2024).
Playlists' Description Examples
Playlist Name Playlist Description
Classique Indulge in the timeless beauty of classical music with this refined playlist. From baroque to romantic periods, this collection showcases renowned compositions.
Poi Find your flow with this dynamic playlist tailored for poi, staff, and ball juggling. Featuring rhythmic tracks that complement your movements.
Pro Sound Boost your productivity and focus with this carefully selected mix of concentration-enhancing music. Ideal for work or study sessions.
ChillySleep Drift off to dreamland with this soothing playlist of sleep-inducing tracks. Gentle melodies and ambient sounds create a peaceful atmosphere for restful sleep.
To Sing Warm up your vocal cords and sing your heart out with karaoke-friendly tracks. Featuring popular songs, perfect for solo performances or group sing-alongs.
1990s Relive the diverse musical landscape of the 90s with this eclectic mix. From grunge to pop, hip-hop to electronic, this playlist showcases defining genres.
1980s Take a nostalgic trip back to the era of big hair and neon with this 80s playlist. Packed with iconic hits and forgotten gems, capturing the energy of the decade.
Groove Up Elevate your mood and energy with this upbeat playlist. Featuring a mix of feel-good tracks across various genres to lift your spirits and get you moving.
Reggae & Dub Relax and unwind with the laid-back vibes of reggae and dub. This playlist combines classic reggae tunes with deep, spacious dub tracks for a chilled-out vibe.
Psytrance Embark on a mind-bending journey with this collection of psychedelic trance tracks. Ideal for late-night dance sessions or intense focus.
Cumbia Sway to the infectious rhythms of Cumbia with this lively playlist. Blending traditional Latin American sounds with modern interpretations for a danceable mix.
Funky Groove Get your body moving with this collection of funk and disco tracks. Featuring irresistible basslines and catchy rhythms, perfect for dance parties.
French Chanson Experience the romance and charm of France with this mix of classic and modern French songs, capturing the essence of French musical culture.
Workout Motivation Push your limits and power through your exercise routine with this high-energy playlist. From warm-up to cool-down, these tracks will keep you motivated.
Cinematic Instrumentals Immerse yourself in a world of atmospheric sounds with this collection of cinematic instrumental tracks, perfect for focus, relaxation, or contemplation."
create e-mail responses with fastmail and OpenAI,https://n8n.io/workflows/2441-create-e-mail-responses-with-fastmail-and-openai/,"Workflow Description:
This n8n workflow automates the drafting of email replies for Fastmail using OpenAI's GPT-4 model. Here‚Äôs the overall process:
Email Monitoring: The workflow continuously monitors a specified IMAP inbox for new, unread emails.
Email Data Extraction: When a new email is detected, it extracts relevant details such as the sender, subject, email body, and metadata.
AI Response Generation: The extracted email content is sent to OpenAI's GPT-4, which generates a personalized draft response.
Get Fastmail Session and Mailbox IDs: Connects to the Fastmail API to retrieve necessary session details and mailbox IDs.
Draft Identification: Identifies the ""Drafts"" folder in the mailbox.
Draft Preparation: Compiles all the necessary information to create the draft, including the generated response, original email details, and specified recipient.
Draft Uploading: Uploads the prepared draft email to the ""Drafts"" folder in the Fastmail mailbox.
Prerequisites:
IMAP Email Account: You need to configure an IMAP email account in n8n to monitor incoming emails.
Fastmail API Credentials: A Fastmail account with JMAP API enabled. You should set up HTTP Header authentication in n8n with your Fastmail API credentials.
OpenAI API Key: An API key from OpenAI to access GPT-4. Make sure to configure the OpenAI credentials in n8n.
Configuration Steps:
Email Trigger (IMAP) Node:
Provide your email server settings and credentials to monitor emails.
HTTP Request Nodes for Fastmail:
Set up HTTP Header authentication in n8n using your Fastmail API credentials.
Replace the httpHeaderAuth credential IDs with your configured credential IDs.
OpenAI Node:
Configure the OpenAI API key in n8n.
Replace the openAiApi credential ID with your configured credential ID.
By following these steps and setting up the necessary credentials, you can seamlessly automate the creation of email drafts in response to new emails using AI-generated content. This workflow helps improve productivity and ensures timely, personalized communication."
Manipulate PDF with Adobe developer API,https://n8n.io/workflows/2424-manipulate-pdf-with-adobe-developer-api/,"Adobe developer API
Did you know that Adobe provides an API to perform all sort of manipulation on PDF files :
Split PDF, Combine PDF
OCR
Insert page, delete page, replace page, reorder page
Content extraction (text content, tables, pictures)
...
The free tier allows up to 500 PDF operation / month. As it comes directly from Adobe, it works often better than other alternatives.
Adobe documentation:
https://developer.adobe.com/document-services/docs/overview/pdf-services-api/howtos/
https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/gettingstarted/
What does this workflow do
The API is a bit painful to use. To perform a transformation on a PDF it requires to
Authenticate and get a temporal token
Register a new asset (file)
Upload you PDF to the registered asset
Perform a query according to the transformation requested
Wait for the query to be proccessed by Adobe backend
Download the result
This workflow is a generic wrapper to perform all these steps for any transformation endpoint. I usually use it from other workflow with an Execute Workflow node.
Examples are given in the workflow.
Example use case
This service is useful for example to clean PDF data for an AI / RAG system.
My favorite use-case is to extract table as images and forward images to an AI for image recognition / description which is often more accuarate than feedind raw tabular data to a LLM."
Venafi Cloud Slack Cert Bot,https://n8n.io/workflows/2422-venafi-cloud-slack-cert-bot/,"Enhance Security Operations with the Venafi Slack CertBot!
Venafi Presentation - Watch Video
Our Venafi Slack CertBot is strategically designed to facilitate immediate security operations directly from Slack. This tool allows end users to request Certificate Signing Requests that are automatically approved or passed to the Secops team for manual approval depending on the Virustotal analysis of the requested domain. Not only does this help centralize requests, but it helps an organization maintain the security certifications by allowing automated processes to log and analyze requests in real time.
Workflow Highlights:
Interactive Modals: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations.
Dynamic Workflow Execution: Integrates seamlessly with Venafi to execute CSR generation and if any issues are found, AI can generate a custom report that is then passed to a slack teams channel for manual approval with the press of a single button.
Operational Flow:
Parse Webhook Data: Captures and parses incoming data from Slack to understand user commands accurately.
Execute Actions: Depending on the user's selection, the workflow triggers other actions within the flow like automatic Virustotal Scanning.
Respond to Slack: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses.
Setup Instructions:
Verify that Slack and Qualys API integrations are correctly configured for seamless interaction.
Customize the modal interfaces to align with your organization's operational protocols and security policies.
Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected.
Need Assistance?
Explore Venafi's Documentation or get help from the n8n Community for more detailed guidance on setup and customization.
Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of CSR's."
Extract spending history from gmail to google sheet,https://n8n.io/workflows/2414-extract-spending-history-from-gmail-to-google-sheet/,"How it works
Fetch transaction notification emails (including attachments)
Clean up data
Let AI (Basic LLM Chain node) generate bookkeeping item
Send to Google sheet
Details
The example fetch email from Gmail lables, suggested using filters to automatically orgianize email into the labels
Data will send to ""raw data"" sheet
Example google sheet:
https://docs.google.com/spreadsheets/d/1_IhdHj8bxtsfH2MRqKuU2LzJuzm4DaeKSw46eFcyYts/edit?gid=1617968863#gid=1617968863"
Use AI to organize your Todoist Inbox,https://n8n.io/workflows/2394-use-ai-to-organize-your-todoist-inbox/,"How it works
This workflow adds a priority to each Todoist item in your inbox, based on a list of projects that you add in the workflow.
Setup
Add your Todoist credentials
Add your OpenAI credentials
Set your project names and add priority"
"Community Insights using Qdrant, Python and Information Extractor",https://n8n.io/workflows/2374-community-insights-using-qdrant-python-and-information-extractor/,"This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings:
Survey Insights
Customer Insights
Community Insights
This template demonstrates the Community Insights scenario where HN commments can be quickly grouped by similarity and an AI agent can generate insights on those groupings.
With this workflow, Researchers or HN users can quickly breakdown community consensus on a particular topic and identify frequently mentioned positives and negatives.
Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vQXaQU9XxsxnUIIeqmmf1PuYRuYtwviVXTv6Mz9Vo6_a4ty-XaJHSeZsptjWXS3wGGDG8Z4u16rvE7l/pubhtml
How it works
HN comments are imported via the Hacknews API node.
Comments are then inserted into a Qdrant collection carefully tagged with the Hackernews API metadata.
Comments are then fetched and are put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups.
Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for.
The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the researcher or the HN user.
Requirements
Works best with lots of comments!
Qdrant Vectorstore for storing embeddings.
OpenAI account for embeddings and LLM.
Customising the Template
Adjust clustering parameters which make sense for your data.
Adjust sentimentality setting if comments are overwhelmingly negative at times."
"Survey Insights with Qdrant, Python and Information Extractor",https://n8n.io/workflows/2372-survey-insights-with-qdrant-python-and-information-extractor/,"This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings:
Survey Insights
Customer Insights
Community Insights
This template demonstrates the Survey Insights scenario where survey participant responses can be quickly grouped by similarity and an AI agent can generate insights on those groupings.
With this workflow, researchers can save days and even weeks of work breaking down cohorts of participants and identify frequently mentioned positives and negatives.
Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vT6m8XH8JWJTUAfwojc68NAUGC7q0lO7iV738J7aO5fuVjiVzdTRRPkMmT1C4N8TwejaiT0XrmF1Q48/pubhtml#
How it works
All survey questions and responses are imported from a Google Sheet.
Responses are then inserted into a Qdrant collection carefully tagged with the question and survey metadata.
For each question, all relevant response are put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups.
Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for.
The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the researcher.
Requirements
Survey data and format as shown in the attached google sheet.
Qdrant Vectorstore for storing embeddings.
OpenAI account for embeddings and LLM.
Customising the Template
Adjust clustering parameters which make sense for your data. Add more clusters for open-ended questions and less clusters when responses are multiple choice."
Query n8n Credentials with AI SQL Agent,https://n8n.io/workflows/2347-query-n8n-credentials-with-ai-sql-agent/,"This n8n workflow is a fun way to query and search over your credentials on your n8n instance.
Good to know
Your credentials should remain safe as this workflow does not decrypt or use any decrypted data.
Example Usage
""Which workflows are using Slack and Google Calendar?""
""Which workflows have AI in their name but are not using openAI?""
How it works
Using the n8n API, it fetches all workflow data on the instance. Workflow data contains references to credentials used so this will be extracted.
With some necessary reformatting, the workflows and their credentials metadata are stored to a SQLite database.
Next, an AI agent is used with a custom SQL tool that reads the SQLite database created in the previous step.
The AI agent is instructed to perform SQL queries against our workflow credential table when asked about credentials by the user.
Requirements
You'll need an n8n API key. Please note that only workflows will be scoped to your API key.
Customising the workflow
Add extra table fields to the SQLite database to answer even more complex queries such as:
workflow status to differentiate between active and inactive workflows."
"Build Your Own Image Search Using AI Object Detection, CDN and ElasticSearch",https://n8n.io/workflows/2331-build-your-own-image-search-using-ai-object-detection-cdn-and-elasticsearch/,"This n8n workflow demonstrates how to automate indexing of images to build a object-based image search.
By utilising a Detr-Resnet-50 Object Classification model, we can identify objects within an image and store these associations in Elasticsearch along with a reference to the image.
How it works
An image is imported into the workflow via HTTP request node.
The image is then sent to Cloudflare's Worker AI API where the service runs the image through the Detr-Resnet-50 object classification model.
The API returns the object associations with their positions in the image, labels and confidence score of the classification.
Confidence scores of less the 0.9 are discarded for brevity.
The image's URL and its associations are then index in an ElasticSearch server ready for searching.
Requirements
A Cloudflare account with Workers AI enabled to access the object classification model.
An ElasticSearch instance to store the image url and related associations.
Extending this workflow
Further enrich your indexed data with additional attributes or metrics relevant to your users.
Use a vectorstore to provide similarity search over the images."
Enrich Property Inventory Survey with Image Recognition and AI Agent,https://n8n.io/workflows/2330-enrich-property-inventory-survey-with-image-recognition-and-ai-agent/,"This n8n workflow assists property managers and surveyors by reducing the time and effort it takes to complete property inventory surveys.
In such surveys, articles and goods within a property may need to be captured and reported as a matter of record. This can take a sizable amount of time if the property or number of items is big enough.
Our solution is to delegate this task to a capable AI Agent who can identify and fill out the details of each item automatically.
How it works
An AirTable Base is used to capture just the image of an item within the property
Our workflow monitoring this AirTable Base sends the photo to an AI image recognition model to describe the item for purpose of identification.
Our AI agent uses this description and the help of Google's reverse image search in an attempt to find an online product page for the item.
If found, the product page is scraped for the item's specifications which are then used to fill out the rest of the details of the item in our Airtable.
Requirements
Airtable for capturing photos and product information
OpenAI account to for image recognition service and AI for agent
SerpAPI account for google reverse image search.
Firecrawl.dev account for webspacing.
Customising this workflow
Try building an internal inventory database to query and integrate into the workflow. This could save on costs by avoiding fetching new each time for common items."
Speed Up Social Media Banners With BannerBear.com,https://n8n.io/workflows/2322-speed-up-social-media-banners-with-bannerbearcom/,"This n8n workflow shows an easy way to automate the creation of social media assets using AI and a service like BannerBear.
Designed for the busy marketer, leveraging AI image generation capabilities can help cut down production times and allow reinvesting into higher quality content.
How it works
This workflow generates social media banners for online events. Using a form trigger, a user can define the banner text and suggest an image to be generated.
This request is passed to OpenAI's Dalle-3 image generation service to produce a relevant graphic for the event banner.
This generated image is uploaded and sent to BannerBear where a template will use it and the rest of the form data to produce the banner.
BannerBear returns the final banner which can now be used in an assortment of posts and publications.
Requirements
A BannerBear.com account and template is required
An OpenAI account to use the Dalle-3 service.
Customising the workflow
We've only shown a small section of what BannerBear has to offer. With experimentation and other asset generating services such as AI audio and video, you should be able to generate more than just static banners!"
Enrich Pipedrive's Organization Data with OpenAI GPT-4o & Notify it in Slack,https://n8n.io/workflows/2318-enrich-pipedrives-organization-data-with-openai-gpt-4o-and-notify-it-in-slack/,"This workflow enriches new Pipedrive organization's data by adding a note to the organization object in Pipedrive. It assumes there is a custom ""website"" field in your Pipedrive setup, as data will be scraped from this website to generate a note using OpenAI. Then, a notification is sent in Slack.
‚ö†Ô∏è Disclaimer
This workflow uses a scraping API. Before using it, ensure you comply with the regulations regarding web scraping in your country or state.
Important Notes
The OpenAI model used is GPT-4o, chosen for its large input token capacity. However, it is not the cheapest model if cost is very important to you.
The system prompt in the OpenAI Node generates output with relevant information, but feel free to improve or modify it according to your needs.
How It Works
Node 1: Pipedrive Trigger - An Organization is Created
This is the trigger of the workflow. When an organization object is created in Pipedrive, this node is triggered and retrieves the data. Make sure you have a ""website"" custom field in Pipedrive (the name of the field in the n8n node will appear as a random ID and not with the Pipedrive custom field name).
Node 2: ScrapingBee - Get Organization's Website's Homepage Content
This node scrapes the content from the URL of the website associated with the Pipedrive Organization created in Node 1. The workflow uses the ScrapingBee API, but you can use any preferred API or simply the HTTP request node in n8n.
Node 3: OpenAI - Message GPT-4o with Scraped Data
This node sends HTML-scraped data from the previous node to the OpenAI GPT-4o model. The system prompt instructs the model to extract company data, such as products or services offered and competitors (if known by the model), and format it as HTML for optimal use in a Pipedrive Note.
Node 4: Pipedrive - Create a Note with OpenAI Output
This node adds a Note to the Organization created in Pipedrive using the OpenAI node output. The Note will include the company description, target market, selling products, and competitors (if GPT-4o was able to determine them).
Node 5 & 6: HTML To Markdown & Code - Markdown to Slack Markdown
These two nodes format the HTML output to Slack Markdown.
The Note created in Pipedrive is in HTML format, as specified by the System Prompt of the OpenAI Node. To send it to Slack, it needs to be converted to Markdown and then to Slack Markdown.
Node 7: Slack - Notify
This node sends a message in Slack containing the Pipedrive Organization Note created with this workflow."
Store Notion's Pages as Vector Documents into Supabase with OpenAI,https://n8n.io/workflows/2290-store-notions-pages-as-vector-documents-into-supabase-with-openai/,"Workflow updated on 17/06/2024:
Added 'Summarize' node to avoid creating a row for each Notion content block in the Supabase table.
Store Notion's Pages as Vector Documents into Supabase
This workflow assumes you have a Supabase project with a table that has a vector column. If you don't have it, follow the instructions here: Supabase Langchain Guide
Workflow Description
This workflow automates the process of storing Notion pages as vector documents in a Supabase database with a vector column. The steps are as follows:
Notion Page Added Trigger:
Monitors a specified Notion database for newly added pages. You can create a specific Notion database where you copy the pages you want to store in Supabase.
Node: Page Added in Notion Database
Retrieve Page Content:
Fetches all block content from the newly added Notion page.
Node: Get Blocks Content
Filter Non-Text Content:
Excludes blocks of type ""image"" and ""video"" to focus on textual content.
Node: Filter - Exclude Media Content
Summarize Content:
Concatenates the Notion blocks content to create a single text for embedding.
Node: Summarize - Concatenate Notion's blocks content
Store in Supabase:
Stores the processed documents and their embeddings into a Supabase table with a vector column.
Node: Store Documents in Supabase
Generate Embeddings:
Utilizes OpenAI's API to generate embeddings for the textual content.
Node: Generate Text Embeddings
Create Metadata and Load Content:
Loads the block content and creates associated metadata, such as page ID and block ID.
Node: Load Block Content & Create Metadata
Split Content into Chunks:
Divides the text into smaller chunks for easier processing and embedding generation.
Node: Token Splitter"
Classify lemlist replies using OpenAI and automate reply handling,https://n8n.io/workflows/2287-classify-lemlist-replies-using-openai-and-automate-reply-handling/,"Who this is for
This workflow is for sales people who want to quickly and efficiently follow up with their leads
What this workflow does
This workflow starts every time a new reply is received in lemlist. It then classifies the response using openAI and creates the correct follow up task. The follow-up tasks currently include:
Slack alerts when a lead for each new replies
Tag interested leads in lemlist
Unsubscription of leads when they request it
The Slack alerts include:
Lead email address
Sender email address
Reply type (positive, not interested...etc)
A preview of the reply
Setup
To set this template up, simply follow the stickies steps in it
How to customize this workflow to your needs
Adjust the follow up tasks to your needs
Change the Slack notification to your needs
..."
AI-Powered Children's Arabic Storytelling on Telegram,https://n8n.io/workflows/2234-ai-powered-childrens-arabic-storytelling-on-telegram/,"Template for Kids' Story in Arabic
The n8n template for creating kids' stories in Arabic offers a versatile platform for storytellers to captivate young audiences with educational and interactive tales. It allows for customization to suit various use cases and can be set up effortlessly.
Check this example: https://t.me/st0ries95
Use Cases
Educational Platforms:
Educational platforms can automate the creation and distribution of educational stories in Arabic for children using this template. By incorporating visual and auditory elements into the storytelling process, educational platforms can enhance learning experiences and engage young learners effectively.
Children's Libraries:
Children's libraries can utilize this template to curate and share a diverse collection of Arabic stories with young readers. The automated generation of visual content and audio files enhances the storytelling experience, encouraging children to immerse themselves in new worlds and characters through captivating narratives.
Language Learning Apps:
Language learning apps focused on Arabic can integrate this template to offer culturally rich storytelling experiences for children learning the language. By translating stories into Arabic and supplementing them with visual and auditory components, these apps can facilitate language acquisition in an enjoyable and interactive manner.
Configuration Guide for Nodes
OpenAI Chat Model Nodes:
Functionality: Allows interaction with the OpenAI GPT-4 Turbo model.
Purpose: Enables communication with advanced chat capabilities.
Create a Prompt for DALL-E Node:
Customization: Tailor prompts for generating relevant visual content.
Summarization: Define prompts for visual content generation without text.
Generate an Image for the Story Node:
Resource Type: Specifies image as the resource.
Prompt Setup: Configures prompt for textless image creation within the visual content.
Generate Audio for the Story Node:
Resource Type: Chooses audio as the resource.
Input Definition: Sets input text for audio file generation.
Translate the Story to Arabic Node:
Chunking Mode Selection: Allows advanced chunking mode choice.
Summarization Configuration: Sets method and prompts for story translation into Arabic.
Send the Story To Channel Node:
Channel ID: Specifies the channel ID for sending the story text.
Text Configuration: Sets up the text to be sent to the channel.
By following these node descriptions, users can effectively configure the n8n template for kids' stories in Arabic, tailoring it to specific use cases for a seamless and engaging storytelling experience for young audiences."
AI-Powered Children's English Storytelling on Telegram with OpenAI,https://n8n.io/workflows/2233-ai-powered-childrens-english-storytelling-on-telegram-with-openai/,"Unleashing Creativity: Transforming Children's English Storytelling with Automation and AI
Check this example: https://t.me/st0ries95
Summary
In the realm of children's storytelling, automation is revolutionizing the way captivating tales are created and shared. This article highlights the transformative power of setting up a workflow for AI-powered children's English storytelling on Telegram. By delving into the use cases and steps involved, we uncover how this innovative approach is inspiring young minds and fostering a love for storytelling in children.
Usecase
The workflow for children's stories is a game-changer for content creators, educators, and parents seeking to engage children through imaginative and educational storytelling. Here's how this workflow is making a difference:
Streamlined Content Creation: By providing a structured framework and automation for story generation, audio creation, and image production, the workflow simplifies the process of crafting captivating children's stories.
Enhanced Educational Resources: Teachers can leverage this workflow to develop interactive educational materials that incorporate storytelling, making learning more engaging for students.
Personalized Parental Engagement: Parents can share personalized stories with their children, nurturing a passion for reading and creativity while strengthening family bonds through shared storytelling experiences.
Community Connection: Organizations and community groups can use the workflow to connect with their audience and promote literacy and creativity by creating and sharing children's stories.
Inspiring Imagination: Through automated creation and sharing of enchanting stories, the workflow aims to spark imagination, inspire young minds, and instill a love for storytelling in children.
Node Explanation
OpenAI Chat Model: Utilizes the OpenAI Chat Model to generate text for the children's stories.
Schedule Trigger: Triggers the workflow at set intervals (every 12 hours) to generate new stories.
Recursive Character Text Splitter: Splits text into smaller chunks for processing.
OpenAI Chat Model2: Another OpenAI Chat Model node for generating prompts for image creation.
Send Story Text: Sends the generated story text to a specified Telegram chat.
Send Audio for the Story: Sends audio files of the stories to the Telegram chat.
Send Story Picture: Shares images related to the stories on Telegram.
Create a Kids Stories: Generates captivating short tales for kids using prompts provided.
Generate Audio for the Story: Converts generated text into audio files for storytelling.
Create a Prompt for DALL-E: Creates prompts for generating images related to the stories.
Generate a Picture for the Story: Generates pictures based on the prompts for visual storytelling.
By embracing automation in children's storytelling, we unleash creativity, inspire young minds, and create magical experiences that resonate with both storytellers and listeners alike."
Configure your own Image Creation API Using OpenAI DALLE-3,https://n8n.io/workflows/2217-configure-your-own-image-creation-api-using-openai-dalle-3/,"How it works:
Webhook URL that responds to Requests with an AI generated Image based on the prompt provided in the URL.
Setup Steps:
Ideate your prompt
URL Encode The Prompt (as shown in the Template)
Authenticate with your OpenAI Credentials
Put together the Webhook URL with your prompt and enter into a webbrowser
In this way you can expose a public url to users, employee's etc. without exposing your OpenAI API Key to them.
Click here to find a blog post with additional information."
Traveler Co-Pilot: AI-Powered Telegram for Easy Language and Image Translation,https://n8n.io/workflows/2213-traveler-co-pilot-ai-powered-telegram-for-easy-language-and-image-translation/,"Introduction
The Traveler Co-Pilot empowers you to confidently traverse the world, connecting with ease and breaking language barriers:
Engage in conversations with locals
Navigate menus at foreign eateries
Comprehend road signs effortlessly.
Features
Seamless Speech-to-Speech Translation
Communicate in any of the 55 supported languages, and witness the bot translate your words into another language in real-time, all through speech.
Visual Translation Magic
Capture images containing text, and the bot will work its magic by recognizing and translating the text into the desired language, right before your eyes.
Setup Steps
Open the Settings node and specify the languages you would like to work with"
Automate Screenshots with URLbox & Analyze them with AI,https://n8n.io/workflows/2207-automate-screenshots-with-urlbox-and-analyze-them-with-ai/,"In this automation we first make a screenshot with a screenshot API called URLbox and then send this screenshot into the OpenAI API and analyze it.
You can extend this automation by the way you want to ingest the website url's & names into this workflow.
Options as data source:
Postgres
Google Sheets
Your CRM
...
Setup:
Replace Website & URL in Setup Node
Put in your URLbox API Key
Put in your OpenAI credentials
Click here for a blog article with more information on the automation."
Assistant for Hubspot Chat using OpenAi and Airtable,https://n8n.io/workflows/2189-assistant-for-hubspot-chat-using-openai-and-airtable/,"This workflow will allow you to use OpenAI Assistant API together with a chatting platform. This version is configured to work with Hubspot, however, the Hubspot modules can be replaced by other platform and it will work similarly.
Prerequisites:
Create a Hubspot Chat (Live chat available on free plan) or Chatflow (paid hubspot only) and configure it to send all replies toward an n8n webhook (you need to create a custom app for that. I will create a separate article on how to do it, meanwhile, feel free to message me if you need support.
Setup:
Create a OpenAI Assistant, define its functionality and functions
Update the Hubspot modules with the Hubspot API Key
Update the OpenAI modules with OpenAI API Key
Create an Airtable or any other database where you keep a reference between the thread id in Hubspot and Assistant API
If you need help deploying this solution don't hesitate to email me or schedule a call here."
Summarize Google Sheets form feedback via OpenAI's GPT-4,https://n8n.io/workflows/2164-summarize-google-sheets-form-feedback-via-openais-gpt-4/,"This n8n workflow was developed to collect and summarize feedback from an event that was collected via a Google Form and saved in a Google Sheets document. The workflow is triggered manually by clicking on the ""Test workflow"" button.
The Google Sheets node retrieves the responses from the feedback form. The Aggregate node then combines all responses for each question into arrays and prepares the data for analysis.
The OpenAI node processes the aggregated feedback data. System Prompt instructs the model to analyze the responses and generate a summary report that includes the overall sentiment regarding the event and constructive suggestions for improvement.
The Markdown node converts the summary report, which is in Markdown format, into HTML. Finally, the Gmail node sends an HTML-formatted email to the specified email address."
Classify new bugs in Linear with OpenAI's GPT-4 and move them to the right team,https://n8n.io/workflows/2154-classify-new-bugs-in-linear-with-openais-gpt-4-and-move-them-to-the-right-team/,"Use case
When working with multiple teams, bugs must get in front of the right team as quickly as possible to be resolved. Normally this includes a manual grooming of new bugs that have arrived in your ticketing system (in our case Linear). We found this way too time-consuming. That's why we built this workflow.
What this workflow does
This workflow triggers every time a Linear issue is created or updated within a certain team. For us at n8n, we created one general team called Engineering where all bugs get added in the beginning. The workflow then checks if the issue meets the criteria to be auto-moved to a certain team. In our case, that means that the description is filled, that it has the bug label, and that it's in the Triage state. The workflow then classifies the bug using OpenAI's GPT-4 model before updating the team property of the Linear issue. If the AI fails to classify a team, the workflow sends an alert to Slack.
Setup
Add your Linear and OpenAi credentials
Change the team in the Linear Trigger to match your needs
Customize your teams and their areas of responsibility in the Set me up node. Please use the format [Teamname][Description/Areas of responsibility]. Also, make sure that the team names match the names in Linear exactly.
Change the Slack channel in the Set me up node to your Slack channel of choice.
How to adjust it to your needs
Play around with the context that you're giving to OpenAI, to make sure the model has enough knowledge about your teams and their areas of responsibility
Adjust the handling of AI failures to your needs
How to enhance this workflow
At n8n we use this workflow in combination with some others. E.g. we have the following things on top:
We're using an automation that enables everyone to add new bugs easily with the right data via a /bug command in Slack (check out this template if that's interesting to you)
This workflow was built using n8n version 1.30.0"
Ask a human for help when the AI doesn't know the answer,https://n8n.io/workflows/2095-ask-a-human-for-help-when-the-ai-doesnt-know-the-answer/,"This is a workflow that tries to answer user queries using the standard GPT-4 model. If it can't answer, it sends a message to Slack to ask for human help. It prompts the user to supply an email address.
This workflow is used in Advanced AI examples | Ask a human in the documentation.
To use this workflow:
Load it into your n8n instance.
Add your credentials as prompted by the notes.
Configure the Slack node to use your Slack details, or swap out Slack for a different service."
Convert text to speech with OpenAI,https://n8n.io/workflows/2092-convert-text-to-speech-with-openai/,"How It Works
This workflow sends an HTTP request to OpenAI's Text-to-Speech (TTS) model, returning an .mp3 audio recording of the provided text.
This template is meant to be adapted for your individual use case, and requires a valid OpenAI credential.
Gotchas
Per OpenAI's Usage Policies, you must provide a clear disclosure to end users that the TTS voice they are hearing is AI-generated and not a human voice, if you are using this workflow to provide audio output to users."
Translate audio using AI,https://n8n.io/workflows/2083-translate-audio-using-ai/,"Overview
This workflow takes some French text, and translates it into spoken audio.
It then transcribes that audio back into text, translates it into English and generates an audio file of the English text.
To do so, it uses ElevenLabs (which has a free tier) and OpenAI.
Setup
These steps should only take a few minutes:
In ElevenLabs, add a voice to your voice lab and copy its ID. Add it to the 'Set voice ID' node
Get your ElevenLabs API key (click your name in the bottom-left of ElevenLabs and choose ‚Äòprofile‚Äô)
In the 'Generate French audio' node, create a new header auth cred. Set the name to xi-api-key and the value to your API key
In the 'credential' field of the 'Transcribe audio' node, create a new OpenAI cred with your OpenAI API key
Run the workflow by clicking the orange button at the bottom of the canvas"
Use any LangChain module in n8n (with the LangChain code node),https://n8n.io/workflows/2082-use-any-langchain-module-in-n8n-with-the-langchain-code-node/,"LangChain is a framework for building AI functionality that users large language models. By leveraging the functionality of LangChain, you can write even more powerful workflows.
This workflow shows how you can write LangChain code within n8n, including importing LangChain modules.
The workflow itself produces a summary of a YouTube video, when given the video's ID.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Qualify replies from Pipedrive persons with AI,https://n8n.io/workflows/2039-qualify-replies-from-pipedrive-persons-with-ai/,"About the workflow
The workflow reads every reply that is received from a cold email campaign and qualifies if the lead is interested in a meeting. If the lead is interested, a deal is made in pipedrive. You can add as many email inboxes as you need!
Setup:
Add credentials to the Gmail, OpenAI and Pipedrive Nodes.
Add a in_campaign field in Pipedrive for persons. In Pipedrive click on your credentials at the top right, go to company settings > Data fields > Person and click on add custom field. Single option [TRUE/FALSE].
If you have only one email inbox, you can delete one of the Gmail nodes.
If you have more than two email inboxes, you can duplicate a Gmail node as many times as you like. Just connect it to the Get email node, and you are good to go!
In the Gmail inbox nodes, select Inbox under label names and uncheck Simplify."
OpenAI assistant with custom tools,https://n8n.io/workflows/2025-openai-assistant-with-custom-tools/,"This workflow shows how you can get your OpenAI assistant to call an n8n workflow as a tool. Since you can put almost any functionality in an n8n workflow, this means you can give your assistant access to almost any data source.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
AI Customer feedback sentiment analysis,https://n8n.io/workflows/1996-ai-customer-feedback-sentiment-analysis/,"LOOM Walkthrough: https://www.loom.com/share/4fbe06872cb8483993e7792018594f08
How it works
Create a form for customer feedback, have OpenAI classify the sentiment of the feedback (positive/neutral/negative) and store it in Google sheets!
Set up steps
Connect Google sheets
Connect your OpenAi account (api key + org Id)
Create a customer feedback form, use an existing one or use the one below as example.
All set!
Here is the example google sheet being used in this workflow: https://docs.google.com/spreadsheets/d/1omWdRbiT6z6GNZ6JClu9gEsRhPQ6J0EJ2yXyFH9Zng4/edit?usp=sharing. You can download it to your account."
Prepare CSV files with GPT-4,https://n8n.io/workflows/1967-prepare-csv-files-with-gpt-4/,"This workflow generates CSV files containing a list of 10 random users with specific characteristics using OpenAI's GPT-4 model. It then splits this data into batches, converts it to CSV format, and saves it to disk for further use.
The execution of the workflow begins from here when triggered manually.
""OpenAI"" Node. This uses the OpenAI API to generate random user data. The input to the OpenAI API is a fixed string, which asks for a list of 10 random users with some specific attributes. The attributes include a name and surname starting with the same letter, a subscription status, and a subscription date (if they are subscribed). There is also a short example of the JSON object structure. This technique is called one-shot prompting.
""Split In Batches"" Node. This node is used to handle the OpenAI responses one by one.
""Parse JSON"" Node. This node converts the content of the message received from the OpenAI node (which is in string format) into a JSON object.
""Make JSON Table"" Node. This node is used to convert the JSON data into a tabular format, which is easier to handle for further data processing.
""Convert to CSV"" Node. This node converts the table format data received from the ""Make JSON Table"" node into CSV format and assigns a file name.
""Save to Disk"" Node. This node is used to save the CSV generated in the previous node to disk in the "".n8n"" directory.
The workflow is designed in a circular manner. So, after saving the file to disk, it goes back to the ""Split In Batches"" node to process the OpenAI output, until all batches are processed."
AI: Ask questions about any data source (using the n8n workflow retriever),https://n8n.io/workflows/1958-ai-ask-questions-about-any-data-source-using-the-n8n-workflow-retriever/,"This template aims to perform Q&A on data retrieved from another n8n workflow. Since that workflow can be used to retrieve any data from any service, this template can be used to ask questions about any data.
It uses a manual trigger, various AI nodes, and an OpenAI Chat Model to extract and provide relevant information based on a specific query.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Force AI to use a specific output format,https://n8n.io/workflows/1957-force-ai-to-use-a-specific-output-format/,"This workflow is for anyone looking to automatically fetch, validate, and parse complex language-based queries into a structured format. Its unique capability lies in not only processing language but also fixing invalid outputs before structuring them.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
AI: Summarize podcast episode and enhance using Wikipedia,https://n8n.io/workflows/1956-ai-summarize-podcast-episode-and-enhance-using-wikipedia/,"The workflow automates the process of creating a summarized and enriched podcast digest, which is then sent via email.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Custom LangChain agent written in JavaScript,https://n8n.io/workflows/1955-custom-langchain-agent-written-in-javascript/,"This workflow has multiple functionalities. It starts with a manual trigger, ""When clicking 'Execute Workflow'"", that activates two separate paths.
The first path takes a preset string ""Tell me a joke"" and processes it through a custom Language Learning Model (LLM) chain node. This node interacts with an OpenAI node for query processing.
The second path takes another preset string ""What year was Einstein born?"" and passes it to an ""Agent"" node. This agent further interacts with a Chat OpenAI node and a custom Wikipedia node to produce the required information.
The workflow uses both built-in and custom nodes, and integrates with OpenAI for both paths. It's built for experimenting with language models, specifically in the context of conversational agents and information retrieval.
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Discord AI-powered bot,https://n8n.io/workflows/1938-discord-ai-powered-bot/,This workflow is a template example for making Discord GPT-powered bots. Incoming user requests are analysed and categorised via OpenAI node with the help of a hand-crafted prompt. The response message is then routed to a different Discord channel.
"OpenAI examples: ChatGPT, DALLE-2, Whisper-1 ‚Äì 5-in-1",https://n8n.io/workflows/1900-openai-examples-chatgpt-dalle-2-whisper-1-5-in-1/,"Primer workflow for OpenAI models: ChatGPT, DALLE-2, Whisper
This workflow contains 5 examples on how to work with OpenAI API.
Transcribe voice into text via Whisper model (disabled, please put your own mp3 file with voice)
The old way of using OpenAI conversational model via text-davinci-003
Examples 1.x. Simple ChatGPT calls. Text completion and text edit
Example 2. Provide system and user content into ChatGPT
Examples 3.x. Create system / user / assistanc content via Code Node. Promtp chaining technique example
Example 4. Generate code via ChatGPT
Example 5. Return multiple answers. Useful for providing picking the most relevant reply
IMPORTANT!
Do not run the whole workflow, it's rather slow
Better execute the last node of each branch or simply disconnect branches that are not needed"
Send a ChatGPT email reply and save responses to Google Sheets,https://n8n.io/workflows/1898-send-a-chatgpt-email-reply-and-save-responses-to-google-sheets/,"This workflow sends a OpenAI GPT reply when an email is received from specific email recipients. It then saves the initial email and the GPT response to an automatically generated Google spreadsheet. Subsequent GPT responses will be added to the same spreadsheet.
Additionally, when feedback is given for any of the GPT responses, it will be recorded to the spreasheet, which can then be used later to fine-tune the GPT model.
Prerequisites
OpenAI credentials
Google credentials
How it works
This workflow is essentially a two-in-one workflow. It triggers off from two different nodes and have very different functionality from each trigger.
The flow triggered from On email received node is as follows:
Triggers off on the On email received node.
Extract the email body from the email.
Generate a response from the email body using the OpenAI node.
Reply to the email sender using the Send reply to recipient node. A feedback link is also included in the email body which will trigger the On feedback given node. This is used to fine-tune the GPT model.
Save the email body and OpenAI response to a Google Sheet. If a sheet does not exist, it will be created.
The flow triggered from On feedback given node is as follows:
Triggers off when a feedback link is clicked in the emailed GPT response.
The feedback, either positive or negative, for that specific GPT response is then recorded to the Google Sheet."
Send specific PDF attachments from Gmail to Google Drive using OpenAI,https://n8n.io/workflows/1897-send-specific-pdf-attachments-from-gmail-to-google-drive-using-openai/,"This workflow reads PDF textual content and sends the text to OpenAI. Attachments of interest will then be uploaded to a specified Google Drive folder. For example, you may wish to send invoices received from an email to an inbox folder in Google Drive for later processing. This workflow has been designed to easily change the search term to match your needs. See the workflow for more details.
Prerequisites
OpenAI credentials.
Google credentials.
How it works
Triggers off on the On email received node.
Iterates over the attachments in the email.
Uses the OpenAI node to filter out the attachments that do not match the search term set in the Configure node. You could match on various PDF files (i.e. invoice, receipt, or contract).
If the PDF attachment matches the search term, the workflow uses the Google Drive node to upload the PDF attachment to a specific Google Drive folder."
Reddit AI digest,https://n8n.io/workflows/1895-reddit-ai-digest/,"This workflow digests mentions of n8n on Reddit that can be sent as an single email or Slack summary each week. We use OpenAI to classify if a specific Reddit post is really about n8n or not, and then the summarise it into a bullet point sentence.
How it works
Get posts from Reddit that might be about n8n;
Filter for the most relevant posts (posted in last 7 days and more than 5 upvotes and is original content);
Check if the post is actually about n8n;
If it is, categorise with OpenAI.
Bear in mind: Workflow only considers first 500 characters of each reddit post. So if n8n is mentioned after this amount, it won't register as being a post about n8n.io.
Next steps
Improve OpenAI Summary node prompt to return cleaner summaries;
Extend to more platforms/sources - e.g. it would be really cool to monitor larger Slack communities in this way;
Do some classification on type of user to highlight users likely to be in our ICP;
Separate a list of data sources (reddit, twitter, slack, discord etc.), extract messages from there and have them go to a sub workflow for classification and summarisation."
lemlist <> GPT-3: Supercharge your sales workflows,https://n8n.io/workflows/1838-lemlist-lessgreater-gpt-3-supercharge-your-sales-workflows/,"Use GPT-3 to classify email responses in lemlist.
And automate:
Slack alerts when a lead is interested
the creation of tasks when a lead is OOO
unsubscription of leads when they request it"
Automate testimonials in Strapi with n8n,https://n8n.io/workflows/1535-automate-testimonials-in-strapi-with-n8n/,"This is the workflow powering the n8n demo shown at StrapiConf 2022.
The workflow searches matching Tweets every 30 minutes using the Interval node and listens to Form submissions using the Webhook node.
Sentiment analysis is handled by Google using the Google Cloud Natural Language node before the result is stored in Strapi using the Strapi node.
(These were originally two separate workflows that have been combined into one to simplify sharing.)"
OpenAI-powered tweet generator,https://n8n.io/workflows/1520-openai-powered-tweet-generator/,"This workflow uses OpenAI to generate tweets to be stored in Airtable for review.
A JS snippet handles the topics to be tweeted about in the form of hashtags."
Send a random recipe once a day to Telegram,https://n8n.io/workflows/1425-send-a-random-recipe-once-a-day-to-telegram/,"This telegram bot is designed to send one random recipe a day.
This specific bot has filtered out only vegan recipes, so you can choose your diet type and send only recipes for a specific diet.
What credentials you need:
Set up a telegram bot.
Airtable for listing who has joined your bot. This is needed to send one random recipe a day.
Recipe (or other) API. This one uses Spoonacular.
I hope you enjoy your bot!"
Create dynamic Twitter profile banner,https://n8n.io/workflows/1357-create-dynamic-twitter-profile-banner/,"This workflow updates your Twitter profile banner when you have a new follower.
To use this workflow:
Configure Header Auth in the Fetch New Followers to connect to your Twitter account.
Update the URL of the template image in the Fetch BG node.
Create and configure your Twitter OAuth 1.0 credentials in the last HTTP Request node.
You can configure the size, and position of the avatar images in the Edit Image nodes.
Check out this video to learn how to build it from scratch: How to automatically update your Twitter Profile Banner"
Update Twitter banner using HTTP request,https://n8n.io/workflows/1338-update-twitter-banner-using-http-request/,"This workflow demonstrates the use of the HTTP Request node to upload binary files for form-data-multipart type. This example workflow updates the Twitter banner.
HTTP Request node: This node fetches an image from Unsplash. Replace this node with any other node to fetch the image file.
HTTP Request1 node: This node uploads the Twitter Profile Banner. The Twitter API requires OAuth 1.0 authentication. Follow the Twitter documentation to learn how to configure the authentication."
Detect toxic language in Telegram messages,https://n8n.io/workflows/1216-detect-toxic-language-in-telegram-messages/,"This workflow detects toxic language (such as profanity, insults, threats) in messages sent via Telegram. This blog tutorial explains how to configure the workflow nodes step-by-step.
Telegram Trigger: triggers the workflow when a new message is sent in a Telegram chat.
Google Perspective: analyzes the text of the message and returns a probability value between 0 and 1 of how likely it is that the content is toxic.
IF: filters messages with a toxic probability value above 0.7.
Telegram: sends a message in the chat with the text ""I don't tolerate toxic language"" if the probability value is above 0.7.
NoOp: takes no action if the probability value is below 0.7."
Add positive feedback messages to a table in Notion,https://n8n.io/workflows/1109-add-positive-feedback-messages-to-a-table-in-notion/,"This workflow allows you to add positive feedback messages to a table in Notion.
Prerequisites
Create a Typeform that contains Long Text filed question type to accepts feedback from users.
Get your Typeform credentials by following the steps mentioned in the documentation.
Follow the steps mentioned in the documentation to create credentials for Google Cloud Natural Language.
Create a page on Notion similar to this page.
Create credentials for the Notion node by following the steps in the documentation.
Follow the steps mentioned in the documentation to create credentials for Slack.
Follow the steps mentioned in the documentation to create credentials for Trello.
Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form.
Google Cloud Natural Language node: This node analyses the sentiment of the response the user has provided and gives a score.
IF node: The IF node uses the score provided by the Google Cloud Natural Language node and checks if the score is positive (larger than 0). If the score is positive we get the result as True, otherwise False.
Notion node: This node gets connected to the true branch of the IF node. It adds the positive feedback shared by the user in a table in Notion.
Slack node: This node will share the positive feedback along with the score and username to a channel in Slack.
Trello node: If the score is negative, the Trello node is executed. This node will create a card on Trello with the feedback from the user."
ETL pipeline for text processing,https://n8n.io/workflows/1045-etl-pipeline-for-text-processing/,"This workflow allows you to collect tweets, store them in MongoDB, analyse their sentiment, insert them into a Postgres database, and post positive tweets in a Slack channel.
Cron node: Schedule the workflow to run every day
Twitter node: Collect tweets
MongoDB node: Insert the collected tweets in MongoDB
Google Cloud Natural Language node: Analyse the sentiment of the collected tweets
Set node: Extract the sentiment score and magnitude
Postgres node: Insert the tweets and their sentiment score and magnitude in a Posgres database
IF node: Filter tweets with positive and negative sentiment scores
Slack node: Post tweets with a positive sentiment score in a Slack channel
NoOp node: Ignore tweets with a negative sentiment score"
Analyze feedback using AWS Comprehend and send it to a Mattermost channel,https://n8n.io/workflows/965-analyze-feedback-using-aws-comprehend-and-send-it-to-a-mattermost-channel/,"This workflow analyzes the sentiments of the feedback provided by users and sends them to a Mattermost channel.
Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form.
AWS Comprehend node: This node analyses the sentiment of the response the user has provided and gives a score.
IF node: The IF node uses the data provided by the AWS Comprehend node and checks if the sentiment is negative. If the sentiment is negative we get the result as true, otherwise false.
Mattermost node: If the score is negative, the IF node returns true and the true branch of the IF node is executed. We connect the Mattermost node with the true branch of the IF node. Whenever the score of the sentiment analysis is negative, the node gets executed and a message is posted on a channel in Mattermost.
NoOp: This node here is optional, as the absence of this node won't make a difference to the functioning of the workflow.
This workflow can be used by Product Managers to analyze the feedback of the product. The workflow can also be used by HR to analyze employee feedback. You can even use this node for sentiment analysis of Tweets.
To perform a sentiment analysis of Tweets, replace the Typeform Trigger node with the Twitter node.
Note: You will need a Trigger node or Start node to start the workflow.
Instead of posting a message on Mattermost, you can save the results in a database or a Google Sheet, or Airtable. Replace the Mattermost node with (or add after the Mattermost node) the node of your choice to add the result to your database."
Analyze feedback and send a message on Mattermost,https://n8n.io/workflows/786-analyze-feedback-and-send-a-message-on-mattermost/,"This workflow analyzes the sentiments of the feedback provided by users and sends them to a Mattermost channel.
Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form.
Google Cloud Natural Language node: This node analyses the sentiment of the response the user has provided and gives a score.
IF node: The IF node uses the score provided by the Google Cloud Natural Language node and checks if the score is negative (smaller than 0). If the score is negative we get the result as True, otherwise False.
Mattermost node: If the score is negative, the IF node returns true and the true branch of the IF node is executed. We connect the Mattermost node with the true branch of the IF node. Whenever the score of the sentiment analysis is negative, the node gets executed and a message is posted on a channel in Mattermost.
NoOp: This node here is optional, as the absence of this node won't make a difference to the functioning of the workflow.
This workflow can be used by Product Managers to analyze the feedback of the product. The workflow can also be used by HR to analyze employee feedback. You can even use this node for sentiment analysis of Tweets.
To perform a sentiment analysis of Tweets, replace the Typeform Trigger node with the Twitter node.
Note: You will need a Trigger node or Start node to start the workflow.
Instead of posting a message on Mattermost, you can save the results in a database or a Google Sheet, or Airtable. Replace the Mattermost node with (or add after the Mattermost node) the node of your choice to add the result to your database.
You can learn to build this workflow on the documentation page of the Google Cloud Natural Language node."
"Create, update, and get a profile in Humantic AI",https://n8n.io/workflows/784-create-update-and-get-a-profile-in-humantic-ai/,
Create AI-Powered Website Chatbot with Langflow Backend and Custom Branding,https://n8n.io/workflows/4645-create-ai-powered-website-chatbot-with-langflow-backend-and-custom-branding/,"This workflow integrates a chatbot frontend with a backend powered by Langflow, a visual low-code AI development tool. The flow is triggered whenever a chat message is received via the n8n chatbot widget embedded on a website. It then sends the message to a Langflow flow for processing and returns the generated response to the user.
How It Works
Chat Trigger: The workflow starts with a webhook trigger (When chat message received) that listens for incoming chat messages from the n8n Chat interface.
Langflow Integration: The chat input is sent to a Langflow instance via an HTTP request (Langflow node). The request includes the user's message and expects a response from the Langflow flow.
Response Processing: The output from Langflow is extracted and formatted using the Edit Fields node, ensuring the chatbot displays the response correctly.
Customization: Sticky notes provide instructions for embedding the n8n Chat widget on a website and customizing its appearance, including welcome messages, language settings, and branding.
Set Up Steps
Configure Langflow Connection:
Replace LANGFLOW_URL and FLOW_ID in the HTTP request node with your Langflow instance details.
Ensure the API headers (e.g., Content-Type: application/json) and authentication (if required) are correctly set.
Deploy n8n Chat:
Add the provided CDN script to your website, replacing YOUR_PRODUCTION_WEBHOOK_URL with the webhook URL generated by the When chat message received node.
Customize the chatbot‚Äôs UI (e.g., title, placeholder text, initial messages) using the JavaScript snippet in the sticky notes.
Activate Workflow:
Toggle the workflow to ""Active"" in n8n.
Test the chatbot by sending a message and verifying the Langflow response is processed and displayed correctly.
Advantages
‚úÖ Seamless Langflow Integration
It allows n8n to communicate directly with a Langflow flow via API, enabling AI responses using custom-designed Langflow logic.
‚úÖ No-Code Chatbot Deployment
With just a script snippet, the chatbot widget can be embedded into any website. Minimal coding is required to launch a fully functioning AI chatbot.
‚úÖ Customizable UI/UX
The included embed code offers full control over the chatbot's appearance, language, welcome message, input placeholder, and branding‚Äîideal for white-label or customer-facing deployments.
‚úÖ Modular and Extensible
Because it's built in n8n, this chatbot can be easily extended with other services like CRMs, email alerts, or databases, without leaving the platform.
‚úÖ Real-Time AI Interactions
Thanks to Langflow's API and chat response support, users get immediate and dynamic AI-driven replies.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Auto-Respond to Gmail Enquiries using GPT-4o, Dumpling AI & LangChain Agent",https://n8n.io/workflows/4057-auto-respond-to-gmail-enquiries-using-gpt-4o-dumpling-ai-and-langchain-agent/,"Who is this for?
This workflow is perfect for customer support teams, sales departments, or solopreneurs who receive frequent email enquiries and want to automate the initial response process using AI. If you spend too much time answering similar questions, this system helps respond faster and more intelligently‚Äîwithout writing a single line of code.
What problem is this workflow solving?
Manually responding to repeated customer enquiries slows productivity and increases delay. This workflow classifies if an incoming email is a real enquiry, analyzes the content with a LangChain-powered agent, fetches helpful context using Dumpling AI, and sends a personalized reply using Gmail‚Äîall within minutes.
What this workflow does
Listens for new incoming Gmail messages using the Gmail Trigger node.
Classifies whether the email is an enquiry using a GPT-4o classification prompt.
Uses a Filter node to continue only if the email was classified as an enquiry.
Passes the email content to a LangChain Agent, enhanced with memory, AI tools, and Dumpling AI to search for relevant information.
The agent constructs a smart, relevant response, then sends it to the original sender via Gmail.
Setup
Connect Gmail
Use the Gmail Trigger node to connect to the Gmail account that receives enquiries.
Make sure Gmail OAuth2 credentials are authenticated.
Configure Dumpling AI Agent
Sign up at Dumpling AI.
Create an agent trained to search your help docs, site content, or FAQs.
Copy your Dumpling agent ID and API key.
Paste it in the Dumpling AI Agent ‚Äì Search for Relevant Info HTTP Request node.
Set Up LangChain Agent
No extra setup needed beyond connecting OpenAI credentials.
GPT-4o is used for classification and reply generation.
Enable Gmail Reply Node
The final Send Email Response via Gmail node will send the AI-generated reply back to the same thread.
How to customize this workflow to your needs
Change the classification prompt to include other email types like ‚Äúsupport‚Äù, ‚Äúcomplaint‚Äù, or ‚Äúsales‚Äù.
Add additional logic if you want to CC someone or forward certain types of enquiries.
Add a Notion or Google Sheets node to log the conversation for analytics.
Replace Gmail with Outlook or another email provider by switching the nodes.
Improve context by adding more AI tools like database queries or preloaded FAQs."
Automated PDF Invoice Processing & Approval Flow using OpenAI and Google Sheets,https://n8n.io/workflows/4452-automated-pdf-invoice-processing-and-approval-flow-using-openai-and-google-sheets/,"Who is this for?
This workflow is ideal for:
Finance teams that need to process incoming invoices faster with minimal errors
Small to mid-sized businesses that want to automate invoice intake, review, and storage
Operations managers who require approval workflows and centralized record-keeping
What problem is this workflow solving?
Manually processing invoices is time-consuming, error-prone, and often lacks structure. This workflow solves those challenges by:
Automating the intake of invoices from multiple sources (email, Google Drive, web form)
Extracting invoice data using AI, eliminating manual data entry
Implementing an email-based approval system to add human oversight
Automatically storing approved invoice data in Google Sheets for easy access and reporting
Notifying stakeholders when invoices are approved or rejected
What this workflow does
This end-to-end invoice processing workflow includes:
Three invoice input methods: Google Drive folder monitor, Gmail attachments, and web form uploads
PDF to text extraction for each input method using native PDF parsing
AI-powered invoice analysis with GPT-4 to extract structured fields such as vendor, total, and due date
Dynamic categorization of invoice type (e.g., Travel, Software, Utilities) via AI
Email-based approval workflow with embedded forms to collect decisions and notes
Automated Google Sheets logging of all invoice data, approval status, and reviewer feedback
Rejection notifications sent automatically to your finance team for transparency and follow-up
Setup
Copy the Google Sheet template here:
üëâ PDF Invoice Parser with Approval Workflow ‚Äì Google Sheet Template
Connect your Google Drive account and specify the invoice folder ID
Set up Gmail to monitor incoming invoices with PDF attachments
Enable your form trigger to accept direct uploads from your internal or external users
Enter your OpenAI API key in the AI processing node for data extraction
Configure Google Sheets with a target spreadsheet to store invoice data
Set recipient email addresses for invoice approvals and rejection notifications
Test with a sample invoice to ensure end-to-end flow is working
How to customize this workflow to your needs
Change input sources: Replace Gmail with Outlook or use Slack uploads instead
Add validation steps: Include regex or keyword checks before AI analysis
Customize the AI schema: Modify the expected JSON structure based on your internal finance system
Integrate with accounting tools: Add Xero, QuickBooks, or custom API nodes to push data
Route based on category: Add conditional logic to handle invoices differently based on vendor or category
Multi-level approvals: Add additional email steps if higher-level signoff is needed
Audit logging: Use database or Google Sheets to maintain a historical log of approvals and rejections"
AI-Powered Social Media Content Generator & Publisher,https://n8n.io/workflows/2950-ai-powered-social-media-content-generator-and-publisher/,"AI-Powered Social Media Content Generator & Publisher üöÄ
This AI-driven n8n workflow automates social media content creation and publishing across LinkedIn, Instagram, Facebook, and Twitter (X). It generates engaging, platform-optimized posts using Google Gemini AI, based on user inputs such as a post title, keywords, and an uploaded image. The workflow ensures seamless content generation and publishing, making it a perfect tool for marketers, business owners, influencers, and content creators.
üåü Features & Benefits
‚úÖ AI-Generated Social Media Posts ‚Äì Uses Google Gemini AI to create high-quality, optimized content.
‚úÖ Multi-Platform Support ‚Äì Automatically generates posts for LinkedIn, Instagram, Facebook, and Twitter (X).
‚úÖ Hashtag & SEO Optimization ‚Äì Includes trending hashtags to enhance visibility and engagement.
‚úÖ Image Upload & Processing ‚Äì Allows image uploads for Instagram and Facebook using imgbb and Facebook Graph API.
‚úÖ Automated Publishing ‚Äì Posts are automatically published on all selected platforms.
‚úÖ Custom Call-to-Action ‚Äì Each platform's post is optimized with CTAs for better engagement.
‚úÖ User-Friendly Form Submission ‚Äì Easy-to-use form where users can enter post titles, keywords, links, and images.
‚úÖ Performance Tracking ‚Äì Provides confirmation and tracking links for published posts.
üìå How It Works
1Ô∏è‚É£ User Submission Form
Fill out the form with Post Title, Keywords, and an Optional Link.
Upload an image for Instagram & Facebook posts.
2Ô∏è‚É£ AI Content Generation
Google Gemini AI generates optimized content for each platform.
The AI ensures professional, engaging, and audience-specific content.
3Ô∏è‚É£ Content Review
Users review and approve the AI-generated content before publishing.
4Ô∏è‚É£ Automated Publishing
Posts are automatically published on LinkedIn, Facebook, Instagram, and Twitter (X).
Uses Facebook Graph API, LinkedIn API, Twitter API, and Instagram API.
5Ô∏è‚É£ Post Confirmation & Tracking
Get links to track published posts on each platform.
üõ†Ô∏è Prerequisites
Before using this workflow, ensure you have:
‚úÖ n8n Instance (Cloud or Self-Hosted)
‚úÖ Social Media API Credentials (Facebook, Instagram, LinkedIn, Twitter API)
‚úÖ Google Gemini AI API Key
‚úÖ imgbb API Key (for image hosting)
Get N8n
My Affiliate Link
Buy My Book:
Mastering n8n on Amazon
Full Courses & Tutorials:
http://lms.syncbricks.com
üì∫ YouTube Video Tutorial üé•
Watch the step-by-step tutorial on how to set up and use this n8n workflow template:
üîó YouTube Tutorial - AI-Powered Social Media Posting in n8n
üéØ Use Cases
üìå Marketing Agencies ‚Äì Automate client content scheduling.
üìå Businesses & Brands ‚Äì Maintain a consistent brand presence on social media.
üìå Content Creators & Influencers ‚Äì Generate high-quality posts quickly.
üìå E-commerce & Startups ‚Äì Promote products and services effortlessly.
üìå Corporate & Enterprise Teams ‚Äì Streamline internal and external communications.
Important
Start with n8n
Learn n8n with Amjid
Get n8n Book
üë®‚Äçüíª Creator Information
üë§ Developed by: Amjid Ali
üåê Website: SyncBricks
üìß Email: info@syncbricks.com
üíº LinkedIn: Amjid Ali
üì∫ YouTube: SyncBricks
üí° Support & Contributions
If you find this workflow helpful, consider supporting my work:
üëâ Donate via PayPal
For full courses on ** AI Automation**, visit:
üìö SyncBricks LMS
üìö AI and Auotmation Course
üëâ Get Started with N8N"
Find Content Gaps in Competitors' Websites with InfraNodus GraphRAG for SEO,https://n8n.io/workflows/4403-find-content-gaps-in-competitors-websites-with-infranodus-graphrag-for-seo/,"This template can be used to find the content gaps in your competitors' discourse: identifying the topics they are not yet connecting and giving you an opportunity to fill in this gap with your content and product ideas. It will also generate research questions that will help bridge the gaps and generate new ideas.
The template showcases the use of multiple n8n nodes and processes:
enriching Google sheets file with the new data
data extraction
content enhancement using GraphRAG approach
content gap / research question generation
This approach can be very useful for research, marketing, and SEO applications as you can quickly get an overview of the main topics that are available online for a certain niche and understand what is missing.
What are Content Gaps in Marketing and SEO?
In the context of SEO, content gaps are usually understood as the topics that your competitors rank for but you do not.
However, it's hard to rank for these topics because there's very high competition. So a much more effective way is to identify the gaps between the topics your competitors are talking about that are not yet bridged in their discourse.
If you address these gaps in your content, you will increase the informational gain that your content offers and also offer a novel perspective while touching upon the topics that are relevant in your field.
For example, if we analyze the top websites for ""body and physical practices, fitness, etc."" we will see that most of them are talking about the health and fitness aspects and another big topic is the community aspect.
However, there is a gap between the two topics: which means that most of the websites (companies) that talk about this topic don't mention the two in the same context. This might be an opportunity: bridging the gap between health, fitness but also emphasizing the community aspect that comes with a collective practice.
How it works
This template consists of the two stages:
Data enrichment of a Google sheet file with a list of your competitors using InfraNodus' GraphRAG to generate topical summaries and graph summaries for every URL you're analyzing.
Insight generation (using InfraNodus to identify the main topical clusters and gaps in those summaries, these insights are then added to the Google sheet file.
Additionally, it contains a sub workflow that you can activate and launch to ask Perplexity model to conduct a market research and find the companies that operate in your field and populate the original Google sheet file.
Here's a description step by step:
Step 0: Populate the Google sheets file with the company data (either manually or using the sub-workflow provided or Manus AI / Deep Research)
Steps 1-2: Triggering and Launching the workflow, extracting the company URL from the Google sheet row
Step 3: Scraping the url content from the companies' websites and cleaning the data
Steps 5-7: Use InfraNodus GraphRAG Content Enhancer to get a topical summary and graph summary. This is what you're going to get:
Steps 8-10: Use InfraNodus AI to generate insight advice and research questions based on the content gaps
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Create a separate knowledge graph for each expert (using PDF / content import options) in InfraNodus
For each graph, go to the workflow, paste the name of the graph into the body name field.
Keep other settings intact or learn more about them at the InfraNodus access points page.
Once you add one or more graphs as experts to your flow, add the LLM key to the OpenAI node and launch the workflow
Requirements
An InfraNodus account and API key
A Google Sheet account and an authorization key
Note: OpenAI key is not required. But you might want to get a Perplexity AI key if you'd like to use the sub-workflow that populates the Google sheet with your competitors' website addresses (if you don't have this list yet).
Customizing this workflow
You can use this same workflow with a Telegram bot or Slack (to be notified of the summaries and ideas).
You can also hook up automated social media content creation workflows in the end of this template, so you can generate posts that are relevant (covering the important topics in your niche) but also novel (because they connect them in a new way).
Check out our n8n templates for ideas at https://n8n.io/creators/infranodus/
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20234254556828-Find-Content-Gaps-in-Websites-Market-Research-and-SEO-n8n-Workflow
Also check the full tutorial with a conceptual explanation at https://support.noduslabs.com/hc/en-us/articles/20454382597916-Beat-Your-Competition-Target-Their-Content-Gaps-with-this-n8n-Automation-Workflow
Also check out the video tutorial with a demo:
For support and help with this workflow, please, contact us at https://support.noduslabs.com"
AI-Powered Restaurant Order Chatbot with GPT-4o for POS Integration,https://n8n.io/workflows/3942-ai-powered-restaurant-order-chatbot-with-gpt-4o-for-pos-integration/,"This workflow automates the restaurant POS (Point of Sale) data management process, facilitating seamless order handling, customer tracking, inventory management, and sales reporting. It retrieves order details, processes payment information, updates inventory, and generates real-time sales reports, all integrated into a centralized system that improves restaurant operations.
The workflow integrates various systems, including a POS terminal to gather order data, payment gateways to process transactions, inventory management tools to update stock, and reporting tools like Google Sheets or an internal database for generating sales and performance reports.
Who Needs Restaurant POS Automation?
This POS automation workflow is ideal for restaurant owners, managers, and staff looking to streamline their operations:
Restaurant Owners ‚Äì Automate order processing, track sales, and monitor inventory to ensure smooth operations.
Managers ‚Äì Access real-time sales data and performance reports to make informed decisions.
Staff ‚Äì Reduce manual work, focusing on providing better customer service while the system handles orders and payments.
Inventory Teams ‚Äì Automatically update inventory levels based on orders and ingredient usage.
If you need a reliable and automated POS solution to manage restaurant orders, payments, inventory, and reporting, this workflow minimizes human error, boosts efficiency, and saves valuable time.
Why Use This Workflow?
End-to-End Automation ‚Äì Automates everything from order input to inventory updates and sales reporting.
Seamless Integration ‚Äì Connects POS, payment systems, inventory management, and reporting tools for smooth data flow.(if needed)
Real-Time Data ‚Äì Provides up-to-the-minute reports on sales, stock levels, and order statuses.
Scalable & Efficient ‚Äì Supports multiple locations, multiple users, and high order volumes.
Step-by-Step: How This Workflow Manages POS Data
Collect Orders ‚Äì Retrieves order details from the POS system, including customer information, ordered items, and payment details.
Update Inventory ‚Äì Decreases inventory levels based on sold items, ensuring stock counts are always accurate.
Generate Reports ‚Äì Compiles sales, revenue, and inventory data into real-time reports and stores them in Google Sheets or an internal database.
Track Customer Data ‚Äì Keeps a log of customer details and order history for better service and marketing insights.
Customization: Tailor to Your Needs
Multiple POS Systems ‚Äì Adapt the workflow to work with different POS systems or terminals based on your restaurant setup.
Custom Reporting ‚Äì Modify the reporting format or include specific sales metrics (e.g., daily totals, best-selling items, employee performance).
Inventory Management ‚Äì Adjust inventory updates to include alerts when stock reaches critical levels or needs reordering.
Integration with Accounting Software ‚Äì Connect with platforms like QuickBooks for automated financial tracking.
üîë Prerequisites
POS System Integration ‚Äì Ensure the POS system can export order data in a compatible format.
Payment Gateway API ‚Äì Set up the necessary API keys for payment processing (e.g., Stripe, PayPal).
Inventory Management Tools ‚Äì Use inventory software or databases that can automatically update stock levels.
Reporting Tools ‚Äì Use Google Sheets or an internal database to store and generate sales and inventory reports.
üöÄ Installation & Setup
Configure Credentials
Set up API credentials for payment gateways and inventory management tools.
Import Workflow
Import the workflow into your automation platform (e.g., n8n, Zapier).
Link POS system, payment gateway, and inventory management systems.
Test & Run
Process a test order to ensure that data flows correctly through each step.
Verify that inventory updates and reports are generated as expected.
‚ö† Important
Data Privacy ‚Äì Ensure compliance with data protection regulations (e.g., GDPR, PCI DSS) when handling customer payment and order data.
System Downtime ‚Äì Monitor system performance to ensure that the workflow runs without disruptions during peak hours.
Summary
This restaurant POS automation workflow integrates order management, payment processing, inventory updates, and real-time reporting, enabling efficient restaurant operations. Whether you are running a single location or a chain of restaurants, this solution streamlines daily tasks, reduces errors, and provides valuable insights, saving time and improving customer satisfaction. üöÄ"
"AI Agent Creates Content to Be Picked by ChatGPT, Gemini, Google",https://n8n.io/workflows/4652-ai-agent-creates-content-to-be-picked-by-chatgpt-gemini-google/,"üß† Who is this for?
Marketing teams, content creators, solopreneurs, and agencies who want to generate emotionally-resonant, SEO-optimized content tailored to audience psychology and buyer journey stages ‚Äî and get picked up by AI discovery engines like ChatGPT, Gemini, and Perplexity.
How it works:
‚úÖ Decodes why people buy (using buyer psychology)
‚úÖ Creates SEO + emotionally resonant content for 4 formats:
‚Üí Blog Posts, Newsletters, Landing Pages, Social Media
‚úÖ Structures the content to be picked up by ChatGPT, Gemini, Perplexity & Google
‚úÖ Automatically routes it to Google Sheets, Gmail, or even WordPress
This isn‚Äôt just about writing better content ‚Äî it‚Äôs about getting seen by the tools that shape the internet.
How long does it take to set-up: 30 Mins"
"Automate Lead Qualification with RetellAI Phone Agent, OpenAI GPT & Google Sheet",https://n8n.io/workflows/3912-automate-lead-qualification-with-retellai-phone-agent-openai-gpt-and-google-sheet/,"üëâ Build a Phone Agent to qualify outbound leads and schedule inbound calls
Who is this for?
This workflow is designed for sales teams, call centers, and businesses handling both outbound and inbound lead calls who want to automate their qualification, follow-up, and call documentation process without manual intervention. It‚Äôs ideal for teams using Google Sheets, RetellAI, OpenAI, and Gmail as part of their tech stack.
Real-World Use Cases
üõç E-commerce ‚Äì Instantly handle product FAQs and order status checks, 24/7.
üè¨ Retail Stores ‚Äì Share store hours, directions, and return policies without lifting a finger.
üçΩ Restaurants ‚Äì Take reservations or answer menu questions automatically.
üíº Service Providers ‚Äì Book appointments or consultations while you focus on your craft.
üìû Any Local Business ‚Äì Deliver friendly, consistent phone support ‚Äî no live agent required.
What problem is this workflow solving?
Managing lead calls at scale can be chaotic‚Äîbetween scheduling outbound qualification calls, handling inbound appointment requests, and making sure every call is documented and followed up. This workflow automates the entire process, reducing human error and saving time by:
‚úÖ Sending reminders to reps for outbound calls
‚úÖ Automatically placing calls with RetellAI
‚úÖ Handling inbound calls and checking caller details
‚úÖ Generating and emailing call summaries automatically
What this workflow does
This n8n template connects Google Sheets, RetellAI, OpenAI, and Gmail into a seamless workflow:
Outbound Lead Qualification Workflow
Triggers when a new lead is added to Google Sheets
Sends an SMS notification to remind the rep to call in 5 minutes
(Optional) Waits 5 minutes
Initiates an automated call to the lead via RetellAI
Inbound Call Appointment Scheduler
Receives inbound calls from RetellAI (via webhook)
Checks if the caller‚Äôs number exists in Google Sheets
Responds to RetellAI with a success or error message
Post-Call Workflow
Receives post-call data from RetellAI
Filters only analyzed calls
Updates the lead‚Äôs record in Google Sheets
Uses OpenAI to generate a call summary
Emails the summary to a team inbox or rep
Setup
‚úÖ You need an active RetellAI API key
Sign up for RetellAI, create an agent, and set the webhook URLs (n8n_call for call events).
Purchase a Twilio phone number and link it to the agent.
‚úÖ Your Google Sheet must have a column for phone numbers (e.g., ""Phone"")
‚úÖ Gmail account connected and authorized in n8n
‚úÖ OpenAI API key added to your environment variables or credentials
Configure your Google Sheets node with the correct spreadsheet ID and range
Add your RetellAI API key to the HTTP request nodes
Connect your Gmail account in the Gmail node
Add your OpenAI key in the OpenAI node
üëâ See full setup guide here: Notion Documentation
How to customize this workflow to your needs
Change SMS content: Edit the text in the ‚ÄúSend SMS reminder‚Äù node to match your team‚Äôs tone
Modify call wait time: Enable and adjust the ‚ÄúWait 5 minutes‚Äù node to any delay you prefer
Add CRM integration: Replace or extend the Google Sheets node to update your CRM instead of a spreadsheet
Customize call summary prompts: Edit the prompt sent to OpenAI to change the summary style or add extra insights
Send email to different recipients: Change the recipient address in the Gmail node or make it dynamic from the lead record
Need help customizing?
Contact me for consulting and support : Linkedin"
"Generate & Enrich LinkedIn Leads with Apollo.io, LinkedIn API, Mail.so & GPT-3.5",https://n8n.io/workflows/3791-generate-and-enrich-linkedin-leads-with-apolloio-linkedin-api-mailso-and-gpt-35/,"Note: Now includes an Apify alternative for Rapid API (Some users can't create new accounts on Rapid API, so I have added an alternative for you. But immediately you are able to get access to Rapid API, please use that option, it returns more detailed data). Scroll to bottom for APify setup guide
This n8n workflow automates LinkedIn lead generation, enrichment, and activity analysis using Apollo.io, RapidAPI, Google Sheets and Mail.so.
Perfect for sales teams, founders, B2B marketers, and cold outreach pros who want personalized lead insights to drive better conversion rates.
‚öôÔ∏è How This Workflow Works
The workflow is broken down into several key steps, each designed to help you build and enrich a valuable list of LinkedIn leads:
1. üîë Lead Discovery (Keyword Search via Apollo)
Pulls leads using Apollo.io's API based on keywords, industries, or job titles.
Saves lead name, title, company, and LinkedIn URL to your Google Sheet.
You can replace the trigger node from the form node to a webhook, whatsapp, telegram, etc, any way for you to send over your query variables over to initiate the workflow.
2. üß† Username Extraction (from LinkedIn URL)
Extracts the LinkedIn username from profile URLs using a simple script node.
This is required for further enrichment via RapidAPI.
3. ‚úâÔ∏è Email Lookup (via Apollo User ID)
Uses the Apollo User ID to retrieve the lead‚Äôs verified work email.
Ensures high-quality leads with reliable contact info.
To double check that the email is currently valid, we use the mail.so api and filter out emails that fail deliverability and mx-record check. We don't wanna risk sending emails to no longer existent addresses, right?
4. üßæ Profile Summary Enrichment (via RapidAPI)
Queries the LinkedIn Data API to fetch a lead‚Äôs profile summary/bio.
Gives you a deeper understanding of their background and expertise.
5. üì∞ Recent Activity Collection (Posts & Reposts)
Retrieves recent posts or reposts from each lead‚Äôs profile.
Great for tailoring outreach with reference to what they‚Äôre currently talking about.
6. üóÇÔ∏è Leads Database Update
All enriched data is written to the same Google Sheet.
New columns are filled in without overwriting existing data.
‚úÖ Smart Retry & Row Status Logic
Every subworkflow includes a fail-safe mechanism to ensure:
‚úÖ Each row has status columns (e.g., done, failed, pending).
üïí A scheduled retry workflow resets failed rows to pending after 2 weeks (customizable).
üí¨ This gives failed enrichments another chance to be processed later, reducing data loss.
üìã Google Sheets Setup
Template 1: Apollo Leads Scraping & Enrichment
Template 2: Enriched Leads Database
Make a copy to your Drive and use.
Columns will be filled as each subworkflow runs (email, summary, interests, etc.)
üîê Required API Keys
To use this workflow, you‚Äôll need the following credentials:
üß© Apollo.io
Sign up and get your key here: Apollo.io API Keys
‚ö†Ô∏è Important: Toggle the ‚ÄúMaster API Key‚Äù option to ON when generating your key.
This ensures the same key can be used for all Apollo endpoints in this workflow.
üåê RapidAPI (LinkedIn Data API)
Subscribe to the API here: LinkedIn Data API on RapidAPI
Use the key in the x-rapidapi-key header in the relevant nodes.
‚úâÔ∏è Mail.so
Sign up and get your key here: Mail.so API
üí° For both APIs, set up the credentials in n8n as ‚ÄúGeneric Credential‚Äù types.
This way, you won‚Äôt need to reconfigure the headers in each node.
üõ†Ô∏è Customization Options
Modify the Apollo filters (location, industry, seniority) to target your ideal customers.
Change retry interval in the scheduler (e.g., weekly instead of 2 weeks).
Connect the database to your email campaign tool like Mailchimp or Instantly.ai.
Replace the AI nodes with your desired AI agents and customize the system messages further to get desired results.
üÜï Apify Update Guide
To use this workflow, you‚Äôll need the following credentials:
Login to Apify, then open this link; https://console.apify.com/actors/2SyF0bVxmgGr8IVCZ/
Click on integrations and scroll down to API Solutions and select ""Use API endpoints"". Scroll to ""Run Actor synchronously and get dataset items"" and copy the actor endpoint url then paste it in the placeholder inside the http node of Apify alternative flow ""apify-actor-endpoint"". That's it, you are set to go.
I am available for custom n8n workflows, if you like my work, please get in touch with me on email at joseph@uppfy.com"
Schedule & Publish All Instagram Content Types with Facebook Graph API,https://n8n.io/workflows/4498-schedule-and-publish-all-instagram-content-types-with-facebook-graph-api/,"Automated Instagram posting with Facebook Graph API and content routing
Who is this for?
This workflow is perfect for social media managers, content creators, digital marketing agencies, and small business owners who need to automate their Instagram posting process. Whether you're managing multiple client accounts or maintaining consistent personal branding, this template streamlines your social media operations.
What problem is this workflow solving?
Manual Instagram posting is time-inconsistent and prone to inconsistency. Content creators struggle with:
Remembering to post at optimal times
Managing different content types (images, videos, reels, stories, carousels)
Maintaining posting schedules across multiple accounts
Ensuring content is properly formatted for each post type
This workflow eliminates manual posting, reduces human error, and ensures consistent content delivery across all Instagram format types.
What this workflow does
The workflow automatically publishes content to Instagram using Facebook's Graph API with intelligent routing based on content type. It handles image posts, video stories, Instagram reels, carousel posts, and story content. The system creates media containers, monitors processing status, and publishes content when ready. It supports both HTTP requests and Facebook SDK methods for maximum reliability and includes automatic retry mechanisms for failed uploads.
Setup
Connect Instagram Business Account to a Facebook Page
Configure Facebook Graph API credentials with instagram_basic permissions
Update the ""Configure Post Settings"" node with your Instagram Business Account ID
Set media URLs and captions in the configuration section
Choose post type (http_image, fb_reel, http_carousel, etc.)
Test workflow with sample content before going live
How to customize this workflow to your needs
Modify the post_type variable to control content routing:
Use http_* prefixes for direct API calls
Use fb_* prefixes for Facebook SDK calls
Use both HTTP and Facebook SDK nodes as fallback mechanisms - if one method fails, automatically try the other for maximum success rate
Add scheduling by connecting a Cron node trigger
Integrate with Google Sheets or Airtable for content management
Connect webhook triggers for automated posting from external systems
Customize wait times based on your content file sizes
Set up error handling to switch between HTTP and Facebook SDK methods when API limits are reached"
üè† Find your Home with Real Estate Agent and Bright Data,https://n8n.io/workflows/4872-find-your-home-with-real-estate-agent-and-bright-data/,"üìù Overview
This workflow transforms n8n into a smart real-estate concierge by combining an AI chat interface with Bright Data‚Äôs marketplace datasets. Users interact via chat to specify city, price, bedrooms, and bathrooms‚Äîand receive a curated list of three homes for sale, complete with images and briefings.
üé• Workflow in Action
Want to see this workflow in action? Play the video
üîë Key Features
AI-Powered Chat Trigger: Instantly start conversations using LangChain‚Äôs Chat Trigger node.
Contextual Memory: Retain up to 30 recent messages for coherent back-and-forth.
Bright Data Integration: Dynamically filter ‚ÄúFOR_SALE‚Äù properties by city, price, bedrooms, and bathrooms (limit = 3).
Automated Snapshot Retrieval: Poll for dataset readiness and fetch full snapshot content.
HTML-Formatted Output: Present results as a &lt;ul&gt; of &lt;li&gt; items, embedding property images.
üöÄ How It Works (Step-by-Step)
Prerequisites:
n8n ‚â• v1.0
Community nodes: install n8n-nodes-brightdata (the unverified community node)
API credentials: OpenAI, Bright Data
Webhook endpoint to receive chat messages
Node Configuration:
Chat Trigger: Listens for incoming chat messages; shows a welcome screen.
Memory Buffer: Stores the last 30 messages for context.
OpenAI Chat Model: Uses GPT-4o-mini to interpret user intent.
Real Estate AI Agent: Orchestrates filtering logic, calls tools, and formats responses.
Bright Data ‚ÄúFilter Dataset‚Äù Tool: Applies user-defined filters plus homeStatus = FOR_SALE.
Wait & Recover Snapshot: Polls until snapshot is ready, then fetches content.
Get Snapshot Content: Converts raw JSON into a structured list.
Workflow Logic:
User sends search criteria ‚Üí Agent validates inputs.
Agent invokes ‚ÄúFilter Dataset‚Äù once all filters are present.
Upon dataset readiness, the snapshot is retrieved and parsed.
Final output rendered as a bullet list with property images.
Testing & Optimization:
Use the built-in Execute Workflow trigger for rapid dry runs.
Inspect node outputs in n8n‚Äôs UI; adjust filter defaults or snapshot limits.
Tune OpenAI model parameters (e.g., maxIterations) for faster responses.
Deployment & Monitoring:
Activate the main workflow and expose its webhook URL.
Monitor executions in the ‚ÄúExecutions‚Äù panel; set up alerts for errors.
Archive or duplicate workflows as needed; update credentials via credential manager.
‚úÖ Pre-requisites
Bright Data Account: API key for marketplaceDataset.
OpenAI Account: Access to GPT-4o-mini model.
n8n Version: v1.0 or later with community node support.
Permissions: Webhook access, credential vault read/write.
üë§ Who Is This For?
Real-estate agencies and brokers seeking to automate client queries.
PropTech startups building conversational search tools.
Data analysts who want on-demand property snapshots without manual scraping.
üìà Benefits & Use Cases
Time Savings: Replace manual MLS searches with an AI-driven chat.
Scalability: Serve multiple clients simultaneously via webchat or embedded widget.
Consistency: Always report exactly three properties, ensuring concise results.
Engagement: Visual listings with images boost user satisfaction and conversion.
Workflow created and verified by Miquel Colomer https://www.linkedin.com/in/miquelcolomersalas/ and N8nHackers https://n8nhackers.com"
Startup Founder Discovery and AI-Powered Outreach with CrunchBase and Gmail,https://n8n.io/workflows/4729-startup-founder-discovery-and-ai-powered-outreach-with-crunchbase-and-gmail/,"üöÄ Automated Founder Discovery: CrunchBase to Gmail Outreach Workflow!
Workflow Overview
This cutting-edge n8n automation is a sophisticated founder intelligence and outreach tool designed to transform startup research into actionable networking opportunities. By intelligently connecting CrunchBase, OpenAI, and Gmail, this workflow:
Discovers Startup Founders:
Automatically retrieves founder profiles
Tracks latest company updates
Eliminates manual research efforts
Intelligent Profile Processing:
Extracts key professional information
Filters most relevant details
Prepares comprehensive founder insights
AI-Powered Summarization:
Generates professional email-ready summaries
Crafts personalized outreach content
Ensures high-quality communication
Seamless Email Distribution:
Sends automated founder digests
Integrates with Gmail
Enables rapid professional networking
Key Benefits
ü§ñ Full Automation: Zero-touch founder research
üí° Smart Profiling: Intelligent founder insights
üìä Comprehensive Intelligence: Detailed professional summaries
üåê Multi-Platform Synchronization: Seamless data flow
Workflow Architecture
üîπ Stage 1: Founder Discovery
Manual Trigger: Workflow initiation
CrunchBase API Integration: Profile retrieval
Intelligent Filtering:
Identifies key startup founders
Prepares for detailed analysis
üîπ Stage 2: Profile Extraction
Detailed Information Capture
Key Field Mapping
Structured Data Preparation
üîπ Stage 3: AI Summarization
OpenAI GPT Processing
Professional Summary Generation
Contextual Insight Creation
üîπ Stage 4: Email Distribution
Gmail Integration
Automated Outreach
Personalized Communication
Potential Use Cases
Venture Capitalists: Startup scouting
Sales Teams: Lead generation
Recruitment Specialists: Talent discovery
Networking Professionals: Strategic connections
Startup Ecosystem Researchers: Market intelligence
Setup Requirements
CrunchBase API
API credentials
Configured access permissions
Founder tracking setup
OpenAI API
GPT model access
Summarization configuration
API key management
Gmail Account
Connected email
Outreach email configuration
Appropriate sending permissions
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced founder scoring
üìä Multi-source intelligence gathering
üîî Customizable alert mechanisms
üåê Expanded networking platform integration
üß† Machine learning insights generation
Technical Considerations
Implement robust error handling
Use secure API authentication
Maintain flexible data processing
Ensure compliance with API usage guidelines
Ethical Guidelines
Respect professional privacy
Maintain transparent outreach practices
Ensure appropriate communication
Provide opt-out mechanisms
Hashtag Performance Boost üöÄ
#StartupNetworking #FounderDiscovery #AIOutreach #ProfessionalNetworking #TechInnovation #BusinessIntelligence #AutomatedResearch #StartupScouting #ProfessionalGrowth #NetworkingTech
Workflow Visualization
[Manual Trigger]
    ‚¨áÔ∏è
[Updated Profiles List]
    ‚¨áÔ∏è
[Founder Profiles]
    ‚¨áÔ∏è
[Extract Key Fields]
    ‚¨áÔ∏è
[AI Summarization]
    ‚¨áÔ∏è
[Send Email]
Connect With Me
Ready to revolutionize your professional networking?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your founder research with intelligent, automated workflows!"
Generate and Publish Carousels for TikTok and Instagram with GPT-Image-1,https://n8n.io/workflows/4028-generate-and-publish-carousels-for-tiktok-and-instagram-with-gpt-image-1/,"Description
This n8n automation template provides an end-to-end solution for generating a series of themed images for Instagram and TikTok carousels using OpenAI's GPT Image (via the image generation API) and automatically publishing them to both platforms. It uses a sequence of prompts to create a narrative or themed carousel, generating each image based on the previous one, and then posts them with an AI-generated caption.
Who Is This For?
Social Media Managers: Quickly create and schedule engaging image carousels for Instagram and TikTok.
Content Creators: Automate the visual content creation process for thematic posts or storytelling carousels.
Digital Marketers: Efficiently produce visual assets for campaigns that require sequential imagery.
Small Businesses: Generate unique promotional content for social media without needing advanced design skills.
What Problem Does This Workflow Solve?
Manually creating a series of related images for a carousel and then publishing them across multiple platforms can be repetitive and time-consuming. This workflow addresses these issues by:
Automating Image Generation: Uses OpenAI to generate a sequence of 5 images, where each new image is an evolution based on the previous one and a new prompt.
Automating Caption Generation: Leverages OpenAI (GPT) to create a suitable description/caption for the carousel based on the image prompts.
Streamlining Multi-Platform Publishing: Automatically uploads the generated image carousel and caption to both Instagram and TikTok.
Reducing Manual Effort: Significantly cuts down the time spent on designing individual images and manually uploading them.
Ensuring Visual Cohesion: The sequential image generation method (editing the previous image) helps maintain a consistent style or narrative across the carousel.
How It Works
Trigger: The workflow is initiated manually (can be adapted to a schedule or webhook).
Define Prompts: Five distinct prompts are pre-set within the workflow to guide the generation of each image in the carousel.
AI Caption Generation: OpenAI (GPT-4.1) generates a concise (‚â§ 90 characters for TikTok) description for the social media posts based on all five image prompts.
Sequential AI Image Generation:
Image 1: OpenAI's image generation API (specified as gpt-image-1) creates the first image based on prompt1.
Image 2-5: For each subsequent image, the workflow uses the OpenAI image edits API. It takes the previously generated image and a new prompt (prompt2 for image 2, prompt3 for image 3, and so on) to create the next image in the sequence.
Images are converted from base64 JSON to binary format.
Content Aggregation: The five generated binary image files (named photo1 through photo5) are merged.
Multi-Platform Distribution:
The merged images and the AI-generated description are sent to api.upload-post.com for publishing as a carousel to Instagram.
The same content is sent to api.upload-post.com for publishing as a carousel to TikTok, with an option to automatically add music.
The TikTok description is truncated if it exceeds 90 characters.
Setup
Accounts & API Keys: You will need:
An n8n instance.
An OpenAI API key.
An API key for upload-post.com.
Configure Credentials:
Add your OpenAI API key to the ""OpenAI"" credentials in n8n. This will be used by the ""Generate Description for Tiktok and Instagram"" node and the HTTP Request nodes calling the OpenAI image generation/edit APIs.
In the ""POST TO INSTAGRAM"" and ""POST TO TIKTOK"" nodes, replace ""Apikey add_api_key_here"" with your actual upload-post.com API key.
Update the user field in the ""POST TO INSTAGRAM"" and ""POST TO TIKTOK"" nodes if ""upload_post"" is not your user identifier for that service.
Customize Prompts: Modify the five prompts (prompt1 to prompt5) in the ""Set All Prompts"" node to define the story or theme of your image carousel.
Review Image Generation Parameters: In the ""Set API Variables"" node, you can adjust:
size_of_image (e.g., ""1024x1536"" for vertical carousels).
openai_image_model (ensure this matches a valid OpenAI model identifier for image generation/edits, like dall-e-2 or dall-e-3 if gpt-image-1 is a placeholder).
response_format_image (should generally remain b64_json for this workflow).
(Optional) TikTok Auto Music: The ""POST TO TIKTOK"" node has an auto_add_music parameter set to true. Change this to false if you prefer to add music manually or not at all.
Requirements
Accounts: n8n, OpenAI, upload-post.com.
API Keys & Credentials: API Keys for OpenAI and https://upload-post.com.
(Potentially) Paid Plans: OpenAI and upload-post.com usage may incur costs depending on your volume and their respective pricing models.
This template empowers you to automate the creation and distribution of visually consistent image carousels, saving time and enhancing your social media presence."
Get Binance Spot Market Financial Analysis via Telegram with GPT-4o,https://n8n.io/workflows/4741-get-binance-spot-market-financial-analysis-via-telegram-with-gpt-4o/,"This workflow powers the Binance Spot Market Quant AI Agent, acting as the Financial Market Analyst. It fuses real-time market structure data (price, volume, kline) with multiple timeframe technical indicators (15m, 1h, 4h, 1d) and returns a structured trading outlook‚Äîperfect for intraday and swing traders who want actionable analysis in Telegram.
üîó Requires the following sub-workflows to function:
‚Ä¢ Binance SM 15min Indicators Tool
‚Ä¢ Binance SM 1hour Indicators Tool
‚Ä¢ Binance SM 4hour Indicators Tool
‚Ä¢ Binance SM 1day Indicators Tool
‚Ä¢ Binance SM Price/24hStats/Kline Tool
‚öôÔ∏è How It Works
Triggered via webhook (typically by the Quant AI Agent).
Extracts user symbol + timeframe from input (e.g., ""DOGE outlook today"").
Calls all linked sub-workflows to retrieve indicators + live price data.
Merges the data and formats a clean trading report using GPT-4o-mini.
Returns HTML-formatted message suitable for Telegram delivery.
üì• Sample Input
{
  ""message"": ""SOLUSDT"",
  ""sessionId"": ""654321123""
}
‚úÖ Telegram Output Format
üìä SOLUSDT Market Snapshot

üí∞ Price: $156.75  
üìâ 24h Stats: High $160.10 | Low $149.00 | Volume: 1.1M SOL

üß™ 4h Indicators:
‚Ä¢ RSI: 58.2 (Neutral-Bullish)  
‚Ä¢ MACD: Crossover Up  
‚Ä¢ BB: Squeezing Near Upper Band  
‚Ä¢ ADX: 25.7 (Rising Trend)

üìà Resistance: $163  
üìâ Support: $148
üîç Use Cases
Scenario Outcome
User asks for ‚ÄúBTC outlook‚Äù Returns 1h + 4h + 1d indicators + live price + key levels
Telegram bot prompt: ‚ÄúDOGE now‚Äù Returns short-term 15m + 1h analysis snapshot
Strategy trigger inside n8n Enables other workflows to consume structured signal data
üé• Watch the Live Demo
üì∫ https://youtu.be/k9VuU2h5wwI
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected.
No unauthorized rebranding or redistribution permitted.
üîó For support: LinkedIn ‚Äì Don Jayamaha"
LinkedIn Post Automation with AI (GPT-4o) Generation & Slack Approval,https://n8n.io/workflows/4483-linkedin-post-automation-with-ai-gpt-4o-generation-and-slack-approval/,"LinkedIn Post Automation with AI Generation (Gpt-4o) & Slack Approval
How it Works
This workflow automates the creation and publishing of LinkedIn posts with AI-generated content and human approval via Slack, using Google Sheets, OpenAI (GPT-4), Slack Interactive Messages, and the LinkedIn API.
Whether you're a social media manager, content creator, or marketing professional, this workflow helps you maintain consistent LinkedIn presence and scale content creation while keeping human oversight ‚Äî all managed from a simple spreadsheet.
üéØ Use Case
Ideal for:
Content Marketers managing multiple LinkedIn accounts
Personal Brand Builders maintaining regular posting schedule
Agencies handling client social media presence
Teams requiring content approval workflows
Setup Instructions
1. Prepare the Spreadsheet
File name: Linkedin Post
Main sheet structure:
| ID | Linkedin Post Title | Status | Image URL |
Groups sheet structure:
| GroupIds |
Add post topics and set their Status as Pending
2. Configure Google Sheets Nodes
Connect your Google account to:
Linkedin-Post-Topic (Trigger node)
Update-Status
Get-Group-id
3. Add API Credentials
OpenAI API Key ‚Üí for GPT-4 content generation
Slack OAuth Token ‚Üí for approval messages
LinkedIn Access Token ‚Üí for posting content
HTTP Header Auth ‚Üí Bearer token for LinkedIn API
4. Configure Webhooks
Set up Slack Interactive Components webhook
Point Request URL to your N8N webhook endpoint
Enable interactive messages in Slack app
5. Activate the Workflow
Once live, the workflow will:
Monitor spreadsheet for new topics every minute
Generate LinkedIn post using GPT-4
Send to Slack for approval/editing
Upload image to LinkedIn
Publish to profile and groups
Update spreadsheet status to Posted
üîÅ Workflow Logic
Trigger: New/updated row with Pending status
Generate: AI creates engaging LinkedIn post
Approve: Slack message for review/edit
Process: Handle approval response
Upload: Register and upload image
Publish: Post to LinkedIn profile & groups
Update: Mark as Posted in sheet
üß© Node Descriptions
Node Name Description
Linkedin-Post-Topic Monitors spreadsheet for new post topics
Validate-Status Filters only 'Pending' items
Limit Processes one item at a time
Linedin-Post-Creator Generates post content using GPT-4
Format-Content Prepares content for Slack display
Approval-on-Slack Sends interactive approval message
Webhook Receives Slack button responses
Format-Response Extracts edited content from Slack
Set-Final-Message Prepares approved content
Linkedin-User-Detail Fetches LinkedIn user info
Register Image Initiates LinkedIn image upload
Upload Image Uploads image to LinkedIn
Linkedin-post Publishes to personal profile
Get-Group-id Retrieves LinkedIn group IDs
Post-Linkedin-Groups Posts to multiple groups
Update-Status Marks as completed in sheet
üõ†Ô∏è Customization Tips
Adjust AI prompt for brand voice and hashtags
Change Slack approver or add multiple reviewers
Modify posting schedule with delay nodes
Add analytics tracking with additional API calls
Filter groups based on specific criteria
Include URL shortening for tracking
üìí Suggested Sticky Notes for Workflow
Node/Section Sticky Note Content
Linkedin-Post-Topic ""Triggers every minute for new posts in your spreadsheet""
Validate-Status ""Filters to process only 'Pending' items - prevents duplicates""
Linedin-Post-Creator ""Uses GPT-4 to generate LinkedIn content - customize prompt for your brand voice""
Approval-on-Slack ""Sends for human review - edit the Slack user to change approver""
Webhook ""Receives approval responses - ensure URL is configured in Slack app""
Register Image ""Initiates LinkedIn media upload - requires valid image URL from sheet""
Linkedin-post ""Publishes to your profile - update with your LinkedIn credentials""
Post-Linkedin-Groups ""Posts to multiple groups - add group IDs in Groups sheet""
Update-Status ""Marks as 'Posted' to prevent reprocessing""
üí° AI Prompt Configuration
The workflow uses a sophisticated prompt that:
Creates compelling hooks
Includes 3-4 informative paragraphs
Adds engagement questions
Inserts relevant emojis
Generates 4-6 hashtags
Formats with proper spacing
üîí Security & Permissions
LinkedIn App Requirements:
r_liteprofile - Read profile data
r_emailaddress - Access email
w_member_social - Post content
rw_organization_admin - Group posting
Slack Bot Permissions:
chat:write - Send messages
im:write - Direct messages
users:read - User information
üìé Required Components
Component Purpose
Google Sheet Store post topics and status
LinkedIn App API access for posting
Slack App Interactive approval flow
OpenAI Account GPT-4 content generation
N8N Instance Workflow execution
üß™ Testing Tips
Start with one test topic marked as Pending
Verify Slack message appears correctly
Test both ""Approve"" and ""Edit"" buttons
Check image upload completes
Confirm post appears on LinkedIn
Verify status updates to Posted
‚ö†Ô∏è Common Issues & Solutions
LinkedIn API Errors:
Token expiration ‚Üí Refresh access token
Rate limits ‚Üí Add delays between posts
Group restrictions ‚Üí Check posting permissions
Slack Integration:
Missing responses ‚Üí Verify webhook URL
Button not working ‚Üí Check interactive components
User not found ‚Üí Confirm Slack user ID
Image Upload Failures:
Invalid URL ‚Üí Validate image accessibility
Size limits ‚Üí Compress images under 10MB
Format issues ‚Üí Use JPEG or PNG only
üìä Workflow Benefits
Time Savings: 80% reduction in content publishing time
Consistency: Regular posting schedule maintained
Quality Control: Human review ensures brand standards
Scalability: Handle multiple accounts and groups
Flexibility: Easy to modify and extend
üìé Required Files
File Name Purpose
Linkedin Post Google Sheet to hold post topics and status
üè∑ Suggested Tags & Categories
#LinkedIn
#AI
#ContentAutomation
#SocialMedia
#Slack
#GPT4
#Marketing
#WorkflowAutomation"
"Manage Calendar with Voice & Text using GPT-4, Telegram & Google Calendar",https://n8n.io/workflows/4102-manage-calendar-with-voice-and-text-using-gpt-4-telegram-and-google-calendar/,"Manage Calendar with Voice & Text Commands using GPT-4, Telegram & Google Calendar
This n8n workflow transforms your Telegram bot into a personal AI calendar assistant, capable of understanding both voice and text commands in Romanian, and managing your Google Calendar using the GPT-4 model via LangChain.
Whether you want to create, update, fetch, or delete events, you can simply speak or write your request to your Telegram bot ‚Äî and the assistant takes care of the rest.
üöÄ Features
Voice command support using Telegram voice messages (.ogg)
Transcription using OpenAI Whisper
Natural language understanding with GPT-4 via LangChain
Google Calendar integration:
‚úÖ Create Events
üîÅ Update Events
‚ùå Delete Events
üìÖ Fetch Events
Responses sent back via Telegram
üõ†Ô∏è Step-by-Step Setup Instructions
1. Create a Telegram Bot
Go to @BotFather on Telegram.
Send /newbot and follow the instructions.
Save the Bot Token.
2. Configure Telegram Trigger Node
Paste the Telegram token into the Telegram Trigger and Telegram nodes.
Set updates to [""message""].
3. Set up OpenAI Credentials
Get an OpenAI API key from https://platform.openai.com
Create a credential in n8n for OpenAI.
This is used for both transcription and AI reasoning.
4. Set up Google Calendar
In Google Cloud Console:
Enable Google Calendar API
Set up OAuth2 credentials
Add your n8n redirect URI (usually https://yourdomain/rest/oauth2-credential/callback)
Create a credential in n8n using Google Calendar OAuth2
Grant access to your calendar (e.g., ""Family"" calendar).
‚öôÔ∏è Customization Options
üó£Ô∏è Change Language or Locale
The transcription node uses ""en"" for English. Change to another locale if needed.
‚úèÔ∏è Edit Prompt
You can modify the prompt in the AI Agent node to include your name, work schedule, or specific behavior expectations.
üìÜ Change Calendar Logic
Adjust time ranges or filters in the Get Events node
Add custom logic before Create Event (e.g., validation, conflict checks)
üìö Helpful Tips
Make sure n8n has HTTPS enabled to receive Telegram updates.
You can test the flow first using only text, then voice.
Use AI memory or vector stores (like Supabase) if you want context-aware planning in the future."
WhatsApp Group Chat with Your Vector Database ‚Äî No Facebook Business Required,https://n8n.io/workflows/4838-whatsapp-group-chat-with-your-vector-database-no-facebook-business-required/,"Enable smart, real-time answers in your WhatsApp groups using a custom webhook, Pinecone vector database, and no Facebook Business setup.
üü° Note: This template uses a custom WhatsApp webhook. It does not use the official WhatsApp Business API.
üë• Who is this for?
This workflow is designed for individuals and teams who want to enable smart WhatsApp group automation ‚Äî without going through Meta‚Äôs official WhatsApp Business API. Ideal for small businesses, internal teams, communities, and personal power users.
‚ùì What problem is this solving?
Setting up WhatsApp bots with intelligent responses often requires approval from Meta and a verified business account. This workflow removes those barriers by using a self-hosted webhook to handle incoming messages and respond using a document-trained AI via Pinecone.
‚öôÔ∏è What this workflow does
Connects a regular WhatsApp number to a custom webhook
Adds the bot to any group chat (it stays silent unless mentioned)
Indexes documents from Google Drive into Pinecone
Responds with intelligent, context-aware answers from your custom knowledge base
Auto-updates its knowledge every minute as the document changes
üõ†Ô∏è Setup
Step 1: Connect Google Drive
Set up your Google Drive credentials in n8n
Step 2: Configure Pinecone
Create an index in Pinecone
Dimension: 1536
Select this index in both Pinecone nodes
Click Test Workflow to ingest your document into Pinecone
Step 3: Get Access to the WhatsApp Webhook
Fill out this form to request access
You‚Äôll receive a WhatsApp confirmation for linking
Step 4: Test WhatsApp Integration
‚úÖ One-on-one test: Send a message from another number
üë• Group test: Add the bot to a group; it will only respond when tagged
üß© How to customize this workflow
Modify the system prompt inside the AI agent node to control tone and behavior
Update the connected Google Doc to match your specific domain (e.g. FAQs, SOPs, product manuals)
Adjust the Pinecone sync frequency if you want updates more or less often
üìö Use cases
Customer Support: Instant, intelligent replies in WhatsApp without live agents
Team Knowledge Bot: Tag the bot for quick access to SOPs and internal docs
Community Groups: Automate common questions while keeping noise low
Personal AI Assistant: A WhatsApp chatbot trained on your notes and files
üìù Sticky Note Suggestion
üí¨ What this template does:
Enables an AI bot in your WhatsApp group that answers questions based on a Google Doc you provide. It uses a custom webhook, Google Drive, and Pinecone.
üîß Requirements:
Google Drive account
Pinecone account with an index (dimension 1536)
Access to the custom WhatsApp webhook (see setup steps)"
"AI-Powered Meta Ads Analysis & Creation with Gemini, GPT-4.1 Mini & Ads Manager",https://n8n.io/workflows/4684-ai-powered-meta-ads-analysis-and-creation-with-gemini-gpt-41-mini-and-ads-manager/,"AI-Powered Meta Ads Creation & Analysis Workflow
Overview
This comprehensive n8n workflow automates the entire Meta (Facebook/Instagram) advertising process, from asset analysis to ad creation. It combines AI-powered content analysis with automated ad deployment, streamlining the creation of high-converting social media advertisements.
Key Features
ü§ñ AI-Powered Asset Analysis
Video Analysis: Uses Google Gemini to analyze video content, extracting transcripts, scene descriptions, hooks, offers, and branding elements
Image Analysis: Employs GPT-4.1 Mini to analyze static images, identifying visual elements, USPs, and marketing potential
Structured Output: Generates detailed descriptions and creative insights for informed ad creation
‚úçÔ∏è Automated Ad Copy Generation
Creates 3 variations each of primary text, headlines, and link descriptions
Optimized for Meta's character limits and best practices
Maintains brand voice and messaging consistency
Leverages AI analysis to create compelling, conversion-focused copy
üéØ Meta Ads Manager Integration
Video Ads: Uploads videos, creates ad creatives with multiple text variations
Image Ads: Supports both single and multi-image campaigns (1:1 and 9:16 formats)
Asset Feed Optimization: Implements placement-specific customization rules
Automated Preview Generation: Creates ad previews for different placements
üìä Smart Workflow Management
Google Drive Integration: Monitors designated folders for new creative assets
Google Sheets Tracking: Maintains comprehensive records of all assets and campaigns
Status Management: Tracks processing stages and prevents duplicate work
Error Handling: Includes retry logic and status checking for reliable operation
Workflow Components
Asset Discovery & Processing
Google Drive Trigger: Monitors specified folder for new image/video files
File Analysis: Extracts metadata, dimensions, and file specifications
Asset Registration: Logs all assets in Google Sheets for tracking
AI Analysis Pipeline
Content Type Detection: Automatically identifies videos vs. images
Video Processing:
Uploads to Google Gemini for analysis
Generates comprehensive content breakdown
Extracts all marketing-relevant elements
Image Processing:
Analyzes visual content with GPT-4.1 Mini
Identifies key messaging and visual elements
Creates detailed creative descriptions
Ad Copy Creation
AI Agent Processing: Transforms analysis into marketing copy
Multi-Variant Generation: Creates 3 versions of each text element
Platform Optimization: Ensures compliance with Meta's requirements
Quality Assurance: Structured output validation
Meta Ads Deployment
Asset Upload: Pushes images/videos to Meta Ads Manager
Creative Assembly: Builds ad creatives with generated copy variations
Campaign Creation: Sets up ads with proper targeting and placement rules
Status Tracking: Updates spreadsheet with campaign IDs and status
Setup Requirements
Required Credentials
Meta Developer Account: App access token for Meta Graph API
Google Drive OAuth: For file monitoring and asset access
Google Sheets OAuth: For workflow tracking and management
Google Gemini API: For video analysis capabilities
OpenAI API: For image analysis and copy generation
Configuration Steps
Google Drive Setup:
Create dedicated folder for creative assets
Configure folder monitoring in the trigger node
Google Sheets Template:
Use provided template for asset and campaign tracking
Configure account settings and campaign parameters
Meta Developer Setup:
Create Meta App and obtain access tokens
Set up ad account permissions
API Credentials:
Configure all required authentication credentials
Test connections before activation"
Convert boring images to stunning photos and videos,https://n8n.io/workflows/4275-convert-boring-images-to-stunning-photos-and-videos/,"AI Image Editor with Telegram Bot
Tutorial Video:
https://www.youtube.com/watch?v=imi7xKQvKh8
‚ú® Transform ordinary photos into AI masterpieces
This powerful workflow creates a complete AI image editing system that your Telegram contacts can use with a simple message. Let users send images with creative instructions and watch as cutting-edge AI transforms their ideas into reality.
ü§ñ What This Workflow Does
Connect your Telegram bot to OpenAI and Replicate's advanced image models to:
Edit photos based on text descriptions
Generate creative variations of original images
Deliver professional-quality results in seconds
Handle the entire process automatically
üé® Perfect For
Digital creators seeking quick design iterations
Photography enthusiasts wanting AI enhancements
Product marketers creating concept visualizations
Community managers offering image editing services
Anyone looking to explore AI image capabilities
üîß Easy Setup
Just configure your API credentials, customize a few prompts, and your Telegram bot becomes a powerful AI image editor. The workflow handles all the technical complexity including file conversions, API communications, and asynchronous processing.
üí° Expandable Design
Built with flexibility in mind, you can easily:
Add additional AI models
Implement image moderation
Create branching paths based on user requests
Integrate with other services like cloud storage
Elevate your image editing capabilities with this seamless blend of messaging and AI technology!"
Compose/Stitch Separate Images together using n8n & Gemini AI Image Editing,https://n8n.io/workflows/4817-composestitch-separate-images-together-using-n8n-and-gemini-ai-image-editing/,"This n8n template demonstrates how to use AI to compose or ""stitch"" separate images together to generate a new image which retains the source assets and consistent style.
Use cases are many: Try producing storyboard scenes with consistent characters, marketing material with existing product assets or trying on different articles on fashion!
Good to know
At time of writing, each image generated will cost $0.039 USD. See Gemini Pricing for updated info.
The model used in this workflow is geo-restricted! If it says model not found, it may not be available in your country or region.
How it works
We'll import our required assets via our Cloud storage using the HTTP node.
The images are then converted to base64 strings and aggregated so we can use it for our AI model.
Gemini's image generation model is used which takes all 3 images and a prompt that we define. Our prompt instructs the model on how to compose the final image.
Gemini generates a new image but uses the original 3 assets to do so. The consistency to the source images is very high and shows little signs of hallucinations!
Gemini's output is base64 so we use a ""Convert to file"" node to convert the data to binary.
The final binary image is then uploaded to Google Drive to complete the demonstration.
How to use
The manual trigger node is used as an example but feel free to replace this with other triggers such as webhook or even a form.
Technically, you should be able to compose even more images but of course, the generation will take longer and cost more.
Requirements
Gemini account for LLM and Image generation
Google drive for upload
Customising this workflow
AI Image editing can be used for many use-cases. Try a popular use-case such as virtual try-on for fashion or applying branding on editing image assets."
Track CVE Vulnerability Details & History with NVD API and Google Sheets,https://n8n.io/workflows/4797-track-cve-vulnerability-details-and-history-with-nvd-api-and-google-sheets/,"Who is this for?
NVD (National Vulnerability Database) data is essential for security analysts, vulnerability managers, and DevSecOps professionals who need to perform both CVE lookups and monitor historical change logs. This workflow helps streamline those efforts by providing structured outputs for audit, triage, or compliance tracking purposes.
üìù Note: While this example uses Google Sheets as the destination, you can easily modify the final destination node (e.g., send to Slack, email, database, etc.) based on your specific automation needs.?
What problem is this solving?
Security teams often manually look up CVE data and track changes across multiple tools. This process is inefficient and error-prone. This workflow automates the CVE lookup and historical change tracking by logging enriched vulnerability data into Google Sheets in real-time.
What this workflow does
This workflow is designed for CVE API lookup and change history tracking. In many vulnerability automation pipelines, it is essential to determine not only the metadata of a CVE but also how it has evolved over time. Based on the operational need‚Äîwhether it's enrichment, risk scoring, or remediation validation‚Äîthis workflow becomes particularly handy in surfacing both current and historical CVE data. This template performs the following actions:
Accepts incoming webhook requests containing a CVE ID
Queries the NVD CVE Lookup API to fetch vulnerability metadata
Queries the NVD CVE History API to retrieve all historical changes
Flattens both datasets into a sheet-compatible structure
Appends vulnerability metadata to one sheet and change history to another within the same Google Spreadsheet
Setup
üîë Request an NVD API Key
To request an NVD API Key, please provide your organization name, a valid email address, and indicate your organization type at NVD API Key Request. You must scroll to the end of the Terms of Use Agreement and check ""I agree to the Terms of Use"" to obtain an API Key. After submission, you will receive a single-use hyperlink via email to activate and view your API Key. If not activated within seven days, a new request must be submitted.
üìä API Rate Limits
Without an API key, you're limited to 5 requests per 30-second window. With an API key, you‚Äôre allowed up to 50 requests in the same period. To prevent request throttling, it's recommended to introduce slight delays between consecutive API calls in production setups.
Clone or import this workflow into your n8n instance.
Set up the following credentials:
Google Sheets OAuth2
NVD API Key (via HTTP Header Auth)
The workflow logs data to a Google Sheet titled NVD Database, with Sheet 1 named CVE Lookup and Sheet 2 named CVE History.
Trigger each workflow using the respective webhook URL, appending ?cveId=CVE-XXXX-XXXX as a query parameter.
üîç Example Webhook Request (CVE Change History)
You can test this workflow with the following example:
GET https://your-domain.com/webhook/cve-history?cveId=CVE-2023-34362
How to customize this workflow
Use the Edit Fields node (optional) to centralize configuration like sheet name or query input
Extend the CVE flattening logic to include more nested metadata if needed
Integrate notification systems (e.g., Slack or email) by branching from the processing nodes
Modify webhook paths for better endpoint organization
üîê Production Security Tips
Use HTTP Header Auth on the webhook for secure access
‚ö†Ô∏è This template uses webhooks and NVD API access with authentication headers.
This template uses two flows:
Webhook 1: NVD CVE Lookup ‚Äî Lookup CVE vulnerability metadata from NVD and sync to Google Sheet
Webhook 2: NVD CVE Change History ‚Äî Track change history for CVEs via NVD and log each update
Each flow:
Hits NVD‚Äôs respective endpoint
Uses custom JS Code node to flatten the nested JSON
Syncs data to dedicated Google Sheet tabs
üß© 4 nodes: Webhook ‚Üí API Call ‚Üí Parse ‚Üí Sheet Sync
Make sure both flows are activated and webhooks exposed for external access. Based on your needs, ensure you have a secure setup‚Äîwhether hosted internally or in a cloud environment‚Äîwhen running n8n in production."
"Build a Chatbot, Voice and Phone Agent with Voiceflow, Google Calendar and RAG",https://n8n.io/workflows/3657-build-a-chatbot-voice-and-phone-agent-with-voiceflow-google-calendar-and-rag/,"Voiceflow is a no-code platform that allows you to design, prototype, and deploy conversational assistants across multiple channels‚Äîsuch as chat, voice, and phone‚Äîwith advanced logic and natural language understanding. It supports integration with APIs, webhooks, and even tools like Twilio for phone agents. It's perfect for building customer support agents, voice bots, or intelligent assistants.
This workflow connects n8n and Voiceflow with tools like Google Calendar, Qdrant (vector database), OpenAI, and an order tracking API to power a smart, multi-channel conversational agent.
There are 3 main webhook endpoints in n8n that Voiceflow interacts with:
n8n_order ‚Äì receives user input related to order tracking, queries an API, and responds with tracking status.
n8n_appointment ‚Äì processes appointment booking, reformats date input using OpenAI, and creates a Google Calendar event.
n8n_rag ‚Äì handles general product/service questions using a RAG (Retrieval-Augmented Generation) system backed by:
Google Drive document ingestion,
Qdrant vector store for search,
and OpenAI models for context-based answers.
Each webhook is connected to a corresponding ""Capture"" block inside Voiceflow, which sends data to n8n and waits for the response.
How It Works
This n8n workflow integrates Voiceflow for chatbot/voice interactions, Google Calendar for appointment scheduling, and RAG (Retrieval-Augmented Generation) for knowledge-based responses. Here‚Äôs the flow:
Trigger:
Three webhooks (n8n_order, n8n_appointment, n8n_rag) receive inputs from Voiceflow (chat, voice, or phone calls).
Each webhook routes requests to specific functions:
Order Tracking: Fetches order status via an external API.
Appointment Scheduling: Uses OpenAI to parse dates, creates Google Calendar events, and confirms via WhatsApp.
RAG System: Queries a Qdrant vector store (populated with Google Drive documents) to answer customer questions using GPT-4.
AI Processing:
OpenAI Chains: Convert natural language dates to Google Calendar formats and generate responses.
RAG Pipeline: Embeds documents (via OpenAI), stores them in Qdrant, and retrieves context-aware answers.
Voiceflow Integration: Routes responses back to Voiceflow for multi-channel delivery (chat, voice, or phone).
Outputs:
Confirmation messages (e.g., ""Event created successfully"").
Dynamic responses for orders, appointments, or product support.
Setup Steps
Prerequisites:
APIs:
Google Calendar & Drive OAuth credentials.
Qdrant vector database (hosted or cloud).
OpenAI API key (for GPT-4 and embeddings).
Configuration:
Qdrant Setup:
Run the ""Create collection"" and ""Refresh collection"" nodes to initialize the vector store.
Populate it with documents using the Google Drive ‚Üí Qdrant pipeline (embeddings generated via OpenAI).
Voiceflow Webhooks:
Link Voiceflow‚Äôs ""Captures"" to n8n‚Äôs webhook URLs (n8n_order, n8n_appointment, n8n_rag).
Google Calendar:
Authenticate the Google Calendar node and set event templates (e.g., summary, description).
RAG System:
Configure the Qdrant vector store and OpenAI embeddings nodes.
Adjust the Retrieve Agent‚Äôs system prompt for domain-specific queries (e.g., electronics store support).
Optional:
Add Twilio for phone-agent capabilities.
Customize OpenAI prompts for tone/accuracy.

PS. You can import a Twilio number to assign it to your agent for becoming a Phone Agent
Need help customizing?
Contact me for consulting and support or add me on Linkedin"
WhatsApp Dietitian AI Chatbot Workflow,https://n8n.io/workflows/4858-whatsapp-dietitian-ai-chatbot-workflow/,"WhatsApp Dietitian AI Chatbot Workflow
Transform your WhatsApp into a professional nutrition consultant with this comprehensive AI-powered dietitian assistant. Built using n8n automation and Google Gemini AI, this workflow provides instant, intelligent nutrition advice to your clients 24/7.
üçΩÔ∏è What It Does
Meal Analysis: Users can send photos of their meals with captions like ""analyze this meal"" and receive detailed nutritional breakdowns including calories, macronutrients, and personalized recommendations.
Nutrition Guidance: Answers questions about general nutrition, dietary restrictions, meal planning, and healthy eating habits with evidence-based information.
Meal Recommendations: Suggests customized meals based on specific criteria like calorie targets, dietary preferences (vegan, keto, etc.), or available ingredients.
üöÄ Key Features
Dual Input Processing: Handles both text questions and image analysis seamlessly
Professional AI Responses: Uses Google Gemini 2.0 Flash for accurate, contextual nutrition advice
WhatsApp Optimized: Responses formatted specifically for mobile messaging with emojis and clear structure
Smart Routing: Automatically detects message type (text vs image) and processes accordingly
Structured Output: Consistent, professional formatting for all responses
Scope Management: Stays within nutrition expertise and includes appropriate medical disclaimers
üõ†Ô∏è Technical Stack
Platform: n8n workflow automation
AI Model: Google Gemini 2.0 Flash
Integration: WhatsApp Business API
Message Types: Text messages, images with captions
Output: Structured, conversational responses
üì± Perfect For
Nutritionists and dietitians offering remote consultations
Health coaches providing 24/7 client support
Fitness professionals adding nutrition guidance
Wellness businesses expanding their service offerings
Anyone wanting to monetize nutrition expertise
üéØ Ready to Deploy
This workflow comes with comprehensive documentation, clear setup instructions, and professional prompt engineering. Simply configure your WhatsApp Business API and Google Gemini credentials to start serving clients immediately.
Transform your nutrition expertise into an automated, scalable WhatsApp service that works around the clock!"
"Voice Creation, TTS, Sound Effects, Voicechanger & more! üéß Elevenlabs MCP Server",https://n8n.io/workflows/4672-voice-creation-tts-sound-effects-voicechanger-and-more-elevenlabs-mcp-server/,"This workflow provides a complete set of tools for interacting with the ElevenLabs voice API, enabling AI-powered text-to-speech, voice management, and audio processing capabilities.
Features üöÄ
Text-to-Speech Operations
Convert text to speech with customizable voice settings
Generate speech with timestamps for precise audio control
Stream text-to-speech in real-time
Stream text-to-speech with timestamps for live applications
Voice Management üó£Ô∏è
List all available voices
Get detailed information about specific voices
Delete custom voices
Edit voice properties (name and description)
Find similar voices based on voice ID
Audio Processing üéß
Create transcripts from audio
Apply voice changing effects
Generate sound effects
Isolate audio components
Design and preview new voices
Save voice previews as new voices
Requirements ‚öôÔ∏è
An ElevenLabs API key
n8n instance with HTTP Request Tool node
MCP Server Trigger node for AI integration
Setup üîß
Import the workflow into your n8n instance
Configure your ElevenLabs API key in the workflow settings
The workflow uses AI expressions for voice IDs with a fallback value of Z9hrfEHGU3dykHntWvIY
Usage üìù
All nodes are connected to the MCP Server Trigger for AI-powered interactions
Example Use Cases üí°
Generate natural-sounding speech from text
Create and manage custom voices
Process and transform audio files
Build voice-based applications
Integrate ElevenLabs capabilities into existing workflows
Technical Details üîç
All endpoints use proper error handling
Voice ID parameters use AI expressions with fallback values
Nodes are organized in logical groups for better workflow management
Includes comprehensive parameter validation
Supports streaming and real-time processing
Support ü§ù
For issues or questions, please refer to:
ElevenLabs API Documentation
n8n Documentation
n8n Community"
Index Documents from Google Drive to Pinecone with OpenAI Embeddings for RAG,https://n8n.io/workflows/4552-index-documents-from-google-drive-to-pinecone-with-openai-embeddings-for-rag/,"üß† Google Drive Upload Trigger ‚Üí Pinecone Vector Upsert for Document Indexing
Category: AI & LLM / Document Indexing
Level: Intermediate
Tags: Google Drive, Pinecone, OpenAI, Embeddings, Vector Store, LangChain, RAG
üìÑ What This Workflow Does
This workflow watches a specific Google Drive folder and automatically uploads any newly added document to a Pinecone vector database ‚Äî complete with OpenAI-generated embeddings.
Perfect for setting up retrieval-augmented generation (RAG) pipelines, semantic search, or document Q&A systems. Once configured, your knowledge base stays up-to-date with zero manual effort.
Watch Full Step By Stey Tutorial Video Here:
https://www.youtube.com/@Automatewithmarc
üîß How It Works
üìÅ Google Drive Trigger
Watches a specific folder and triggers when new documents are uploaded.
üîç Google Drive File Search & Download
Finds and fetches all files in the folder.
üîÑ Loop Over Each File
Handles batch processing for multiple files.
üìÉ Document Loader
Parses each file as binary and applies custom metadata like document type.
‚úÇÔ∏è Text Splitter
Breaks content into manageable chunks for embedding (e.g., 600 characters, 60 overlap).
üß† OpenAI Embeddings
Generates vector embeddings using OpenAI.
üì¶ Pinecone Vector Store
Inserts/upserts documents into a specific Pinecone namespace for search-ready indexing.
üß† Why This is Useful
This is a production-grade setup for:
Building vector search tools over internal docs
Feeding up-to-date data into RAG agents or chatbots
Auto-tagging and chunking files for scalable AI workflows
Whether you‚Äôre indexing course outlines, SOPs, or technical docs ‚Äî this automation keeps your vector store fresh and organized.
ü™ú Setup Instructions
Connect your Google Drive, OpenAI, and Pinecone accounts.
Specify the Google Drive folder to monitor.
Customize metadata, chunk size, or vector namespace as needed.
Activate the workflow and drop a file into the folder ‚Äî magic happens behind the scenes.
üìå Notes
Works best with PDFs or text-based documents.
You can swap out OpenAI with other embedding models if needed.
Consider adding notifications or logging (e.g., via Slack or email) for better observability."
Airbnb Telegram Agent - AI-powered accommodation search with voice support,https://n8n.io/workflows/4494-airbnb-telegram-agent-ai-powered-accommodation-search-with-voice-support/,"Welcome to my Airbnb Telegram Agent Workflow!
This workflow creates an intelligent Telegram bot that helps users search and find Airbnb accommodations using natural language queries and voice messages.
DISCLAIMER: This workflow only works with self-hosted n8n instances! You have to install the n8n-nodes-mcp-client Community Node!
What this workflow does
This workflow processes incoming Telegram messages (text or voice) and provides personalized Airbnb accommodation recommendations. The AI agent understands natural language queries, searches through Airbnb data using MCP tools, and returns mobile-optimized results with clickable links, prices, and key details.
Key Features:
Voice message support (speech-to-text and text-to-speech)
Conversation memory for context-aware responses
Mobile-optimized formatting for Telegram
Real-time Airbnb data access via MCP integration
This workflow has the following sequence:
Telegram Trigger - Receives incoming messages from users
Text or Voice Switch - Routes based on message type
Voice Processing (if applicable) - Downloads and transcribes voice messages
Text Preparation - Formats text input for the AI agent
Airbnb AI Agent - Core logic that:
Lists available MCP tools for Airbnb data
Executes searches with parsed parameters
Formats results for mobile display
Response Generation - Sends formatted text response
Voice Response (optional) - Creates and sends audio summary
Requirements:
Telegram Bot API: Documentation
Create a bot via @BotFather on Telegram
Get bot token and configure webhook
OpenAI API: Documentation
Used for speech transcription (Whisper)
Used for chat completion (GPT-4)
Used for text-to-speech generation
MCP Community Client Node: Documentation
Custom integration for Airbnb data
Requires MCP server setup with Airbnb/Airtable connection
Provides tools for accommodation search and details
Important: You need to set up an MCP server with Airbnb data access. The workflow uses MCP tools to retrieve real accommodation data, so ensure your MCP server is properly configured with the Airtable/Airbnb integration.
Configuration Notes:
Update the Telegram chat ID in the trigger for your specific bot
Modify the system prompt in the Airbnb Agent for different use cases
The workflow supports both individual users and can be extended for group chats
Feel free to contact me via LinkedIn, if you have any questions!"
"Automate Call Scheduling with Voice AI Receptionist using Vapi, Google Calendar & Airtable",https://n8n.io/workflows/3427-automate-call-scheduling-with-voice-ai-receptionist-using-vapi-google-calendar-and-airtable/,"Who is this template for?
This template is ideal for small businesses, agencies, and solo professionals who want to automate appointment scheduling and caller follow-up through a voice-based AI receptionist. If you‚Äôre using tools like Google Calendar, Airtable, and Vapi (Twilio), this setup is for you.
What problem does this workflow solve?
Manual call handling, appointment booking, and email coordination can be time-consuming and prone to errors. This workflow solves that by automating the receptionist role: answering calls, checking calendar availability, managing appointments, and storing call summaries‚Äîall without human intervention.
What this workflow does
This Agent Receptionist manages inbound voice calls and scheduling tasks using Vapi and Google Calendar. It checks availability, books or updates calendar events, sends email confirmations, and logs call details into Airtable. The workflow includes built-in logic for slot management, email triggers, and storing call transcripts.
Setup Instructions
Duplicate Airtable Base: Use this Airtable base templateBASE LINK
Import Workflow: Load provided JSON into your n8n instance.
Credentials: Connect your Google Calendar and Airtable credentials in n8n.
Activate Workflow: Enable workflow to get live webhook URLs.
Vapi Configuration:
Paste provided system prompt into Vapi Assistant.
Link the appropriate webhook URLs from n8n (GetSlots, BookSlots, UpdateSlots, CancelSlots, and end-of-call report).
Disclaimer
Optimized for cloud-hosted n8n instances. Self-hosted users should verify webhook and credential setups."
üîçüõ†Ô∏èGenerate SEO-Optimized WordPress Content with AI Powered Perplexity Research,https://n8n.io/workflows/3291-generate-seo-optimized-wordpress-content-with-ai-powered-perplexity-research/,"Generate SEO-Optimized WordPress Content with Perplexity Research
Who is This For?
This workflow is ideal for content creators, marketers, and businesses looking to streamline the creation of SEO-optimized blog posts for WordPress. It is particularly suited for professionals in the AI consulting and workflow automation industries.
What Problem Does This Workflow Solve?
Creating high-quality, SEO-friendly blog posts can be time-consuming and challenging, especially when trying to balance research, formatting, and publishing. This workflow automates the process by integrating research capabilities, AI-driven content creation, and seamless WordPress publishing. It reduces manual effort while ensuring professional-grade output.
What This Workflow Does
Research: Gathers detailed insights from Perplexity AI based on user-provided queries.
Content Generation: Uses OpenAI models to create structured blog posts, including titles, slugs, meta descriptions, and HTML content optimized for WordPress.
Image Handling: Automatically fetches and uploads featured images to WordPress posts.
Publishing: Drafts the blog post directly in WordPress with all necessary formatting and metadata.
Notification: Sends a success message via Telegram upon completion.
Setup Guide
Prerequisites:
A WordPress account with API access.
OpenAI API credentials.
Perplexity AI API credentials.
Telegram bot credentials for notifications.
Steps:
Import the workflow into your n8n instance.
Configure API credentials for WordPress, OpenAI, Perplexity AI, and Telegram.
Customize the form trigger to define your research query.
Test the workflow using sample queries to ensure smooth execution.
How to Customize This Workflow to Your Needs
Modify the research query prompt in the ""Form Trigger"" node to suit your industry or niche.
Adjust content generation guidelines in the ""Copywriter AI Agent"" node for specific formatting preferences.
Replace the image URL in the ""Set Image URL"" node with your own source or dynamic image selection logic."
Weekly AI News Digest with Perplexity AI and Gmail Newsletter,https://n8n.io/workflows/4412-weekly-ai-news-digest-with-perplexity-ai-and-gmail-newsletter/,"Overview
This automated workflow delivers a weekly digest of the most important AI news directly to your inbox. Every Monday at 9 AM, it uses Perplexity AI to research the latest developments and organizes them into four key categories: New Technology, Trending Topics, Top Stories, and AI Security. The workflow then formats this information into a beautifully designed HTML email with summaries, significance explanations, and source links.
What It Does
Automatically searches for the latest AI news using Perplexity AI
Categorizes content into four focused areas most relevant to AI enthusiasts and professionals
Generates comprehensive summaries explaining why each story matters
Creates a professional HTML email with styled sections and clickable links
Sends weekly on Monday at 9 AM (customizable schedule)
Includes error handling with fallback content if news parsing fails
Setup Instructions
Import the Workflow
Copy the JSON code and import it into your n8n instance
The workflow will appear as ‚ÄúDaily AI News Summary‚Äù
Configure Perplexity API
Sign up for a Perplexity API account at perplexity.ai
Create new credentials in n8n:
Type: ‚ÄúOpenAI‚Äù
Name: ‚Äúperplexity-credentials‚Äù
API Key: Your Perplexity API key
Base URL: https://api.perplexity.ai
Set Up Email Credentials
Configure SMTP credentials in n8n:
Name: ‚Äúemail-credentials‚Äù
Add your email provider‚Äôs SMTP settings
Test the connection to ensure emails can be sent
Customize Email Settings
Open the ‚ÄúSend Email Summary‚Äù node
Update the toEmail field with your email address
Modify the fromEmail if needed (must match your SMTP credentials)
Optional Customizations
Change Schedule: Modify the ‚ÄúDaily Trigger‚Äù node to run at your preferred time
Adjust Categories: Edit the Perplexity prompt to focus on different AI topics or change the theme altogether
Modify Styling: Update the HTML template in the ‚ÄúFormat Email Content‚Äù node
Test and Activate
Run a test execution to ensure everything works correctly
Activate the workflow to start receiving daily AI news summaries
Requirements
n8n instance (cloud or self-hosted)
Perplexity API account and key
SMTP email access (Gmail, Outlook, etc.)"
YouTube Comment Scraper & Analyzer with GPT-4o + Email Summary Report,https://n8n.io/workflows/4863-youtube-comment-scraper-and-analyzer-with-gpt-4o-email-summary-report/,"How it Works
This workflow automates the collection and analysis of YouTube comments from a video and sends a summary report via email, using Google Sheets, the YouTube API, OpenAI (GPT-4o), and Gmail.
Whether you're a content creator, brand manager, or social media analyst, this workflow helps you automate sentiment analysis and receive insights directly in your inbox ‚Äî all triggered from a simple spreadsheet.
üéØ Use Case
Ideal for:
YouTubers monitoring audience sentiment
Marketing teams analyzing campaign feedback
Community managers summarizing engagement
Setup Instructions
1. Upload the Spreadsheet
File name: Youtube_Video
Sheet structure:
| ID | Video Title | YouTube Video ID | Status |
Add video IDs and set their Status as Pending
2. Configure Google Sheets Nodes
Connect your Google account to:
Pick Video IDs from Google Sheet
Update Status on Google Sheet
3. Add API Credentials
YouTube API Key ‚Üí for comment + video scraping nodes
OpenAI API Key ‚Üí for analyzing comments
Gmail Account ‚Üí for sending the summary email
4. Activate the Workflow
Once live, the workflow will:
Watch for new or updated rows in the spreadsheet
Scrape comments using the YouTube API
Analyze sentiment and key themes via GPT-4o
Send a formatted HTML email with the summary
Update the spreadsheet status to Mail sent
üîÅ Workflow Logic
Trigger: New/updated row in Google Sheet
Retrieve: YouTube video metadata + comments
Analyze: Comments using GPT-4o
Email: Summary report via Gmail
Update: Spreadsheet status to Mail sent
üß© Node Descriptions
Node Name Description
Pick Video IDs from Google Sheet Watches the spreadsheet and retrieves pending video IDs
If Checks whether status is 'Pending'
Limit Restricts the number of processed rows
Set Video Details Prepares video info (e.g., title, channel)
Get YouTube Video Details Fetches metadata (title, channel, etc.)
Get YouTube Video Comments Pulls top-level comments using YouTube API
Prepare Comments Data Formats comment text for OpenAI
AI Agent Summarizes comments using OpenAI's GPT-4o
Prepare HTML for Email Converts summary into HTML for email body
Gmail Account Configuration Sends the email report via Gmail
Update Status on Google Sheet Marks the row as 'Mail sent'
üõ†Ô∏è Customization Tips
Change the AI prompt for tone, length, or custom metrics
Send results to Slack or Telegram instead of Gmail
Export summaries to Notion, Airtable, or PDF
Schedule it daily/weekly for recurring analysis
üìí Suggested Sticky Notes for Workflow
Node/Section Sticky Note Content
Pick Video IDs from Google Sheet ""Triggers on new YouTube videos in your spreadsheet""
AI Agent ""Uses OpenAI to generate an analysis summary ‚Äì customize prompt as needed""
Gmail ""Sends summary report ‚Äì you can update subject, recipients, or style""
Update Status ""Marks video as processed to avoid duplicate runs""
üìé Required Files
File Name Purpose
Youtube_Video Google Sheet to hold YouTube video IDs and status
Youtube_Comment_Scraper.json Main n8n workflow export for this automation
üß™ Testing Tips
Add one test video with a valid YouTube video ID and status = Pending
Monitor the workflow logs to confirm API responses
Confirm summary delivery in your inbox
Verify that status updates in the sheet
üè∑ Suggested Tags & Categories
#YouTube
#OpenAI
#Automation
#Marketing
#Email
#Analytics"
Auto-Generate AI News Commentary with Dumpling AI and GPT-4o,https://n8n.io/workflows/4884-auto-generate-ai-news-commentary-with-dumpling-ai-and-gpt-4o/,"This workflow turns trending news into thoughtful first-person commentary for platforms like LinkedIn. It uses Dumpling AI‚Äôs News Search and Scraping APIs to find and extract article content, then feeds the cleaned text to GPT-4o to write personalized insights. The final output is saved back to Google Sheets as a draft for easy review or posting.
‚úÖ What this workflow does
Triggers daily using a Schedule node.
Fetches a list of content topics from a Google Sheet.
Uses Dumpling AI to search for relevant news articles based on each topic.
Scrapes the article content with Dumpling AI‚Äôs /scrape endpoint.
Cleans and aggregates the article content using a Code node.
Generates first-person commentary with GPT-4o tailored for LinkedIn.
Appends the generated post back to the Google Sheet next to its topic.
üß© Nodes in this workflow
Schedule Trigger: Starts the workflow daily.
Google Sheets (Read Topics): Pulls topic rows that don‚Äôt have a generated commentary yet.
Split In Batches: Processes each topic one at a time.
Wait: Adds a delay to manage API limits.
HTTP Request (Search News): Uses Dumpling AI's /search-news to find relevant articles for the topic.
Split Out: Iterates over the list of article results.
HTTP Request (Scrape Article): Extracts the full article text using Dumpling AI‚Äôs /scrape.
Aggregate: Collects and merges article content fields.
Code (Clean Article): Strips links, formatting, and irrelevant text.
OpenAI (GPT-4o): Generates a short, first-person LinkedIn post-style commentary using a custom prompt.
Google Sheets (Write Back): Appends the final output next to the original topic in the sheet.
üßë‚Äçüíº Who is this for?
Founders, content creators, marketers, or agency teams looking to maintain an active presence on LinkedIn or newsletters by sharing smart takes on industry trends.
üí° What problem does this solve?
Most people want to comment on current events but don't have the time to summarize articles or write well-structured posts. This automation saves hours of manual work by:
Finding the right article.
Extracting and cleaning the content.
Writing it in a natural, first-person voice using AI.
‚öôÔ∏è What you need to use this:
A Google Sheet with at least two columns: topic and generated commentary.
A Dumpling AI API Key with access to the /search-news and /scrape endpoints.
An OpenAI GPT-4o connection."
"Automated Lead Generation & Qualification with Google Maps, GPT-4 & HubSpot",https://n8n.io/workflows/4824-automated-lead-generation-and-qualification-with-google-maps-gpt-4-and-hubspot/,"This n8n workflow automates CVE tracking by retrieving vulnerability details from the NVD API üõ°Ô∏è, organizing and updating the data in Google Sheets üìä, and optionally alerting teams via Slack or Email üì©üí¨.
Who is this for?
This workflow is ideal for:
Security operations (SecOps) teams üßë‚Äçüíª
DevSecOps engineers üõ†Ô∏è
IT compliance officers üßæ
Vulnerability management analysts üïµÔ∏è
Sysadmins or cloud engineers in regulated industries üè¢
What problem does this workflow solve?
Manually checking for the latest CVE information is inefficient and error-prone. This automation:
Monitors NVD for CVE entries based on product or keyword filters üîç
Tracks new vulnerabilities and changes to existing ones ‚è±Ô∏è
Logs all CVE data in a structured Google Sheet for ongoing review and audit üßæ
Can trigger alerts or actions for high-severity CVEs üö®
What this workflow does
This workflow builds an automated CVE monitoring system that:
Queries the NVD API for vulnerability data matching keywords (e.g. ""Apache"", ""Log4j"") üì°
Extracts relevant fields: CVE ID, description, severity (CVSS scores), published/modified dates, and affected products üóÇÔ∏è
Saves or updates the information in Google Sheets üìë
Optionally filters for critical severity (e.g., CVSS > 8.0) and sends Slack alerts or emails üì¨
Supports historical tracking and change detection over time üïí
Includes a Google Sheets template for tracking:
CVE IDs and metadata
Severity levels and scores
Product/component tags
Resolution/patch status tracking
Setup
Prerequisites
You'll need:
An n8n instance (cloud or self-hosted) ‚òÅÔ∏è
A Google account + Google Sheets API credentials üìë
(Optional) Slack webhook URL or email setup for notifications üí¨
Step 1: Configure API Inputs
Open the üîß Configuration node and provide:
NVD API parameters (keyword filters, date ranges, etc.)
Google Sheet ID and tab name for output
Slack webhook URL (optional)
Step 2: Set Filters & Preferences
Define:
Target keywords or CPE filters (e.g. ‚ÄúCisco ASA‚Äù, ‚ÄúWindows 10‚Äù) üß©
CVSS threshold for high/critical alerts üéöÔ∏è
Update frequency (manual trigger, scheduled cron, webhook, etc.) üîÅ
Step 3: Connect to Google Sheets
Update Sheet node with your destination Sheet ID
Ensure columns like CVE ID, Description, Severity, Last Updated exist
Step 4: Enable Alerts (Optional)
Set up Slack node with your webhook URL or connect SMTP/Email node
Format alert message with key CVE data
Step 5: Activate and Run
Save and activate the workflow üîõ
Run manually or schedule it to run periodically (e.g., every 6 hours) ‚è±Ô∏è
Customization Tips
Add deduplication logic to avoid reprocessing the same CVEs ‚ôªÔ∏è
Use filters to monitor only critical CVEs or specific vendors/vendors üîç
Extend with GitHub Security Advisories or Exploit DB integration üß©
Track remediation status and link to patch notes or fixes ü©π
Troubleshooting
Common Issues
Empty results from NVD: Check if your keywords are too narrow or if NVD API rate limits apply üìâ
Google Sheets error: Ensure the Sheet ID and tab names are correct and accessible üîë
Alerts not sending: Check Slack webhook or email configurations üîß
Getting Help
Read inline comments in n8n üìù
Visit the n8n Docs üìö
Contact template creator: dimejicole21@gmailcom
This template was created by David Olusola. üõ°Ô∏è"
AI-Powered HR Interview System with BeyondPresence,https://n8n.io/workflows/4514-ai-powered-hr-interview-system-with-beyondpresence/,"What problem does it solve?
Manual candidate screening is time-consuming and inconsistent. This workflow automates initial interviews, providing 24/7 availability, consistent questioning, and objective assessments for every candidate.
Who is it for?
HR teams handling high-volume recruiting
Small businesses without dedicated recruiters
Companies scaling their hiring process
Remote-first organizations needing asynchronous screening
What this workflow does
Creates AI interviewers from job descriptions that conduct natural conversations with candidates via BeyondPresence Agents. Automatically analyzes interviews and saves structured assessments to Google Sheets.
Setup
Copy template sheet: BeyondPresence HR Interview System Template
Add credentials:
BeyondPresence API Key
OpenAI API
Google Sheets
Configure webhook in BeyondPresence dashboard:
https://[your-n8n-instance]/webhook/beyondpresence-hr-interviews
Paste job description and run setup
Share generated link with candidates
How it works
Agent Creation: Converts job description into conversational AI interviewer
Interview Conduct: Candidates chat naturally with AI via shared link
Webhook Trigger: Completed interviews sent to n8n
AI Analysis: OpenAI evaluates responses against job requirements
Results Storage: Assessments saved to Google Sheets with scores and recommendations
Resources
Google Sheets Template
BeyondPresence Documentation
Webhook Setup Guide
Example Use Case
Tech startup screens 200 applicants for engineering role. Creates AI interviewer in 2 minutes, sends link to all candidates. Receives structured assessments within 24 hours, identifying top 20 candidates for human interviews. Reduces initial screening time from 2 weeks to 2 days."
Extract Invoice Data from PDFs with AI - Google Sheets Email Alerts,https://n8n.io/workflows/4763-extract-invoice-data-from-pdfs-with-ai-google-sheets-email-alerts/,"Built by Setidure Technologies
This smart n8n automation extracts invoice details from PDF files uploaded to Google Drive using AI, logs them to a Google Sheet, and notifies the billing team via email ‚Äî all without manual intervention.
‚ö†Ô∏è Note: This workflow requires a self-hosted n8n instance with LangChain, LLM, and Google integrations configured.
üì¶ What This Workflow Does
Monitors a Google Drive folder for new invoice uploads
Extracts text and parses key invoice details using LLM via LangChain
Logs extracted data into a Google Sheet (Invoice Database)
Generates a summary email using GPT-4O-MINI (Greenie)
Sends the email to the billing team via Gmail
‚úÖ Prerequisites
A Google Drive folder to monitor for PDF uploads
A Google Sheet named Invoice Database with the following columns:
Invoice Number, Client Name, Client Email, Client Address, Client Phone, Invoice Date, Due Date, Total Amount
Service account or OAuth credentials for:
Google Drive
Google Sheets
Gmail
LangChain + Ollama integration for LLM responses
üîß Step-by-Step Setup Instructions
Clone this workflow into your self-hosted n8n instance
Set up credentials:
Google Drive (for folder trigger)
Google Sheets (for data logging)
Gmail (for sending email)
Ollama (local LLM) or any connected LangChain provider
Configure the trigger node to watch your specific Invoice Uploads folder
Update the Google Sheet node with your Invoice Database sheet URL and column mapping
Test with a sample invoice to validate the AI extraction and email generation
üîÑ Workflow Steps
Step 1: Trigger on New File in Google Drive
Node Name: Watch for New Invoices
Type: Google Drive Trigger
Event: fileCreated
Triggers when a new PDF file is uploaded to a designated folder
Step 2: Download the Uploaded File
Node Name: Download Invoice PDF
Type: Download Binary
Downloads the invoice file from Google Drive
Step 3: Extract Raw Text from PDF
Node Name: Extract PDF Text
Type: Extract from File
Extracts unstructured text content from the downloaded PDF
Step 4: Parse Invoice Fields Using AI
Node Name: Parse Invoice Data with LLM
Type: LangChain Agent
LLM is prompted to extract:
Invoice Number
Client Name, Email, Address, Phone
Invoice Date, Due Date, Total Amount
Fields not found are skipped
Step 5: Log Extracted Data to Google Sheet
Node Name: Log to Invoice Database
Type: Google Sheets
Appends a new row with the extracted fields to the Invoice Database spreadsheet
Step 6: Create Email Notification via LLM
Node Name: Generate Billing Email Summary
Type: LangChain Agent (GPT-4O-MINI)
Prompt instructs AI to:
Act as ‚ÄúGreenie‚Äù from Green Grass Corp
Inform billing that a new invoice was processed
Confirm logging into the Invoice Database
Step 7: Send the Email to Billing Team
Node Name: Email Billing Team
Type: Gmail Send
To: billing@example.com
Subject and body injected from LLM output
Step 8: End Workflow Gracefully
Node Name: End
Type: No Operation
Used to cleanly terminate the flow
üß† Example Output (Email)
Subject: New Invoice Logged ‚Äì Client: ABC Corp
Hi Billing Team,
A new invoice has been received and processed automatically. The following details have been extracted and logged into the Invoice Database:
Invoice Number: INV-1024
Client: ABC Corp
Amount: $1,450
Due Date: July 15, 2025
Please review the Invoice Database for full details.
Regards,
Greenie
Green Grass Corp"
"Create AI Videos with OpenAI Scripts, Leonardo Images & HeyGen Avatars",https://n8n.io/workflows/4107-create-ai-videos-with-openai-scripts-leonardo-images-and-heygen-avatars/,"Short Content Automation üé¨ (AI Video System with Bulk Gen, Avatar & Music Customization)
Overview üéØ
The ""Short Content"" automation is a powerful, all-in-one solution designed to streamline the creation of short videos for social media, marketing, or personal projects. Leveraging cutting-edge AI tools and seamless workflows, this automation handles everything from scriptwriting to video assembly‚Äîsaving time and effort while delivering professional results.
<div>
<a href=""https://www.loom.com/share/4e8af8c4cc6b4214a245b0f2c2458577"">
<p>Best Short Content AI System - Watch Video</p>
</a>
<a href=""https://www.loom.com/share/4e8af8c4cc6b4214a245b0f2c2458577"">
<img src=""https://cdn.loom.com/sessions/thumbnails/4e8af8c4cc6b4214a245b0f2c2458577-37ff2cf2330935d3-full-play.gif"">
</a>
</div>
Problem üõ†Ô∏è
Creating short videos manually is time-consuming and expensive. Businesses and creators struggle with:
Hiring writers, designers, and editors.
Juggling multiple tools for scripts, visuals, and editing.
Inconsistent quality and slow turnaround times.
This automation solves these challenges with a fully AI-driven, one-click workflow.
Solution ‚úÖ
A no-code, end-to-end automation that:
Writes scripts using AI (LLM) or manual input.
Generates visuals (Leonardo AI) or acting scenes (RunwayML).
Edits and assembles videos (JSON2Video).
Generates avatars using HeyGen or Captions.ai.
Tracks progress and analytics in Baserow.
üî• Result:
High-quality short videos in minutes, not days‚Äîwith zero manual effort.
Setup ‚öôÔ∏è (4‚Äì8 minutes)
Upload Blueprint to N8N
Import the provided JSON workflow into your N8N instance.
Upload Database to Baserow (Free)
Follow the setup video step-by-step.
How It Works üåü
Start with a Pre-Built Form
Fill in the included Baserow form with your video requirements.
Generate Videos Instantly
Single Video Mode: Run one video at a time for quick results.
Bulk Mode: Upload a CSV or use Baserow to queue unlimited videos‚Äîperfect for scaling content.
Workflow Handles the Rest
Manage & Iterate
Track all videos in Baserow: Edit inputs, retry failed jobs, or adjust styles anytime.
What‚Äôs Included üì¶
‚úÖ Baserow Template: Pre-configured database.
‚úÖ N8N JSON Blueprint: Ready-to-use workflow.
‚úÖ Step-by-Step Video Guides: Setup & usage instructions.
Customization üé®
üîß Video Generation Customization
Script Options: Auto-generate using AI or write manually.
Captions: Customize type, color, and style.
Video Types: Storytelling, trends, educational, etc.
Audio: Add background music easily.
Avatars:
Switch between HeyGen and Captions.ai.
Adjust size, position, voice, and appearance.
‚öôÔ∏è System Customization
N8N Workflow: Easily customizable for new integrations, prompts, and logic.
Baserow Database: Modify options or add new ones to match your workflow.
üåê Explore more workflows
‚ù§Ô∏è Buy more workflows at: adamcrafts
ü¶æ Custom workflows at: adamcrafts@cloudysoftwares.com
Build once, customize endlessly, and scale your video content like never before. üöÄ"
Building Your First WhatsApp Chatbot,https://n8n.io/workflows/2465-building-your-first-whatsapp-chatbot/,"This n8n template builds a simple WhatsApp chabot acting as a Sales Agent. The Agent is backed by a product catalog vector store to better answer user's questions.
This template is intended to help introduce n8n users interested in building with WhatsApp.
How it works
This template is in 2 parts: creating the product catalog vector store and building the WhatsApp AI chatbot.
A product brochure is imported via HTTP request node and its text contents extracted.
The text contents are then uploaded to the in-memory vector store to build a knowledgebase for the chatbot.
A WhatsApp trigger is used to capture messages from customers where non-text messages are filtered out.
The customer's message is sent to the AI Agent which queries the product catalogue using the vector store tool.
The Agent's response is sent back to the user via the WhatsApp node.
How to use
Once you've setup and configured your WhatsApp account and credentials
First, populate the vector store by clicking the ""Test Workflow"" button.
Next, activate the workflow to enable the WhatsApp chatbot.
Message your designated WhatsApp number and you should receive a message from the AI sales agent.
Tweak datasource and behaviour as required.
Requirements
WhatsApp Business Account
OpenAI for LLM
Customising this workflow
Upgrade the vector store to Qdrant for persistance and production use-cases.
Handle different WhatsApp message types for a more rich and engaging experience for customers."
Query and Monitor Shopify Orders via Telegram Bot Commands,https://n8n.io/workflows/4714-query-and-monitor-shopify-orders-via-telegram-bot-commands/,"This n8n workflow integrates Shopify order management with Telegram, allowing you to query open orders and order details directly through Telegram chat commands. It provides an interactive way to monitor your Shopify store orders using Telegram as an interface.
Key Features
Telegram Trigger: Listens for messages and callback queries from your Telegram bot.
Switch Node: Routes incoming Telegram messages to different flows based on message content:
/orders command to fetch all open orders
Callback queries starting with /order_ to fetch details of a specific order
Shopify Get Orders: Retrieves all open orders from your Shopify store using your Shopify API credentials.
Conditional Check (If Node): Determines if there are any open orders; branches accordingly:
If orders exist, prepare an interactive Telegram message with a list of orders.1
If no orders exist, send a ‚ÄúNo Order‚Äù message.
Orders Code Node: Formats the list of open orders into a Telegram message with inline buttons. Each button corresponds to an order and sends a callback data containing the order ID.
Get Order Details: When a user selects an order button, the workflow extracts the order ID from the callback data, fetches detailed order information from Shopify, and formats the order items into a readable message.
Send Messages to Telegram: Sends formatted messages back to Telegram:
The list of open orders with clickable buttons.
Detailed information about a selected order.
‚ÄúNo Order‚Äù notification if there are no open orders.
How It Works
A Telegram user sends /orders to the bot.
The workflow fetches open orders from Shopify and sends a message with buttons listing each order.
When a user clicks an order button, the workflow fetches and displays detailed information about that specific order in Telegram.
If there are no open orders, the bot replies accordingly.
Setup Instructions
Create a Telegram Bot:
Use @BotFather on Telegram to create a bot and get the bot token.
Obtain Shopify API Credentials:
Create a private app in your Shopify admin dashboard with permission to read orders.
Obtain the API key and access token.
Configure n8n Credentials:
Add your Telegram bot token as Telegram API credentials in n8n.
Add your Shopify API credentials in n8n Shopify credentials.
Import the Workflow:
Import this workflow into your n8n instance.
Update the Telegram and Shopify credential nodes to use your credentials.
Set Webhook URLs:
Ensure your Telegram bot webhook is set correctly to receive messages.
n8n webhook URLs should be publicly accessible.
Test the Workflow:
Send /orders to your Telegram bot to verify it retrieves and lists open orders.
Customization Guidance
Modify Commands: Update the Switch node to add more Telegram commands or change existing ones.
Change Message Formats: Edit the Code nodes to customize how order lists and details appear.
Expand Shopify Integration: Add nodes to handle other Shopify operations like updating orders, managing products, etc.
Multi-User Support: Adapt the workflow to handle multiple Telegram chat IDs dynamically.
Security and Implementation Notes
The native Telegram node in n8n has limitations: it does not support sending dynamic inline keyboard arrays in JSON format, which is essential for displaying a variable number of buttons depending on how many orders are retrieved from Shopify.
To overcome this, this workflow uses the HTTP Request node to call Telegram‚Äôs API directly, allowing full flexibility to send dynamic inline keyboards as JSON objects. (I will make an update once Telegram Node support dynamic inline keyboards).
Security Considerations:
Always store your Telegram bot token securely in n8n credentials and never expose it in the HTTP Request node‚Äôs URL or body directly.
Use environment variables or n8n credentials to inject tokens safely.
Be mindful of Telegram API rate limits and add error handling in your workflow.
While using HTTP Request nodes increases flexibility, it also requires careful management of request payloads and authentication, as opposed to the built-in Telegram node which abstracts much of this complexity.
Benefits
Quickly access Shopify order data without leaving Telegram.
Interactive inline buttons improve user experience.
Automated, real-time integration between Shopify and Telegram."
Forex News & Sentiment Telegram Alerts,https://n8n.io/workflows/4659-forex-news-and-sentiment-telegram-alerts/,"Purpose & Audience
This n8n workflow template is designed for Forex traders, analysts, and enthusiasts who want to automate the process of staying updated on the latest news and sentiment for any currency pair. By leveraging advanced news aggregation and sentiment analysis, the workflow delivers concise, actionable updates directly to your Telegram‚Äîhelping you make more informed trading decisions without manual research on news for broader market sentiment.
What It Does?
Aggregates the latest news from major Forex platforms for your chosen currency pair.
Analyzes the news to extract market sentiment (bullish, bearish, or neutral) using an LLM.
Summarizes key headlines, technical levels (support/resistance), and highlights high-impact economic events.
Sends a neatly formatted update to your selected Telegram chat or group at your preferred schedule.
Who Is It For?
Forex traders seeking a competitive edge by staying ahead of market sentiment shifts.
Analysts who want automated, unbiased news curation and sentiment summaries.
Anyone looking for a hands-off, plug-and-play solution to monitor any currency pair.
Setup once, and you can use this workflow for a lifetime. Simply duplicate the workflow for as many currency pairs as you want‚Äîcustomize the inputs, and you‚Äôre set! No recurring fees, no coding required, and you control the update frequency and destinations.
How to Set Up?
Choose your currency pair and set your preferred update schedule.
Connect your LLM and Telegram credentials (simple, guided steps included).
Specify where you want to receive the updates.
That‚Äôs it‚Äîreceive real-time, actionable Forex news and sentiment updates automatically."
"Automate Product Training & Customer Support via WhatsApp, GPT-4 & Google Sheets",https://n8n.io/workflows/3379-automate-product-training-and-customer-support-via-whatsapp-gpt-4-and-google-sheets/,"WhatsApp AI Agent: Auto-Train Product Data & Handle Customer Support
Who Is This For
This workflow is ideal for eCommerce founders, product managers, customer support teams, and automation builders who rely on WhatsApp to manage product information and interact with clients.
It‚Äôs perfect for businesses that want to automate product data entry and support responses directly from WhatsApp messages using GPT-4 and Google Sheets.
What Problem Does This Workflow Solve
Manual Product Data Entry: Collecting and organizing product data from links is tedious and error-prone.
Slow Customer Response Times: Responding to client questions manually leads to delays and inconsistent support.
No Logging System for Issues: Without automation, support issues often go undocumented, making it harder to learn and improve.
What This Workflow Does
Step 1 ‚Äì Incoming Message Detection
Listens for incoming messages via WhatsApp.
If the message starts with train:, it routes to the product training process.
Otherwise, it routes to the customer support assistant.
Step 2 ‚Äì Product Data Training
Extracts URL from the message using a regex script.
Fetches HTML content from the URL.
Cleans HTML data to extract readable product description.
Saves raw data (URL + description) into Google Sheets.
Uses GPT-4 to enhance product data:
‚Üí Name, price (one-time or subscription), topic, and FAQs.
Updates the product row in Google Sheets with structured information.
Step 3 ‚Äì Customer Support Flow
Analyzes user messages with GPT-4 to understand the request or issue.
Looks up relevant product info in Google Sheets.
Detects potential problems (e.g. payment, login, delivery).
Suggests an appropriate solution.
Logs the problem, solution, and category to the Customer Issues sheet.
Sends a response back to the client via WhatsApp.
Step 4 ‚Äì Client Response
Sends the AI-generated response to the client via WhatsApp.
Keeps the communication fast, clear, and professional.
Setup Guide
Prerequisites
WhatsApp Business API access
OpenAI API Key
Google Account with Google Sheets access
A hosted instance of n8n (Cloud or self-hosted)
Setup Steps
Import the Workflow into your n8n instance.
Connect your credentials for WhatsApp, OpenAI, and Google Sheets.
Customize Google Sheet IDs and names as needed.
Test by sending a train: message or a regular customer message to WhatsApp.
Activate the workflow to make it live.
How to Customize This Workflow
Edit AI prompts to reflect your product type, language style, or tone.
Change the trigger keyword (e.g. from train: to add: or anything else).
Add integrations like Notion, Airtable, or CRM tools.
Expand the Sheets structure with more product fields (e.g. stock status, image link).
Add notifications to Slack or email after product updates or issue logging.
üìÑ Documentation: Notion Guide
Need help customizing?
Contact me for consulting and support : Linkedin / Youtube"
"AI-Powered Customer Service Automation with GPT, LangChain & Smart Routing",https://n8n.io/workflows/4626-ai-powered-customer-service-automation-with-gpt-langchain-and-smart-routing/,"AI-Powered Customer Service Automation with Smart Routing
How it works
Core Intelligence Pipeline
‚Ä¢ Multi-Layer Message Analysis - Every customer interaction passes through three specialized AI classifiers: privacy detection (identifies sensitive data and security requirements), intent recognition (categorizes requests as purchases, inquiries, complaints, technical support, or order tracking), and sentiment analysis (monitors emotional tone from neutral to critical frustration levels)
‚Ä¢ Dynamic Knowledge Integration - The system maintains live connections to your company's knowledge base and order management systems, automatically querying relevant information before crafting responses. This ensures accuracy and eliminates outdated information while providing real-time order status updates
‚Ä¢ Conversational Memory & Context - Advanced chat memory preserves conversation history across sessions, enabling the AI to maintain context, avoid repetitive responses, and build on previous interactions for more natural, human-like conversations
‚Ä¢ Intelligent Response Generation - The AI agent synthesizes information from multiple sources (knowledge base, order systems, conversation history) while adapting its tone and approach based on detected customer sentiment and privacy requirements
Smart Escalation System
‚Ä¢ Automated Triage Classification - A sophisticated routing engine categorizes each interaction into four escalation levels: Normal (AI-handled routine inquiries), Human Request (explicit agent requests), Critical Complaint (serious issues requiring immediate attention), and Owner Escalation (extreme situations with legal implications or persistent anger)
‚Ä¢ Context-Aware Handoffs - When escalation is required, the system automatically generates comprehensive situation summaries for human agents, including conversation history, customer sentiment analysis, and specific issue classification
‚Ä¢ Multi-Channel Notifications - Escalated cases trigger appropriate alerts via email to designated team members based on severity level, ensuring proper resource allocation and response times
Data Intelligence & Analytics
‚Ä¢ Comprehensive Interaction Logging - Every conversation is captured with full context including customer sentiment, intent classification, AI responses, and escalation decisions, creating a rich dataset for performance analysis and system improvement
‚Ä¢ Conversation Context Generation - For escalated cases, the system automatically produces detailed conversation summaries and context reports to help human agents understand the full situation before taking over
Set up steps
Platform Integration (20-30 minutes)
‚Ä¢ Messaging Platform Connection - Configure your primary communication channel (supports multiple messaging platforms) with proper API credentials and webhook setup for real-time message processing
‚Ä¢ AI Service Configuration - Connect OpenAI API credentials for the language models powering the classification engines and response generation
‚Ä¢ Database Setup - Establish connections to your customer database and order management systems for personalized responses and order tracking capabilities
Knowledge Base Preparation (45-60 minutes)
‚Ä¢ Company Information Import - Upload or connect your existing knowledge base, including product catalogs, policy documents, FAQ sections, and troubleshooting guides
‚Ä¢ Order Database Integration - Link your order management system to enable real-time order status queries, shipping tracking, and return processing
‚Ä¢ Response Templates - Configure standard response patterns and company voice guidelines to ensure consistent brand communication
Escalation & Routing Setup (15-25 minutes)
‚Ä¢ Team Structure Configuration - Define escalation paths and assign notification recipients for different severity levels (standard support, critical complaints, owner-level issues)
‚Ä¢ Email Integration - Connect email service for automated escalation notifications with customizable templates for different situation types
‚Ä¢ Escalation Triggers - Fine-tune the classification thresholds that determine when conversations require human intervention
Testing & Optimization (30-45 minutes)
‚Ä¢ Conversation Flow Testing - Run comprehensive test scenarios covering various customer intents, sentiment levels, and escalation triggers to validate system responses
‚Ä¢ Knowledge Base Validation - Verify that the AI can accurately retrieve and apply information from your knowledge base for common customer queries
‚Ä¢ Escalation Path Verification - Test all escalation routes to ensure proper notifications and handoff procedures are functioning correctly
‚Ä¢ Performance Monitoring Setup - Configure analytics tracking to monitor response accuracy, escalation rates, and customer satisfaction metrics
Advanced Configuration (Optional - 30-60 minutes)
‚Ä¢ Multi-Language Support - Configure language detection and response capabilities for international customer base
‚Ä¢ Custom Classification Rules - Adjust intent and sentiment classification parameters based on your specific business context and customer communication patterns
‚Ä¢ Integration Extensions - Connect additional business systems (CRM, billing, inventory) for enhanced customer service capabilities
Technical Specifications
AI Models & Processing
‚Ä¢ Powered by GPT-4 family models for natural language understanding and generation
‚Ä¢ Real-time voice transcription capabilities for multi-modal customer interactions
‚Ä¢ Structured output parsing for consistent data classification and routing decisions
‚Ä¢ Context-aware memory management with configurable conversation history retention
Data Security & Privacy
‚Ä¢ Automatic detection of sensitive information with privacy-first handling protocols
‚Ä¢ Configurable data retention policies and secure storage of conversation logs
‚Ä¢ Customer verification requirements for accessing sensitive account information
‚Ä¢ GDPR-compliant data processing and storage practices
Scalability & Performance
‚Ä¢ Modular architecture supporting easy integration of additional messaging platforms
‚Ä¢ Database-agnostic design (supports PostgreSQL, Supabase, and other systems)
‚Ä¢ Horizontal scaling capabilities for high-volume customer service operations
‚Ä¢ Real-time processing with minimal latency for immediate customer responses
Detailed technical implementation guides, API configuration examples, and troubleshooting documentation are embedded within the workflow nodes for development teams."
Generate Guerrilla Marketing Campaign Plans with AI Swarm Intelligence,https://n8n.io/workflows/4553-generate-guerrilla-marketing-campaign-plans-with-ai-swarm-intelligence/,"üß† Who is this for?
Startup founders designing creative growth strategies
Marketing teams seeking low-cost, high-impact campaigns
Consultants and agencies needing fast guerrilla plans
Creators exploring AI-powered content and campaigns
‚ùì What problem does this workflow solve?
Building a full guerrilla marketing strategy usually takes hours of brainstorming, validation, and formatting. This template does all of that in minutes using a swarm of AI agents, from idea generation to KPIs, and even kills bad ideas before you waste time on them.
‚öôÔ∏è What this workflow does
Starts with a chat input where you describe your business or idea
A ‚ÄúSwarm Intelligence‚Äù loop:
One AI agent generates guerrilla ideas
Another agent critically validates the idea and gives honest feedback
If the idea is weak, it asks for a new one
If accepted, the swarm continues with 16 AI specialists generating:
üéØ Objectives
üßç‚Äç‚ôÇÔ∏è Personas
üé§ Messaging
üß® Tactics
üì¢ Channels
üßÆ Budget
üìä KPIs
üìã Risk plan and more
Merges all chapters into a final Markdown file
Lets you download the campaign in seconds
üõ†Ô∏è Setup
Import the workflow to your n8n instance
(Optional) Configure your LLM (OpenAI or Ollama) in the ‚ÄúOpenAI Chat Model‚Äù node
Type your business idea (e.g., ‚ÄúLuxury dog collar brand for Instagram dads‚Äù)
Wait for flow completion
Download the final marketing plan file
ü§ñ LLM Flexibility (Choose Your Model)
Supports any LLM via LangChain:
Ollama (LLaMA 3.1, Mistral, DeepSeek)
OpenAI (GPT-4, GPT-3.5)
To switch models, just replace the ‚ÄúLanguage Model‚Äù node, no other logic needs updating
üìå Notes
Output is professional and ready-to-pitch
Built-in pessimistic validator filters out bad ideas before wasting time
üì© Need help?
Email: sinamirshafiee@gmail.com
Happy to support setup or customization!"
AI-Powered Candidate Screening and Evaluation Workflow using OpenAI and Airtable,https://n8n.io/workflows/4481-ai-powered-candidate-screening-and-evaluation-workflow-using-openai-and-airtable/,"Who is this for?
This workflow is ideal for:
HR professionals and recruiters who want to automate and enhance the hiring process
Organizations seeking AI-driven, consistent, and data-backed candidate evaluations
Hiring managers using Airtable as their recruitment database
What problem is this workflow solving?
Screening candidates manually is time-consuming, inconsistent, and difficult to scale. This workflow solves that by:
Automating resume intake and AI evaluation
Matching candidates to job postings dynamically
Generating standardized suitability reports
Notifying HR only when candidates meet the criteria
Storing all applications in a structured Airtable database
What this workflow does
This workflow builds an end-to-end AI-powered hiring pipeline using Airtable, OpenAI, and Google Drive. Here's how it works:
Accept candidate applications via a public web form, including resume upload (PDF only)
Extract text from uploaded resumes for processing
Store resumes in Google Drive and generate shareable links
Match the application to a job posting stored in Airtable
Use AI to evaluate candidates (via OpenAI GPT-4) against job descriptions and requirements
Generate suitability results including:
Match percentage
Screening status: Suitable, Not Suitable, Under Review
Detailed notes
Combine AI output and files into one data object
Create a new candidate record in Airtable with all application data
Automatically notify HR via Gmail if a candidate is marked ‚ÄúSuitable‚Äù
Setup
View & Copy the Airtable base here:
üëâ Candidate Screening ‚Äì Airtable Base Template
Set up Google Drive folder
Connect your OpenAI API key for the AI agent model
Connect your Gmail account for email notifications
Deploy the public-facing form to start receiving applications
Test the workflow using a sample job and resume
How to customize this workflow to your needs
Expand file support: Allow DOC or DOCX uploads by adding format conversion nodes
Add multi-recipient email alerts: Extend Gmail node for multiple HR recipients
Handle ‚ÄúUnder Review‚Äù differently: Add additional logic to notify or flag these candidates
Send rejection emails automatically: Extend the IF branch for ‚ÄúNot Suitable‚Äù candidates
Schedule interviews: Integrate with Google Calendar or Calendly APIs
Add Slack notifications: Send alerts to team channels for real-time updates"
Google SERP + Trends and Recommendations with Bright Data & Google Gemini,https://n8n.io/workflows/4861-google-serp-trends-and-recommendations-with-bright-data-and-google-gemini/,"Who this is for?
Google SERP Tracker + Trends and Recommendations is an AI-powered n8n workflow that extracts Google search results via Bright Data, parses them into structured JSON using Google Gemini, and generates actionable recommendations and search trends. It outputs CSV reports and sends real-time Webhook notifications.
This workflow is ideal for:
SEO Agencies needing automated rank & trend tracking
Growth Marketers seeking daily/weekly search-based insights
Product Teams monitoring brand or competitor visibility
Market Researchers performing search behavior analysis
No-code Builders automating search intelligence workflows
What problem is this workflow solving?
Traditional tracking of search engine rankings and search trends is often fragmented and manual. Analyzing SERP changes and trends requires:
Manual extraction or using unstable scrapers
Unstructured or cluttered HTML data
Lack of actionable insights or recommendations
This workflow solves the problem by:
Automating real-time Google SERP data extraction using Bright Data
Structuring unstructured search data using Google Gemini LLM
Generating actionable recommendations and trends
Exporting both CSV reports automatically to disk for downstream use
Notifying external systems via Webhook
What this workflow does
Accepts search input, zone name, and webhook notification URL
Uses Bright Data to extract Google Search Results
Uses Google Gemini LLM to parse the SERP data into structured JSON
Loops over structured results to:
Extract recommendations
Extract trends
Saves both as .csv files (example below):
Google_SERP_Recommendations_Response_2025-06-10T23-01-50-650Z.csv
Google_SERP_Trends_Response_2025-06-10T23-01-38-915Z.csv
Sends a Webhook with the summary or file reference
LLM Usage
Google Gemini LLM handles:
Parsing Google Search HTML into structured JSON
Summarizing recommendation data
Deriving trends from the extracted SERP metadata
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).
The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Set input fields with the search criteria, Bright Data Zone name, Webhook notification URL.
How to customize this workflow to your needs
Input Customization
Set your target keyword/phrase in the search field
Add your webhook_notification_url for external triggers or notifications
SERP Source
You can extend the Bright Data search logic to include other engines like Bing or DuckDuckGo.
Output Format
Edit the .csv structure in the Convert to File nodes if you want to include/exclude specific columns.
LLM Prompt Tuning
The Gemini LLM prompt inside the Recommendation or Trends extractor nodes can be fine-tuned for domain-specific insight (e.g., SEO vs eCommerce focus)."
Automated Music Video Creation & YouTube Publishing with AI-Generated Metadata from Google Drive,https://n8n.io/workflows/4848-automated-music-video-creation-and-youtube-publishing-with-ai-generated-metadata-from-google-drive/,"The Problem That it Solves
Uploading music manually to YouTube is a chore. Renaming files, filling metadata, picking the right playlist, adding keywords, and scheduling the release can eat up your creative time. This workflow removes the busy work so you can focus on making tracks instead of managing uploads.
How it works
Drop an MP3 into your shared Google Drive ‚ÄúAudio‚Äù folder and walk away. The workflow detects the new file, grabs the genre from the filename based on a set of keywords, uses the genre to map to cover art styles, and YouTube playlists, generates the description and tags for YouTube metadata, and schedules and posts to YouTube channel, while sending messages to discord for updates."
"Family Assistant: Schedule, Meal & Routine Management with Email & Telegram",https://n8n.io/workflows/4855-family-assistant-schedule-meal-and-routine-management-with-email-and-telegram/,"The FamilyFlow Assistant is your n8n-powered üöÄ companion designed to streamline daily parenting tasks, reduce mental load, and bring a bit more organization and fun into your family life. This versatile workflow automates reminders, generates helpful ideas, and provides gentle nudges to keep your household running smoothly.
Why FamilyFlow Assistant? ü§î
Parenting is a demanding (and rewarding!) job. Juggling schedules, meals, routines, and remembering everything can be overwhelming. FamilyFlow Assistant aims to:
Reduce Mental Clutter: Offload common reminders and to-dos to an automated system.
Enhance Organization: Keep track of activities, appointments, and important checks.
Spark Joy & Creativity: Generate meal ideas, story starters, and positive thoughts.
Promote Routine: Help establish and maintain consistent morning and evening routines.
Save Time: Automate repetitive prompts and checks, freeing you up for what matters most.
Core Features & Automations üß©
This workflow bundle comes packed with several independent yet complementary automations:
üìÖ Daily & Weekly Organization:
Activity & Appointment Reminders: Get a daily heads-up on scheduled events for the family.
Morning Routine Checklist: Start the day right with a friendly checklist for kids (or parents!).
Bedtime Routine Nudge: A gentle reminder to begin the wind-down process.
Evening ‚ÄúDid You Remember?‚Äù Scan: A quick mental checklist for common end-of-day to-dos.
Weekly ‚ÄúPlan Ahead‚Äù Prompt: A Sunday reminder to review the upcoming week.
School/Daycare Comms Check: Prompts to check for important updates.
üí° Inspiration & Well-being:
Meal Idea Generator: Never wonder ‚Äúwhat‚Äôs for dinner?‚Äù again (or at least, less often!).
Positive Thought / Fun Fact Sender: A little daily dose of brightness.
Story Starter / Quick Activity Prompt: Spark imagination or a quick family interaction.
Weekly Wins & Gratitude Reminder: Encourage reflection on positive moments.
‚è±Ô∏è Optional Helpers:
Screen Time Break Reminder: (Disabled by default) A nudge for offline time.
Getting Started: Setup & Customization üõ†Ô∏è
Make FamilyFlow Assistant truly yours with these simple steps:
Import the Workflow: Add the provided .json file to your n8n instance.
Configure Credentials:
Email: Set up your preferred email sending credential (e.g., SMTP, Gmail) in all ‚ÄúEmail Send‚Äù nodes. You‚Äôll likely use the same one for all.
Personalize Content (Crucial!):
Email Addresses: Update parents@example.com to your actual email(s) in all Email nodes.
Schedules & Activities: Edit the ‚ÄúDefine Daily Schedules‚Äù (Set node) with your family‚Äôs actual events.
Meal Ideas: Customize the mealIdeas array in the ‚ÄúGenerate Meal Idea‚Äù (Function node).
Checklists & Prompts: Tailor the text/HTML in Email nodes for morning/bedtime routines, check-ins, story starters, etc., to fit your family‚Äôs style.
Positive Thoughts/Facts: Update the lists in the ‚ÄúGet Positive Thought/Fact‚Äù (Function node).
Adjust Triggers (Cron Nodes):
Modify the ‚ÄúRule‚Äù in each Cron node to set the exact time and frequency for each automation.
Activate Automations:
The main workflow and most automations are inactive by default. Toggle the ‚ÄúActive‚Äù switch ON for the entire workflow or for individual automations/nodes you wish to use.
Remember, the ‚ÄúScreen Time Break Reminder‚Äù is disabled by default; enable its Email node if desired.
How Each Automation Works (Briefly) ‚öôÔ∏è
Each automation generally follows this pattern:
TRIGGER: An event starts the flow (e.g., a scheduled time via Cron).
PROCESS: Data is defined (e.g., list of activities, meal ideas) or generated (e.g., a random selection, a formatted message).
RESPOND: An action is taken, typically sending an email with the relevant information or reminder.
For instance, the ‚ÄúDaily Activity Reminder‚Äù uses a Cron trigger, a Set node to hold your schedule, a Function node to filter for today‚Äôs events, an IF node to check if there are any, and finally an Email node to send the reminder.
Expanding Your Assistant üöÄ
FamilyFlow Assistant is a great starting point. Consider these ideas to extend its capabilities:
Integrate Calendars: Connect to Google Calendar or Outlook Calendar to fetch events dynamically.
Use Different Notifiers: Send messages via Telegram, WhatsApp, Slack, or push notifications.
Smart Home Integration: If you have smart home devices, trigger lights or announcements for routines.
Shared To-Do Lists: Update a shared Todoist or Microsoft To Do list.
Interactive Input: Use n8n forms or Telegram commands to add events or trigger specific actions.
Requirements üìã
An active n8n instance (self-hosted or cloud).
Email sending credentials configured in n8n.
A little time to customize the content to perfectly match your family‚Äôs needs!
Embrace the flow and let your FamilyFlow Assistant help make parenting a little smoother!"
AI-Powered Employee Database Management via Telegram using OpenAI and Airtable,https://n8n.io/workflows/4545-ai-powered-employee-database-management-via-telegram-using-openai-and-airtable/,"Who is this for?
This workflow is perfect for:
HR professionals seeking to automate employee and department management
Startups and SMBs that want an AI-powered HR assistant on Telegram
Internal operations teams that want to simplify onboarding and employee data tracking
What problem is this workflow solving?
Managing employee databases manually is error-prone and inefficient‚Äîespecially for growing teams. This workflow solves that by:
Enabling natural language-based HR operations directly through Telegram
Automating the creation, retrieval, and deletion of employee records in Airtable
Dynamically managing related data such as departments and job titles
Handling data consistency and linking across relational tables automatically
Providing a conversational interface backed by OpenAI for smart decision-making
What this workflow does
Using Telegram as the interface and Airtable as the backend database, this intelligent HR workflow allows users to:
Chat in natural language (e.g. ‚ÄúShow me all employees‚Äù or ‚ÄúCreate employee: Sarah, Marketing‚Ä¶‚Äù)
Interpret and route requests via an AI Agent that acts as the orchestrator
Query employee, department, and job title data from Airtable
Create or update records as needed:
Add new departments and job titles automatically if they don‚Äôt exist
Create new employees and link them to the correct department and job title
Delete employees based on ID
Respond directly in Telegram, providing user-friendly feedback
Setup
View & Copy the Airtable base here:
üëâ Employee Database Management ‚Äì Airtable Base Template
Telegram Bot: Set up a Telegram bot and connect it to the Telegram Trigger node
Airtable: Prepare three Airtable tables:
Employees with links to Departments and Job Titles
Departments with Name & Description
Job Titles with Title & Description
Connect your Airtable API key and base/table IDs into the appropriate Airtable nodes
Add your OpenAI API key to the AI Agent nodes
Deploy both workflows: the main chatbot workflow and the employee creation sub-workflow
Test with sample messages like:
‚ÄúCreate employee: John Doe, john@company.com, Engineering, Software Engineer‚Äù
‚ÄúRemove employee ID rec123xyz‚Äù
How to customize this workflow to your needs
Switch databases: Replace Airtable with Notion, PostgreSQL, or Google Sheets if desired
Enhance security: Add authentication and validation before allowing deletion
Add approval flows: Integrate Telegram button-based approvals for sensitive actions
Multi-language support: Expand system prompts to support multiple languages
Add logging: Store every user action in a log table for auditability
Expand capabilities: Integrate payroll, time tracking, or Slack notifications
Extra Tips
This is a two-workflow setup. Make sure the sub-workflow is deployed and accessible from the main agent.
Use Simple Memory per chat ID to preserve context across user queries.
You can expand the orchestration logic by adding more tools to the main agent‚Äîsuch as ‚ÄúGet active employees only‚Äù or ‚ÄúList employees by job title.‚Äù"
"AI Real Estate Agent: End-to-End Ops Automation (Web, Data, Voice)",https://n8n.io/workflows/4368-ai-real-estate-agent-end-to-end-ops-automation-web-data-voice/,"This suite automates distinct aspects of real estate operations: incoming web lead qualification, scheduled/manual data research and content generation, and automated voice call outreach with lead qualification. It leverages AI (primarily OpenAI GPT-4o Mini via Langchain), data processing nodes, and integrations with external APIs and Google Workspace.
Workflow 1: Incoming Web Lead Qualification & Scoring
This workflow captures leads from a web source, validates their input, uses AI to classify intent and urgency, checks against a property database, scores the lead, and prepares a structured lead object.
Tools & Services Used:
AI Core & Processing: OpenAI (GPT-4o Mini via Langchain Agent and Chat Model nodes)
Data Processing: n8n Set, If, Code nodes
External API: HTTP Request node (for PropertyCheckAPI)
Workflow Overview:
Trigger:
Incoming Web Lead Webhook (/incoming-lead): Captures new leads submitted via a web form.
Lead Data Ingestion & Validation:
Set & Rename Incoming Lead Fields: Standardizes input field names from the webhook.
IF User Message Valid: Checks if the userMessage field is present and meets a minimum length.
Clean User Message Text (Code): Pre-processes the user's message (e.g., removes extra spaces).
AI-Powered Lead Analysis:
AI Classify Lead Intent & Urgency (Langchain Agent): Analyzes the cleaned user message to determine the lead's primary intent (e.g., buying, selling, inquiry) and the urgency of their request.
Powered by: LLM for Lead Intent/Urgency Classification (OpenAI Chat Model - GPT-4o Mini).
Extract Intent & Urgency from AI Output (Set): Structures the AI's classification into distinct fields.
Data Standardization & Enrichment:
Standardize All Lead Data Fields (Code): Further standardizes all collected data points for consistency.
Call Property Check API (HTTP Request): Sends lead/property details to an external API (https://api.example.com/property-check) to verify listing status or gather more information.
Lead Scoring & Finalization:
IF Property is Known Listing: Checks the API response to see if a matchFound was ""true"".
Calculate Web Lead Score (Code): If a known listing (or based on other criteria), this node assigns a numerical or categorical score to the lead.
Set Final Structured Web Lead Data (Set): Consolidates all original, processed, classified, and scored data into a final, comprehensive lead object.
Workflow 2: Scheduled/Manual AI-Powered Data Research & Content Generation (Red Background)
This workflow fetches data from external URLs, extracts information using AI, allows a sophisticated AI agent to perform research and generate analysis using various tools, and outputs results to Google Sheets, Google Docs, and potentially other AI processing steps.
Tools & Services Used:
Orchestration & Automation: n8n
AI Core & Processing: OpenAI (GPT-4o Mini via Langchain Information Extractor, Langchain Agent, and direct OpenAI nodes)
AI Tools: Langchain Calculator, Langchain SerpAPI
Data Storage & Output: Google Sheets, Google Docs
Data Input: HTTP Request node
Workflow Overview:
Triggers:
Manual Trigger for Data Analysis Flow: Allows on-demand execution.
Scheduled Trigger for Data Analysis Flow: Automates execution on a defined schedule.
Data Ingestion & Initial Extraction:
Fetch External Data for AI Analysis (HTTP Request): Retrieves content from a specified URL.
AI Extract Information from Fetched Data (Langchain Information Extractor): Uses an AI model to extract structured data from the fetched content.
Powered by: LLM for Data Information Extractor (OpenAI Chat Model - GPT-4o Mini).
Advanced AI Analysis & Tasking:
AI Agent for Research & Content Generation (Langchain Agent): Processes the extracted information to perform in-depth research, analysis, or content creation.
Powered by: LLM for Research & Content AI Agent (OpenAI Chat Model - GPT-4o Mini).
Utilizes Tools:
Calculator Tool for AI Agent: For numerical calculations.
SerpAPI Web Search Tool for AI Agent: For performing real-time web searches to gather additional context or verify information.
Output & Dissemination:
The AI Agent's output is routed to multiple destinations:
Update Google Doc with AI Agent Analysis (Google Docs): Inserts the generated analysis/content into a Google Document.
Split AI Agent Output Items (Split Out) -> Log AI Analysis Data to Google Sheets (Google Sheets): If the agent produces multiple data items, they are split and logged individually into a Google Sheet.
OpenAI: Generate Text from Agent (Output 1) & OpenAI: Generate Text from Agent (Output 2): These nodes likely take the agent's output for further specialized AI processing (e.g., summarization for different purposes, reformatting).
Workflow 3: Automated Lead Outreach & Voice Call Qualification (Green Background)
This workflow automates the initial contact with new leads via a voice call, uses AI to understand the lead's responses during the call, qualifies them, and logs the detailed interaction and a summary.
Tools & Services Used:
Orchestration & Automation: n8n
AI Core & Processing: OpenAI (GPT-4o Mini via Langchain Agent and Chat Model nodes)
Voice Services: ElevenLabs (Text-to-Speech via HTTP Request), Twilio (Place Call via HTTP Request)
Data Storage & Output: Google Sheets
Error Handling: n8n Execute Workflow Trigger
Workflow Overview:
Trigger:
Webhook for Voice Call Lead (/new-lead): Captures new leads designated for an automated voice call.
Call Preparation & Initiation:
Set Initial Voice Call Lead Details: Extracts basic lead info (name, phone, property ref, email) from the webhook.
Generate Voice Call Introduction Script (Function): Creates a personalized script for the call.
ElevenLabs: Convert Intro Script to Voice (HTTP Request): Sends the script to ElevenLabs API to generate natural-sounding audio.
Twilio: Initiate Voice Call to Lead (HTTP Request): Uses Twilio API to place the call and play the generated audio.
AI-Powered Call Interaction Analysis:
AI Agent: Extract Info from Voice Call (Langchain Agent): Processes the interaction from the call (e.g., a transcript of the lead's responses, or DTMF inputs if designed to capture them) to extract key qualification data like budget, timeline, and interest level.
Powered by: LLM for Voice Call Info Extraction Agent (OpenAI Chat Model - GPT-4o Mini).
Structure Extracted Voice Call Info (Function): Organizes the AI-extracted data into a structured JSON object.
Lead Qualification & Data Logging:
Set Lead Status Based on Call Interest: Updates the lead's status (e.g., ""Interested"" or ""Not Interested"") based on the AI's interpretation of the call.
IF Lead Interested (from Voice Call): Branches the workflow based on lead status.
If Interested:
Assign Score to Interested Voice Lead (Function): Calculates a lead score based on budget, timeline, etc.
Format Current Timestamp for Logging (DateTime): Generates a timestamp.
Log Qualified Voice Lead to Google Sheets: Appends the detailed, qualified lead information to a 'Leads' Google Sheet.
AI Agent: Generate Voice Call Lead Summary (Langchain Agent): Creates a concise summary of the entire lead interaction and qualification.
Powered by: LLM for Voice Call Lead Summary Agent (OpenAI Chat Model - GPT-4o Mini).
Log Voice Call Lead Summary to Google Sheets: Appends this summary to a separate 'LeadsSummary' Google Sheet.
Error Handling (for Sheets Logging):
IF Voice Lead Logging to Sheets Failed: Checks if the Google Sheets logging operation was unsuccessful.
Error Trigger: Notify Admin of Sheets Failure (Execute Workflow Trigger): If logging fails, triggers a separate workflow to alert an administrator."
"Extract Trends, Auto-Generate Social Content with AI, Reddit, Google & Post",https://n8n.io/workflows/3560-extract-trends-auto-generate-social-content-with-ai-reddit-google-and-post/,"Extract Trends and Auto-Generate Social Media Content with OpenAI, Reddit, and Google Trends: Approve and Post to Instagram, TikTok, and More
Description
What Problem Does This Solve? üõ†Ô∏è
This workflow automates trend extraction and social media content creation for businesses and marketers. It eliminates manual trend research and content generation by fetching trends, scoring them with AI, and posting tailored content to multiple platforms. Target audience: Social media managers, digital marketers, and businesses aiming to streamline content strategies.
What Does It Do? üåü
Fetches trending topics from Reddit, X and Google Trends
Scores trends for relevance using OpenAI.
Generates content for Twitter/X, LinkedIn, Instagram and Facebook
Posts to supported platforms
Stores results in Google Sheets for tracking
Key Features üìã
Real-time trend fetching from Reddit and Google Trends.
AI-driven trend scoring and content generation (OpenAI).
Automated posting to Twitter/X, LinkedIn, Instagram, and Facebook.
Persistent storage in Google Sheets.
Setup Instructions
Prerequisites ‚öôÔ∏è
n8n Instance: Self-hosted or cloud n8n instance.
API Credentials:
Reddit API: Client ID and secret from Reddit.
SerpApi (Google Trends): API key from SerpApi, stored in n8n credentials
OpenAI API: API key with GPT model access.
Twitter/X API: OAuth 1.0a credentials with write permissions.
LinkedIn API: OAuth 2.0 credentials with w_organization_social scope.
Instagram/Facebook API: Meta Developer app with posting permissions.
Google Sheets API: Credentials from Google Cloud Console.
Installation Steps üì¶
Import the Workflow:
Copy the workflow JSON from the ""Template Code"" section below.
Import it into n8n via ""Import from File"" or ""Import from URL"".
Configure Credentials:
Add API credentials in n8n‚Äôs Credentials section for Reddit, SerpApi, OpenAI, Twitter/X, LinkedIn, Instagram/Facebook, and Google Sheets.
Assign credentials to respective nodes. For example:
In the Fetch Google Trends node (HTTP Request), use n8n credentials for SerpApi instead of hardcoding the API key.
Example: Set the API key in n8n credentials as SerpApiKey and reference it in the node‚Äôs query parameter: api_key={{ $credentials.SerpApiKey }}.
Set Up Google Sheets with the following columns (exact column names are case-sensitive)
-Timestamp | Trend | Score | BrandVoice | AudienceMood |
Customize Nodes:
OpenAI Nodes (Trend Relevance Scoring, Generate Social Media Content): Update the model (e.g., gpt-4o) and prompt as needed.
HTTP Request Nodes (Post to Twitter/X, Post to LinkedIn, etc.): Verify URLs, authentication, and payloads.
Brand Voice/Audience Mood: Adjust Prepare Trend Scoring Input for your desired brand_voice (e.g., ""casual"") and audience_mood (e.g., ""curious"").
Test the Workflow:
Fetch Reddit Trends to Store Selected Trends- to score and store trends.
Retrieve Latest Trends to end) to generate and post content
Check Google Sheets for posting statuses
How It Works
High-Level Steps üîç
Fetch Trends: Pulls trends from Reddit,X and Google Trends.
Score Trends: Uses OpenAI to score trends for relevance.
Generate Content: Creates platform-specific social media content.
Post Content: Posts to LinkedIn, Facebook or X
Detailed descriptions are available in the sticky notes within the workflow screenshot above.
Node Names and Actions
Trend Extraction and Scoring
Daily Trigger Idea: Triggers the workflow daily.
Set Default Inputs: Sets default brand_voice and inputs.
Fetch Reddit Trends: Fetches Reddit posts.
Extract Reddit Trends: Extracts trends from Reddit.
Fetch Google Trends: Fetches Google Trends via SerpApi.
Extract Google Trends2: Processes Google Trends data.
Fetch Twitter Mentions: Fetches Twitter mentions.
Translate Tweets to English: Translates tweets.
Fix Tweet Translation Output: Fixes translation format.
Detect Audience Mood: Detects audience mood.
Fix Audience Mood Output: Fixes mood output format.
Analyze News Sentiment: Analyzes news sentiment.
Combine Data (Merge): Merges all data sources.
Merge Items into Single Item: Combines data into one item.
Combine Trends and UGC: Combines trends with UGC.
Prepare Trend Scoring Input: Prepares data for scoring.
Trend Relevance Scoring: Scores trends with OpenAI.
Parse Trend Scores: Parses scoring output.
Store Selected Trends: Stores trends in Google Sheets.
Content Generation and Posting
Retrieve Latest Trends: Retrieves trends from Google Sheets.
Parse Retrieved Trends: Parses retrieved trends.
Select Top Trends: Selects the top trend.
Generate Social Media Content: Generates platform-specific content.
Parse Social Media Content: Parses generated content.
Generate Images: Generates images for posts (if applicable).
-Handle Approvals/Rejection before Posting
Post to Instagram: Posts to Instagram.
Post to Facebook: Posts to Facebook.
Post to LinkedIn: Posts to LinkedIn.
Customization Tips
Add Trend Sources üì°: Include more sources (e.g., Instagram trends) by adding nodes to Combine Data (Merge).
Change Content Tone ‚úçÔ∏è: Update the Generate Social Media Content prompt for a different tone (e.g., ""humorous"").
Adjust Schedule ‚è∞: Modify Daily Trigger Idea to run hourly or weekly.
Automate TikTok/YouTube üé•: Add video generation (e.g., FFMPEG) to post TikTok and YouTube Shorts"
"Personalized LinkedIn Connection Requests with Apollo, GPT-4, Apify & PhantomBuster",https://n8n.io/workflows/4803-personalized-linkedin-connection-requests-with-apollo-gpt-4-apify-and-phantombuster/,"AI LinkedIn Outreach Automation with Apollo, OpenAI & PhantomBuster
Categories:
Sales Automation
Lead Generation
AI Personalization
This workflow creates a complete LinkedIn outreach automation system that generates targeted lead lists from Apollo using natural language, enriches profiles with AI-personalized icebreakers, and automatically sends connection requests through PhantomBuster. Built by someone who's made over $1 million with AI automation, this system demonstrates the real-world approach to building profitable automation workflows.
Benefits
Natural Language Lead Targeting - Describe your ideal prospects in plain English and automatically generate Apollo search URLs
AI-Powered Personalization - Creates custom icebreakers based on LinkedIn profile data, employment history, and professional background
Complete Outreach Pipeline - From lead discovery to personalized connection requests, fully automated end-to-end
Smart Data Management - Automatically tracks all prospects in Google Sheets with deduplication and status tracking
Cost-Effective Scraping - Uses Apify to extract Apollo data without expensive subscription costs
Scalable Architecture - Processes hundreds of leads while respecting LinkedIn's connection limits
How It Works
Natural Language Lead Generation:
Form input accepts audience descriptions in plain English
AI converts descriptions into properly formatted Apollo search URLs
Automatically includes location, company size, job titles, and keyword filters
Apollo Data Extraction:
Uses Apify actor to scrape targeted lead lists from Apollo
Extracts LinkedIn URLs, email addresses, employment history, and profile data
Processes 500+ leads per run with detailed professional information
AI Personalization Engine:
Analyzes LinkedIn profile data including job history and company information
Generates personalized icebreakers using proven connection request templates
Creates human-like messages that reference specific career details and achievements
Google Sheets Integration:
Automatically stores all lead data in organized spreadsheet format
Tracks prospect information, contact details, and generated icebreakers
Provides easy data management and campaign tracking
PhantomBuster Automation:
Connects to PhantomBuster API to trigger LinkedIn connection campaigns
Sends personalized connection requests with custom icebreakers
Respects LinkedIn's daily limits and mimics human behavior patterns
Business Use Cases
Sales Teams - Automate prospecting for B2B outreach campaigns
Agencies - Scale client acquisition through targeted LinkedIn outreach
Recruiters - Find and connect with qualified candidates efficiently
Entrepreneurs - Build professional networks in specific industries
Business Development - Generate qualified leads for partnership opportunities
Revenue Potential
This system can replace expensive LinkedIn outreach tools that cost $200-500/month. Users typically see:
400% improvement in response rates through personalization
10x faster lead generation compared to manual prospecting
Ability to process 500+ leads per hour vs. 10-20 manually
Difficulty Level: Intermediate
Estimated Build Time: 1-2 hours
Monthly Operating Cost: ~$50 (Apollo + PhantomBuster + AI APIs)
Watch My Complete 1-Hour Build
Want to see exactly how I built this system from scratch? I walk through the entire development process live, including all the debugging, API integrations, and real-world testing that goes into building profitable automation systems.
üé• See My Live Build Process: ""Build This Automated AI LinkedIn DM System in 1 Hour (N8N)""
This comprehensive tutorial shows my actual development approach - including the detours, problem-solving, and iterative testing that real automation building involves.
Required Google Sheets Setup
Create a Google Sheet with these exact column headers:
Essential Lead Columns:
id - Unique prospect identifier
first_name - Contact's first name
last_name - Contact's last name
name - Full name
linkedin_url - LinkedIn profile URL
title - Current job title
email_status - Email verification status
photo_url - Profile photo URL
icebreaker - AI-generated personalized message
Setup Instructions:
Create Google Sheet with these headers in row 1
Connect Google Sheets OAuth in n8n
Update the document ID in the ""Add to Google Sheet"" node
PhantomBuster will read from this sheet for automated outreach
Set Up Steps
Apollo & Apify Configuration:
Set up Apify account and obtain API credentials
Configure Apollo scraper actor with proper parameters
Test lead extraction with sample audience descriptions
AI Personalization Setup:
Configure OpenAI API for natural language processing and personalization
Set up prompt templates for audience targeting and icebreaker generation
Test personalization quality with sample LinkedIn profiles
Google Sheets Integration:
Create lead tracking spreadsheet with proper column structure
Configure Google Sheets API credentials and permissions
Set up data mapping for automatic lead storage
PhantomBuster Connection:
Set up PhantomBuster account and LinkedIn connection
Configure LinkedIn auto-connect agent with custom message templates
Connect API for automated campaign triggering
Form and Workflow Setup:
Configure form trigger for audience input collection
Set up data flow between all components
Add proper error handling and rate limiting
Testing and Optimization:
Start with small batches (5-10 connections daily)
Monitor LinkedIn account health and response rates
Optimize icebreaker templates based on performance data
Important Compliance Notes
LinkedIn Limits: Respect 100 connection requests per week limit
Account Safety: Use PhantomBuster's human-like behavior patterns
Message Quality: Regularly update templates to avoid automation detection
Response Management: Monitor and respond to replies within 24 hours
Advanced Extensions
This system can be enhanced with:
Multi-channel Outreach: Add email sequences for comprehensive campaigns
A/B Testing: Test different icebreaker templates automatically
CRM Integration: Connect to Salesforce, HubSpot, or other sales systems
Response Tracking: Monitor reply rates and optimize messaging
Explore My Channel
For more advanced automation systems that generate real business results, check out my YouTube channel where I share the exact strategies I've used to make over $1 million with AI automation."
Auto-Publish Latest News on X with AI Content Generation using Keywords and Bright Data,https://n8n.io/workflows/4556-auto-publish-latest-news-on-x-with-ai-content-generation-using-keywords-and-bright-data/,"üì∞ Publish Latest News on X and Other Social Media Platforms Using Keyword
A comprehensive n8n automation that fetches the latest news based on keywords, generates AI-powered social media content, and automatically publishes to X (Twitter) with complete tracking and notification systems.
üìã Overview
This workflow provides a professional news publishing solution that automatically discovers breaking news, creates engaging social media content using AI, and publishes to X (Twitter) with comprehensive tracking. Perfect for news organizations, content creators, social media managers, and businesses wanting to stay current with automated news sharing. The system uses BrightData's Google News dataset, OpenAI's GPT-4o for content generation, and multi-platform integration for complete automation.
‚≠ê Key Features
üìù Form-Based Input: Clean web form for keyword and country submission
üì∞ Real-Time News Fetching: BrightData Google News integration for latest articles
ü§ñ AI Content Generation: GPT-4o powered tweet creation with hashtags
üì± Auto X Publishing: Direct posting to X (Twitter) with URL tracking
üìä Complete Tracking: Google Sheets logging of all published content
üîî Email Notifications: Success alerts with tweet links
üåç Multi-Country Support: Localized news for US, India, UK, Australia
‚ö° Status Monitoring: Real-time progress tracking with retry logic
üõ° Error Handling: Robust error management and validation
üîÑ Loop Management: Intelligent waiting for news processing completion
üéØ What This Workflow Does
Input:
News Name: Keyword or topic for news search (required)
Country: Target country for localized news (dropdown: US/IN/GB/AU)
Processing:
Form Submission: Captures news keyword and target country
News Triggering: Initiates BrightData Google News scraping job
Status Monitoring: Checks scraping progress with intelligent retry loop
Data Retrieval: Fetches latest news articles when ready
AI Content Creation: Generates engaging tweet content using GPT-4o
Social Publishing: Posts content to X (Twitter) automatically
URL Generation: Creates direct tweet links for tracking
Data Logging: Saves content and URLs to Google Sheets
Email Notification: Sends success confirmation with tweet link
Completion: Workflow ends with full audit trail
üìã Output Data Points
Field Description Example
TweetMessage AI-generated social media content ""Breaking: AI revolution transforming healthcare with 40% efficiency gains. New study shows promising results in patient care automation. #AI #Healthcare #Innovation #TechNews #US""
TweetURL Direct link to published tweet https://twitter.com/i/web/status/1234567890123456789
üõ†Ô∏è Setup Instructions
Prerequisites:
n8n instance (self-hosted or cloud)
X (Twitter) account with API v2 access
OpenAI account with GPT-4o access
Gmail account for notifications
Google account with Sheets access
BrightData account with Google News dataset access
Basic understanding of social media automation
Step 1: Import the Workflow
Copy the JSON workflow code from the provided file.
In n8n, click ""+ Add workflow"".
Select ""Import from JSON"".
Paste the workflow code and click ""Import"".
The workflow will appear with all nodes properly connected.
Step 2: Configure API Credentials
X (Twitter) API Setup:
Create X Developer Account at developer.twitter.com.
Create new app and generate API keys.
In n8n: Credentials ‚Üí + Add credential ‚Üí Twitter OAuth2 API.
Add your Twitter API credentials:
API Key
API Secret Key
Bearer Token
Access Token
Access Token Secret
Test the connection with a sample tweet.
OpenAI API Configuration:
Get API key from platform.openai.com.
Ensure GPT-4o model access is available.
In n8n: Credentials ‚Üí + Add credential ‚Üí OpenAI API.
Add your OpenAI API key.
Verify model access in the ""OpenAI Chat Model"" node.
Gmail Integration:
Create ""Gmail OAuth2"" credential.
Follow OAuth setup process.
Grant email sending permissions.
Test with sample email.
BrightData News API:
The workflow uses pre-configured token: 5662edde-6735-4c5d-a6c6-693043a5a9a5.
Dataset ID: gd_lnsxoxzi1omrwnka5r (Google News).
Verify access to Google News dataset.
Test API connection.
Google Sheets Integration:
Create ""Google Sheets OAuth2 API"" credential.
Complete OAuth authentication.
Grant read/write permissions.
Test connection.
Step 3: Configure Google Sheets Integration
Create Google Sheets Structure:
Sheet Name: ""Publish Latest News on Social Media Platforms Using Keyword""
Tab: ""Data"" (default)
Columns:
Tweet Message: AI-generated content posted to X
Tweet URL: Direct link to published tweet
Sheet Configuration:
Create new Google Sheet or use existing one.
Add the required column headers.
Copy Sheet ID from URL: https://docs.google.com/spreadsheets/d/SHEET_ID_HERE/edit.
Current configured Sheet ID: 1koxNrwdeuaSBdREuKc7JQh3d9blEk0sQDJ8VgVLjPOo.
Update Workflow Settings:
Open ""Google Sheets"" node.
Replace Document ID with your Sheet ID.
Select your Google Sheets credential.
Choose ""Data"" sheet/tab.
Verify column mapping is correct.
Step 4: Configure Form Interface
Form Settings:
Open ""On form submission"" node.
Form configuration:
Title: ""News Publisher""
Description: ""publish latest news to direct social media""
Fields:
News Name (text, required)
Country (dropdown: US, IN, GB, AU, required)
Webhook URL: Copy webhook URL from form trigger node.
Current webhook ID: 8d320705-688c-4150-a393-cf899d2bbb52.
Test form accessibility and submission.
Step 5: Configure Email Notifications
Gmail Setup:
Open ""Gmail"" node.
Update recipient email: raushan@iwantonlinemarketing.com.
Email template includes:
Success confirmation
Direct tweet link
Professional formatting
Test email delivery.
Step 6: Test the Workflow
Sample Test Data:
Use these examples for testing:
News Name Country Expected Results
artificial intelligence US Latest AI news with US-specific hashtags
cricket world cup IN Sports news with India-focused content
brexit update GB UK political news with British hashtags
bushfire news AU Australian environmental news
Testing Process:
Activate the workflow (toggle switch).
Navigate to the webhook form URL.
Submit test data.
Monitor execution progress:
News fetching (30-60 seconds)
AI content generation (10-15 seconds)
X publishing (5-10 seconds)
Sheet update and email (5 seconds)
Verify results in all platforms.
üìñ Usage Guide
Using the Form Interface
Navigate to the webhook URL provided by the form trigger.
Enter news keyword or topic (e.g., ""climate change"", ""stock market"", ""technology"").
Select target country from dropdown.
Click submit and wait for processing.
Check email for success notification with tweet link.
Example Inputs to Test
News Name Country Expected
""artificial intelligence breakthrough"" ""US"" Latest AI developments with tech hashtags
""football premier league"" ""GB"" UK football news with sports hashtags
""stock market updates"" ""IN"" Indian market news with finance hashtags
""hollywood movies"" ""AU"" Entertainment news with Australian perspective
Country-Specific Considerations
United States (US):
Focus on national news and global impact.
Hashtags: #USA, #American, #Breaking, #News.
Time zone considerations for optimal posting.
India (IN):
Emphasis on regional relevance.
Hashtags: #India, #Indian, #News, #Breaking.
Cultural context in content generation.
United Kingdom (GB):
British perspective and terminology.
Hashtags: #UK, #British, #News, #Breaking.
Focus on European context.
Australia (AU):
Australian angle and regional focus.
Hashtags: #Australia, #Australian, #News, #Breaking.
Pacific region context.
üìä Reading the Results
Google Sheets Data
The output sheet contains:
Complete tweet content with hashtags and formatting.
Direct tweet URLs for easy access and sharing.
Chronological record of all published content.
Audit trail for content management.
Email Notifications
Success emails include:
Confirmation that content was published.
Direct link to view the tweet.
Professional formatting for easy reference.
X (Twitter) Posts
Published content features:
AI-optimized messaging within 260 character limit.
Relevant hashtags based on topic and country.
Engaging format designed for social media.
Professional tone suitable for news sharing.
üîß Customization Options
Expanding Social Media Platforms
Add more platforms to the publishing workflow:
// Add LinkedIn publishing
{
  ""node"": ""LinkedIn"",
  ""type"": ""n8n-nodes-base.linkedin"",
  ""parameters"": {
    ""text"": ""={{ $json.output }}"",
    ""additionalFields"": {}
  }
}
// Add Facebook posting
{
  ""node"": ""Facebook"",
  ""type"": ""n8n-nodes-base.facebook"",
  ""parameters"": {
    ""pageId"": ""YOUR_PAGE_ID"",
    ""message"": ""={{ $json.output }}""
  }
}"
AI-Powered Telegram Task Manager with MCP Server,https://n8n.io/workflows/3656-ai-powered-telegram-task-manager-with-mcp-server/,"Detailed Description
The ToDo App workflow is designed to streamline task management through Telegram and Google Tasks integration. This workflow allows users to create, update, and manage tasks via Telegram messages, leveraging AI capabilities to enhance user interaction. The expected outcome is a seamless experience where users can manage their tasks efficiently without needing to switch between applications.
Who is this for?
This workflow is intended for:
Individuals looking for an efficient way to manage their tasks directly from Telegram.
Teams that require a collaborative task management solution integrated with Google Tasks.
Developers interested in automating task management processes using n8n and Telegram.
What problem does this workflow solve?
Managing tasks can often be cumbersome, especially when switching between different applications. This workflow addresses the following problems:
Fragmented Task Management: Users can manage tasks directly from Telegram, reducing the need to switch to Google Tasks.
Inefficient Communication: By integrating AI, users can interact with the task management system in a conversational manner, making it more intuitive.
Task Updates: Users can easily update task statuses and details through simple messages, enhancing productivity.
What this workflow does
The ToDo App workflow performs the following functions:
Incoming Message Handling: Listens for messages sent to a Telegram bot.
Task Creation: Allows users to create new tasks based on their messages.
Task Updates: Users can update existing tasks by sending specific commands.
Task Retrieval: Retrieves today's and upcoming tasks from Google Tasks.
Voice Note Transcription: Supports voice messages, converting them into text for task management.
AI Assistance: Utilizes an AI agent to assist users in managing their tasks effectively.
Setup
Prerequisites
Before setting up the workflow, ensure you have the following:
n8n Account: Sign up for an n8n account if you don't have one.
Telegram Bot: Create a Telegram bot and obtain the API token.
Google Tasks API: Set up Google Tasks API and obtain OAuth2 credentials.
OpenAI API Key: Sign up for OpenAI and obtain an API key for AI functionalities.
Setup Process
Upload the JSON for this workflow and setup the authentication for the different tools.
How to customize this workflow
To adapt the ToDo App workflow to different needs, consider the following customizations:
Change Task Management Platform: If you prefer a different task management tool, replace the Google Tasks nodes with your preferred service's API.
Modify AI Responses: Adjust the AI agent's system message to change how it interacts with users.
Add Additional Commands: Expand the workflow by adding more commands for different task management functionalities (e.g., deleting tasks).
Integrate Other Messaging Platforms: If you want to use a different messaging service, replace the Telegram nodes with the appropriate nodes for that service.
Conclusion
The ToDo App workflow provides a powerful solution for managing tasks through Telegram, enhancing productivity and user experience. By following the setup instructions and customization options, users can tailor the workflow to meet their specific needs, making task management more efficient and accessible."
ü§ñ AI Powered RAG Chatbot for Your Docs + Google Drive + Gemini + Qdrant,https://n8n.io/workflows/2982-ai-powered-rag-chatbot-for-your-docs-google-drive-gemini-qdrant/,"ü§ñ AI-Powered RAG Chatbot with Google Drive Integration
This workflow creates a powerful RAG (Retrieval-Augmented Generation) chatbot that can process, store, and interact with documents from Google Drive using Qdrant vector storage and Google's Gemini AI.
How It Works
Document Processing & Storage üìö
Retrieves documents from a specified Google Drive folder
Processes and splits documents into manageable chunks
Extracts metadata using AI for enhanced search capabilities
Stores document vectors in Qdrant for efficient retrieval
Intelligent Chat Interface üí¨
Provides a conversational interface powered by Google Gemini
Uses RAG to retrieve relevant context from stored documents
Maintains chat history in Google Docs for reference
Delivers accurate, context-aware responses
Vector Store Management üóÑÔ∏è
Features secure delete operations with human verification
Includes Telegram notifications for important operations
Maintains data integrity with proper version control
Supports batch processing of documents
Setup Steps
Configure API Credentials:
Set up Google Drive & Docs access
Configure Gemini AI API
Set up Qdrant vector store connection
Add Telegram bot for notifications
Add OpenAI Api Key to the 'Delete Qdrant Points by File ID' node
Configure Document Sources:
Set Google Drive folder ID
Define Qdrant collection name
Set up document processing parameters
Test and Deploy:
Verify document processing
Test chat functionality
Confirm vector store operations
Check notification system
This workflow is ideal for organizations needing to create intelligent chatbots that can access and understand large document repositories while maintaining context and providing accurate responses through RAG technology."
Telegram AI Chatbot with Document-Based Answers using OpenAI and PGVector RAG,https://n8n.io/workflows/4799-telegram-ai-chatbot-with-document-based-answers-using-openai-and-pgvector-rag/,"ü§ñ AI Q&A Chatbot Workflow ‚Äì Build Your Own AI Agent Trained on Private Documents
This powerful AI automation add-on upgrades your Telegram Bot Starter Template by integrating a fully functional AI chatbot and a context-aware AI agent that answers user questions using your internal documents.
Unlike generic bots, this chatbot uses your own data to respond with deeply personalized, context-relevant information ‚Äî perfect for support, onboarding, internal knowledge access, and client-facing interactions.
It connects to any PostgreSQL database ‚Äî including Neon.tech, Supabase, or a self-hosted Postgres setup ‚Äî allowing you to build custom AI-powered FAQ assistants, internal support bots, or knowledge-based customer service tools.
üß† Why It Works: Contextual Retrieval
The secret is Contextual Retrieval ‚Äî a powerful technique where your documents are stored in a way that preserves meaning and context. This allows the AI to fetch highly relevant, source-backed responses, eliminating hallucinations and guesswork.
Data is embedded, chunked, and saved in a vector database (Postgres + PGVector), enabling smart semantic search tailored to your needs.
üìñ Learn more about this approach in this article by Anthropic ‚Üí
‚ú® Key Features
Chat with your internal documents: Uses your content to answer questions with precision
Built-in document vectorization: Pre-configured Google Drive ingestion flow (Notion, Airtable, Dropbox available separately)
Contextual memory: Past chats stored in PostgreSQL for personalized conversations
Plug-and-play architecture: Connect Supabase, OpenAI, custom APIs via n8n‚Äôs interface
üë§ Who Can Use This Workflow?
Entrepreneurs & startups building branded AI chatbots without code
Customer support teams automating answers using documentation
Ops teams creating internal FAQ bots for onboarding and training
No-code developers using n8n to build Telegram bots with AI features
‚öôÔ∏è Setup Instructions
You'll find step-by-step instructions inside the workflow.
Quick Setup Overview:
Import the workflow into n8n (cloud or self-hosted)
Add your Telegram Bot credentials
Connect your PostgreSQL DB (Neon, Supabase, etc.)
Set up document ingestion from Google Drive
Activate the workflow and start chatting
üß© Extensibility
This workflow is modular and ready to expand. Build powerful assistants by connecting additional workflows:
Vectorization modules for Notion, Airtable, Dropbox, and others
Any vector DB: Neon.tech, Supabase, or self-hosted PGVector
‚úçüèª Telegram User Registration Module ‚Üí
üíµ Telegram Payment, Invoicing & Refund Module ‚Üí
üß† More Smart AI Agents
Explore more AI workflows and agents on my Gumroad ‚Üí
üåê Agent: Find in the Internet ‚Äî fetches live info from the web
üìÅ Agent: Search Internal Docs ‚Äî queries Notion, Google Drive, etc.
üì¶ Agent: Check Order Status ‚Äî reads status from Airtable or CRM
üí∞ Agent: Calculate Cost or Quote ‚Äî builds pricing logic from inputs
üì® Submit your idea here for a custom AI agent ‚Üí"
Personal Budget & Expense Tracker with Google Sheets and Alerts MCP,https://n8n.io/workflows/4612-personal-budget-and-expense-tracker-with-google-sheets-and-alerts-mcp/,"This template provides a set of MCP tools to manage personal budgets and expenses. This MCP tools can be integrated to any AI client that support MCP integration.
How it works
It stores transaction records and budget in google sheet
It will give warning if expense is above budget
How to setup
Sign in with google in google sheet nodes
Copy google sheet template (link available in the sticky note)
Target google sheet nodes to the right sheet
Integrate with AI client
Enjoy!!"
Automated News Summarizer with GPT-4o + Email Delivery,https://n8n.io/workflows/4864-automated-news-summarizer-with-gpt-4o-email-delivery/,"How it Works
This workflow fetches top news headlines every 10 minutes from NewsAPI, summarizes them using OpenAI's GPT-4o model, and sends a concise email digest to a list of recipients defined in a Google Spreadsheet. It's ideal for anyone who wants to stay updated with the latest news in a short, digestible format.
üéØ Use Case
Professionals who want summarized daily news
Newsletters or internal communication updates
Teams that require contextual summaries of the latest events
Setup Instructions
1. Upload the Spreadsheet
File name: Emails
Column: Email with recipient addresses
2. Configure Google Sheets Nodes
Connect your Google account to:
Email List
Send Email
3. Add API Credentials
NewsAPI Key ‚Üí for fetching top headlines
OpenAI API Key ‚Üí for summarizing headlines
Gmail Account ‚Üí for sending the email digest
4. Activate the Workflow
Once active, the workflow runs every 10 minutes via a cron trigger
Summarized news is sent to the list of emails in the spreadsheet
üîÅ Workflow Logic
Trigger: Every 10 minutes via Cron
Fetch News: HTTP request to NewsAPI for top headlines
Summarize: Headlines are passed to OpenAI's GPT-4o for 5-bullet summary
Read Recipients: Google Sheet is used to collect email recipients
Send Email: Summary is formatted and sent via Gmail
üß© Node Descriptions
Node Name Description
Cron Triggers the workflow every 10 minutes.
HTTP Request - NewsAPI Fetches top news headlines using NewsAPI.
Set Formats or structures raw news data before processing.
AI Agent Summarizes the news content using OpenAI into 5 bullet points.
Email List Reads recipient email addresses from the 'Emails' Google Spreadsheet.
Send Email Sends the email digest to all recipients using Gmail.
üõ†Ô∏è Customization Tips
Modify the AI prompt for tone, length, or content type
Send summaries to Slack, Telegram, or Notion instead of Gmail
Adjust cron interval for more/less frequent updates
Change email formatting (HTML vs plain text)
üìé Required Files
File Name Purpose
Emails spreadsheet Google Sheet containing the list of email recipients
daily_news.json Main n8n workflow file to automate daily news digest
üß™ Testing Tips
Add 1‚Äì2 test email addresses in your spreadsheet
Temporarily change the Cron node to run every minute for testing
Check email inbox for delivery and formatting
Inspect the execution logs for API errors or formatting issues
üè∑ Suggested Tags & Categories
#News #OpenAI #Automation #Email #Digest #Marketing"
AI Email Auto-Responder System- AI RAG Agent for Email Inbox,https://n8n.io/workflows/4748-ai-email-auto-responder-system-ai-rag-agent-for-email-inbox/,"AI Email Auto-Responder ‚Äì Smart Client Reply Automation with RAG
This workflow is built for individuals, teams, and businesses that receive regular inquiries via email and want to automate responses in a way that‚Äôs intelligent, brand-aligned, and always up to date. Its core purpose is to generate high-quality, professional email replies using internal company data, brand voice, and semantic search ‚Äî fully automated through Gmail, Pinecone, and OpenAI.
The system is divided into three steps. First, it allows you to index your internal knowledge base (Docs, Sheets, PDFs) with embeddings. Second, it injects a consistent brand brief into every interaction to ensure tone and positioning. Finally, the main flow listens for incoming emails, understands the user query, retrieves all needed data, and writes a full HTML reply ‚Äî sending it directly to the original thread via Gmail.
This solution is ideal for support teams, solopreneurs, B2B service providers, or anyone looking to scale high-quality client communication without scaling manual work. It can be extended to handle multilingual queries, intent routing, or CRM logging.
How it works
When a new email arrives in Gmail, the workflow checks whether it's a valid client inquiry. If so, it:
Extracts the subject and message content
Sends the message through OpenAI to understand the question
Queries a Pinecone vector database (populated via a separate embedding workflow) to find relevant internal knowledge
Loads a brand brief from a Google Doc or Notion block
Combines retrieved data and brand context to generate a clear, structured HTML reply using OpenAI
Sends the reply via Gmail and logs the message
This process ensures every reply is relevant, accurate, and consistent with your brand ‚Äî and takes under 10 seconds.
Set up steps
Getting started takes about 30‚Äì60 minutes.
Create three workflows: one for embedding documents (Step 1), one sub-workflow for the brand brief (Step 2), and one main responder flow (Step 3)
Connect the following APIs: Gmail (OAuth2), OpenAI, Pinecone, Google Drive, and optionally Notion
Replace all placeholders: folder ID in Google Drive, Pinecone index and namespace, your brand brief URL or doc ID, and Gmail credentials
Test your embedding workflow by uploading a document and verifying its presence in Pinecone
Trigger the responder by sending an email and reviewing the AI‚Äôs reply
Detailed setup instructions are stored in sticky notes within each workflow to guide you through configuration."
Daily Validated Business Ideas using n8n and Upwork,https://n8n.io/workflows/4715-daily-validated-business-ideas-using-n8n-and-upwork/,"How it works
Runs at set times to fetch new Upwork job listings.
Checks each job‚Äôs total or hourly budget and keeps only higher-budget ones.
Extracts the job description for those selected jobs.
Uses AI to identify the core business idea from each description.
Records the idea and job details into a Google Sheet.
Can also be triggered manually to process a single job description.
Set up steps
Required: Upwork API key, AI service key, and a Google Sheet.
Import: Bring the JSON into n8n.
Configure: Enter your API keys and connect to your Google Sheet.
Time: About 25‚Äì35 minutes to complete all steps.
Notes: Detailed setup instructions are added as sticky notes inside the workflow."
"Create Animated Stories using GPT-4o-mini, Midjourney, Kling and Creatomate API",https://n8n.io/workflows/3655-create-animated-stories-using-gpt-4o-mini-midjourney-kling-and-creatomate-api/,"What does the workflow do?
This workflow is designed to generate high-quality short videos, primarily uses GPT-4o-mini (unofficial), Midjourney (unofficial) and Kling (unofficial) APIs from PiAPI and Creatomate API mainly for content creator, social media bloggers and short-form video creators. Through this short video workflow, users can quickly validate their creative ideas and focus more on enhancing the quality of their video concepts.
Who is the workflow for?
Social Media Influencers: produce content videos based on inspiration efficiently.
Vloggers: generate vlogs based on inspiration.
Educational Creators: explain specific topics via animated short videos or demonstrate a specific imagined scenario to students for enhanced educational impact.
Advertising Agencies: generate short videos based on specific products.
AI Tool Developers: automatically generate product demo videos.
Step-by-step Instructions
Fill in X-API-key of PiAPI account in Basic Params node.
Fill in the scenario of the image and video prompt.
Set a video template on Creatomate and make an API call in the final node with core and processing modules provided in Creatomate. Before full video generation, you can first use basic assets in Creatomate for a prototype demo, then integrate with n8n after verifying the expected results.
Fill in your Creatomate account settings following the image guildline.
Click Test Workflow and wait for a generation (within 10~20min).
In this workflow, we've established a basic structure for image-to-video generation with subtitle integration. You can further enhance it by adding music nodes using either PiAPI's audio models or your preferred music solution. All video elements will ultimately be composited through Creatomate. For best practice, please refer to PiAPI's official API documentation or Creatomate's API documentation to comprehend more use cases.
Use Case
Params Settings
style: a children‚Äôs book cover, ages 6-10. --s 500 --sref 4028286908 --niji 6
character: A gentle girl and a fluffy rabbit explore a sunlit forest together, playing by a sparkling stream
situational_keywords: Butterflies flutter around them as golden sunlight filters through green leaves. Warm and peaceful atmosphere
Output Video
<video src=""https://static.piapi.ai/n8n-instruction/short-video/example1.mp4"" controls />"
‚úçÔ∏è Blog Image SEO & Size Auditor with Ghost and Google Sheets,https://n8n.io/workflows/4802-blog-image-seo-and-size-auditor-with-ghost-and-google-sheets/,"Tags: Ghost CMS, SEO Audit, Image Optimisation, Alt Text, Google Sheets, Automation
Context
Hi! I‚Äôm Samir ‚Äî a Supply Chain Engineer and Data Scientist based in Paris, and founder of LogiGreen Consulting.
I help companies and content creators use automation and analytics to improve visibility, enhance performance, and reduce manual work.
Let‚Äôs use n8n to automate SEO audits to increase your traffic!
üì¨ For business inquiries, feel free to connect on LinkedIn
Who is this template for?
This workflow is perfect for bloggers, marketers, or content teams using Ghost CMS who want to:
Extract and review all images from articles
Detect missing or short alt texts
Check image file size and filename SEO compliance
Push the audit results into a Google Sheet
How does it work?
This n8n workflow extracts all blog posts from Ghost CMS, scans the HTML to collect all embedded images, then evaluates each image for:
‚úÖ Presence and length of alt text
üìè File size in kilobytes
üî§ Filename SEO quality (e.g. lowercase, hyphenated, no special chars)
All findings are written to Google Sheets for further analysis or manual cleanup.
üß≠ Workflow Steps:
üöÄ Trigger the workflow manually or on schedule
üì∞ Extract blog post content from Ghost CMS
üñºÔ∏è Parse all &lt;img&gt; tags with src and alt attributes
üì§ Store image metadata in a Google Sheet (step 1)
üåê Download each image using HTTP request
üßÆ Extract file size, extension, and filename SEO flag
üìÑ Update the audit sheet with size and format insights
What do I need to get started?
This workflow requires:
A Ghost Content API key
A Google Sheet (to log audit results)
No AI or external APIs required ‚Äî works fully with built-in nodes
Next Steps
üóíÔ∏è Follow the sticky notes inside the workflow to:
Plug in your Ghost blog credentials
Select or create a Google Sheet
Run the audit and start improving your SEO!
This template was built using n8n v1.93.0
Submitted: June 8, 2025"
AI-Powered Post-Sales Call Automated Proposal Generator,https://n8n.io/workflows/4359-ai-powered-post-sales-call-automated-proposal-generator/,"AI-Powered Proposal Generator - Sales Automation Workflow
Overview

This n8n workflow automates the entire proposal generation process using AI, transforming client requirements into professional, customized proposals delivered via email in seconds.
Use Case
Perfect for agencies, consultants, and sales teams who need to generate high-quality proposals quickly. Instead of spending hours writing proposals manually, this workflow captures client information through a web form and uses GPT-4 to generate contextually relevant, professional proposals.
How It Works
Form Trigger - Captures client information through a customizable web form
OpenAI Integration - Processes form data and generates structured proposal content
Google Drive - Creates a copy of your proposal template
Google Slides - Populates the template with AI-generated content
Gmail - Automatically sends the completed proposal to the client
Key Features
AI Content Generation: Uses GPT-4 to create personalized proposal content
Professional Templates: Integrates with Google Slides for polished presentations
Automated Delivery: Sends proposals directly to clients via email
Form Integration: Captures all necessary client data through web forms
Customizable Output: Generates structured proposals with multiple sections
Template Sections Generated
Proposal title and description
Problem summary analysis
Three-part solution breakdown
Project scope details
Milestone timeline with dates
Cost integration
Requirements
n8n instance (cloud or self-hosted)
OpenAI API key for content generation
Google Workspace account for Slides and Gmail
Basic n8n knowledge for setup and customization
Setup Complexity
Intermediate - Requires API credentials setup and basic workflow customization
Benefits
Time Savings: Reduces proposal creation from hours to minutes
Consistency: Ensures all proposals follow the same professional structure
Personalization: AI analyzes client needs for relevant content
Automation: Eliminates manual copy-paste and formatting work
Scalability: Handle multiple proposal requests simultaneously
Customization Options
Modify AI prompts for different industries or services
Customize Google Slides template design
Adjust form fields for specific information needs
Personalize email templates and signatures
Configure milestone templates for different project types
Error Handling
Includes basic error handling for API failures and form validation to ensure reliable operation.
Security Notes
All credentials have been removed from this template. Users must configure their own:
OpenAI API credentials
Google OAuth2 connections for Slides, Drive, and Gmail
Form webhook configuration
This workflow demonstrates practical AI integration in business processes and showcases n8n's capabilities for complex automation scenarios."
"AI Sales Agent: WhatsApp, FB, IG, OpenAI, Airtable, Supabase Auto-Booking",https://n8n.io/workflows/4083-ai-sales-agent-whatsapp-fb-ig-openai-airtable-supabase-auto-booking/,"This workflow automates multi-channel AI-driven sales engagement for lead qualification, service information delivery, and consultation booking. It integrates WhatsApp, Facebook Messenger, Instagram DM, and an n8n chat interface with a backend CRM (Airtable), a knowledge base (Supabase), and conversational AI (OpenAI), all orchestrated by n8n.
Tools & Services Used
Messaging Platforms: WhatsApp, Facebook Messenger, Instagram DM, n8n Built-in Chat
AI Core & Processing: OpenAI (GPT-4o for main agent logic, Whisper for audio transcription)
CRM & Data Management: Airtable (for initial WhatsApp lead lookup, lead form submissions, and as the backend for the crmAgent sub-workflow operations)
Knowledge Base: Supabase (Vector Store for technical_and_sales_knowledge tool)
Chat Memory: PostgreSQL (for the main AI Agent's conversation history)
Orchestration & Automation: n8n (Self-hosted, utilizing Langchain community nodes)
Calendar Service: Integrated via the calendarAgent sub-workflow
CRM Service: Integrated via the crmAgent sub-workflow (interacting with Airtable)
Workflow Overview
This automation performs the following steps:
Trigger: A new interaction is initiated through one of the following channels:
A new message is received via the WhatsApp Trigger.
A new message is received via the Facebook Trigger (Webhook).
A new message is received via the Instagram Trigger (Webhook).
A new message is received via the n8n Chat Trigger.
Alternatively, a new lead is submitted via the Airtable Form Submitted Webhook.
Channel-Specific Ingestion & Pre-processing:
For WhatsApp:
The system attempts to find an existing lead in Airtable using the sender's phone number.
Incoming messages are routed by the Handle Message Types switch:
Text messages are passed to the Edit Fields - chat1 node to prepare input for the AI Agent, including any found lead information.
Audio messages are processed: the WhatsApp Business Cloud node gets the media URL, the HTTP Request node downloads the audio, OpenAI transcribes it to text, and Edit Fields - chat2 prepares this transcribed text and lead information for the AI Agent.
Unsupported message types trigger the Reply To User1 node to send a notification that the message type cannot be processed.
For Facebook Messenger:
The system responds to webhook verification (Respond to Webhook - facebook get) and acknowledges new messages (Respond to Webhook - facebook post).
The If is not echo - facebook node filters out messages sent by the page.
The Sales Agent Demo - typing_on node sends a typing indicator.
The Edit Fields - facebook node prepares the message text, sender ID, and Facebook-specific context for the AI Agent.
For Instagram DM:
The system responds to webhook verification (Respond to Webhook - instagram get) and acknowledges new messages (Respond to Webhook - instagram post).
The If is not echo - instagram node filters out messages sent by the business account.
The Edit Fields - instagram node prepares the message text, sender ID, and Instagram-specific context for the AI Agent.
For n8n Chat:
The Edit Fields - chat node prepares the user's input and session information for the AI Agent.
Input Aggregation for AI Agent:
Processed data from all active messaging channels (WhatsApp text/audio, Facebook, Instagram, n8n Chat) is funneled through the No Operation, do nothing node to the main AI Agent.
AI Sales Conversation & Tool Utilization:
The AI Agent (using OpenAI Chat Model - GPT-4o, and Postgres Chat Memory) engages the user according to its system prompt, aiming to qualify them for Paint Protection Film (PPF), Ceramic Coating, or Window Tint.
The AI Agent uses the technical_and_sales_knowledge tool (which queries the Demo Supabase vector store via Embeddings OpenAI and OpenAI Chat Model1) to provide service details and answer questions.
The AI Agent uses the crmAgent tool (a sub-workflow) to log contact details (Name, Email, service interest) and update opportunity statuses in Airtable.
The AI Agent uses the calendarAgent tool (a sub-workflow) to book consultation appointments once preferred dates/times are provided. This occurs after contact details are logged in the CRM.
Response Delivery:
The AI Agent's final textual response is passed to the Switch node.
The Switch node routes the response to the appropriate node for delivery on the original channel:
Reply To User for WhatsApp.
Facebook Graph API - Sales Agent Demo for Facebook Messenger.
Instagram Graph API - smb.sales.agent.demo for Instagram DM.
Output - chat for the n8n Chat interface.
Airtable Form Submission Processing (Separate Branch):
When the Airtable Form Submitted webhook receives data, the Airtable node fetches the full record.
The Create Contact node creates a new contact in the Airtable 'Contacts' table.
The Edit Fields - form node prepares data for a notification.
The WhatsApp Business Cloud2 node sends a templated WhatsApp message to the lead, confirming their form submission."
Summarise Slack Channel Activity for Weekly Reports with AI,https://n8n.io/workflows/3969-summarise-slack-channel-activity-for-weekly-reports-with-ai/,"This n8n template lets you summarize team member activity on Slack for the past week and generates a report.
For remote teams, chat is a crucial communication tool to ensure work gets done but with so many conversations happening at once and in multiple threads, ideas, information and decisions usually live in the moment and get lost just as quickly - and all together forgotten by the weekend!
Using this template, this doesn't have to be the case. Have AI crawl through last week's activity, summarize all threads and generate a casual and snappy report to bring the team back into focus for the current week. A project manager's dream!
How it works
A scheduled trigger is set to run every Monday at 6am to gather all team channel messages within the last week.
Each message thread are grouped by user and data mined for replies.
Combined, an AI analyses the raw messages to pull out interesting observations and highlights.
The summarized threads of the user are then combined together and passed to another AI agent to generate a higher level overview of their week. These are referred to as the individual reports.
Next, all individual reports are summarized together into a team weekly report. This allows understanding of group and similar activities.
Finally, the team weekly report is posted back to the channel. The timing is important as it should be the first message of the week and ready for the team to glance over coffee.
How to use
Ideally works best per project and where most of the comms happens on a single channel. Avoid combining channels and instead duplicate this workflow for more channels.
You may need to filter for specific team members if you want specific team updates.
Customise the report to suit your organisation, team or the channel. You may prefer to be more formal if clients or external stakeholders are also present.
Requirements
Slack for chat platform
Gemini for LLM (or switch for other models)
Customising this workflow
If the slack channel is busy enough already, consider posting the final report to email.
Pull in project metrics to include in your report. As extra context, it may be interesting to tie the messages to production performance.
Use an AI Agent to query for knowledgebase or tickets relevant to the messages. This may be useful for attaching links or references to add context.
Channel not so busy or way too busy for 1 week? Play with the scheduled trigger and set an interval which works for your team."
"Amazon Product Search Scraper with BrightData, GPT-4, and Google Sheets",https://n8n.io/workflows/3901-amazon-product-search-scraper-with-brightdata-gpt-4-and-google-sheets/,"This workflow automates web scraping of Amazon search result pages by retrieving raw HTML, cleaning it to retain only the relevant product elements, and then using an LLM to extract structured product data (name, description, rating, reviews, and price), before saving the results back to Google Sheets.
It integrates Google Sheets to supply and collect URLs, BrightData to fetch page HTML, a custom n8n Function node to sanitize the HTML, LangChain (OpenRouter GPT-4) to parse product details, and Google Sheets again to store the output.

URL to scape
.

Result
Who Needs Amazon Search Result Scraping?
This scraping workflow is ideal for teams and businesses that need to monitor Amazon product listings at scale:
E-commerce Analysts ‚Äì Track competitor pricing, ratings, and inventory trends.
Market Researchers ‚Äì Collect data on product popularity and reviews for market analysis.
Data Teams ‚Äì Automate ingestion of product metadata into BI pipelines or data lakes.
Affiliate Marketers ‚Äì Keep affiliate catalogs up to date with latest product details and prices.
If you need reliable, structured data from Amazon search results delivered directly into your spreadsheets, this workflow saves you hours of manual copy-and-paste.
Why Use This Workflow?
End-to-End Automation ‚Äì From URL list to clean JSON output in Sheets.
Robust HTML Cleaning ‚Äì Strips scripts, styles, unwanted tags, and noise.
Accurate Structured Parsing ‚Äì Leverages GPT-4 via LangChain for reliable extraction.
Scalable & Repeatable ‚Äì Processes thousands of URLs in batches.
Step-by-Step: How This Workflow Scrapes Amazon
Get URLs from Google Sheets ‚Äì Reads a list of search result URLs.
Loop Over Items ‚Äì Iterates through each URL in controlled batches.
Fetch Raw HTML ‚Äì Uses BrightData‚Äôs Web Unlocker proxy to retrieve the page.
Clean HTML ‚Äì A Function node removes doctype, scripts, styles, head, comments, classes, and non-whitelisted tags, collapsing extra whitespace.
Extract with LLM ‚Äì Passes cleaned HTML into LangChain ‚Üí GPT-4 to output JSON for each product:
name, description, rating, reviews, price
Save Results ‚Äì Appends the JSON fields as columns back into a ‚Äúresults‚Äù sheet in Google Sheets.
Customization: Tailor to Your Needs
Adaptable Sites ‚Äì This workflow can be adapted to any e-commerce or other website, for example Walmart or eBay.
Whitelist Tags ‚Äì Modify the allowedTags array in the Code node to keep additional HTML elements.
Schema Changes ‚Äì Update the Structured Output Parser schema to include more fields (e.g., availability, SKU).
Alternate Data Sink ‚Äì Instead of Sheets, route output to a database, CSV file, or webhook.
üîë Prerequisites
Google Sheets Credentials ‚Äì OAuth credentials configured in n8n.
BrightData API token ‚Äì Stored in n8n credentials as BRIGHTDATA_TOKEN.
OpenRouter API Key ‚Äì Configured for the LangChain node to call GPT-4.
n8n Instance ‚Äì Self-hosted or cloud with sufficient quota for HTTP requests and LLM calls.
üöÄ Installation & Setup
Configure Credentials
In n8n, set up Google Sheets OAuth under ‚ÄúCredentials.‚Äù
Add BrightData token as a new HTTP Request credential.
Create an OpenRouter API key credential for the LangChain node.
Import the Workflow
Copy the JSON workflow into n8n‚Äôs ‚ÄúImport‚Äù dialog.
Map your Google Sheet IDs and GIDs to the {{WEB_SHEET_ID}}, {{TRACK_SHEET_GID}}, and {{RESULTS_SHEET_GID}} placeholders.
Ensure the BRIGHTDATA_TOKEN credential is selected on the HTTP Request node.
Test & Run
Add a few Amazon search URLs to your ‚Äútrack‚Äù sheet.
Execute the workflow and verify product data appears in your ‚Äúresults‚Äù sheet.
Tweak batch size or parser schema as needed.
‚ö† Important
API Rate Limits ‚Äì Monitor your BrightData and OpenRouter usage to avoid throttling.
Amazon‚Äôs Terms ‚Äì Ensure your scraping complies with Amazon‚Äôs policies and legal requirements.
Summary
This workflow delivers a fully automated, scalable solution to extract structured product data from Amazon search pages directly into Google Sheets‚Äîstreamlining your competitive analysis and data collection. üöÄ
Phil | Inforeole"
Automate GPT-4o Fine-Tuning with Google Sheets or Airtable Data,https://n8n.io/workflows/4853-automate-gpt-4o-fine-tuning-with-google-sheets-or-airtable-data/,"Who is this for?
Anyone curating before/after text examples in a spreadsheet and wanting a push-button path to a fine-tuned GPT model‚Äîwithout touching curl. Works with Google Sheets or Airtable.
What problem does it solve?
Manually downloading CSVs, converting to JSONL, uploading, and polling OpenAI is a slog.
This flow automates the whole loop: grab examples flagged Ready, build the JSONL file, start the fine-tune, then log the resulting model ID back to a registry sheet/table for reuse.
How it works
# Node Purpose
1 Schedule Trigger Runs weekly by default (change as needed).
2a Get Examples from Sheet Pulls rows where Ready = TRUE from your Google Sheet. Uses the JSONL-Template Sheet as the expected column layout.
2b Get Examples from Airtable (disabled) Alternate source for Airtable users.
3 Create JSONL File (Code) Converts each example to chat-format JSONL and splits into train.jsonl / val.jsonl (80/20).
4 Upload JSONL Uploads the training file to OpenAI (purpose: fine-tune).
5 Begin Fine-Tune Starts a fine-tune job on gpt-4o (editable).
6 Wait ‚Üí Check Job ‚Üí IF Polls every minute until status = succeeded.
7a Write Model to Sheet Appends the new model ID + meta to your Model Registry sheet.
7b Write Model to Airtable (disabled) Equivalent logging step for Airtable.
Setup steps
Import & connect credentials
Import the JSON flow into n8n.
Add your OpenAI API key.
Google Sheets: create an OAuth2 credential and link it to both Sheets nodes.
Airtable (optional): create a Personal Access Token and attach it to the Airtable nodes.
Copy the template sheet
Duplicate the JSONL-Template Sheet linked above into your own Drive.
Required columns (exact names):
| systemPrompt | userPrompt | assistantResponse | Ready |
Tick Ready = TRUE for rows you want to include.
Create the registry sheet/table
Google Sheet (or Airtable table) named Model Registry with columns:
Model ID, Training Examples, Epochs, Batch Size, Learning Rate, Finished At.
Tweak model & schedule
Change the base model in Begin Fine-Tune if desired.
Adjust the Schedule Trigger for daily / on-demand runs.
Test it
Mark a few examples Ready = TRUE.
Run the flow manually.
Check OpenAI for the new fine-tune job and confirm the model ID is logged in your registry.
Resources
n8n Docs ‚Äì <https://docs.n8n.io/&gt;
OpenAI Fine-Tuning ‚Äì <https://platform.openai.com/docs/guides/fine-tuning&gt;
Google Sheets API ‚Äì <https://developers.google.com/sheets/api&gt;
Airtable API ‚Äì <https://airtable.com/api&gt;
Extending the flow
Webhook trigger ‚Äì swap the schedule for a webhook to train on demand.
Multi-source merge ‚Äì enable both Sheets and Airtable nodes to combine datasets.
Auto-deploy ‚Äì save the new model name to an env-var or Secrets Manager for downstream generation workflows."
"Automate Travel Agent Outreach with Web Scraping, OpenAI, and Google Sheets",https://n8n.io/workflows/4606-automate-travel-agent-outreach-with-web-scraping-openai-and-google-sheets/,"üîß Automated Workflow: Scrape Travel Agent Contacts and Send Personalized Survey Emails
This workflow is designed to automate the process of scraping travel agent contact data, standardizing the information, storing it, and then sending out personalized survey emails using AI. It‚Äôs especially useful for outreach campaigns, research, or lead generation.
‚öôÔ∏è Workflow Breakdown
üìç Part 1: Scraping and Storing Travel Agent Data
HTTP Scrape Website
Type: HTTP Request (POST)
Function: Calls a third-party scraping API (https://api.firecrawl.dev...) to scrape data from a travel agent listing site.
Purpose: Extract raw HTML or structured data from a website containing contact info.
OpenAI Standardise Data
Type: OpenAI Message Model
Function: Uses AI to clean and standardize the raw scraped data into structured JSON (e.g., name, email, agency, location).
Purpose: Ensures uniformity in formatting, making data easier to process downstream.
Split Out
Type: Item Splitter
Function: Splits the standardized array of agent records into individual items.
Purpose: Allows appending each agent as a separate row in Google Sheets.
Google Sheet - Data Store
Type: Google Sheets (Append)
Function: Stores each individual record in a spreadsheet.
Purpose: Maintains a centralized and accessible log of scraped and processed contacts.
üìç Part 2: Read Records and Send Personalized Survey Emails
Triggered Manually ‚Äì when ‚ÄúTest Workflow‚Äù button is clicked.
Trigger ‚Äì When clicking 'Test workflow'
Type: Manual Trigger
Function: Starts the second part of the workflow manually.
Use Case: Testing or running the outreach email process on demand.
Google Sheet Data Store (Read)
Type: Google Sheets (Read)
Function: Reads the stored travel agent records from the spreadsheet.
Purpose: Retrieves contact details and context for personalized messaging.
OpenAI Mail Composer
Type: OpenAI Message Model
Function: Generates a custom email for each agent using their details.
Purpose: Creates human-like, engaging emails that include a survey link (optional input).
Google Sheet Update Records
Type: Google Sheets (Update)
Function: Optionally marks the record as ""emailed"" or logs the date of outreach.
Purpose: Prevents duplicate outreach and helps track campaign status.
Send Email
Type: Email Node (SMTP or integrated service)
Function: Sends the personalized email generated by OpenAI.
Purpose: Delivers the survey to each travel agent with contextually relevant messaging.
üß† Use Case:
Targeted email outreach to travel agents.
Collect insights or feedback via survey links.
Use personalized messaging to improve response rates.
üìå Benefits:
‚úÖ Fully automated scraping and processing.
‚úÖ Personalized at scale using OpenAI.
‚úÖ Easily repeatable for different domains or campaigns.
‚úÖ Centralized recordkeeping in Google Sheets.
üõ†Ô∏è Tech Stack:
n8n: Automation and workflow management
OpenAI: AI-based text standardization and email generation
Firecrawl (or similar): Web scraping API
Google Sheets: Data storage and tracking
Email Node: Survey email delivery"
Dynamic AI Model Router for Query Optimization with OpenRouter,https://n8n.io/workflows/4237-dynamic-ai-model-router-for-query-optimization-with-openrouter/,"The Agent Decisioner is a dynamic, AI-powered routing system that automatically selects the most appropriate large language model (LLM) to respond to a user's query based on the query‚Äôs content and purpose.
This workflow ensures dynamic, optimized AI responses by intelligently routing queries to the best-suited model.
Advantages
üîÅ Automatic Model Routing:
Automatically selects the best model for the job, improving efficiency and relevance of responses.
üéØ Optimized Use of Resources:
Avoids overuse of expensive models like GPT-4 by routing simpler queries to lightweight models.
üìö Model-Aware Reasoning:
Uses detailed metadata about model capabilities (e.g., reasoning, coding, web search) for intelligent selection.
üì• Modular and Extendable:
Easy to integrate with other tools or expand by adding more models or custom decision logic.
üë®‚Äçüíª Ideal for RAG and Multi-Agent Systems:
Can serve as the brain behind more complex agent frameworks or Retrieval-Augmented Generation pipelines.
How It Works
Chat Trigger: The workflow starts when a user sends a message, triggering the Routing Agent.
Model Selection: The AI Agent analyzes the query and selects the best-suited model from the available options (e.g., Claude 3.7 Sonnet for coding, Perplexity/Sonar for web searches, GPT-4o Mini for reasoning).
Structured Output: The agent returns a JSON response with the user‚Äôs prompt and the chosen model.
Execution: The selected model processes the query and generates a response, ensuring optimal performance for the task.
Set Up Steps
Configure Nodes:
Chat Trigger: Set up the webhook to receive user messages.
Routing Agent (AI Agent): Define the system message with model strengths and JSON output rules.
OpenRouter Chat Model: Connect to OpenRouter for model access.
Structured Output Parser: Ensure it validates the JSON response format (prompt + model).
Execution Agent (AI Agent1): Configure it to forward the prompt to the selected model.
Connect Nodes:
Link the Chat Trigger to the Routing Agent.
Connect the OpenRouter Chat Model and Output Parser to the Routing Agent.
Route the parsed JSON to the Execution Agent, which uses the chosen model via OpenRouter Chat Model1.
Credentials:
Ensure OpenRouter API credentials are correctly set for both chat model nodes.
Test & Deploy:
Activate the workflow and test with sample queries to verify model selection logic.
Adjust the routing rules if needed for better accuracy.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Generate Knowledge Base Articles with GPT & Perplexity AI for Contentful CMS,https://n8n.io/workflows/4012-generate-knowledge-base-articles-with-gpt-and-perplexity-ai-for-contentful-cms/,"Workflow: Auto Knowledge Base Article Generator
‚ö° About the Creators
This workflow was created by Varritech Technologies, an innovative agency that leverages AI to engineer, design, and deliver software development projects 500% faster than traditional agencies. Based in New York City, we specialize in custom software development, web applications, and digital transformation solutions. If you need assistance implementing this workflow or have questions about content management solutions, please reach out to our team.
üèóÔ∏è Architecture Overview
This workflow automates the end-to-end creation of a structured knowledge-base article from a simple chat prompt:
Chat Trigger ‚Üí Receives user request
AI Drafting Loop ‚Üí Generates & refines JSON article via AI agents
Perplexity Research Call ‚Üí Deep-dive content generation
Editorial Loop ‚Üí Up to 3 AI-driven revisions
Contentful Publish ‚Üí Pushes final JSON into CMS
üì¶ Node-by-Node Breakdown
flowchart LR
  A[Webhook: Chat Trigger] --&gt; B[AI Writer Agent]
  B --&gt; C[HTTP Request: Perplexity Content]
  C --&gt; D[Function: Format Output & Citations]
  D --&gt; E[Loop Start: Initialize Count]
  E --&gt; F[AI Editor Agent]
  F --&gt; G{action == ""rewrite""?}
  G -- yes --&gt; H[Function: Merge Improvements]
  H --&gt; I[Increment Count] --&gt; F
  G -- no --&gt; J[Stop Loop]
  J --&gt; K[HTTP Request: Publish to Contentful]
Webhook: Chat Trigger
Type: HTTP Webhook (POST /webhook/knowledge-article)
Payload:
{ 
 ""chatInput"": ""What topics should I write about?"" 
}
Purpose: Kicks off the workflow on that chat prompt.
AI Writer Agent
Inputs: chatInput or existing article JSON
Outputs:
{
  ""title"": ""..."",
  ""slug"": ""..."",
  ""category.id"": ""..."",
  ""description"": ""..."",
  ""keywords"": [...],
  ""content"": ""..."",
  ""metaTitle"": ""..."",
  ""metaDescription"": ""..."",
  ""readingTime"": 5,
  ""difficulty"": ""intermediate""
}
Purpose: Generates the article skeleton (metadata + initial content).
HTTP Request: Perplexity Content
Method: POST
URL: https://api.perplexity.ai/research
Body:
{
  ""model"": ""sonar-deep-research"",
  ""query"": ""{{ $json.title }}"",
  ""length"": 1000
}
Purpose: Retrieves a long-form, deeply researched draft for the article body.
Function: Format Output & Citations
Logic:
Parse raw Perplexity response
Extract source URIs
Append them under a sources markdown list
Editorial Loop
Initialize Counter to 0
AI Editor Agent
Reads draft JSON
Returns either:
action: ""rewrite"" + improvements: [...]
or action: ""submit""
Merge Improvements (if rewriting)
Applies suggested updates to JSON fields
Limit Check
Stops after 3 iterations or on ""submit""
HTTP Request: Publish to Contentful
Method: PUT
URL: https://cdn.contentful.com/spaces/{space}/environments/master/entries/{entryId}
Headers:
Authorization: Bearer <token>
Content-Type: application/vnd.contentful.management.v1+json
Body: Maps JSON ‚Üí Contentful entry fields
Outcome: Publishes the finalized article live.
üîç Design Rationale & Best Practices
Separation of Concerns
Writer vs. Editor agents isolate creative drafting from quality review.
Idempotent Loop
Counter + action flags prevent infinite retries.
Extensibility
Swap in different research APIs or CMS targets with minimal changes.
Structured JSON
Ensures predictable input/output for each node."
Create a Session-Based Telegram Chatbot with GPT-4o-mini and Google Sheets,https://n8n.io/workflows/3798-create-a-session-based-telegram-chatbot-with-gpt-4o-mini-and-google-sheets/,"How It Works
This workflow creates an AI-powered Telegram chatbot with session management, allowing users to:
Start new conversations (/new).
Check current sessions (/current).
Resume past sessions (/resume).
Get summaries (/summary).
Ask questions (/question).
Key Components:
Session Management:
Uses Google Sheets to track active/expired sessions (storing SESSION IDs and STATE).
/new creates a session; /resume reactivates past ones.
AI Processing:
OpenAI GPT-4 generates responses with contextual memory (via Simple Memory node).
Summarization: Condenses past conversations when requested.
Data Logging:
All interactions (prompts/responses) are saved to Google Sheets for audit and retrieval.
User Interaction:
Telegram commands trigger specific actions (e.g., /question [query] fetches answers from session history).
Main Advantages
1. Multi-session Handling
Each user can create, manage, and switch between multiple sessions independently, perfect for organizing different conversations without confusion.
2. Persistent Memory
Conversations are stored in Google Sheets, ensuring that chat history and session states are preserved even if the server or n8n instance restarts.
3. Commands for Full Control
With intuitive commands like /new, /current, /resume, /summary, and /question, users can manage sessions easily without needing a web interface.
4. Smart Summarization and Q&A
Thanks to OpenAI models, the workflow can summarize entire conversations or answer specific questions about past discussions, saving time and improving the chatbot‚Äôs usability.
5. Easy Setup and Scalability
By using Google Sheets instead of a database, the workflow is easy to clone, modify, and deploy ‚Äî ideal for quick prototyping or lightweight production uses.
6. Modular and Extensible
Each logical block (new session, get current session, resume, summarize, ask question) is modular, making it easy to extend the workflow with additional features like analytics, personalized greetings, or integrations with CRM systems.
Setup Steps
Prerequisites:
Telegram Bot Token: Create via BotFather.
Google Sheets:
Duplicate this template.
Two sheets: Session (active/inactive sessions) and Database (chat logs).
OpenAI API Key: For GPT-4 responses.
Configuration:
Telegram Integration:
Add your bot token to the Telegram Trigger and Telegram Send nodes.
Google Sheets Setup:
Authenticate the Google Sheets nodes with OAuth.
Ensure sheet names (Session, Database) and column mappings match the template.
OpenAI & Memory:
Add your API key to the OpenAI Chat Model nodes.
Adjust contextWindowLength in the Simple Memory node for conversation history length.
Testing:
Use Telegram commands to test:
/new: Starts a session.
/question [query]: Tests AI responses.
/summary: Checks summarization.
Deployment:
Activate the workflow; the bot will respond to Telegram messages in real-time.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Analyze Any Website with OpenAI And Get On-Page SEO Audit,https://n8n.io/workflows/3224-analyze-any-website-with-openai-and-get-on-page-seo-audit/,"Instantly Find & Fix What‚Äôs Holding Your Page Back
You‚Äôve put in the work. Your content is strong. Your design is polished.
But‚Ä¶
‚ùå Your page isn‚Äôt ranking where it should.
‚ùå Your competitors are outranking you‚Äîeven with weaker content.
‚ùå You have no idea what‚Äôs wrong‚Äîor how to fix it.
The truth? SEO isn‚Äôt just about keywords.
Your technical setup, content structure, and on-page elements must work together seamlessly.
And if anything is off? Google won‚Äôt rank your page.
Who Is This For?
SaaS Founders & Startups ‚Äì Get higher rankings & organic traffic that converts.
Marketing Teams & Agencies ‚Äì Audit & optimize pages in seconds.
E-commerce & Content Sites ‚Äì Improve rankings for product pages, blogs, and landing pages.
How It Works
Paste your URL
Get an instant audit + recommendations list
Implement changes & watch your rankings jump
The workflow scrapes the url you input, gets the htlm source code of the landing page, and sends it to OpenAI AI Agent.
The Agent makes a deep analysis, audit the Technical + Content SEO of the page, and provides 10 Recommendations to improve your SEO.
Setup Guide
You will need OpenAI Credentials with an API Key to run the workflow.
The workflow is using the OpenAI-o1 model to deliver the best results. It costs between $0.20/0.30 per run.
You can adjust the prompt to your wish in the AI Agent parameters.
Once the audit has been completed, it will send an email (don't forget to add your email address here)
Below is an example of what you can expect"
Transform Documents into Engaging LinkedIn Posts with GPT-4.1 and Email Approval,https://n8n.io/workflows/4777-transform-documents-into-engaging-linkedin-posts-with-gpt-41-and-email-approval/,"üöÄ How it works (Fonctionnement r√©sum√©) :
Ce template permet de transformer un document (PDF, TXT, DocX...) en post LinkedIn engageant, pr√™t √† √™tre publi√© ou valid√© par email, le tout avec l‚Äôaide d‚Äôune IA sp√©cialis√©e en copywriting LinkedIn. Voici les √©tapes cl√©s :
Formulaire de d√©p√¥t : L'utilisateur charge un fichier ou colle un texte.
D√©tection du type de contenu : Un Switch analyse le type de fichier (PDF, DOCX, TXT, ou texte brut). Attention pour DocX n√©cessite un compte Make pour transformer le doc (mais cela fonctionne aussi sans docX)
Extraction du contenu : Selon le format, le bon module d'extraction est utilis√©.
G√©n√©ration d‚Äôun post LinkedIn : L'IA transforme le contenu en post LinkedIn selon une m√©thodologie de copywriting optimis√©e.
Validation par email : Un email est envoy√© √† l‚Äôutilisateur pour approbation avec possibilit√© d‚Äôajouter une image.
Publication automatique : Si l'utilisateur valide, le post est publi√© sur LinkedIn.
‚öôÔ∏è Setup Steps :
Connecte tes comptes :
Google Docs OAuth
LinkedIn OAuth
OpenAI (via gpt-4.1-mini ou un autre mod√®le)
SMTP + IMAP pour l'envoi et la lecture d'emails
Configure les champs du formulaire dans le n≈ìud Form Trigger selon ton usage.
Personnalise le prompt IA dans le n≈ìud AI Agent si tu veux adapter le ton ou la m√©thodologie.
V√©rifie les emails dans le n≈ìud d'envoi (Send Email) et de lecture (Email Trigger (IMAP)), pour que la validation fonctionne.
Teste le workflow avec diff√©rents fichiers pour t'assurer que tous les types sont bien trait√©s (PDF, DOCX, TXT, etc.).
üß© Cas d‚Äôusage typiques :
Cr√©er des posts √† partir de notes de r√©union ou de rapports.
Valoriser un article ou une publication professionnelle sous forme de contenu LinkedIn.
D√©l√©guer √† l'IA le premier jet de ton contenu r√©seau.
Bonus surveille une newsletter de ta messagerie pour proposer un post pertinent sur LinkedIn (vous pouvez supprimer il fonctionne en parall√®le)"
Automate News Publishing to LinkedIn with Gemini AI and RSS Feeds,https://n8n.io/workflows/4414-automate-news-publishing-to-linkedin-with-gemini-ai-and-rss-feeds/,"üì∞ LinkedIn News Auto-Publisher
Overview üìã
This project is an automated news publisher for LinkedIn. It uses RSS feeds to fetch news, processes the content with the Gemini API to generate precise summaries, and automatically publishes to LinkedIn via its API.
How It Works
Architecture and Workflow ‚öôÔ∏è
n8n: Efficient orchestration of workflow with automation.
RSS: News sources such as TechCrunch and MIT Technology Review.
Gemini API: Dynamic generation of content and precise summaries.
LinkedIn API: Automatic publication on profiles and corporate pages.
Content Processing üß†
Fetching news through RSS feeds.
Processing and generating summaries with the Gemini API.
Automatic publication on LinkedIn.
Key Benefits ‚úÖ
Complete automation of the news publishing process.
Dynamic generation of precise and relevant content.
Integration with reliable news sources and publication on a professional platform.
Use Cases üíº
Automation of news publishing for businesses and professionals.
Keeping corporate profiles and pages updated with relevant content.
Saving time in managing content on social networks.
Requirements üë®‚Äçüíª
n8n instance (self-hosted or cloud).
Gemini API credentials.
LinkedIn bot setup and API credentials.
Configured RSS feeds to fetch news.
Authors üë•
Joel Choez
Alan Baja√±a
Jaren Pazmi√±o
David Sandoval
Members of CIAP"
AI Agent To Chat With Files In Supabase Storage and Google Drive,https://n8n.io/workflows/4086-ai-agent-to-chat-with-files-in-supabase-storage-and-google-drive/,"Video Guide
I prepared a detailed guide that illustrates the entire process of building an AI agent using Supabase and Google Drive within N8N workflows.
Youtube Link
Who is this for?
This workflow is designed for developers, data scientists, and business users who wish to automate document management and enable AI-powered interactions over their stored files. It's especially beneficial for scenarios where users need to process, analyze, and retrieve information from uploaded documents rapidly.
What problem does this workflow solve?
Managing files across multiple platforms often involves tedious manual processes. This workflow facilitates automated file handling, making it easier for users to upload, parse, and interact with documents through an AI agent. It reduces redundancy and enhances the efficiency of data retrieval and management tasks.
What this workflow does
This workflow integrates Supabase storage with Google Drive and employs an AI agent to manage files effectively. The agent can:
Upload files to Supabase storage and activate processes based on file changes in Google Drive.
Retrieve and parse documents, converting them into a structured format for easy querying.
Utilize an AI agent to answer user queries based on saved document data.
Data Collection: The workflow initially gathers files from Supabase storage, ensuring no duplicates are processed in the 'files' table.
File Handling: It processes files to be parsed based on their type, leveraging LlamaParse for effective data transformation.
Google Drive Integration: The workflow monitors a designated Google Drive folder to upload files automatically and refresh document records in the database with new data.
AI Interaction: A webhook is established to enable the AI agent to converse with users, facilitating queries and leveraging stored document knowledge.
Setup
Supabase Storage Setup:
Create a private bucket in Supabase storage, modifying the default name in the URL.
Upload your files using the provided upload options.
Database Configuration:
Establish the 'file' and 'document' tables in Supabase with the necessary fields.
Execute any required SQL queries for enabling vector matching features.
N8N Workflow Logic:
Start with a manual trigger for the initial workflow segment or consider alternative triggers like webhooks.
Replace all relevant credentials across nodes with your own to ensure seamless operation.
File Processing and Google Drive Monitoring:
Set up file processing to take care of downloading and parsing files based on their types.
Create triggers to monitor the designated Google Drive folder for file uploads and updates.
Integrate AI Agent:
Configure the webhook for the AI agent to accept chat inputs while maintaining session context for enhanced user interactions.
Utilize PostgreSQL to store user interactions and manage conversation states effectively.
Testing and Adjustments:
Once everything is set up, run tests with the AI agent to validate its responses based on the documents in your database.
Fine-tune the workflow and AI model as needed to achieve desired performance."
Chat with Postgresql Database,https://n8n.io/workflows/2859-chat-with-postgresql-database/,"Who is this template for?
This workflow template is designed for any professionals seeking relevent data from database using natural language.
How it works
Each time user ask's question using the n8n chat interface, the workflow runs.
Then the message is processed by AI Agent using relevent tools - Execute SQL Query, Get DB Schema and Tables List and Get Table Definition, if required. Agent uses these tool to form and run sql query which are necessary to answer the questions.
Once AI Agent has the data, it uses it to form answer and returns it to the user.
Set up instructions
Complete the Set up credentials step when you first open the workflow. You'll need a Postgresql Credentials, and OpenAI api key.
Template was created in n8n v1.77.0"
Generate Content Ideas with Gemini Pro and Store in Google Sheets,https://n8n.io/workflows/4432-generate-content-ideas-with-gemini-pro-and-store-in-google-sheets/,"Automated Content Idea Generation and Expansion with Google Gemini and Google Sheets
This n8n workflow automates the process of generating content ideas based on a user-defined topic, then expands each idea into a more detailed content piece (like a blog post) using Google Gemini, and finally saves all the generated data (idea title, description, and full content) into a Google Sheet. It's a powerful tool for streamlining content creation workflows.
This workflow includes:
Generation of multiple content ideas from a single topic.
Expansion of each idea into detailed content using AI.
Storage of ideas and generated content in a structured Google Sheet.
Sticky Notes within the workflow for inline documentation and setup guidance.
Prerequisites
n8n Instance: You need a running n8n instance (self-hosted or cloud).
Google AI Account: Access to Google AI (Gemini). You will need an API key.
Google Account: Access to Google Sheets. You will need to create or use an existing spreadsheet with specific column headers.
Installation and Setup
Import the Workflow:
Copy the entire JSON code provided.
In your n8n instance, go to ""Workflows"".
Click ""New"" -> ""Import from JSON"".
Paste the JSON code and click ""Import"".
Configure Credentials:
Google AI (Gemini):
Find the ""Google Gemini Chat Model for Content Idea Generator"" node and the ""Google Gemini Chat Model for Content Generation"" node.
Click on the ""Credentials"" field in both nodes (it will likely show a placeholder name like ""Google Gemini(PaLM) Api account"").
Click ""Create New"".
Select ""Google AI API"".
Enter your Google AI API Key.
Save the credential. (You can reuse the same credential for both nodes).
Google Sheets:
Find the ""Google Sheets"" node.
Click on the ""Credentials"" field (it will likely show a placeholder name like ""Google Sheets account"").
Click ""Create New"".
Select ""Google Sheets OAuth2 API"".
Follow the steps to connect your Google Account and grant n8n access to Google Sheets.
Save the credential.
Configure Google Sheets Node:
Open the ""Google Sheets"" node settings.
Spreadsheet ID: Replace the placeholder value with the actual ID of your Google Sheet. You can find the Spreadsheet ID in the URL of your Google Sheet (it's the long string of characters between /d/ and /edit).
Sheet Name: Select or enter the name or GID of the sheet within your spreadsheet where you want to save the data (e.g., Sheet1 or gid=0).
Columns: Ensure your Google Sheet has columns named title, description, and content. The node is configured to map the generated data to these specific column headers.
Save the node settings.
Review Sticky Notes:
Look at the Sticky Notes placed around the workflow canvas. They provide helpful context and reminders for setup, required Google Sheet columns, and the AI models used.
How to Use
Activate the Workflow: Toggle the workflow switch to ""Active"".
Trigger the Workflow:
Since this workflow uses a ""When clicking ‚ÄòExecute workflow‚Äô"" node as the trigger, you can run it directly from the n8n editor.
Click the ""Execute Workflow"" button.
The workflow will start automatically.
Set the Topic:
Open the ""Set the input fields"" node.
Modify the topic value to the subject you want to generate content ideas about.
Save the node settings.
Monitor Execution: Watch the workflow execute. The nodes will light up as they process. The ""Loop Over Items"" node will show multiple executions as it processes each generated idea.
Check Results:
The generated content ideas (title, description) and the expanded content will be written as new rows in the Google Sheet you configured. Each row will correspond to one generated idea and its content.
This workflow provides a robust starting point for AI-assisted content creation. You can customize the AI prompts in the ""Content Idea Generator"" and ""LLM Content Generator"" nodes to refine the output style and format, or integrate additional steps like sending notifications or further processing the generated content."
"Automate Multi-Channel Customer Support with Gmail, Telegram, and GPT AI",https://n8n.io/workflows/4474-automate-multi-channel-customer-support-with-gmail-telegram-and-gpt-ai/,"Smart Customer Support AI Agent with Gmail and Telegram
Who is this for?
This workflow is perfect for:
Small to medium businesses looking to automate customer support
E-commerce stores handling order inquiries and customer questions
SaaS companies providing technical support to users
Service providers managing appointment bookings and general inquiries
Startups wanting to provide 24/7 customer service without hiring full-time staff
Agencies managing client communications across multiple channels
What problem is this workflow solving?
Customer support is essential but resource-intensive. Common challenges include:
Slow response times leading to frustrated customers
Repetitive questions consuming valuable staff time
Inconsistent responses across different support agents
Limited availability outside business hours
Scaling support costs as business grows
Context loss when customers switch between channels
This workflow eliminates these pain points by providing instant, consistent, and intelligent responses 24/7.
What this workflow does
Core Functionality
Multi-Channel Monitoring: Simultaneously watches Gmail and Telegram for customer inquiries
Intelligent Processing: Uses AI to understand customer intent and context
Knowledge Base Integration: Accesses your company's FAQ and support information
Contextual Responses: Provides personalized, helpful replies maintaining conversation history
Smart Escalation: Automatically escalates complex issues to human agents
Comprehensive Logging: Tracks all interactions for analytics and improvement
AI Agent Capabilities
Natural Language Understanding: Comprehends customer questions in plain English
Context Awareness: Remembers previous conversations with each customer
Knowledge Retrieval: Searches your knowledge base for accurate information
Response Generation: Creates professional, brand-appropriate responses
Escalation Decision: Identifies when human intervention is needed
Multi-Channel Support: Handles Gmail and Telegram with channel-specific formatting
Automation Features
Auto-Response: Replies to customers within seconds
Email Management: Marks processed emails as read
Conversation Threading: Maintains context in email threads and Telegram chats
Error Handling: Gracefully handles failures with admin notifications
Analytics Tracking: Logs interactions for performance monitoring
Setup
Prerequisites
Active Google Workspace or Gmail account
Telegram account for bot creation
OpenAI API access
Google Sheets access
n8n instance (cloud or self-hosted)
Step 1: Credential Setup
Gmail OAuth2 Configuration
Go to Google Cloud Console
Create new project or select existing one
Enable Gmail API
Create OAuth 2.0 credentials
Add authorized redirect URIs for n8n
In n8n: Settings ‚Üí Credentials ‚Üí Add Gmail OAuth2
Enter Client ID and Client Secret
Complete OAuth flow
Telegram Bot Setup
Message @BotFather on Telegram
Create new bot with /newbot command
Choose bot name and username
Copy the bot token
In n8n: Settings ‚Üí Credentials ‚Üí Add Telegram
Enter bot token
Set webhook URL in bot settings
OpenAI API Configuration
Sign up at OpenAI Platform
Generate API key in API Keys section
In n8n: Settings ‚Üí Credentials ‚Üí Add OpenAI
Enter API key
Choose appropriate model (gpt-4o-mini recommended)
Google Sheets Setup
Use existing Google account from Gmail setup
In n8n: Settings ‚Üí Credentials ‚Üí Add Google Sheets OAuth2
Complete authorization flow
Step 2: Google Sheets Preparation
Create three Google Sheets in your Google Drive:
Knowledge Base Sheet
Sheet Name: ""Knowledge Base""
Columns: ID, Category, Question/Topic, Answer/Response, Keywords, Last_Updated
Import sample data from the Knowledge Base example
Customize with your company's FAQs and policies
Escalation Tracker Sheet
Sheet Name: ""Escalations""
Columns: Timestamp, Customer_Name, Customer_Contact, Inquiry_Summary, Escalation_Reason, Priority, Status, Assigned_To
This will be auto-populated by the AI agent
Interaction Log Sheet
Sheet Name: ""Interaction Log""
Columns: Timestamp, Channel, Customer_Name, Customer_Contact, Inquiry_Subject, Customer_Message, AI_Response, Response_Time, Status
This tracks all customer interactions for analytics
Step 3: Workflow Configuration
Import Template
Copy the workflow JSON from the template
In n8n: Import workflow from JSON
Replace placeholder Sheet IDs with your actual Google Sheet IDs
Update Sheet References
Open each Google Sheets node
Select your created sheets from the dropdown
Verify column mappings match your sheet structure
Customize AI Prompts
Edit the ""Customer Support AI Agent"" node
Update system message with:
Your company name and description
Brand voice and tone guidelines
Specific policies and procedures
Escalation criteria
Configure Error Notifications (Optional)
Set up Slack webhook or email notifications
Update error notification node with your webhook URL
Customize error message format
Step 4: Testing
Test Gmail Integration
Send test email to your support Gmail account
Check workflow execution in n8n
Verify response is sent and email marked as read
Check interaction logging in Google Sheets
Test Telegram Integration
Send message to your Telegram bot
Verify bot responds appropriately
Test conversation memory with follow-up messages
Check escalation functionality with complex request
Test Knowledge Base
Ask questions covered in your knowledge base
Verify AI retrieves and uses correct information
Test with variations of the same question
Ensure responses are consistent and helpful
How to customize this workflow to your needs
Brand Voice Customization
Update the AI system prompt to include:
- Your company's tone (formal, casual, friendly)
- Key phrases and terminology you use
- Brand personality traits
- Communication style preferences
Knowledge Base Expansion
Add industry-specific FAQs
Include product documentation
Add troubleshooting guides
Create category-specific responses
Escalation Rules
Customize when to escalate by modifying the AI agent instructions:
Billing disputes over $X amount
Technical issues requiring developer help
Angry or dissatisfied customers
Requests outside standard services
Legal or compliance questions
Additional Channels
Extend the workflow to support:
Slack: Add Slack triggers and response nodes
WhatsApp: Integrate WhatsApp Business API
Web Chat: Add webhook triggers for website chat
Discord: Connect Discord bot integration
Analytics Enhancement
Add sentiment analysis to customer messages
Implement customer satisfaction scoring
Create automated reporting dashboards
Set up alert thresholds for escalation rates
Integration Opportunities
CRM Integration: Connect to HubSpot, Salesforce, or Pipedrive
Ticketing System: Link to Zendesk, Freshdesk, or Jira Service Desk
E-commerce Platform: Integrate with Shopify, WooCommerce, or Magento
Calendar Booking: Connect to Calendly or Acuity for appointment scheduling
Advanced Features
Multi-language Support: Add translation capabilities
Voice Messages: Integrate speech-to-text for Telegram voice notes
Image Recognition: Process customer screenshots for technical support
Proactive Outreach: Send follow-up messages based on customer behavior
Workflow Maintenance
Daily Tasks
Review escalation queue
Monitor error notifications
Check response quality in interaction log
Weekly Reviews
Analyze customer interaction patterns
Update knowledge base with new common questions
Review escalation reasons and optimize AI prompts
Monthly Optimization
Export interaction data for detailed analysis
Calculate key metrics (response time, resolution rate, escalation rate)
Update AI model parameters based on performance
Expand knowledge base with seasonal or trending topics
Key Metrics to Track
Response Time: Average time from customer message to AI response
Resolution Rate: Percentage of inquiries resolved without escalation
Customer Satisfaction: Based on follow-up surveys or sentiment analysis
Escalation Rate: Percentage of conversations requiring human intervention
Channel Performance: Effectiveness of Gmail vs Telegram vs other channels
Knowledge Base Usage: Which topics are accessed most frequently
Peak Hours: When customers contact support most often
Troubleshooting
Common Issues
Gmail not triggering: Check OAuth permissions and API quotas
Telegram bot not responding: Verify bot token and webhook configuration
AI responses seem off: Review and update system prompts
Escalations not logging: Check Google Sheets permissions and column mapping
High escalation rate: Expand knowledge base and refine AI instructions
Performance Optimization
Monitor OpenAI API usage and costs
Adjust AI model temperature for response consistency
Optimize knowledge base for faster searches
Set appropriate conversation memory limits
This workflow provides a solid foundation for automated customer support that can be extensively customized to match your specific business needs and grow with your company."
Real-time Crypto News & Sentiment Analysis via Telegram with GPT-4o,https://n8n.io/workflows/3751-real-time-crypto-news-and-sentiment-analysis-via-telegram-with-gpt-4o/,"Stay on top of the latest crypto news and market sentiment instantly, all inside Telegram!
This workflow aggregates articles from the top crypto news sources, filters for your topic of interest, and summarizes key news and market sentiment using GPT-4o AI. Ideal for crypto traders, investors, analysts, and market watchers needing fast, intelligent news briefings.
üí¨ Just type a coin name (e.g., ""Bitcoin"", ""Solana"", ""DeFi"") into your Telegram AI Agent‚Äîand get a smart news digest.
How It Works
Telegram Bot Trigger
User sends a keyword (e.g., ""Ethereum"") of questions to the Telegram AI Agent.
Keyword Extraction (AI-Powered)
An AI agent identifies the main topic for better targeting.
News Aggregation
Pulls articles from 9 major crypto news RSS feeds:
Cointelegraph
Bitcoin Magazine
CoinDesk
Bitcoinist
NewsBTC
CryptoPotato
99Bitcoins
CryptoBriefing
Crypto.news
Filtering
Finds and merges articles relevant to the user's keyword.
AI Summarization
GPT-4o generates a 3-part summary:
News Summary
Market Sentiment Analysis
List of Article Links
Telegram Response
Sends a structured, easy-to-read digest back to the user.
üîç What You Can Do with This Workflow
üîπ Summarize breaking news for any crypto project or keyword
üîπ Monitor real-time market sentiment on Bitcoin, DeFi, NFTs, and more
üîπ Stay ahead of FUD, bullish trends, and major news events
üîπ Quickly brief yourself or your team via Telegram
üîπ Use it as a foundation for more advanced crypto alert bots
‚úÖ Example User Inputs
‚úÖ ""Bitcoin"" ‚Üí Latest Bitcoin news and sentiment summary
‚úÖ ""Solana"" ‚Üí Updates on Solana projects, price movements, and community trends
‚úÖ ""NFT"" ‚Üí Aggregated news about NFT markets and launches
‚úÖ ""Layer 2"" ‚Üí Insights on Optimism, Arbitrum, and other L2s
üõ†Ô∏è Setup Instructions
Create a Telegram Bot
Use @BotFather and obtain the Bot Token.
Configure Telegram Credentials in n8n
Add your bot token under Telegram API Credentials.
Configure OpenAI API
Add your OpenAI credentials for GPT-4o access.
Update Telegram Send Node
In the Telegram Send node, replace the placeholder chatId with your real Telegram user or group chat ID.
Deploy and Test
Start chatting with your bot: e.g., ""Ethereum"" or ""DeFi"".
üìå Workflow Highlights
9 major crypto news sources combined
Smart keyword matching with AI query parsing
Summarized insights in human-readable format
Reference links included for deeper reading
Instant delivery via Telegram
üöÄ Get ahead of the crypto market‚Äîautomate your news and sentiment monitoring with AI inside Telegram!"
Build your own CUSTOM API MCP server,https://n8n.io/workflows/3638-build-your-own-custom-api-mcp-server/,"This n8n demonstrates how any organisation can quickly and easily build and offer MCP servers to their customers or internal staff to improve productivity.
This MCP example uses PayCaptain.com as an example and shows how to create an MCP server which can search for and update employee data.
How it works
A MCP server trigger is used and connected to 3 custom workflow tools: Search Employee, Get Employee by ID and Update Employee.
Each tool makes calls to the PayCaptain API to perform their respective tasks. Extra care is performed to strip out sensitive data and ensure we're not sharing too much.
The Update Employee too also guards against updating fields which would preferably remain readonly. When you control the MCP server, you can determine behaviour of the tool.
Finally, a Google Sheet node is used to log all operations for later audit. This will add a tiny bit of latency but recommended if sensitive data is being accessed.
How to use
This MCP server allows any compatible MCP client to manage their PayCaptain employee database. You will need to have a PayCaptain account and developer key to use it.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""When did Sarah start here employment at the company?""
""Does Jack work Wednesdays or Fridays?""
""Please update Tracy's NI number to ABCD123456""
Requirements
PayCaptain Account and Developer Key.
Google Sheets to log actions for later audit.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
Add or remove employee attributes as required for your user case.
If Google Sheets is too slow, consider an API call to a faster service to log calls to the MCP server.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
"Learn Anything, Write a Book, Design a Curriculum",https://n8n.io/workflows/4492-learn-anything-write-a-book-design-a-curriculum/,"Hey n8n Innovators & Knowledge Seekers! üëã
Ever felt that spark? That ""aha!"" moment when you realize you need to dive deep into a new subject? Or maybe you've got a brilliant idea for a course, a comprehensive guide, or even the seeds of your next groundbreaking book, but the thought of structuring it all makes your brain do a backflip?
What if you had an AI co-pilot, right within n8n, to help you LEARN ANYTHING profoundly, DESIGN A DETAILED CURRICULUM from scratch, or even OUTLINE YOUR NEXT BESTSELLER ‚Äì and then beam it all into a beautifully organized Notion workspace?
Well, dream no more! I'm super stoked to share a set of n8n workflows I've cooked up that does exactly this.





Picture This: You Want To...
üöÄ Master the Unfamiliar: Finally conquer the intricacies of Decentralized Finance (DeFi), understand the core principles of Permaculture Design, or even get a solid grasp on Cognitive Bias in Decision Making. Just tell the AI your starting point!
üìö WRITE THAT BOOK! Been mulling over a guide on Sustainable Urban Living? Or a deep dive into The History of Ancient Philosophies? Let AI generate your chapter outlines, key concepts, and foundational content.
üéì CREATE A WORLD-CLASS CURRICULUM: Planning a workshop on Advanced Data Visualization Techniques? Or an online course on The Fundamentals of Ethical AI Development? Get a comprehensive, Harvard-level syllabus structure in minutes.
üß† Personalize Your Knowledge Journey: Get content adapted not just for your current skill level, but with presentation hints based on cognitive styles (yep, we're playing with MBTI!).
So, How Does This AI Magic Work in n8n?
It's a dynamic duo of workflows:
The ""Learn Anything"" Maestro:
You feed it your topic, how much you know, (optionally) your MBTI, and how deep you want to go via a cool, custom-styled form.
AI models (via OpenRouter) then get to work, sketching out a full hierarchical curriculum and brainstorming a catchy title and description.
Then, for every single section in your new plan, our ""SeniorWriter"" AI crafts rich, detailed content. Think of it as your personal subject matter expert on tap!
The ""Notion Block Ninja"" Sub-Workflow:
This trusty sidekick takes all that awesome Markdown content.
It intelligently converts it into the precise JSON that Notion loves, ensuring every heading, list, and table looks perfect.
Then whoosh ‚Äì it‚Äôs all uploaded to your Notion page, organized and ready.
Why Share This? Because Knowledge Should Be Accessible & Creation Fun!
I'm a firm believer that n8n isn't just for business tasks; it's an incredible platform for personal growth, learning, and creative exploration. This project is all about making it easier to dive deep, structure your thoughts, and bring big ideas to life.
Ready to Unleash Your AI Co-Pilot?
I'll be dropping the workflow JSONs and a setup guide (with a handy Notion database template link!) in the comments or a follow-up. You'll just need your Notion API key and an OpenRouter API key (they've got free tiers to get you started exploring!).
Think of the Possibilities! You could be:
Building custom learning paths for yourself or your team.
Rapidly prototyping educational content.
Finally writing that book you've always talked about.
Just satisfying your endless curiosity on any subject under the sun!
I can't wait to hear what you think, the amazing topics you explore, and any brilliant ideas you have to make this even more powerful!
Let the learning (and creating) begin! üöÄ"
Find relevant X tweets based on your profile and suggest responses,https://n8n.io/workflows/4851-find-relevant-x-tweets-based-on-your-profile-and-suggest-responses/,"What This Workflow Does
Step 1: Analyzes your recent tweets for personality & style
Step 2: Generates strategic keywords based on your profile
Step 3: Searches for trending tweets in your niche
Step 4: Creates personalized responses & original tweets
Step 5: Displays results in beautiful HTML format
How to Use
Execute the workflow
Fill out the form with:
Your X username (from your URL)
Your goals on X (multiple selection)
Optional additional info
Submit the form
Wait for processing (2-3 minutes)
Double-click the ""Click to show Result"" node
example clip can be found here"
Medical Q&A Chatbot for Urology using RAG with Pinecone and GPT-4o,https://n8n.io/workflows/3670-medical-qanda-chatbot-for-urology-using-rag-with-pinecone-and-gpt-4o/,"Medical Q&A Chatbot for Urology using RAG with Pinecone and GPT-4o
This template provides an AI-powered Q&A assistant for the Urology domain using Retrieval-Augmented Generation (RAG). It uses Pinecone for vector search and GPT-4o for conversational responses.
üß† Use Case
This chatbot is designed for clinics or medical pages that want to automate question answering for Urology-related conditions. It uses a vector store of domain knowledge to return verified responses.
üîß Requirements
‚úÖ OpenAI API key (GPT-4o or GPT-4o-mini)
‚úÖ Pinecone account with an active index
‚úÖ Verified Urology documents embedded into Pinecone
‚öôÔ∏è Setup Instructions
Create a Pinecone vector index and connect it using the Pinecone credentials node.
Upload Urology-related documents to embed using the Create Embeddings for Urology Docs node.
Customize the chatbot system message to reflect your medical specialty.
Deploy this chatbot on your website or link it with Telegram via the chat trigger node.
üõ†Ô∏è Components
chatTrigger: Listens for user messages and starts the workflow.
Medical AI Agent: GPT-based agent guided by domain-specific instructions.
RAG Tool Vector Store: Fetches relevant documents from Pinecone using vector search.
Memory Buffer: Maintains conversation context.
Create Embeddings for Urology Docs: Encodes documents into vector format.
üìù Customization
You can replace the knowledge base with any other medical domain by:
Updating the documents stored in Pinecone.
Modifying the system prompt in the AI Agent node.
üì£ CTA
This chatbot is ideal for clinics, medical consultants, or educational websites wanting a reliable AI assistant in Urology."
WhatsApp Expense Tracker with PostgreSQL Database & AI-Powered Reports,https://n8n.io/workflows/4136-whatsapp-expense-tracker-with-postgresql-database-and-ai-powered-reports/,"Track Personal Finances with WhatsApp and AI Assistant
Transform your WhatsApp into a powerful personal finance command center. This AI-powered workflow converts natural language messages into structured financial data, automates record-keeping, and delivers instant insights‚Äîall within your favorite messaging app.
Who is this for?
This template is perfect for:
Personal finance enthusiasts who want effortless expense tracking
Small business owners managing personal and business expenses
Freelancers tracking income and expenses across projects
Anyone who prefers messaging over complex finance apps
Users seeking privacy with self-hosted financial data
What problem is this workflow solving?
Traditional expense tracking requires switching between apps, manual data entry, and complex spreadsheets. Most people abandon these systems within weeks. This workflow solves the friction by:
Eliminating app-switching‚Äîeverything happens in WhatsApp
Converting natural language to structured data automatically
Providing instant confirmations and reports
Requiring zero learning curve or behavior change
What this workflow does
Smart Transaction Processing
Send natural messages like Spent 300 on groceries at Walmart and the AI automatically extracts:
Date: Today's date (or specified date)
Category: Groceries
Type: Expense/Income/Debt
Amount: 300
Person/Company: Walmart
Intelligent Message Classification
The workflow automatically routes messages to three processing branches:
Branch 1: Reports and analytics (show March expenses)
Branch 2: Transaction logging (spent 50 on coffee)
Branch 3: General financial chat (how can I save money?)
Advanced Reporting
Generate instant reports by messaging:
today's report ‚Üí Daily income/expense summary
March vs April report ‚Üí Monthly comparisons with percentages
show groceries spending ‚Üí Category-specific analysis
Automatic daily summaries at your preferred time
Database Integration
All transactions are stored in PostgreSQL with proper schema:
CREATE TABLE financial_transactions (
 date DATE NOT NULL,
 category TEXT NOT NULL, 
 type TEXT NOT NULL,
 amount NUMERIC(12,2) NOT NULL,
 person TEXT
);
Setup
Prerequisites
n8n instance (self-hosted or n8n.cloud)
WhatsApp Business Cloud API credentials
PostgreSQL database (version 12+)
OpenRouter API key for AI processing
Quick Setup Steps
Import the workflow template into your n8n instance
Configure credentials:
WhatsApp Business Cloud API (App Token + Phone Number ID)
PostgreSQL connection details
OpenRouter API key for AI processing
Create database table using the provided SQL schema
Test the connection by sending a sample message
Customize the scheduled report timing (default: 8 AM daily)
Verification Checklist
[ ] WhatsApp webhook receives messages
[ ] AI correctly parses transaction messages
[ ] Database insertions work properly
[ ] Confirmation messages are sent back
[ ] Reports generate with accurate data
How to customize this workflow to your needs
AI Model Configuration
Default: Uses OpenRouter with GPT-3.5-turbo for cost efficiency
Upgrade: Switch to GPT-4 or Claude for better accuracy
Local: Replace with self-hosted Ollama for complete privacy
Database Options
PostgreSQL: Recommended for production use
Google Sheets: Alternative for simpler setups (nodes included)
MySQL/SQLite: Easily adaptable with minor SQL modifications
Message Classification
Customize the classification system:
0: Reports (modify SQL queries for different analytics)
1: Transactions (adjust parsing rules for your language/currency)
2: Chat (customize AI responses for financial advice)
Reporting Customization
Scheduled reports: Change timing, format, and recipients
Custom periods: Add quarterly, yearly, or custom date ranges
Categories: Modify auto-categorization rules for your spending patterns
Currency: Update formatting for your local currency
Advanced Features
Multi-user support: Add user identification for family/team use
Receipt photos: Extend workflow to process image receipts via OCR
Budgets: Add budget tracking and overspend alerts
Integrations: Connect to banks via Plaid or other financial APIs
Complete Package Included
When you download this template, you get everything needed for immediate implementation:
Ready-to-Use n8n Workflow
Fully configured nodes with descriptive names explaining each step
Color-coded sticky notes throughout the workflow explaining:
What each branch does (Reports/Transactions/Chat)
How the AI classification works
Database connection requirements
Error handling and troubleshooting tips
Comprehensive Documentation Bundle
Quick Start Guide: Get running in under 10 minutes
Detailed Setup Guide: Complete configuration walkthrough with screenshots
Branch Explanation Guide: Deep dive into each processing branch:
Branch 0: Reports & Analytics - SQL queries and formatting
Branch 1: Transaction Processing - AI parsing and database insertion
Branch 2: Financial Chat - AI responses and conversation handling
Built-in Workflow Documentation
Sticky notes at every major step explaining the logic
Node descriptions that clarify what each component does
Visual flow indicators showing message routing paths
Dependency callouts highlighting required credentials and connections
Technical Implementation Details
Database schema with complete SQL commands
API configuration examples for all external services
Troubleshooting checklist for common setup issues
Performance optimization recommendations
Bonus Resources
Example message templates to test each workflow branch
Sample data for testing reports and analytics
Customization recipes for common modifications
Integration patterns for extending functionality
Example Usage
Log Expenses:
Spent 1200 on rent this month
Paid 45 for gas at Shell
Coffee 5.50 at Starbucks
Log Income:
Received 5000 salary from Company ABC
Freelance payment 800 from Client XYZ
Generate Reports:
today's summary
show this week's expenses
compare March vs April spending
how much on food this month?
Expected Responses:
‚úÖ Logged: expense | Rent | ‚Çπ1,200.00 | Landlord
‚úÖ Logged: income | salary |‚Çπ12,000.00|company
üìä Today's Summary:
Income: ‚Çπ0.00
Expenses: ‚Çπ1,245.50
Savings: -‚Çπ1,245.50
üìà March vs April:
Expenses: ‚Çπ15,000 vs ‚Çπ12,500 (-16.7%)
Top categories: Rent, Food, Transport"
üèõÔ∏è Daily US Congress Members Stock Trades Report via Firecrawl + OpenAI + Gmail,https://n8n.io/workflows/4509-daily-us-congress-members-stock-trades-report-via-firecrawl-openai-gmail/,"üì¨ What This Workflow Does
This workflow automatically scrapes recent high-value congressional stock trades from Quiver Quantitative, summarizes the key transactions, and delivers a neatly formatted report to your inbox ‚Äî every single day.
It combines Firecrawl's powerful content extraction, OpenAI's GPT formatting, and n8n's automation engine to turn raw HTML data into a digestible, human-readable email.
Watch Full Tutorial on how to build this workflow here:
https://www.youtube.com/@Automatewithmarc
üîß How It Works
üïí Schedule Trigger
Fires daily at a set hour (e.g., 6 PM) to begin the data pipeline.
üî• Firecrawl Extract API (POST)
Targets the Quiver Quantitative ‚ÄúCongress Trading‚Äù page and sends a structured prompt asking for all trades over $50K in the past month.
‚è≥ Wait Node
Allows time for Firecrawl to finish processing before retrieving results.
üì• Firecrawl Get Result API (GET)
Retrieves the extracted and structured data.
üß† OpenAI Chat Model (GPT-4o)
Formats the raw trading data into a readable summary that includes:
Date of Transaction
Stock/Asset traded
Amount
Congress member‚Äôs name and political party
üìß Gmail Node
Sends the summary to your inbox with the subject ‚ÄúCongress Trade Updates - QQ‚Äù.
üß† Why This is Useful
Congressional trading activity often reveals valuable signals ‚Äî especially when high-value trades are made.
This workflow:
Saves time manually tracking Quiver Quant updates
Converts complex tables into a daily, readable email
Keeps investors, researchers, and newsrooms in the loop ‚Äî hands-free
üõ† Requirements
Firecrawl API Key (with extract access)
OpenAI API Key
Gmail OAuth2 credentials
n8n (self-hosted or cloud)
üí¨ Sample Output:
Congress Trade Summary ‚Äì May 21
Nancy Pelosi (D) sold TSLA for $85,000 on April 28
John Raynor (R) purchased AAPL worth $120,000 on May 2
... and more
ü™ú Setup Steps
Add your Firecrawl, OpenAI, and Gmail credentials in n8n.
Adjust the schedule node to your desired time.
Customize the OpenAI system prompt if you want a different summary style.
Deploy the workflow ‚Äî and enjoy your daily edge."
"AI-Powered PDF Invoice Parser with Google Drive, Google Sheets & OpenAI",https://n8n.io/workflows/4451-ai-powered-pdf-invoice-parser-with-google-drive-google-sheets-and-openai/,"Who is this for?
This workflow is perfect for:
Companies that manage invoices through Google Drive
Business owners who want to minimize manual data entry and maximize accuracy
Accounting teams and finance departments seeking to automate invoice processing
What problem is this workflow solving?
Processing invoices manually is time-consuming, error-prone, and inconsistent. This workflow solves those issues by:
Automating invoice processing from detection to data extraction to storage
Improving accuracy by using AI to extract key invoice data fields reliably
Reducing human workload while maintaining compliance and consistency
What this workflow does
This workflow creates a fully automated invoice processing system by:
Monitoring a Google Drive folder for new PDF invoices in real time
Downloading the PDF files and extracting their content using OCR technology
Using AI (OpenAI) to parse and extract key invoice fields such as invoice number, date, total amount, vendor name, itemized details, tax, and category
Validating the extracted data to ensure compliance with a structured JSON schema
Storing structured data in Google Sheets for easy access, review, and reporting
Key Features:
AI-powered extraction handles both text-based and scanned PDF invoices
Provides a structured, searchable invoice database in Google Sheets
Configured to run as frequently as the user needs, ensuring timely processing.
Setup
Copy the Google Sheet template here:
üëâ PDF Invoice Parser ‚Äì Google Sheet Template
Connect your Google Drive account to the Drive Trigger and File Download nodes
Add your OpenAI API key in the AI Parser node
Link the Google Sheet in the final storage node
Drop a test invoice PDF into the monitored Drive folder
Required Credentials:
OpenAI API Key
Google Drive Credentials
Google Sheets Credentials
How to customize this workflow to your needs
Modify the polling interval (default: every minute) for higher/lower frequency.
Integrate with your accounting software by adding nodes (e.g., QuickBooks, Xero).
Use alternative LLM such as Gemini, Claude."
RAG Chatbot for Company Documents using Google Drive and Gemini,https://n8n.io/workflows/2753-rag-chatbot-for-company-documents-using-google-drive-and-gemini/,"This workflow implements a Retrieval Augmented Generation (RAG) chatbot that answers employee questions based on company documents stored in Google Drive. It automatically indexes new or updated documents in a Pinecone vector database, allowing the chatbot to provide accurate and up-to-date information. The workflow uses Google's Gemini AI for both embeddings and response generation.
How it works
The workflow uses two Google Drive Trigger nodes: one for detecting new files added to a specified Google Drive folder, and another for detecting file updates in that same folder.
Automated Indexing: When a new or updated document is detected
The Google Drive node downloads the file.
The Default Data Loader node loads the document content.
The Recursive Character Text Splitter node breaks the document into smaller text chunks.
The Embeddings Google Gemini node generates embeddings for each text chunk using the text-embedding-004 model.
The Pinecone Vector Store node indexes the text chunks and their embeddings in a specified Pinecone index.
7.The Chat Trigger node receives user questions through a chat interface. The user's question is passed to an AI Agent node.
The AI Agent node uses a Vector Store Tool node, linked to a Pinecone Vector Store node in query mode, to retrieve relevant text chunks from Pinecone based on the user's question.
The AI Agent sends the retrieved information and the user's question to the Google Gemini Chat Model (gemini-pro).
The Google Gemini Chat Model generates a comprehensive and informative answer based on the retrieved documents.
A Window Buffer Memory node connected to the AI Agent provides short-term memory, allowing for more natural and context-aware conversations.
Set up steps
Google Cloud Project and Vertex AI API:
Create a Google Cloud project.
Enable the Vertex AI API for your project.
Google AI API Key:
Obtain a Google AI API key from Google AI Studio.
Pinecone Account:
Create a free account on the Pinecone website.
Obtain your API key from your Pinecone dashboard.
Create an index named company-files in your Pinecone project.
Google Drive:
Create a dedicated folder in your Google Drive where company documents will be stored.
Credentials in n8n: Configure credentials in your n8n environment for:
Google Drive OAuth2
Google Gemini(PaLM) Api (using your Google AI API key)
Pinecone API (using your Pinecone API key)
Import the Workflow:
Import this workflow into your n8n instance.
Configure the Workflow:
Update both Google Drive Trigger nodes to watch the specific folder you created in your Google Drive.
Configure the Pinecone Vector Store nodes to use your company-files index."
Translate & Repost Twitter Threads in Multiple Languages with OpenAI,https://n8n.io/workflows/4233-translate-and-repost-twitter-threads-in-multiple-languages-with-openai/,"Twitter Thread (Flood) Translator & Poster
What it does
Thread Extraction: Automatically detects and extracts all tweets from a provided Twitter thread (flood) link.
Translation: Translates each extracted tweet into your target language using OpenAI.
Rewriting: Rewrites each translated tweet to maintain the original meaning while improving clarity or style.
Automated Posting: Posts the rewritten tweets as a new thread on Twitter using twitterapi.io, preserving the original thread structure.
How it works
Accepts a Twitter thread (flood) link as input.
Extracts all tweets from the thread in their original order.
Each tweet is sent to OpenAI for translation into your desired language.
The translated tweets are then rewritten for clarity and natural flow, while keeping the original meaning intact.
The processed tweets are automatically posted as a new thread on your Twitter account via twitterapi.io.
Setup Steps
Create a Notion Database:
Set up a database page in Notion to store and manage your Twitter links and workflow data.
Configure Notion Integration:
Add the created database page ID to the Notion nodes in your workflow.
Set Twitter API Credentials:
Add your twitterapi.io API key to the relevant nodes.
Add Twitter Account Details:
Enter your Twitter account username/email and password for authentication.
Set Up OpenAI Credentials:
Provide your OpenAI API credentials to enable translation and rewriting.
Subworkflow Integration:
Create a separate workflow for subworkflow logic and call it using the Execute Workflow node for modular automation.
Set Desired Language & Thread Link:
Change the target language and Twitter thread (flood) link directly in the Manual Trigger node to customize each run.
Benefits
Ultra Low Cost: Total cost for a 15-tweet thread (flood) is just $0.016 USD ($0.015 for twitterapi.io + $0.001 for OpenAI API). (Actual cost may vary depending on the density of tweets in the thread.)
End-to-End Automation: Go from thread extraction to translation, rewriting, and reposting-all in one workflow.
Multilingual Support: Effortlessly translate and republish Twitter threads in any supported language.
Note: Detailed configuration instructions and node explanations are included as sticky notes within the workflow canvas.
Ideal for:
Content creators looking to reach new audiences by translating and republishing Twitter threads
Social media managers automating multilingual content workflows
Anyone wanting to streamline the process of thread extraction, translation, and posting
Notes
This workflow is not able to post images or videos to Twitter-it handles text-only threads."
Create a Branded AI-Powered Website Chatbot,https://n8n.io/workflows/2786-create-a-branded-ai-powered-website-chatbot/,"Create a Branded AI Website Chatbot
Engage website visitors with an intelligent chat widget powered by OpenAI. This template includes:
üí¨ Natural conversation handling
üìÖ Microsoft Outlook calendar integration
üìù Lead capture and information gathering
üîÑ Human handoff capabilities
Simply add a JavaScript snippet to your website and configure the workflow to match your needs. Follow our detailed setup guide to get started in minutes.
Note: Widget includes a ""Powered By"" affiliate link"
Create a Branded AI Chatbot for Websites with Flowise Multi-Agent Chatflows,https://n8n.io/workflows/4651-create-a-branded-ai-chatbot-for-websites-with-flowise-multi-agent-chatflows/,"This workflow integrates Flowise Multi-Agent Chatflows into a custom-branded n8n chatbot, enabling real-time interaction between users and AI agents powered by large language models (LLMs).
Key Advantages:
‚úÖ Easy Integration with Flowise:
Uses a low-code HTTP node to send user questions to Flowise's API (/api/v1/prediction/FLOWISE_ID) and receive intelligent responses.
Supports multi-agent chatflows, allowing for complex, dynamic interactions.
üé® Customizable Chatbot UI:
Includes pre-built JavaScript for embedding the n8n chatbot into any website.
Provides customization options such as welcome messages, branding, placeholder text, chat modes (e.g., popup or embedded), and language support.
üîê Secure & Configurable:
Authorization via Bearer token headers for Flowise API access.
Clearly marked notes in the workflow for setting environment variables like FLOWISE_URL and FLOW_ID.
How It Works
Chat Trigger: The workflow starts with the When chat message received node, which acts as a webhook to receive incoming chat messages from users.
HTTP Request to Flowise: The received message is forwarded to the Flowise node, which sends a POST request to a Flowise API endpoint (https://FLOWISEURL/api/v1/prediction/FLOWISE_ID). The request includes the user's input as a JSON payload ({""question"": ""{{ $json.chatInput }}""}) and uses HTTP header authentication (e.g., Authorization: Bearer FLOWSIE_API).
Response Handling: The response from Flowise is passed to the Edit Fields node, which maps the output ($json.text) for further processing or display.
Set Up Steps
Configure Flowise Integration:
Replace FLOWISEURL and FLOWISE_ID in the HTTP Request node with your Flowise instance URL and flow ID.
Ensure the Authorization header is set correctly in the credentials (e.g., Bearer FLOWSIE_API).
Embed n8n Chatbot:
Use the provided JavaScript snippet in the sticky notes to embed the n8n chatbot on your website. Replace YOUR_PRODUCTION_WEBHOOK_URL with the webhook URL generated by the When chat message received node.
Customize the chatbot's appearance and behavior (e.g., welcome messages, language, UI elements) using the createChat configuration options.
Optional Branding:
Adjust the sticky note examples to include branding details, such as custom messages, colors, or metadata for the chatbot.
Activate Workflow:
Toggle the workflow to ""Active"" in n8n and test the chat functionality end-to-end.
Ideal Use Cases:
Embedding branded AI assistants into websites.
Connecting Flowise-powered agents with customer support chatbots.
Creating dynamic, smart conversational flows with LLMs via n8n automation.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Automate B2B Lead Generation with Apollo, GPT-4o Scoring, and Brevo Email Outreach",https://n8n.io/workflows/4539-automate-b2b-lead-generation-with-apollo-gpt-4o-scoring-and-brevo-email-outreach/,"Lead Generation, AI Scoring, Personalized Email Creation and Outreach
Overview
This comprehensive automation flow helps you scale your outreach to potential customers through intelligent lead generation and personalized email campaigns. The system combines web scraping, AI-powered lead scoring, and automated email sequences to maximize your conversion rates.
Key Features
Bulk Lead Scraping from Apollo.io
Intelligent Website Analysis with automated scraping
AI-Powered Lead Scoring for prioritization
Personalized Email Generation using AI
Automated Email Outreach with tracking
Real-time Event Monitoring via Brevo webhooks
Centralized Data Management with NocoDB
System Architecture
The workflow integrates multiple platforms to create a seamless lead generation pipeline:
NocoDB - Central database and CRM
Apollo.io - Lead data source
Crawl4AI - Website content analysis
OpenAI - Lead scoring and email personalization
Brevo - Email delivery and tracking
Setup Requirements
Prerequisites
Before starting, ensure you have accounts for the following services:
NocoDB (Database & CRM)
Apify (Web scraping platform)
OpenAI (AI processing)
Brevo (Email delivery service)
üí° Pro Tip: For advanced users, consider self-hosting NocoDB and Crawl4AI to significantly reduce operational costs.
Step-by-Step Setup
1. NocoDB Database Configuration
Create Your NocoDB Account
Sign up at nocodb.com
Create a new workspace
Set up your first base (database)
Database Schema Setup
Create a new table with the following field structure:
Field Name Field Type Configuration
first_name Single Line Text -
last_name Single Line Text -
email Email -
headline Single Line Text -
linkedin_url URL -
industry Single Line Text -
organization_name Single Line Text -
organization_website URL -
organization_size Number -
organization_linkedin_url URL -
organization_facebook_url URL -
market_cap Single Line Text -
job_title Single Line Text -
country Single Line Text -
city Single Line Text -
lead_status Single Select Options: entered, processed, email_created, contacted, trash, failed_to_process, opened_email, warm
website_summary Long Text -
score Number Default: 0
organization_description Long Text -
primary_phone Phone Number -
keywords Single Line Text -
email_subject Single Line Text -
email_body Long Text -
email_opened_times Number Default: 0
personal_email Single Line Text -
2. External Service Setup
Apify Configuration
Create account at apify.com
Navigate to Settings ‚Üí API tokens
Generate new API token
Save token securely for automation setup
OpenAI API Setup
Visit platform.openai.com
Go to API Keys section
Create new secret key
Store API key securely
Ensure billing is configured for API usage
Brevo Email Service
Sign up at brevo.com
Complete account verification
Navigate to SMTP & API settings
Generate API key for integration
Configure sender authentication (SPF/DKIM)
Workflow Process
Phase 1: Lead Discovery & Data Collection
Apollo Scraping - Extract lead data based on target criteria
Data Import - Populate NocoDB with lead information
Status Update - Mark leads as entered
Phase 2: Website Analysis & Lead Scoring
Website Scraping - Analyze company websites using Crawl4AI
Content Processing - Extract key business information
AI Scoring - Evaluate lead quality using OpenAI
Database Update - Store analysis results and scores
Phase 3: Email Personalization & Outreach
Content Generation - Create personalized email content
Email Preparation - Format messages for delivery
Automated Sending - Deploy via Brevo
Status Tracking - Monitor delivery and engagement
Phase 4: Performance Monitoring
Webhook Processing - Capture Brevo email events
Engagement Tracking - Update open/click metrics
Lead Qualification - Adjust lead status based on engagement
Pipeline Management - Maintain lead progression
Lead Status Workflow
entered ‚Üí processed ‚Üí email_created ‚Üí contacted ‚Üí opened_email ‚Üí warm
    ‚Üì
failed_to_process / trash (for low-quality leads)
Status Definitions
entered: Initial lead data captured
processed: Website analyzed and scored
email_created: Personalized email generated
contacted: Email successfully sent
opened_email: Recipient opened the email
warm: High engagement, ready for follow-up
failed_to_process: Technical issues during processing
trash: Low-quality lead, removed from active pipeline
Best Practices
Data Quality
Regularly clean and deduplicate your lead database
Monitor lead scores to refine your targeting criteria
Archive or remove inactive leads to maintain performance
Email Deliverability
Warm up your sending domain before high-volume campaigns
Monitor bounce rates and adjust list quality
Implement proper SPF, DKIM, and DMARC records
Compliance
Ensure GDPR/CAN-SPAM compliance in all communications
Provide clear unsubscribe mechanisms
Respect recipient preferences and opt-outs
Performance Optimization
A/B test email subject lines and content
Analyze open rates and adjust sending times
Segment leads based on engagement levels for targeted follow-up
Next Steps
Once your setup is complete, you'll have a powerful, automated lead generation system that can:
Process hundreds of leads simultaneously
Provide intelligent lead prioritization
Generate personalized outreach at scale
Track and optimize campaign performance
Ready to transform your lead generation process? Start with the NocoDB setup and work through each integration step by step."
"Build an AI Powered Phone Agent üìûü§ñ with Retell, Google Calendar and RAG",https://n8n.io/workflows/3563-build-an-ai-powered-phone-agent-with-retell-google-calendar-and-rag/,"This Workflow simulates an AI-powered phone agent with two main functions:
üìÖ Appointment Booking ‚Äì It can schedule appointments directly into Google Calendar.
üß† RAG-based Information Retrieval ‚Äì It provides answers using a Retrieval-Augmented Generation (RAG) system. For example, it can respond to questions such as store opening hours, return policies, or product details.
The guide also explains how to purchase a dedicated phone number (with a +1 prefix) and link it to the AI agent. This setup is cost-effective, as it uses a FREE $10 credit to operate without additional charges in the beginning.
‚ú® Advantages
üïê 24/7 Availability ‚Äì The AI agent can answer calls and assist customers at any time.
ü§ñ Automation ‚Äì It reduces the workload on human staff by handling repetitive tasks like appointment scheduling and FAQ responses.
üîå Easy Integration ‚Äì Built with n8n, it‚Äôs flexible and customizable for various platforms and tools.
üí∏ Low-cost Setup ‚Äì Using the free credit, businesses can get started without an upfront investment.
üì¶ Use Cases
üõç E-commerce ‚Äì Answer common product questions or order inquiries.
üè¨ Retail Stores ‚Äì Provide store hours, address info, and return policies.
üçΩ Restaurants ‚Äì Take reservations or share menu information.
üíº Service Providers ‚Äì Book appointments or consultations.
üìû Any Local Business ‚Äì Offer phone support without needing a live operator.
How It Works
This Workflow simulates an AI-powered phone agent with two primary functions:
Appointment Booking
The workflow captures call events (e.g., call_ended or call_analyzed) and extracts key details (transcript, caller info, duration, etc.).
Using OpenAI, it summarizes the conversation and parses structured data (e.g., names, contact info, dates).
For scheduling, it converts user-provided dates into Google Calendar-compatible formats and creates events automatically.
RAG-Based Information Retrieval
When a query is received (e.g., store hours, product details), the workflow retrieves relevant information from a Qdrant vector store.
An AI agent processes the query using the retrieved data and responds via a webhook, ensuring accurate, context-aware answers.
Set Up Steps
Prepare Qdrant Vector Store
Create/refresh a Qdrant collection (via HTTP requests).
Upload and vectorize documents (e.g., from Google Drive) using OpenAI embeddings.
Configure RetellAI Agent
Sign up for RetellAI, create an agent, and set the webhook URLs (n8n_call for call events, n8n_rag_function for RAG queries).
Purchase a Twilio phone number and link it to the agent.
n8n Workflow Setup
Connect OpenAI, Qdrant, Google Calendar, and Telegram nodes with credentials.
Customize prompts for summarization, date parsing, and RAG responses.
Test the workflow to ensure data flows from call events ‚Üí processing ‚Üí actions (e.g., calendar bookings, Telegram alerts).
Deploy
Trigger the workflow via RetellAI webhooks during calls.
Monitor outputs (e.g., call summaries in Telegram, calendar events).
Note: Replace placeholders (e.g., QDRANTURL, COLLECTION, CHAT_ID) with actual values.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Payment Processing and Order Tracking with YooKassa and Google Sheets,https://n8n.io/workflows/4749-payment-processing-and-order-tracking-with-yookassa-and-google-sheets/,"Accept YooKassa payments and log transactions in Google Sheets
üßæ Summary
This workflow allows you to accept online payments via YooKassa and log both orders and transactions in Google Sheets ‚Äî all without writing a single line of code. It supports full payment flow: product selection, payment initiation, webhook processing, refund updates, and payment status checks.
üë• Who is this for?
This template is ideal for:
Online stores with simple checkout flows
Sellers of digital products or info-courses
Entrepreneurs using Telegram bots or web forms
Anyone needing quick payment integration with Google Sheets tracking
üéØ What problem does this workflow solve?
Setting up online payments usually requires backend infrastructure. This no-code solution automates the entire payment flow:
Handles product listing and price retrieval
Initiates payments with email and return URL
Listens for payment.succeeded and refund.succeeded events
Records every action into structured Google Sheets
‚öôÔ∏è What this workflow does
1. GET /products
Returns a sorted list of products from a Google Sheet (products).
2. POST /payment
Validates required fields (product_id, email, return_url)
Checks email format
Fetches product data from products
Generates a unique idempotence key
Sends a request to YooKassa API
Saves the order into the orders sheet
Returns a payment confirmation link
3. POST /yoomoney
Webhook to process payment/refund events:
On payment.succeeded, adds entry to transactions
On refund.succeeded, updates transaction status
4. GET /status/:id
Returns real-time payment status from YooKassa
üöÄ Setup
Connect credentials:
Google Sheets (OAuth2)
YooKassa (Basic Auth using shopId and secretKey)
Update the following Google Sheets:
products: should contain product_id, title, price
orders: for saving confirmed purchases
transactions: for logging all successful or refunded payments
Test endpoints using any HTTP client:
Example payload for /payment:
{
  ""product_id"": ""abc123"",
  ""email"": ""user@example.com"",
  ""return_url"": ""https://your.site/success""
}
üîß How to customize this workflow
Add delivery logic (e.g., email with product link after successful payment)
Replace Google Sheets with a database (e.g., PostgreSQL)
Connect Telegram or other messengers for post-payment notifications
Add promo codes, discounts, or subscriptions logic
üíº Use cases
Simple online checkouts
Telegram bots selling access
Educational product sales
MVP e-commerce flows
Donation or membership payments
üìé Notes
‚úÖ Includes Sticky Notes for sections
‚úÖ Includes error handling and validation
‚úÖ No custom code needed except UUID generation"
Transcribe Voice Messages from Telegram using OpenAI Whisper-1,https://n8n.io/workflows/4528-transcribe-voice-messages-from-telegram-using-openai-whisper-1/,"This n8n workflow processes incoming Telegram messages, differentiating between text and voice messages.
How it works:
Message Trigger: The workflow initiates when a new message is received via the Telegram ""Message Trigger"" node.
Switch Node: This node acts as a router. It examines the incoming message:
If the message is text, it directs the flow along the ""text"" branch.
If the message contains voice, it directs the flow along the ""voice"" branch.
Get Audio File: For audio messages, this node downloads the audio file from Telegram.
Transcribe Audio: The downloaded audio file is then sent to an ""OpenAI Transcribe Recording"" node, which uses OpenAI's whisper-1 speech-to-text model to convert the audio into a text transcript.
Send Transcription Message: Regardless of whether the original message was text or transcribed audio, the final text content is then passed to a ""Send transcription message"" node.
Setup Requirements:
Telegram Bot Token: You will need a Telegram bot token configured in the ""Message Trigger"" node to receive messages.
OpenAI API Key: An OpenAI API key is required for the ""Transcribe audio"" node to perform speech transcription.
Additional Notes:
This workflow provides a foundational step for building more complex AI-driven applications. The transcribed text or original text message can be easily piped into an AI agent (e.g., a large language model) for analysis, response generation, or interaction with other tools, extending the bot's capabilities beyond simple message reception and transcription.
üëâ Need Help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
"Multi-Agent AI Clinic Management with WhatsApp, Telegram, and Google Calendar",https://n8n.io/workflows/3694-multi-agent-ai-clinic-management-with-whatsapp-telegram-and-google-calendar/,"Healthcare Clinic Assistant with WhatsApp and Telegram Integration
Version: 1.1.0
n8n Version: 1.88.0+
License: MIT
üìã Description
A comprehensive and modular automation workflow designed for healthcare clinics.
It manages patient communication, appointment scheduling, confirmations, rescheduling, internal tasks, and media processing by integrating WhatsApp, Telegram, Google Calendar, and Google Tasks, combined with AI-powered agents for maximum efficiency.
This system guarantees proactive communication with patients, streamlined internal clinic management, and consistent data synchronization across platforms.
üåü Key Features
ü§ñ AI-Powered Specialized Agents:
Distinct agents handle WhatsApp patient support, appointment confirmations, and internal rescheduling tasks.
üì± Omnichannel Communication:
Handles patient interactions via WhatsApp and staff commands via Telegram.
üìÖ Google Calendar Appointment Management:
Full synchronization for creating, updating, canceling, and confirming appointments.
üìã Task Management with Google Tasks:
Manages shopping lists and administrative tasks efficiently through staff Telegram requests.
üîî Automated Appointment Reminders:
Daily-triggered system proactively sends WhatsApp confirmations to patients for next-day appointments.
üñºÔ∏è Intelligent Media Processing:
Transcribes audios, extracts text from images, and processes documents using OpenAI and OpenRouter AI models.
üõ°Ô∏è Escalation to Human Support:
Automatically detects sensitive or urgent cases and escalates them to a human agent when needed.
üè• Use Cases
Patient Communication:
Respond to inquiries, schedule, reschedule, and confirm appointments seamlessly via WhatsApp.
Internal Clinic Operations:
Allow staff to modify appointments or add shopping list reminders directly from Telegram.
Appointment Confirmation System:
Automatically contacts patients one day prior to appointments for confirmation or rescheduling.
Task and Reminder Management:
Keeps clinic operations organized through automatic task management with Google Tasks.
üõ†Ô∏è Technical Implementation
WhatsApp Patient Interaction Flow
Webhook Reception: Incoming WhatsApp messages captured via Evolution API webhook.
Message Classification: Intelligent routing of messages based on content type (text, image, audio, document).
Media Content Processing:
Audios: Download, convert, and transcribe via OpenAI Whisper.
Images: Analyze and extract text/descriptions with OpenAI Vision model.
Patient Request Handling: Specialized WhatsApp assistant responds appropriately using AI prompts.
Outbound Message Formatting: Ensures messages comply with WhatsApp format standards.
Message Delivery: Sends responses back via Evolution API.
Telegram Staff Management Flow
Telegram Webhook Reception: Captures messages from authorized staff accounts.
Internal Assistant Processing:
Appointment Rescheduling: Identifies and updates appointments through MCP Google Calendar.
Task Creation: Adds new entries to the clinic's shopping list using Google Tasks.
Notifications and Confirmations: Sends confirmations back to staff through Telegram.
Appointment Reminder System
Daily Trigger Activation: Fires every weekday at 08:00 AM.
Calendar Scraping: Lists next day's appointments from Google Calendar.
Patient Contact: Sends WhatsApp confirmation messages for each appointment.
Response Management: Redirects confirmation or rescheduling replies to appropriate agents.
‚öôÔ∏è Setup Instructions
Import the Workflow
n8n ‚Üí Workflows ‚Üí Import from File ‚Üí Upload this JSON file.
Configure Credentials
Evolution API (WhatsApp Communication)
Telegram Bot API (Staff Communication)
Google Calendar OAuth2 (Appointment Management)
Google Tasks OAuth2 (Task Management)
OpenAI and OpenRouter APIs (AI Agents)
PostgreSQL Database (Chat Memory)
Set Sensitive Variables
Replace placeholder values:
{sua inst√¢ncia aqui} ‚Üí Evolution API instance name
{n√∫mero_whatsapp} ‚Üí WhatsApp numbers
{url_do_servidor} ‚Üí Server URLs
{a sua apikey aqui} ‚Üí API keys
{seu_calendario} ‚Üí Google Calendar ID
Customize AI Prompts
Adjust system prompts to fit your clinic‚Äôs tone, service style, and patient communication guidelines.
Set clinic operating hours, escalation rules, and cancellation procedures in AI prompts.
Activate and Test
Simulate patient messages via WhatsApp.
Test Telegram commands from staff members.
Validate daily appointment reminders using the scheduled trigger.
üè∑Ô∏è Tags
Healthcare Clinic Management WhatsApp Integration Telegram Bot Appointment Scheduling Google Calendar Google Tasks AI Agents n8n Automation
üìö Technical Notes
PostgreSQL is used for persistent chat memory across sessions.
Multiple AI Models Used:
OpenAI GPT-4.1-nano
OpenAI GPT-4.1-mini
Google Gemini 2.0 and 2.5
Full media content processing supported (audio, image, text).
Compliant escalation workflows ensure patient safety and proper handoff to human staff when necessary.
All sensitive patient data are securely stored inside calendar event descriptions for easy retrieval by agents.
üìú License
This workflow is provided under the MIT License.
Feel free to adapt and customize it for your clinic‚Äôs specific needs."
MCP Supabase Server for AI Agent with RAG & Multi-Tenant CRUD,https://n8n.io/workflows/3675-mcp-supabase-server-for-ai-agent-with-rag-and-multi-tenant-crud/,"Supabase AI Agent with RAG & Multi-Tenant CRUD
Version: 1.0.0
n8n Version: 1.88.0+
Author: Koresolucoes
License: MIT
Description
A stateful AI agent workflow powered by Supabase and Retrieval-Augmented Generation (RAG). Enables persistent memory, dynamic CRUD operations, and multi-tenant data isolation for AI-driven applications like customer support, task orchestration, and knowledge management.
Key Features:
üß† RAG Integration: Leverages OpenAI embeddings and Supabase vector search for context-aware responses.
üóÉÔ∏è Full CRUD: Manage agent_messages, agent_tasks, agent_status, and agent_knowledge in real time.
üì§ Multi-Tenant Ready: Supports per-user/organization data isolation via dynamic table names and webhooks.
üîí Secure: Role-based access control via Supabase Row Level Security (RLS).
Use Cases
Customer Support Chatbots: Persist conversation history and resolve queries using institutional knowledge.
Automated Task Management: Track and update task statuses dynamically.
Knowledge Repositories: Store and retrieve domain-specific information for AI agents.
Instructions
1. Import Template
Go to n8n > Templates > Import from File and upload this workflow.
2. Configure Credentials
Add your Supabase and OpenAI API keys under Settings > Credentials.
3. Set Up Multi-Tenancy (Optional)
Dynamic Webhook Path:
Replace the default webhook path with /mcp/tool/supabase/:userId to enable per-user routing.
Table Names:
Use a Set Node to dynamically generate table names (e.g., agent_messages_{{userId}}).
4. Activate & Test
Enable the workflow and send test requests to the webhook URL.
Tags
AI Agent RAG Supabase CRUD Multi-Tenant OpenAI Automation
Screenshots

License
This template is licensed under the MIT License."
AI Prospect Researcher +ISCP only need Company Name and Domain,https://n8n.io/workflows/4882-ai-prospect-researcher-iscp-only-need-company-name-and-domain/,"AI Prospect Researcher +ISCP only from Domain and Company Name
This workflow finds available email addresses, researches the company using public data (LinkedIn, website scraping, etc.), calculates the ISCP score, and delivers a complete, ready-to-use report.
This workflow is designed for professionals and teams seeking to scale their B2B research with comprehensive company intelligence. It automates the full prospect analysis process ‚Äî from extracting contact data and scraping website content to gathering market insights and generating structured reports with GPT-4.
Using a combination of Hunter.io, Perplexity AI, Airtop, and OpenAI, this system creates a detailed Google Doc profile for each company, complete with decision-maker identification and ISCP scoring. Whether you're conducting market research, qualifying leads, or preparing investor briefings ‚Äî this tool delivers actionable business intelligence in minutes, without manual data collection.
The process is battle-tested, with built-in error handling, and suitable for sales teams, market researchers, or founders conducting competitive analysis.
How it works
Once triggered (via form or spreadsheet), the workflow:
Uses Hunter.io to discover professional emails associated with the domain
Leverages Perplexity AI to gather web-wide company intelligence and industry context
Scrapes the company website using Airtop to extract services, products, and key pages
Optionally enriches with LinkedIn data via Apify (company size, specialties)
Processes all inputs through GPT-4 to:
Generate an executive summary
Identify potential decision-makers
Calculate an Ideal Service Customer Profile (ISCP) match score
Produces a finalized report in Google Docs with:
Company overview
Key contacts
Market positioning
Recommended outreach approach
This is a turnkey solution for teams who need deep company profiles without the hours of manual research and data stitching."
"Invoice Verification and Validation with Gmail, Drive, Sheets and OCR AI",https://n8n.io/workflows/4860-invoice-verification-and-validation-with-gmail-drive-sheets-and-ocr-ai/,"üìù Description (Full, User-Centric & Sales-Driven):
Tired of manually verifying purchase order invoices every single day? This plug-and-play n8n automation template saves your accounts team hours of work by automatically downloading, storing, extracting, and validating invoice data against your master item sheet ‚Äî all without human intervention.
Perfect for SMEs, startups, procurement teams, and accounts departments, this automation handles PO invoice verification from email to final validation with 99% accuracy.
üöÄ What This Automation Does End-to-End:
üì• Invoice Retrieval & Organization:
Runs Monday to Saturday, 8 hours/day
Reads invoices from specific vendor email(s) (e.g., abc@company.com)
Extracts the invoice date from the email subject
Automatically creates month & day-wise folders in Google Drive
Uploads each invoice PDF into its respective folder
üîç AI + OCR Data Extraction & Validation:
Uses OCR to extract item data (name, quantity, rate, HSN, etc.) from each invoice PDF
Converts messy PDF data into clean structured format using AI Agent Model
Stores the data into a dedicated Google Sheet: ""Store Invoice Data""
‚úÖ Invoice vs. Master Data Validation:
Automatically fetches a second Google Sheet: ""Store Master Data""
Compares each invoice item line-by-line with the master sheet:
Checks for Item Code, Name, Price mismatches
Calculates price differences and total loss/gain per invoice
Marks each line as Matched / Mismatched with notes
üì¨ Final Reporting:
Sends an automated summary email once all invoices are processed
Alerts if any mismatches found, allowing for early intervention
üìÇ Example Output:
Check how data is validated in real time:
üîó Store Invoice Data Sheet
üîó Store Master Data Sheet
üë• Who Is This For?
Accounts & Finance Teams
Procurement Departments
Growing Startups with Vendors
ERP/Automation Enthusiasts
Anyone tired of manually checking vendor bills
üì¶ What You‚Äôll Get:
Ready-to-import n8n workflow (JSON)
Setup Guide (PDF or Notion link)
Sample Google Sheet template
Email + Google Drive + OCR setup guide
AI Agent configuration tips
BONUS: Customization support (optional)"
Personalize Sales Outreach Based on Product Launches with Explorium & Claude AI,https://n8n.io/workflows/4711-personalize-sales-outreach-based-on-product-launches-with-explorium-and-claude-ai/,"Explorium Event-Triggered Outreach
Automatically identify product launches, enrich company & prospect data, and generate fully personalized outbound emails using Explorium MCP and LLM agents.
This n8n and agent-based workflow automates outbound prospecting by monitoring Explorium event data (e.g. product launches), researching companies, identifying key contacts, and generating tailored sales emails leveraging the Explorium MCP server.
Workflow Overview
Node 1: Webhook Trigger
Purpose: Listens for real-time product launch events pushed from Explorium‚Äôs webhook system.
How it works:
Explorium sends HTTP POST requests containing event data.
The webhook payload includes company name, business ID, domain, product name, and event type.
Node 2: Company Research Agent
Agent Type: Tools Agent
Purpose: Enrich company data after an event occurs.
How it works:
Uses Explorium MCP via the MCP Client tool to gather additional company data.
Uses Anthropic Claude (Chat Model) to process and interpret company information for downstream personalization.
Node 3: Employee Data Retrieval
Purpose: Retrieve prospect-level data for targeting.
How it works:
Uses HTTP Request node to call Explorium's fetch_prospects endpoint.
Filters prospects by:
Company business_id
Departments: Product, R&D
Seniority levels: owner, cxo, vp, director, senior, manager, partner
Limits results to top 5 relevant employees.
Code nodes handle:
Filtering logic
Cleaning API response
Formatting data for downstream agents
Node 4: Conditional Branch: Prospect Data Check
Purpose: Checks whether prospect data was successfully retrieved.
Logic:
If prospects found ‚Üí personalized emails per person.
If no prospects ‚Üí fallback to company-level general email.
Node 5A: Email Writer #1 (No Prospect Data)
Agent Type: Tools Agent
Purpose: Write generic outbound email using only company-level research and event info.
Powered by: Anthropic Chat Model
Node 5B: Loop Over Prospects ‚Üí Email Writer #2 (Personalized)
Agent Type: Tools Agent
Purpose: Write highly personalized email for each identified employee.
How it works:
Loops through each individual prospect.
Passes company research + employee data to LLM agent.
Generates customized emails referencing:
Prospect's title & department
Product launch
Role-relevant Explorium value proposition
Node 6: Slack Notifications
Purpose: Posts completed emails to internal Slack channel for review or testing before final deployment.
Future State: Can be swapped with an email sequencing platform in production.
Setup Requirements
Explorium API Access
MCP Client credentials for company enrichment and prospect fetching.
Registered webhook for event listening.
n8n Configuration
Secure environment variables for API keys & webhook secret.
Code nodes configured for JSON transformation, filtering & signature validation.
Customization Options
Personalization Logic
Update LLM prompt instructions to reflect ICP priorities.
Modify email templates based on role, department, or tenure logic.
Adjust fallback behavior when prospect data is unavailable.
API Request Tuning
Adjust page_size for number of prospects retrieved.
Fine-tune seniority and department filters to match evolving targeting.
Future Expansion
Swap Slack notifications for outbound email automation.
Integrate call task assignment directly into CRM.
Introduce engagement scoring feedback loop (opens, clicks, replies).
Troubleshooting Tips
Validate webhook signature matching to prevent unauthorized requests.
Ensure correct business_id is passed to prospect fetching endpoint.
Confirm business enrichment returns sufficient data for company researcher agents.
Review agent LLM responses for correct output structure and parsing consistency."
Automated AI Lead Enrichment: Salesforce to Explorium for Enhanced Prospect Data,https://n8n.io/workflows/4837-automated-ai-lead-enrichment-salesforce-to-explorium-for-enhanced-prospect-data/,"Salesforce Lead Enrichment with Explorium
Template
Download the following json file and import it to a new n8n workflow:
salesforce_Workflow.json
Overview
This n8n workflow monitors your Salesforce instance for new leads and automatically enriches them with missing contact information. When a lead is created, the workflow:
Detects the new lead via Salesforce trigger
Matches the lead against Explorium's database using name and company
Enriches the lead with professional email addresses and phone numbers
Updates the Salesforce lead record with the discovered contact information
This automation ensures your sales team always has the most up-to-date contact information for new leads, improving reach rates and accelerating the sales process.
Key Features
Real-time Processing: Triggers automatically when new leads are created in Salesforce
Intelligent Matching: Uses lead name and company to find the correct person in Explorium's database
Contact Enrichment: Adds professional emails, mobile phones, and office phone numbers
Batch Processing: Efficiently handles multiple leads to optimize API usage
Error Handling: Continues processing other leads even if some fail to match
Selective Updates: Only updates leads that successfully match in Explorium
Prerequisites
Before setting up this workflow, ensure you have:
n8n instance (self-hosted or cloud)
Salesforce account with:
OAuth2 API access enabled
Lead object permissions (read/write)
API usage limits available
Explorium API credentials (Bearer token) - Get explorium api key
Basic understanding of Salesforce lead management
Salesforce Requirements
Required Lead Fields
The workflow expects these standard Salesforce lead fields:
FirstName - Lead's first name
LastName - Lead's last name
Company - Company name
Email - Will be populated/updated by the workflow
Phone - Will be populated/updated by the workflow
MobilePhone - Will be populated/updated by the workflow
API Permissions
Your Salesforce integration user needs:
Read access to Lead object
Write access to Lead object fields (Email, Phone, MobilePhone)
API enabled on the user profile
Sufficient API calls remaining in your org limits
Installation & Setup
Step 1: Import the Workflow
Copy the workflow JSON from the template
In n8n: Navigate to Workflows ‚Üí Add Workflow ‚Üí Import from File
Paste the JSON and click Import
Step 2: Configure Salesforce OAuth2 Credentials
Click on the Salesforce Trigger node
Under Credentials, click Create New
Follow the OAuth2 flow:
Client ID: From your Salesforce Connected App
Client Secret: From your Salesforce Connected App
Callback URL: Copy from n8n and add to your Connected App
Authorize the connection
Save the credentials as ""Salesforce account connection""
Note: Use the same credentials for all Salesforce nodes in the workflow.
Step 3: Configure Explorium API Credentials
Click on the Match_prospect node
Under Credentials, click Create New (HTTP Header Auth)
Configure the header:
Name: Authorization
Value: Bearer YOUR_EXPLORIUM_API_TOKEN
Save as ""Header Auth account""
Apply the same credentials to the Explorium Enrich Contacts Information node
Step 4: Verify Node Settings
Salesforce Trigger:
Trigger On: Lead Created
Poll Time: Every minute (adjust based on your needs)
Salesforce Get Leads:
Operation: Get All
Condition: CreatedDate = TODAY (fetches today's leads)
Limit: 20 (adjust based on volume)
Loop Over Items:
Batch Size: 6 (optimal for API rate limits)
Step 5: Activate the Workflow
Save the workflow
Toggle the Active switch to ON
The workflow will now monitor for new leads every minute
Detailed Node Descriptions
Salesforce Trigger: Polls Salesforce every minute for new leads
Get Today's Leads: Retrieves all leads created today to ensure none are missed
Loop Over Items: Processes leads in batches of 6 for efficiency
Match Prospect: Searches Explorium for matching person using name + company
Filter: Checks if a valid match was found
Extract Prospect IDs: Collects all matched prospect IDs
Enrich Contacts: Fetches detailed contact information from Explorium
Merge: Combines original lead data with enrichment results
Split Out: Separates individual enriched records
Update Lead: Updates Salesforce with new contact information
Data Mapping
The workflow maps Explorium data to Salesforce fields as follows:
Explorium Field Salesforce Field Fallback Logic
emails[0].address Email Falls back to professions_email
mobile_phone MobilePhone Falls back to phone_numbers[1]
phone_numbers[0] Phone Falls back to mobile_phone
Usage & Monitoring
Automatic Operation
Once activated, the workflow runs automatically:
Checks for new leads every minute
Processes any leads created since the last check
Updates leads with discovered contact information
Continues running until deactivated
Manual Testing
To test the workflow manually:
Create a test lead in Salesforce
Click ""Execute Workflow"" in n8n
Monitor the execution to see each step
Verify the lead was updated in Salesforce
Monitoring Executions
Track workflow performance:
Go to Executions in n8n
Filter by this workflow
Review successful and failed executions
Check logs for any errors or issues
Troubleshooting
Common Issues
No leads are being processed
Verify the workflow is activated
Check Salesforce API limits haven't been exceeded
Ensure new leads have FirstName, LastName, and Company populated
Confirm OAuth connection is still valid
Leads not matching in Explorium
Verify company names are accurate (not abbreviations)
Check that first and last names are properly formatted
Some individuals may not be in Explorium's database
Try testing with known companies/contacts
Contact information not updating
Check Salesforce field-level security
Verify the integration user has edit permissions
Ensure Email, Phone, and MobilePhone fields are writeable
Check for validation rules blocking updates
Authentication errors
Salesforce: Re-authorize OAuth connection
Explorium: Verify Bearer token is valid and not expired
Check API quotas haven't been exceeded
Error Handling
The workflow includes built-in error handling:
Failed matches don't stop other leads from processing
Each batch is processed independently
Failed executions are logged for review
Partial successes are possible (some leads updated, others skipped)
Best Practices
Data Quality
Ensure complete lead data: FirstName, LastName, and Company should be populated
Use full company names: ""Microsoft Corporation"" matches better than ""MSFT""
Standardize data entry: Consistent formatting improves match rates
Performance Optimization
Adjust batch size: Lower if hitting API limits, higher for efficiency
Modify polling frequency: Every minute for high volume, less frequent for lower volume
Set appropriate limits: Balance between processing speed and API usage
Compliance & Privacy
Data permissions: Ensure you have rights to enrich lead data
GDPR compliance: Consider privacy regulations in your region
Data retention: Follow your organization's data policies
Audit trail: Monitor who has access to enriched data
Customization Options
Extend the Enrichment
Add more Explorium enrichment by:
Adding firmographic data (company size, revenue)
Including technographic information
Appending social media profiles
Adding job title and department verification
Modify Trigger Conditions
Change when enrichment occurs:
Trigger on lead updates (not just creation)
Add specific lead source filters
Process only leads from certain campaigns
Include lead score thresholds
Add Notifications
Enhance with alerts:
Email sales reps when leads are enriched
Send Slack notifications for high-value matches
Create tasks for leads that couldn't be enriched
Log enrichment metrics to dashboards
API Considerations
Salesforce Limits
API calls: Each execution uses ~4 Salesforce API calls
Polling frequency: Consider your daily API limit
Batch processing: Reduces API usage vs. individual processing
Explorium Limits
Match API: One call per batch of leads
Enrichment API: One call per batch of matched prospects
Rate limits: Respect your plan's requests per minute
Integration Architecture
This workflow can be part of a larger lead management system:
Lead Capture ‚Üí This Workflow ‚Üí Lead Scoring ‚Üí Assignment
Can trigger additional workflows based on enrichment results
Compatible with existing Salesforce automation (Process Builder, Flows)
Works alongside other enrichment tools
Security Considerations
Credentials: Stored securely in n8n's credential system
Data transmission: Uses HTTPS for all API calls
Access control: Limit who can modify the workflow
Audit logging: All executions are logged with details
Support Resources
For assistance with:
n8n issues: Consult n8n documentation or community forum
Salesforce integration: Reference Salesforce API documentation
Explorium API: Contact Explorium support for API questions
Workflow logic: Review execution logs for debugging"
Realtime Job Description & Salary Extraction using Bright Data MCP & OpenAI 4o mini,https://n8n.io/workflows/4829-realtime-job-description-and-salary-extraction-using-bright-data-mcp-and-openai-4o-mini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
This workflow automates the real-time extraction of Job Descriptions and Salary Information from job listing pages using Bright Data MCP and analyzes content using OpenAI GPT-4o mini.
This workflow is ideal for:
Recruiters & HR Tech Startups: Automate job data collection from public listings
Market Intelligence Teams: Analyze compensation trends across companies or geographies
Job Boards & Aggregators: Power search results with structured, enriched listings
AI Workflow Builders: Extend to other career platforms or automate resume-job match analysis
Analysts & Researchers: Track hiring signals and salary benchmarks in real time
What problem is this workflow solving?
Traditional scraping of job portals can be challenging due to cluttered content, anti-scraping measures, and inconsistent formatting. Manually analyzing salary ranges and job descriptions is tedious and error-prone.
This workflow solves the problem by:
Simulating user behavior using Bright Data MCP Client to bypass anti-scraping systems
Extracting structured, clean job data in Markdown format
Using OpenAI GPT-4o mini to analyze and extract precise salary details and refined job descriptions
Merging and formatting the result for easy consumption
Delivering final output via webhook, Google Sheets, or file system
What this workflow does
Components & Flow
Input Nodes
job_search_url: The job listing or search result URL
job_role: The title or role being searched for (used in logging/formatting)
MCP Client Operations
MCP Salary Data Extractor
Simulates browser behavior and scrapes salary-related content (if available)
MCP Job Description Extractor
Extracts full job description as structured Markdown content
OpenAI GPT-4o mini Nodes
Salary Information Extractor
Uses GPT-4o mini to detect, clean, and standardize salary range data (if any)
Job Description Refiner
Extracts role responsibilities, qualifications, and benefits from unstructured text
Merge Node
Combines the refined job description and extracted salary information into a unified JSON response object
Aggregate node
Aggregates the job description and salary information into a single JSON response object
Final Output Handling
The output is handled in three different formats depending on your downstream needs:
Save to Disk
Output stored with filename including timestamp and job role
Google Sheet Update
Adds a new row with job role, salary, summary, and link
Webhook Notification
Pushes merged response to an external system
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
In n8n, configure the OpenAi account credentials.
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
Modify Input Source
Change the job_search_url to point to any job board or aggregator
Customize job_role to reflect the type of jobs being analyzed
Tweak LLM Prompts (Optional)
Refine GPT-4o mini prompts to extract additional fields like benefits, tech stacks, remote eligibility
Change Output Format
Customize the merged object to output JSON, CSV, or Markdown based on downstream needs
Add additional destinations (e.g., Slack, Airtable, Notion) via n8n nodes"
Binance SM Indicators Webhook Tool,https://n8n.io/workflows/4747-binance-sm-indicators-webhook-tool/,"This workflow acts as a central API gateway for all technical indicator agents in the Binance Spot Market Quant AI system. It listens for incoming webhook requests and dynamically routes them to the correct timeframe-based indicator tool (15m, 1h, 4h, 1d). Designed to power multi-timeframe analysis at scale.
üé• Live Demo:
üéØ What It Does
Accepts requests via webhook with a token symbol and timeframe
Forwards requests to the correct internal technical indicator tool
Returns a clean JSON payload with RSI, MACD, BBANDS, EMA, SMA, and ADX
Can be used directly or as a microservice by other agents
üõ†Ô∏è Input Format
Webhook endpoint:
POST /webhook/indicators
Body format:
{
  ""symbol"": ""DOGEUSDT"",
  ""timeframe"": ""15m""
}
üîÑ Routing Logic
Timeframe Routed To
15m Binance SM 15min Indicators Tool
1h Binance SM 1hour Indicators Tool
4h Binance SM 4hour Indicators Tool
1d Binance SM 1day Indicators Tool
üîé Use Cases
Use Case Description
üîó Used by Binance Financial Analyst Tool Automatically triggers all indicator tools in parallel
ü§ñ Integrated in Binance Quant AI System Supports reasoning, signal generation, and summaries
‚öôÔ∏è Can be called independently for raw data access Useful for dashboards or advanced analytics
üì§ Output Example
{
  ""symbol"": ""DOGEUSDT"",
  ""timeframe"": ""15m"",
  ""rsi"": 56.7,
  ""macd"": ""Bearish Crossover"",
  ""bbands"": ""Stable"",
  ""ema"": ""Price above EMA"",
  ""adx"": 19.4
}
‚úÖ Prerequisites
Make sure all the following workflows are installed and operational:
Binance SM 15min Indicators Tool
Binance SM 1hour Indicators Tool
Binance SM 4hour Indicators Tool
Binance SM 1day Indicators Tool
OpenAI credentials (for any agent using LLM formatting)
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
All architectural routing logic and endpoint structuring is IP-protected.
No unauthorized rebranding or resale permitted.
üîó Need help? Connect on LinkedIn ‚Äì Don Jayamaha"
AI SEO Readability Audit: Check Website Friendliness for LLMs,https://n8n.io/workflows/4151-ai-seo-readability-audit-check-website-friendliness-for-llms/,"Who is this for?
This workflow is designed for SEO specialists, content creators, marketers, and website developers who want to ensure their web content is easily accessible, understandable, and indexable by Large Language Models (LLMs) like ChatGPT, Perplexity, and Google AI Overviews. If you're looking to optimize your site for the evolving AI-driven search landscape, this template is for you.
What problem is this workflow solving? / Use case
Modern AI tools often crawl websites without executing JavaScript. This can lead to them ""seeing"" a very different version of your page than a human user or traditional search engine bot might. This workflow helps you:
Identify how much of your content is visible without JavaScript.
Check for crucial on-page SEO elements that AI relies on (headings, meta descriptions, structured data).
Detect if your site presents JavaScript-blocking warnings.
Get an AI-generated readability score and actionable recommendations to improve AI-friendliness.
What this workflow does
Receives a URL via a chat interface.
Sanitizes the input URL to ensure it's correctly formatted.
Fetches the website's HTML content, simulating a non-JavaScript crawler (like Googlebot).
Extracts key HTML features: visible text length, presence of H1/H2/H3 tags, meta description, Open Graph data, structured data (JSON-LD), and &lt;noscript&gt; tags. It also checks for common JavaScript-blocking messages.
Performs an AI SEO Analysis using an LLM (via OpenAI) based on the extracted features.
Provides a report including an AI Readability Score (0-10), a summary, actionable recommendations, and a reminder to check the robots.txt file for AI bot access.
Setup
Estimated setup time: 2-5 minutes.
Import this workflow into your n8n instance.
Ensure you have an OpenAI account and API key.
Configure the ""OpenAI Chat Model"" node with your OpenAI API credentials. If you don't have credentials set up yet, create new ones in n8n.
Activate the workflow.
Interact with the chat interface provided by the ""When chat message received"" trigger node (you can access this via its webhook URL).
How to customize this workflow to your needs
Change LLM Model: In the ""OpenAI Chat Model"" node, you can select a different model that suits your needs or budget.
Adjust AI Prompt: Modify the prompt in the ""AI SEO Analysis"" node (Chain Llm) to change the focus of the analysis or the format of the report. For example, you could ask for more technical details or a different scoring system.
User-Agent: The ""Get HTML from Website"" node uses a Googlebot User-Agent. You can change this to simulate other bots if needed.
JS Block Indicators: The ""Extract HTML Features"" node contains a list of common JavaScript-blocking phrases. You can expand this list with other languages or specific messages relevant to your checks."
"Generate Monthly Financial Reports with Gemini AI, SQL, and Outlook",https://n8n.io/workflows/3617-generate-monthly-financial-reports-with-gemini-ai-sql-and-outlook/,"üöÄ AI-Powered Business Performance Reporting Automation
Unlock executive-level insights with ZERO manual work!
This n8n template empowers you to automate your entire monthly business performance reporting using dynamic SQL queries, AI-driven analysis, and beautiful HTML dashboards ‚Äî all delivered directly to your inbox.
üéØ What This Automation Does
üìÜ Triggers automatically every month (5th of each month)
üßÆ Fetches financial data from SQL (ERPNext or any database)
üîÅ Loops over cost centers to analyze each business unit individually
üìä Generates Profit & Loss reports, WIP, Employee stats, and vertical breakdowns
ü§ñ Uses Google Gemini 2.5 AI to perform advanced financial analysis
üíå Delivers a polished HTML report to your email inbox
üîß Fully modular ‚Äì replace data source with Excel, Google Sheets, or APIs
üßë‚Äçüè´ Step-by-Step Video Tutorial
üé• Watch the full tutorial on YouTube:

üìå Learn how each node works and see the AI-generated report in action.
üåê Useful Links
üîó Sign up for n8n Cloud (recommended for non-tech users):
üëâ https://n8n.syncbricks.com
üìò Download the step-by-step Guidebook (Free):
üëâ https://lms.syncbricks.com/books/n8n
üìö Explore the full course on n8n (includes templates, workflows, and AI integrations):
üëâ https://lms.syncbricks.com/courses/n8n
üõ† Requirements
‚úÖ n8n (Self-hosted or Cloud)
‚úÖ SQL Database (MySQL / PostgreSQL / ERPNext)
‚úÖ Microsoft Outlook or Gmail (to send the report)
‚úÖ Gemini API Key (for AI analysis)
‚úÖ Basic understanding of your data schema
üí° Why Use This Template?
‚è± Saves 2-3 days of manual work every month
üìà Improves financial visibility across business units
ü§ù Great for CFOs, COOs, Finance Analysts, and BI teams
üîÑ Scales across multiple divisions and companies
üß† Leverages AI for actionable insights and recommendations
üß© Customize It Your Way
Replace the SQL nodes with:
Excel / Google Sheets
Airtable / APIs
Custom Applications
Swap the AI model:
OpenAI GPT
Claude
DeepSeek
Adjust the report structure or HTML style
üôå Get Started Now
üéØ Import the JSON template ‚Üí Connect your data ‚Üí Receive business insights via email.
Don‚Äôt let manual reporting slow down your decision-making.
üëâ Sign up for n8n Cloud
üëâ Learn n8n with Amjid
üëâ Download Guide
Created by Amjid Ali | SyncBricks‚Ñ¢ ‚Äì Automation for Everyone"
"Generate Invoices, Save to Drive and Send Email to Customer with JS + G Sheets",https://n8n.io/workflows/4105-generate-invoices-save-to-drive-and-send-email-to-customer-with-js-g-sheets/,"This workflow automates invoice generation from form submissions, ensuring unique order IDs, creating PDF invoices, storing files, emailing customers, and logging invoice data ‚Äî all seamlessly integrated.
üîπ Workflow Overview
Trigger (Webhook)
Starts when an order form is submitted, capturing customer and order details.
Generate Random Order ID
A Function node creates a unique alphanumeric invoice ID (e.g., INV-X92B7D).
Check for Duplicate Order ID
Google Sheets looks up the generated order ID in your invoice log sheet to prevent duplicates.
Conditional Check (IF Node)
If the ID already exists ‚Üí regenerates a new ID (loops back)
If unique ‚Üí proceeds to invoice creation
Prepare Invoice Data
A Set node formats customer info, date, order items, and the unique order ID to fit your invoice template.
Convert HTML to PDF
HTTP Request node sends your invoice HTML to the RapidAPI HTML-to-PDF service and receives the PDF file.
Upload PDF to Cloud Storage
Save the PDF in Google Drive or Dropbox with a clear file name like Invoice-INV-X92B7D.pdf.
Send Invoice Email to Customer
Email node attaches the PDF and includes the order ID in the email subject/body.
Log Invoice Details
Append invoice data (customer info, order ID, total, PDF link) to your Google Sheet for tracking.
‚öôÔ∏è Node Details & Setup
1. Webhook Trigger
Configure to receive form submissions (order details like name, email, items, total).
2. Function: Generate Random Order ID
Sample JS code generates unique IDs prefixed by INV-.
3. Google Sheets: Lookup Row
Set up connection to your invoice log sheet.
Search for existing order ID to avoid duplicates.
4. IF Node: Check Order ID Existence
Condition: If order ID found ‚Üí loop to regenerate.
Else ‚Üí continue workflow.
5. Set Node: Prepare Invoice HTML
Define variables like customer name, date, items, and order ID.
This data populates your HTML invoice template.
6. HTTP Request: Convert HTML to PDF
API URL to get your key
Send invoice HTML in the request body.
Receive PDF file blob or download URL.
7. Google Drive (or Dropbox) Upload
Upload the PDF file.
Use file name format: Invoice-{{$json[""order_id""]}}.pdf
8. Email Node
Recipient: customer email from the form data.
Attach generated PDF.
Include order ID in email subject or body for reference.
9. Google Sheets: Append Row
Log invoice metadata to keep records updated.
üìÅ Google Sheets Template
You can make a copy of the invoice log template here
This sheet includes columns for order\_id, customer name, email, total, and invoice PDF link. Customize it as needed.
üìå Additional Notes
Customize the invoice HTML template inside the Set node to match your branding.
Ensure API credentials for RapidAPI, Google Drive/Dropbox, and email are properly set up in your n8n credentials.
You can expand this workflow by adding payment processing or SMS notifications.
Need help or want a custom workflow?
Reach out via email at joseph@uppfy.com."
Analyze Crypto Markets with the AI-Powered CoinMarketCap Data Analyst,https://n8n.io/workflows/3425-analyze-crypto-markets-with-the-ai-powered-coinmarketcap-data-analyst/,"Meet your AI-powered crypto data analyst‚Äîfully integrated with CoinMarketCap APIs.
This workflow acts as the supervisor agent for a multi-agent architecture built in n8n, connecting three powerful sub-agents to extract real-time insights from centralized and decentralized markets. It‚Äôs the ultimate tool for crypto traders, analysts, developers, and researchers who need strategic multi-source intelligence‚Äîall through Telegram.
This workflow requires 3 sub-agent templates to function correctly. See below.
üîå Required Sub-Workflows (Install First)
CoinMarketCap Crypto Agent Tool
‚Üí Token prices, metadata, conversions, listings
CoinMarketCap Exchange & Community Agent Tool
‚Üí Exchange info, token holdings, Fear & Greed index
CoinMarketCap DEXScan Agent Tool
‚Üí DEX trading pairs, liquidity, OHLCV data
Download all from my Creator Profile:
https://n8n.io/creators/don-the-gem-dealer/
What Makes This Workflow Special?
This is not just another API wrapper‚Äîit‚Äôs an intelligent routing agent powered by GPT-4o-mini, capable of:
Understanding complex user queries
Choosing the appropriate tool workflow
Structuring the API request
Executing sub-workflows
Formatting the output
Returning insights via Telegram
It connects three domains of market data:
Cryptocurrencies (CEX)
Exchanges & Sentiment
DEX trading data
üîç What You Can Do
üí∞ Token Intelligence
Get token metadata, price, volume, supply
Compare rankings and conversions
üè¶ Exchange Insights
View assets held by exchanges
Track the CMC 100 Index and Fear & Greed Score
üåê DEX Market Analysis
Analyze pair quotes, historical OHLCV, live trades
Discover the top DEXs by volume across blockchains
‚úÖ Example Questions to Ask
‚ÄúWhat‚Äôs the market cap of Ethereum today?‚Äù
‚ÄúShow liquidity and volume for SOL/USDT on Solana‚Äù
‚ÄúGet token holdings for Binance‚Äù
‚ÄúCompare BTC price on Uniswap vs Binance‚Äù
‚ÄúWhat‚Äôs the Fear & Greed index right now?‚Äù
üõ†Ô∏è Setup Instructions
Create Telegram Bot
Use @BotFather to get your bot token.
Get CoinMarketCap API Key
Apply here: https://coinmarketcap.com/api/
Install Sub-Agent Templates
Required:
Crypto Agent Tool
Exchange & Community Tool
DEXScan Tool
Configure Credentials in n8n
Add both Telegram and CoinMarketCap keys as HTTP Header Auth.
Deploy & Test
Ask your Telegram bot: ‚ÄúTop 10 tokens by 24h volume‚Äù or ‚ÄúConvert 5 ETH to USD‚Äù
Workflow Architecture
AI Brain: GPT-4o-mini
Memory: Windowed buffer memory via sessionId
Tool Agents:
toolWorkflow() ‚Üí routes requests to the appropriate sub-agent
Executes real-time API queries and returns structured output
Included Sticky Notes
System Overview
Error Handling Guide (200, 400, 401, 429, 500)
Step-by-Step Usage Instructions
Prompt Examples + API Docs
Legal & Licensing Notes
Your crypto insights‚Äîsmarter, faster, and all in one Telegram message."
Generate Instagram Content from Top Trends with AI Image Generation,https://n8n.io/workflows/2803-generate-instagram-content-from-top-trends-with-ai-image-generation/,"How it works
This automated workflow discovers trending Instagram posts and creates similar AI-generated content. Here's the high-level process:
1. Content Discovery & Analysis
Scrapes trending posts from specific hashtags
Analyzes visual elements using AI
Filters out videos and duplicates
2. AI Content Generation
Creates unique images based on trending content
Generates engaging captions with relevant hashtags
Maintains brand consistency while being original
3. Automated Publishing
Posts content directly to Instagram
Monitors publication status
Sends notifications via Telegram
Set up steps
Setting up this workflow takes approximately 15-20 minutes:
1. API Configuration (7-10 minutes)
Instagram Business Account setup
Telegram Bot creation
API key generation (OpenAI, Replicate, Rapid Api)
2. Database Setup (3-5 minutes)
Create required database table
Configure PostgreSQL credentials
3. Workflow Configuration (5-7 minutes)
Set scheduling preferences
Configure notification settings
Test connection and permissions
Detailed technical specifications and configurations are available in sticky notes within the workflow."
Binance SM 15min Indicators Tool,https://n8n.io/workflows/4743-binance-sm-15min-indicators-tool/,"A short-term technical analysis agent for 15-minute candles on Binance Spot Market pairs. Calculates and interprets key trading indicators (RSI, MACD, BBANDS, ADX, SMA/EMA) and returns structured summaries, optimized for Telegram or downstream AI trading agents.
This tool is designed to be triggered by another workflow (such as the Binance SM Financial Analyst Tool or Binance Quant AI Agent) and is not intended for standalone use.
üîß Key Features
‚è±Ô∏è Uses 15-minute kline data (last 100 candles)
üìà Calculates: RSI, MACD, Bollinger Bands, SMA/EMA, ADX
üß† Interprets numeric data using GPT-4.1-mini
üì§ Outputs concise, formatted analysis like:
‚Ä¢ RSI: 72 ‚Üí Overbought  
‚Ä¢ MACD: Cross Up  
‚Ä¢ BB: Expanding  
‚Ä¢ ADX: 34 ‚Üí Strong Trend
üß† AI Agent Purpose
You are a short-term analysis tool for spotting volatility, early breakouts, and scalping setups.
Used by higher agents to determine:
Entry/exit precision
Momentum shifts
Scalping opportunities
‚öôÔ∏è How it Works
Triggered externally by another workflow
Accepts input:
{
  ""message"": ""BTCUSDT"",
  ""sessionId"": ""123456789""
}
Sends POST request to backend endpoint:
https://treasurium.app.n8n.cloud/webhook/15m-indicators
Fetches last 100 candles and calculates indicators
Passes data to GPT for interpretation
Returns summary with indicator tags for human readability
üîó Dependencies
This tool is triggered by:
‚úÖ Binance SM Financial Analyst Tool
‚úÖ Binance Spot Market Quant AI Agent
üöÄ Setup Instructions
Import into your n8n instance
Make sure /15m-indicators webhook is active and calculates indicators correctly
Connect your OpenAI GPT-4.1-mini credentials
Trigger from upstream agent with Binance symbol and session ID
Ensure all external calls (to Binance + webhook) are working
üß™ Example Use Cases
Use Case Result
Short-term trade decision for ETHUSDT Receives 15m signal indicators summary
Input from Financial Analyst Tool Returns real-time volatility snapshot
Telegram bot asks for ‚ÄúDOGE update‚Äù Returns momentum indicators in 15m view
üé• Demo Preview

üì∫ https://youtu.be/k9VuU2h5wwI
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected.
No unauthorized rebranding or resale permitted.
üîó For support: Don Jayamaha ‚Äì LinkedIn"
"Automate SEO Blog Creation + Social Media with GPT-4, Perplexity and WordPress",https://n8n.io/workflows/4082-automate-seo-blog-creation-social-media-with-gpt-4-perplexity-and-wordpress/,"üöÄ AI Blog & Social Media Publisher ‚Äì Fully Automated Workflow
This workflow is ideal for individuals, marketers, agencies, and brands who want to effortlessly automate the entire blogging and social media process‚Äîfrom idea generation to promotion. Its primary goal is to consistently deliver engaging, SEO-optimized blog posts directly to WordPress, accompanied by professionally crafted social media content ready for Instagram, Facebook, and X (Twitter).
No writers, editors, designers, or social media managers required. Save countless hours and resources while boosting your brand's online presence.
üîß Workflow Capabilities:
-Automated Blog Topic Generation: Instantly generates unique, high-value blog ideas tailored to your brand.
-SEO Content Creation: Produces comprehensive, 2000‚Äì2500 word articles optimized for search engines.
-Real-Time Research: Integrates Perplexity AI to ensure factually accurate, authoritative content.
-Visual Content Generation: Creates custom, AI-generated featured images via GPT-4, with automatic fallback to Pexels.
-WordPress Publishing: Automatically publishes articles directly to WordPress with complete formatting (HTML), meta descriptions, featured images, and proper categorization.
-Social Media Automation: Generates and schedules engaging, platform-specific posts for Instagram, Facebook, and X, complete with relevant hashtags, calls-to-action, and links.
-Detailed Logging & Reporting: Logs every step transparently into Google Sheets and optionally sends publication reports to Gmail.
‚öôÔ∏è How It Works:
When activated, the workflow checks your content calendar in Google Sheets to avoid duplicate topics. It generates fresh, brand-aligned blog ideas, conducts live research, and creates complete, ready-to-publish articles. It then automatically designs and sources appropriate visuals, publishes content seamlessly to WordPress, and prepares engaging social media posts optimized for each platform.
üõ†Ô∏è Easy Setup Steps:
-Connect your APIs: OpenAI, WordPress, Google Sheets, Gmail, and optionally Pexels.
-Replace placeholders (WordPress URLs, Google Sheets ID/tab, workflow IDs).
-Customize AI prompts with your brand‚Äôs tone, keywords, and CTAs for personalized content.
-Configure scheduling to automate daily, weekly, or custom publishing intervals.
üìå Additional Features:
-Modular workflow design for easy customization and scalability.
-Comprehensive Markdown and PDF setup guides included.
-Bonus: Reference placeholders and quick-edit Sticky Notes for effortless adjustments.
Launch today. Automate permanently. Amplify your reach effortlessly."
Generate Custom AI Images with OpenAI GPT-Image-1 Model,https://n8n.io/workflows/3705-generate-custom-ai-images-with-openai-gpt-image-1-model/,"How it works
Trigger the workflow manually via the n8n UI.
Define key parameters like the image prompt, number of images, size, quality, and model.
Send a POST request to OpenAI‚Äôs image generation API using those inputs.
Split the API response to handle multiple images.
Convert the base64 image data into downloadable binary files.
Set up steps
Initial setup takes around 5‚Äì10 minutes. You‚Äôll need an OpenAI API key, a configured HTTP Request node with credentials, and to customize the prompt/parameter fields in the ‚ÄúSet Variables‚Äù node. No advanced config or external services needed.
Important Note
You have to make sure to complete OpenAI's new verification requirements to use their new image API:
https://help.openai.com/en/articles/10910291-api-organization-verification
It only takes a few minutes and does not cost any money."
Automated LinkedIn Profile Discovery with Airtop and Google Search,https://n8n.io/workflows/3477-automated-linkedin-profile-discovery-with-airtop-and-google-search/,"About The LinkedIn Profile Discovery Automation
Are you tired of manually searching for LinkedIn profiles or paying expensive data providers for often outdated information? If you spend countless hours trying to find accurate LinkedIn URLs for your prospects or candidates, this automation will change your workflow forever. Just give this workflow the information you have about a contact, and it will automatically augment it with a LinkedIn profile.
How to find a LinkedIn Profile Link
In this guide, you'll learn how to automate LinkedIn profile link discovery using Airtop's built-in node in n8n. Using this automation, you'll have a fully automated workflow that saves you hours of manual searching while providing accurate, validated LinkedIn URLs.
What You'll Need
A free Airtop API key
A Google Workspace account. If you have a Gmail account, you‚Äôre all set
Estimated setup time: 10 minutes
Understanding the Process
This automation leverages the power of intelligent search algorithms combined with LinkedIn validation to ensure accuracy. Here's how it works:
Takes your input data (name, company, etc.) and constructs intelligent search queries
Utilizes Google search to identify potential LinkedIn profile URLs
Validates the discovered URLs directly against LinkedIn to ensure accuracy
Returns confirmed, accurate LinkedIn profile URLs
Setting Up Your Automation
Getting started with this automation is straightforward:
Prepare Your Google Sheet
Create a new Google Sheet with columns for input data (name, company, domain, etc.)
Add columns for the output LinkedIn URL and validation status (see this example)
Configure the Automation
Connect your Google Workspace account to n8n if you haven't already
Add your Airtop API credentials
(Optionally) Configure your Airtop Profile and sign-in to LinkedIn in order to validate profile URL's
Run Your First Test
Add a few test entries to your Google Sheet
Run the workflow
Check the results in your output columns
Customization Options
While the default setup uses Google Sheets, this automation is highly flexible:
Webhook Integration: Perfect for integrating with tools like Clay, Instantly, or your custom applications
Alternatives: Replace Google Sheets with Airtable, Notion, or any other tools you already use for more robust database capabilities
Custom Output Formatting: Modify the output structure to match your existing systems
Batch Processing: Configure for bulk processing of multiple profiles
Real-World Applications
This automation has the potential to transform how we organizations handle profile enrichment.
Recruiting Firm Success Story
With this automation, a recruiting firm could save hundreds of dollars a month in data enrichment fees, achieve better accuracy, and eliminate subscription costs. They would also be able to process thousands of profiles weekly with near-perfect accuracy.
Sales Team Integration
A B2B sales team could integrate this automation with their CRM, automatically enriching new leads with validated LinkedIn profiles and saving their SDRs hours per week on manual research.
Best Practices
To maximize the accuracy of your results:
Always include company information (domain or company name) with your search queries
Use full names rather than nicknames or initials when possible
Consider including location data for more accurate results with common names
Implement rate limiting to respect LinkedIn's usage guidelines
Keep your input data clean and standardized for best results
Use the integrated proxy to navigate more effectively through Google and LinkedIn
What's Next?
Now that you've automated LinkedIn profile discovery, consider exploring related automations:
Automated lead scoring based on LinkedIn profile data
Email finder automation using validated LinkedIn profiles
Integration with your CRM for automated contact enrichment"
Google Calendar Reminder System with GPT-4o and Telegram,https://n8n.io/workflows/3393-google-calendar-reminder-system-with-gpt-4o-and-telegram/,"How many times have you missed a meeting or forgotten an appointment because a calendar reminder got lost in the noise? Traditional notifications are often dry, easy to ignore, or scattered across different apps‚Äîleaving you scrambling at the last minute.
This smart Google Calendar workflow fixes that by sending you a clear, friendly reminder exactly 1 hour before your event starts‚Äîdelivered through Telegram as if a personal assistant were looking out for you. Powered by AI, it transforms cold calendar alerts into warm, conversational nudges you won't ignore.
Why This Works Better:
‚úÖ No More Overlooked Alerts ‚Äì Consolidates reminders into one clear, accessible place (Telegram), so you never miss them.
‚úÖ Friendly & Engaging ‚Äì AI transforms robotic calendar entries into natural, human-like reminders that are harder to ignore.
‚úÖ Works Everywhere ‚Äì Whether you're on your phone, laptop, or tablet, you‚Äôll get the same clear notification, no matter the platform.
How It Works
Scheduled Trigger: The workflow starts with a Schedule Trigger node that runs every minute to check for upcoming events.
Google Calendar Check: The ""Get upcoming event"" node queries Google Calendar for events starting within the next hour (between timeMin and timeMax).
Duplicate Prevention: The ""Already sent?"" node ensures reminders are not sent multiple times for the same event by filtering out duplicates.
AI-Powered Reminder: The ""Secretary Agent"" node, powered by GPT-4, crafts a friendly and professional reminder message. It includes event details like name, description, location, start/end time, and creator, formatted in a conversational tone.
Telegram Notification: The final ""Send reminder"" node delivers the reminder via Telegram, ensuring the user receives it in a clear and accessible format.
Set Up Steps
Configure Schedule Trigger: Set the interval (e.g., every minute) to check for events.
Connect Google Calendar: Link your Google Calendar account and specify the calendar to monitor.
Set Up AI Agent: Customize the ""Secretary Agent"" with the provided system message to ensure reminders are warm, professional, and detailed.
Link Telegram: Add your Telegram credentials and specify the CHAT_ID where reminders will be sent.
Activate Workflow: Ensure the workflow is active and set to the correct timezone (e.g., Europe/Rome).
Why It‚Äôs Useful
Never Miss an Event: Traditional calendar reminders can be easy to overlook, especially when scattered across platforms. This workflow consolidates reminders into a single, accessible channel (Telegram).
Clear and Friendly: The AI agent transforms generic calendar alerts into personalized, conversational reminders, making them harder to ignore.
Cross-Platform Accessibility: By delivering reminders via Telegram, users receive them in a consistent format, regardless of the device or platform they‚Äôre using. No more missed events due to unclear notifications!
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Automate Blog Content Creation with GPT-4, Perplexity & WordPress",https://n8n.io/workflows/3336-automate-blog-content-creation-with-gpt-4-perplexity-and-wordpress/,"Who Is This For
This workflow is ideal for content creators, solo founders, marketers, and AI enthusiasts who want to automate the full process of blog content creation.
It is especially useful for professionals in tech, AI, and automation who publish frequently and need SEO-ready content fast.
What Problem Does This Workflow Solve
Creating SEO-optimized blog content is time-consuming and requires consistency.
Manually researching trending topics slows down the content pipeline.
Formatting, publishing, and promoting across multiple platforms takes effort.
This workflow automates the entire process from research to publication.
What This Workflow Does
Research: Uses Perplexity AI to gather up-to-date content ideas via form input.
Content Generation: GPT-4 creates a short, SEO-optimized article (max 20 lines) with H1, H2 structure and meta-description.
Publishing: Automatically posts the content to WordPress.
Email Notification: Sends the article title and URL via Gmail.
Slack Notification: Notifies a specified Slack channel when the article is live.
Database Logging: Saves the article details to a Notion database.
Setup Guide
Prerequisites
WordPress account with API access
OpenAI API Key
Perplexity API Key
Slack Bot Token
Notion integration (Database ID)
Gmail API credentials (optional)
Community Node Required: This workflow uses n8n-nodes-mcp, which only works on self-hosted instances of n8n.
To install: Go to Settings > Community Nodes > Install n8n-nodes-mcp
Steps
Import the workflow into your n8n instance
Install the required community node (n8n-nodes-mcp)
Set up API credentials for OpenAI, Perplexity, WordPress, Slack, Gmail, and Notion
Customize the form trigger with your preferred prompt
Run a test using a sample topic
How to Customize This Workflow
Modify the research prompt to match your niche or industry
Adjust GPT-4 settings for tone, structure, or content length
Customize Notion fields (e.g., add tags, categories, or labels)
Add logic for generating or assigning featured images automatically"
Automate SEO-Optimized WordPress Posts with AI & Google Sheets,https://n8n.io/workflows/3085-automate-seo-optimized-wordpress-posts-with-ai-and-google-sheets/,"This workflow automates the process of creating a complete SEO-optimized blog post, including generating content, titles, images, and meta tags, and publishing it on WordPress. It leverages AI models (like DeepSeek and OpenRouter) for content generation and SEO optimization, and integrates with Google Sheets, WordPress, and OpenAI for image generation.
This is a powerful tool for automating the creation and optimization of blog posts, saving time and ensuring high-quality, SEO-friendly content. It integrates multiple tools and AI models to deliver a seamless content creation experience.
Below is a breakdown of the workflow:
1. How It Works
The workflow is designed to streamline the creation of SEO-friendly blog posts. Here's how it works:
Trigger: The workflow starts with a Manual Trigger node, which initiates the process when the user clicks ""Test workflow.""
Fetch Context: The Google Sheets node retrieves the blog post context (e.g., topic, keywords) from a predefined Google Sheet.
Generate Content:
The Generate Article node uses an AI model (DeepSeek) to create an SEO-friendly blog post based on the fetched context.
The Generate Title node creates a compelling, keyword-rich title for the blog post.
Publish to WordPress:
The Add Draft to WP node creates a draft post in WordPress with the generated content and title.
Generate and Upload Image:
The Generate Image node uses OpenAI to create a realistic, blog-appropriate image.
The Upload Image node uploads the image to WordPress media.
The Set Image node associates the uploaded image as the featured image for the blog post.
SEO Optimization:
The SEO Expert node analyzes the blog post and generates optimized meta titles and descriptions using an AI model (OpenRouter).
The Set Metatag node updates the WordPress post with the generated meta tags.
Update Google Sheets:
The Update Sheet and Finish Work nodes update the Google Sheet with the post's details, including the URL, title, and metadata.
2. Set Up Steps
To set up and use this workflow in n8n, follow these steps:
Google Sheets Setup:
Create a Google Sheet with columns for ID POST, PROMPT, TITLE, URL, METATITLE, and METADESCRIPTION.
Link the Google Sheet to the Get Context node by providing the Document ID and Sheet Name.
WordPress Integration:
Set up WordPress credentials in n8n for the Add Draft to WP, Upload Image, and Set Image nodes.
Ensure the WordPress site is accessible via its REST API.
AI Model Configuration:
Configure the DeepSeek and OpenRouter credentials in n8n for the Generate Article, Generate Title, and SEO Expert nodes.
Ensure the AI models are correctly set up to generate content and meta tags.
Image Generation:
Set up OpenAI credentials for the Generate Image node to create blog post images.
Meta Tag Optimization:
The SEO Expert node uses OpenRouter to generate meta titles and descriptions. Ensure the node is configured to analyze the blog post content and produce SEO-friendly tags.
Workflow Execution:
Click the ""Test workflow"" button to trigger the workflow.
The workflow will:
Fetch the blog post context from Google Sheets.
Generate the article, title, and image.
Publish the draft to WordPress.
Upload and set the featured image.
Generate and apply meta tags.
Update the Google Sheet with the post details.
Optional Customization:
Modify the workflow to include additional SEO optimizations, such as internal linking, keyword density analysis, or social media sharing.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
ü§ñüß† AI Agent Chatbot + LONG TERM Memory + Note Storage + Telegram,https://n8n.io/workflows/2872-ai-agent-chatbot-long-term-memory-note-storage-telegram/,"This workflow template creates an AI agent chatbot with long-term memory and note storage using Google Docs and Telegram integration.
Google Docs Integration üìÑ
n8n Google Docs Node Setup
Google Credentials
Telegram Integration üí¨
Telegram Setup
Core Features üåü
AI Agent Integration ü§ñ
Implements a sophisticated AI agent with memory management capabilities
Uses GPT-4o-mini and DeepSeek models for intelligent conversation handling
Maintains context awareness through session management
Memory System üß†
Long-term memory storage using Google Docs
Separate note storage system for specific information
Window buffer memory for maintaining conversation context
Intelligent memory retrieval and storage mechanisms
Communication Interface üí¨
Telegram integration for message handling
Real-time message processing and response generation
Technical Components üîß
Memory Architecture üìö
Dual storage system separating memories from notes
Automated memory retrieval before each interaction
Structured memory saving with timestamps
AI Models ü§ñ
Primary GPT-4o-mini mini model for general interactions
DeepSeek-V3 Chat for specialized processing
Custom agent system with tool integration
Storage Integration üíæ
Google Docs integration for persistent storage
Separate document management for memories and notes
Automated document updates and retrievals"
Create a Notion AI Assistant with Google Gemini for Managing Tasks & Content,https://n8n.io/workflows/4857-create-a-notion-ai-assistant-with-google-gemini-for-managing-tasks-and-content/,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.
Build an AI agent for Notion (with Notion official MCP server)
Use case
This template empowers Notion power-users to build their own AI assistant, deeply integrated with their workspace. It solves the constant problem of copy-pasting and context-switching between a separate AI chat and Notion by creating a direct, conversational bridge. Now you can interact with an intelligent agent that can create, retrieve, and update your Notion databases and pages on your behalf, turning your workspace into a truly dynamic productivity hub.
How it works
When you send a message via the chat interface, the workflow passes it to your chosen AI model. The model, connected to the official Notion tool server, analyzes your request to see if it can be fulfilled by one of its available Notion actions. If it matches a tool, the workflow executes the command using the Notion API‚Äîlike creating a new page or searching a database‚Äîand the AI then confirms the action is complete back in the chat.
Setup
Prerequisite: This template is for self-hosted n8n instances only, as it requires a community node.
Copy this workflow into your self-hosted n8n instance
Install the required community node (n8n-nodes-mcp).
Add your credentials for your chosen AI Model and the Notion MCP Server.
Test the workflow by starting chatting with your new Notion assistant.
How to adjust it to your needs
You can use the AI model you want and even easily compare different AI models.
You can start from this template and then provide other tools to your AI agent to build more powerful workflows."
Binance SM 1hour Indicators Tool,https://n8n.io/workflows/4744-binance-sm-1hour-indicators-tool/,"üß™ Binance SM 1hour Indicators Tool
A precision trading signal engine that interprets 1-hour candlestick indicators for Binance Spot Market pairs using a GPT-4.1-mini LLM. Ideal for swing traders seeking directional bias and momentum clarity across medium timeframes.
üé• Live Demo:
üéØ Purpose
This tool provides a structured 1-hour market read using:
RSI (Relative Strength Index)
MACD (Moving Average Convergence Divergence)
BBANDS (Bollinger Bands)
SMA & EMA (Simple and Exponential Moving Averages)
ADX (Average Directional Index)
It‚Äôs invoked as a sub-agent in broader AI workflows, such as the Binance Financial Analyst Tool and the Spot Market Quant AI Agent.
‚öôÔ∏è Key Features
Feature Description
üîÑ Subworkflow Trigger Runs only when called by parent agent (not standalone)
üß† GPT-4.1-mini LLM Translates numeric indicators into natural-language summaries
üìä Real-time Data Pulls latest 40√ó1h candles via internal webhook from Binance
üì• Input Format { ""message"": ""ETHUSDT"", ""sessionId"": ""telegram_chat_id"" }
üì§ Output Format JSON summary + Telegram-friendly HTML overview
üí° Example Output
üìä 1h Technical Overview ‚Äì ETHUSDT

‚Ä¢ RSI: 59 (Neutral)  
‚Ä¢ MACD: Bullish Crossover  
‚Ä¢ BBANDS: Price at Upper Band  
‚Ä¢ EMA &gt; SMA ‚Üí Positive Slope  
‚Ä¢ ADX: 28 ‚Üí Moderate Trend Strength
üß© Use Cases
Scenario Result
Mid-frame market alignment Verifies momentum between 15m and 4h timeframes
Quant AI Agent input Supplies trend context for entry/exit decisions
Standalone medium-term signal snapshot Validates swing trade setups or filters noise
üì¶ Installation Instructions
Import workflow into your n8n instance
Confirm internal webhook /1h-indicators is live and authorized
Insert your OpenAI credentials for GPT-4.1-mini node
Use only when triggered via:
Binance Financial Analyst Tool
Binance Spot Market Quant AI Agent
üßæ Licensing & Support
üîó Don Jayamaha ‚Äì LinkedIn
linkedin.com/in/donjayamahajr
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and signal logic are proprietary. Redistribution or commercial use requires explicit licensing. No unauthorized cloning permitted."
Sync GitHub Workflows to n8n After Pull Request Merges,https://n8n.io/workflows/4500-sync-github-workflows-to-n8n-after-pull-request-merges/,"Who is this for?
This template is ideal for developers, DevOps engineers, and automation managers who manage their n8n workflows using GitHub. It helps teams streamline their CI/CD automation by syncing changes from GitHub directly into n8n after a pull request (PR) is merged.
What problem is this workflow solving?
Manually restoring workflows after reviewing and merging code in GitHub can be tedious and error-prone. This workflow solves that by automating the restore process, ensuring that any new or updated workflow committed to your GitHub repo is automatically imported into your n8n environment.
What this workflow does
Triggers when a GitHub pull request is closed and merged.
Fetches the details of the merge commit.
Retrieves the list of added and modified workflow files.
Downloads and decodes each workflow file.
Creates or updates the corresponding workflow in your n8n instance automatically.
Setup
Connect GitHub: Use the GitHub Trigger node and configure GitHub API credentials.
Note: I'd recommended to use GitHub PAT (Personal Access Token) classic with repo and admin:repo_hook permission scopes enabled.
Connect n8n API: Provide your n8n API credentials in the n8n nodes. - Check this doc
Set repository variables: Update github_owner and repo_name in the Define Local Variables node.
Enable webhook: Make sure your GitHub repository has a webhook for pull_request events pointing to this workflow.
How to customize this workflow to your needs
Modify filters to handle only certain branches or file paths.
Add Slack or email notifications to confirm successful imports.
Insert logging or version tagging for better traceability.
Extend with conditional logic for workflow testing before applying changes.
This automated flow provides a seamless CI/CD loop between GitHub and n8n, empowering teams to manage workflow versioning efficiently and securely."
Discover Hidden Website API Endpoints Using Regex and AI,https://n8n.io/workflows/4627-discover-hidden-website-api-endpoints-using-regex-and-ai/,"üí° What it is for
This workflow helps to automatically discover undocumented API endpoints by analysing JavaScript files from the website's HTML code.
When building automation for platforms without public APIs, we face a significant technical barrier. In a perfect world, every service would offer well-documented APIs with clear endpoints and authentication methods. But the reality is different.
Before we resort to complex web scraping, let's analyse the architecture of the platform and check whether it makes internal API calls. We will examine JavaScript files embedded in the HTML source code to find and extract potential API endpoints.
‚öôÔ∏èKey Features
To discover hidden API endpoints, we can apply two major approaches:
1. Predefined regex extraction: manually insert a fixed regex with the necessary conditions to extract endpoints. Unlike LLM, which creates a custom regex for each JS file, we provide a generic expression to capture all URL strings. We do not want to accidentally miss important API endpoints.
2. AI-supported extraction:
ask LLMs to examine the structure of the JavaScript code. The 1st model will:
capture potential API endpoints
create a detailed description of each identified endpoint with methods and query parameters
the 2nd LLM connected to the AI Agent will generate a regex for each JS file individually based on the output of the 1st model.
In addition to pure endpoint extraction, we supplement our analysis with:
AI regex validation: the AI Agent calls a validation tool to iteratively improve its regex based on the reference data.
Results comparison: side-by-side analysis of API endpoints extracted with a predefined regex against AI-supported results.
‚úÖRequirements:
OpenRouter API access: for AI-powered analysis (Gemini + Claude models by default)
Minimal setup: simply configure the target URL and run
Platforms: JS files must be accessible and have embedded standard API endpoints patterns (/api/, /v1/, etc.)
üí™Use Cases
üìö API documentation: create complete endpoint descriptions for internal APIs
üöÄ Automation & integration projects: find the APIs you need when official documentation is missing
üõ† Web scraping projects: discover data access patterns
üîç Security research: map attack surfaces and explore unprotected endpoints
üéâExtracted the endpoints, what now?
To execute API requests, we often need additional information such as query parameters or JSON body data:
One way to find out exactly how the request is being made on the platform is to navigate to the Network tab in the Dev Tools console while interacting with the platform. Look for anything that resembles API requests and review the request/response headers, payload and query parameters.
Alternatively, you can also check the JS file and the page source code for the required values.
‚ú®Inspiration
As a guitarist who also builds workflows, I wanted to automate communication with the booking platform I use in my music project. While trying to connect to the platform from n8n, I ran into a challenge: no public APIs.
Fortunately, I found out that the platform I work with was built as a modern web app with client-side JavaScript that contained information about the API structure. This led me to the topic of hidden API endpoints and eventually to this workflow.
It is part of my music booking project which I presented at the n8n Community Meetup in Berlin on 22 May 2025."
Auto-Generate & Publish SEO Blog Posts to WordPress with OpenRouter & Runware,https://n8n.io/workflows/4546-auto-generate-and-publish-seo-blog-posts-to-wordpress-with-openrouter-and-runware/,"Automate Blog Creation and Publishing with Ultra-Low Cost AI
This n8n workflow ‚Äî BlogBlizt: Runaware Edition ‚Äî automates the creation, enrichment, and publishing of SEO-optimized blog posts for WordPress using entirely free OpenRouter AI models for text and metadata generation, plus ultra-low-cost Runware AI for realistic featured images (as low as $0.0016 per image).
It‚Äôs triggered every 3 hours or manually via Telegram. Each run generates 1,500‚Äì3,500-word articles on trending topics in Technology, AI, Tech Facts, History, or Tips ‚Äî all complete with catchy titles, slugs, meta descriptions, and visuals.
üí∞ Why This Is So Cost-Efficient
‚úÖ Free metadata + article generation (OpenRouter free-tier models)
üí∏ Only cost is for image generation via Runware:
Image generation: ~$0.0016/image (the cheapest model could be $0,00065)
This is ideal for scale ‚Äî generating dozens of high-quality blog posts with minimal expense.
‚ö° Who Is This For?
Bloggers who want high-quality content without writing
SEO marketers seeking fresh, long-form articles
WordPress site owners automating publishing workflows
Growth hackers targeting high-volume content generation
‚ùì What Problem Does It Solve?
Creating SEO-rich blog content is time-consuming, expensive, and requires expertise. BlogBlizt eliminates this friction by using free models (OpenRouter) to generate text and metadata ‚Äî making this workflow nearly free to operate, with the only cost being the featured image (~$0.0016/image).
üîß What This Workflow Does
‚è± Triggers: Every 3 hours (or via Telegram command: generate)
üß† Generates Metadata: Title, slug, category, focus keyphrase, and meta description using OpenRouter
‚úçÔ∏è Writes SEO Article: 1,500‚Äì3,500 words with headings, H2/H3s, CTA, and outbound links
üñºÔ∏è Creates Image: Generates realistic 1024√ó1024 blog image with Runware AI
üì§ Publishes to WordPress: Auto-publishes with category ID, featured image, and Yoast SEO fields
üì£ Notifies: Sends publish link to Discord and Telegram (optional)
üîê Setup Instructions
Import .json into n8n
Add credentials:
OpenRouter API (free LLMs like LLaMA-3, Nemotron)
Runware API key (get it here)
(Optional) Telegram Bot + Discord Webhook
Test by sending the word generate via Telegram
üß© Pre-Requirements
Self-hosted or cloud-based n8n instance
WordPress with category IDs configured:
Technology ‚Üí 3
AI ‚Üí 4
Tech Fact ‚Üí 7
Tech History ‚Üí 8
Tech Tips ‚Üí 9
OpenRouter API Key (free)
Runware API Key (for images)
WordPress user ID (admin, default is 1)
Telegram bot (optional)
Discord webhook (optional)
üõ†Ô∏è Customize It Further
Change writing tone or prompt in ‚úçÔ∏è Compose SEO Article Content node
Cofigure the category
Swap free LLMs (Claude, Gemini, GPT, etc.) in OpenRouter settings
Adjust schedule (every 6h, daily, etc.)
Extend alerts (email, Slack, Notion)
üß† Nodes Used
Schedule Trigger
Telegram Trigger
@n8n/n8n-nodes-langchain (OpenRouter)
WordPress Node
HTTP Request (Runware API)
Discord Webhook
Sticky Notes (Documentation inside canvas)
üìû Support
Created by Khmuhtadin Automation Studio
Website: khmuhtadin.com"
Convert RSS News to AI Avatar Videos with Heygen & GPT-4o,https://n8n.io/workflows/4288-convert-rss-news-to-ai-avatar-videos-with-heygen-and-gpt-4o/,"üé¨ Automated News-to-Video Workflow (n8n + Heygen + GPT-4o)
üìÑ Overview:
This n8n workflow turns news from an RSS feed (e.g., CNN) into short, AI-generated avatar videos using Heygen. It:
Fetches news from an RSS feed.
Logs headlines to Google Sheets.
Uses GPT-4o or Google Gemini to generate a 30‚Äì60 sec script.
Sends the script to Heygen to create an avatar video.
Monitors and retrieves the final video.
Logs video metadata (title, link, etc.) to Google Sheets.
üéØ Ideal for content creators, marketers, or media pages repurposing written news into video content at scale.
‚öôÔ∏è Setup Guide (No Sensitive Info)
üîë 1. Heygen API
Paid Heygen plan required.
Add your API key in the Setup Heygen node:
""heygen_api_key"": ""your_key_here""
Optional: Set ""avatar_id"" and ""voice_id"" as desired.
üí° 2. AI Model: GPT-4o or Gemini
GPT-4o: Use OpenAI‚Äôs node or HTTP request with your API key.
Gemini: Link your Google Cloud project and connect the Gemini node using OAuth2 credentials.
üì• 3. RSS Feed
Add an RSS node (e.g., CNN).
Extract title, link, and content.
üìä 4. Google Sheets + Drive
Connect via OAuth2:
""Google Sheets account 2""
""Google Drive account 2""
Replace sheet IDs in:
Log news to sheets
Log video URL and title to sheets
üìπ 5. Create Video (Heygen)
Send a POST request to Heygen's API using the generated script, avatar, and voice ID.
‚è≥ 6. Monitor Status
Poll the status endpoint until video is ready.
Capture the download link.
üßæ 7. Log Final Output
Save video metadata to a Google Sheet for publishing or archiving.
Set up video: Link in Workflow"
Gmail Customer Support Auto-Responder with Ollama LLM and Pinecone RAG,https://n8n.io/workflows/4760-gmail-customer-support-auto-responder-with-ollama-llm-and-pinecone-rag/,"Gmail Customer Support Auto-Responder with Ollama LLM and Pinecone RAG
Built by Setidure Technologies
Automate intelligent, friendly replies to customer queries using AI, vector search, and Gmail ‚Äî all without human effort.
Overview
This is a ready-to-deploy smart customer support automation template for businesses that want to reply to emails instantly and accurately with the warmth of a human agent.
It uses Gmail, LangChain agents, Ollama-hosted LLMs, and Pinecone vector search to craft contextual, brand-aligned replies at scale.
Note: This template uses community nodes and requires a self-hosted n8n instance.
What the Workflow Does
1. Triggers on Incoming Emails
Uses Gmail Trigger node to listen for new messages
Activates every minute to ensure fast responses
2. Classifies Email Intent
A LangChain Text Classifier detects whether the email is a Customer Support query
Non-relevant emails are skipped
3. Generates AI Response
An AI Agent powered by Ollama generates the email reply
Follows a predefined tone: ""Mr. Aashit Sharma from Setidure Technologies""
Written in a warm, human tone with natural phrasing
4. Retrieves FAQ-Based Knowledge
Connects to Pinecone vector database for real-time FAQ retrieval
Enhances responses with specific, accurate product or policy information
5. Labels Email in Gmail
Automatically tags emails with labels like Handled or Auto-Replied for easy tracking
6. Sends Email Reply
Sends the generated response back to the customer
Includes personal sign-off and clean formatting
Tech Stack Used
Gmail Trigger & Send Nodes
LangChain AI Agent & Classifier
Ollama LLMs (e.g., phi4, llama3)
Pinecone Vector Store
Custom Prompts for Brand Persona
Local Embeddings using Ollama
Key Features
Fully automated ‚Äî no human action needed
Local LLMs ensure data privacy
Real-time answers powered by vector search
Brand-personality aligned tone
Organized inbox with Gmail labels
Best For
Startups scaling support with limited staff
SaaS companies or e-commerce businesses
Privacy-conscious enterprises using local LLMs
Teams building branded auto-communication workflows
Customization Tips
Modify AI prompt to reflect your brand's voice and tone
Expand classifier for more email categories
Replace Gmail output with Slack, Notion, or your CRM
Update Pinecone FAQ index to match evolving support content"
ü§ñüöö AI agent for Transportation Orders Management with GPT-4o and Open Route API,https://n8n.io/workflows/4692-ai-agent-for-transportation-orders-management-with-gpt-4o-and-open-route-api/,"Tags: AI Agent, Supply Chain, Logistics, Route Planning, Transportation, GPS API
Context
Hi! I‚Äôm Samir ‚Äî a Supply Chain Engineer and Data Scientist based in Paris, and founder of LogiGreen Consulting.
I help companies improve their logistics operations using data, AI, and automation to reduce costs and minimize environmental footprint.
Let‚Äôs use n8n to automate order management for transportation companies!
üì¨ For business inquiries, you can add find me on LinkedIn
Who is this template for?
This workflow is designed for small transportation companies and carriers that handle their delivery request by email.
Two AI Agent nodes and a connection to Openroute Service API are used to parse the content of pickup requests
The results include driving time and driving distance in a comprehensive and concise reply generated by the second AI Agent.
How does it work?
This n8n workflow automates the processing of shipment request emails and enriches them with distance and driving time using the OpenRouteService API.
üì• Extract structured logistics data (pickup, delivery, contact) using an AI Agent
üìå Geocode pickup and delivery addresses into GPS coordinates
üöö Query OpenRouteService using the truck (driving-hgv) profile
üìè Get the driving distance (in km) and estimated time (in minutes)
üì§ Store all data in a connected Google Sheet
üìß Send back a professional confirmation email to the customer
Steps:
üì© Trigger the workflow from a new Gmail message
üß† Use the AI Agent to extract structured data from the email
üìä Record and enrich shipment details in a Google Sheet
üó∫Ô∏è Geocode addresses into coordinates
üö¶ Get driving distance & time via OpenRouteService
üì¨ Generate and send back a confirmation email using AI
What do I need to get started?
This workflow requires:
A Gmail account to receive shipment requests
A Google Sheet to log and update shipment info
A free OpenRouteService API key
üëâ Get one here
OpenAI API access to use the AI Agent node
Next Steps
üóíÔ∏è Follow the sticky notes inside the workflow canvas to:
Plug your Gmail and Google credentials
Add your OpenRouteService API key
Test by sending a sample email and see the response!

üé• Check the Tutorial
This template was built using n8n v1.93.0
Submitted: June 5, 2025"
"Build a Multi-functional Telegram Bot with Gemini, RAG PDF Search & Google Suite",https://n8n.io/workflows/4525-build-a-multi-functional-telegram-bot-with-gemini-rag-pdf-search-and-google-suite/,"The Ultimate Beginner's Guide to an AI-Powered Telegram Assistant (PDF, Brave search & Google Suite)
This comprehensive workflow bundle is designed as a powerful starter kit, enabling you to build a multi-functional AI assistant on Telegram. It seamlessly integrates AI-powered voice interactions, an intelligent PDF document search using a Retrieval-Augmented Generation (RAG) system, and automates various Google Suite tasks like calendar management and document generation. Perfect for beginners looking to explore advanced AI and automation capabilities.
Disclaimer
This template is designed for self-hosted n8n instances.
üöÄ Key Features
Telegram Bot Interface: Interact with your AI assistant using both text and voice commands through Telegram.
AI Voice Bot:
Transcribes user voice messages using OpenAI Whisper.
Processes requests with an AI agent powered by Google Gemini.
Responds with AI-synthesized voice using Replicate API.
PDF RAG System:
Index PDF documents from Google Drive via Telegram commands.
Utilizes Mistral AI for Optical Character Recognition (OCR) on PDFs.
Stores document content and embeddings in a Qdrant vector database.
Answers questions about your documents using Google Gemini, based on retrieved context.
Google Suite Automation:
Manage Google Calendar: Create events, find upcoming holidays, and list birthdays.
Google Drive: Search for PDF files and manage document templates.
Google Docs: Automatically generate invoices from templates.
Intelligent Web Search: Employs Brave Search for fetching real-time information from the web.
Versatile AI Agent: Leverages Google Gemini with a suite of tools including a calculator, date & time utilities, and custom integrations (e.g., example Airbnb tools).
Command-Driven Functionality: Easily trigger specific actions using Telegram commands like /help, /pdf, /rag, /invoice, /chat, /brave, and /birthday.
‚öôÔ∏è How It Works
The workflow is initiated by messages or commands sent to your Telegram bot. A central Switch node directs the flow based on the input received.
1. Telegram Interaction & Command Routing
A Telegram Trigger node listens for new messages (text or voice).
A ""typing..."" indicator is sent to Telegram for better user experience.
The Switch node parses the message content and routes it to the appropriate sub-workflow based on predefined commands (e.g., /pdf, /rag, /voice) or general chat.
2. AI Voice Bot Functionality
For voice messages (or if the voice path is triggered):
The voice file is downloaded from Telegram.
OpenAI Whisper transcribes the audio into text.
The transcribed text is fed to an AI Agent (powered by Google Gemini and equipped with various tools and memory).
The AI Agent's text response is then sent to the Replicate API to generate a natural-sounding voice.
The generated audio response is sent back to the user on Telegram.
3. PDF RAG System with Mistral OCR & Qdrant
Indexing PDFs (via /qdrant &lt;Google_Drive_File_ID&gt; command):
The specified PDF is downloaded from Google Drive.
The PDF is uploaded to Mistral AI for OCR processing.
The extracted text (in markdown format) is retrieved.
The text is split into manageable chunks.
OpenAI Embeddings are generated for each chunk.
These chunks and their embeddings are stored in a Qdrant vector collection.
A confirmation message is sent to Telegram.
Querying PDFs (via /rag &lt;your_question&gt; command):
The user's question is processed by a RetrievalQA Chain.
This chain uses Google Gemini as the Language Model and retrieves relevant document chunks from Qdrant based on semantic similarity (embeddings).
Google Gemini then generates a concise answer based on the user's question and the retrieved contextual information from the documents.
The answer is sent back to the user on Telegram.
Searching PDFs in Drive (via /pdf &lt;search_term&gt; command):
Searches your Google Drive for PDF files matching the provided search term.
Sends a list of found PDF files (name and ID) to the user on Telegram, allowing them to easily identify files for indexing with /qdrant.
4. AI Chat & Task Automation (General Chat & /chat command)
Handles general text messages or transcribed voice inputs that are not specific commands.
An AI Agent (Google Gemini) processes these inputs.
The agent is equipped with tools such as:
Google Calendar Tools: To create events, find the next public holiday, or list upcoming birthdays.
Brave Search Tool: To search the internet for information.
Calculator Tool: For mathematical computations.
Date & Time Utility: For handling and formatting dates.
Example Airbnb Tools: Demonstrates extensibility for custom tool integrations.
The agent utilizes a ""Think Node"" process for reasoning and Window Buffer Memory to maintain conversational context.
Responses are formulated and sent back to the user on Telegram.
5. Google Automations
Invoice Generation (via /invoice command):
Copies a predefined Google Docs invoice template from your Google Drive.
Populates the new document with details (client name, items, etc. ‚Äì currently uses placeholder data from an 'Edit Fields' node, which can be customized to parse input from the command).
Converts the populated Google Doc into a PDF file.
Sends the generated PDF invoice to the user via Telegram.
Calendar Management (via AI Agent or specific commands like /birthday):
The /birthday command fetches upcoming birthdays from a specified Google Calendar.
The AI Agent can also interact with Google Calendar to create events or retrieve information based on natural language requests.
6. Web Search (via /brave &lt;query&gt; command or AI Agent)
Performs a web search using the Brave Search API.
Returns a list of search results, including titles and URLs, to the user on Telegram.
7. Help Command (/help)
Provides the user with a formatted list of all available Telegram commands and their basic usage instructions.
üõ†Ô∏è Setup Steps & Credentials
To get this workflow bundle up and running, you'll need to configure several credentials:
Telegram:
Create a new bot via @BotFather on Telegram to obtain a Bot Token.
Add this token to the Telegram Trigger node and all Telegram (sender) nodes in the workflow.
OpenAI:
Obtain an API Key from platform.openai.com.
Create an OpenAI credential.
Use this credential in the Convert audio to text (OpenAI node for Whisper transcription) and all Embeddings OpenAI nodes (for RAG system).
Replicate API (Text-to-Speech):
Sign up at replicate.com and get your API token.
Create an HTTP Bearer Auth credential using this token.
Assign this credential to the Call Replicate API (HTTP Request) node.
Google Gemini (PaLM API):
Get an API key from Google AI Studio or Google Cloud Console.
Create a Google Gemini(PaLM) Api credential.
Assign it to all Google Gemini Chat Model nodes.
Qdrant (Vector Database):
Set up a Qdrant instance (either cloud-hosted or self-managed).
Note your Qdrant instance URL and API Key (if security is enabled).
Create a QdrantApi credential with these details.
Assign it to the Qdrant Vector Store nodes.
For the Refresh collection node (an HTTP Request node used to clear the collection for /pdf command demo), create an HTTP Header Auth credential with your Qdrant API key if required by your instance.
Mistral AI (PDF OCR):
Obtain an API key from console.mistral.ai.
Create a Mistral Cloud API credential.
Assign this to the Mistral Upload, Mistral Signed URL, and Mistral DOC OCR (HTTP Request) nodes.
Google Drive & Google Docs:
Ensure the Google Drive API and Google Docs API are enabled in your Google Cloud Console project.
Set up OAuth 2.0 credentials (Client ID and Client Secret).
Create Google Drive OAuth2 Api and Google Docs OAuth2 Api credentials.
Assign these to the respective Google Drive and Google Docs nodes throughout the workflow.
Important: Configure relevant Folder IDs (for PDF search, invoice template source, invoice output) and the invoice template Document ID in the Google Drive and Google Docs nodes.
Google Calendar:
Enable the Google Calendar API in your Google Cloud Console project.
Set up OAuth 2.0 credentials.
Create a Google Calendar OAuth2 Api credential.
Assign it to the Google CalendarTool and Google Calendar nodes.
Specify the correct calendar names or IDs in the nodes (e.g., for birthdays, holidays, new event creation).
Brave Search:
Get a Brave Search API key from their developer portal.
Create a Brave Search API credential.
Assign it to the Brave Search nodes and tools.
(Optional) Airbnb MCP Client:
The workflow includes example nodes for Airbnb MCP tools. If you intend to use or expand these, set up the corresponding MCP Client API credentials.
üí° Customization & Learning
This ""Beginner Bundle"" is not just a ready-to-use solution but also a fantastic learning resource:
Explore AI Agent Prompts: Dive into the AI Agent nodes to see how prompts are structured to guide the LLM's behavior, including the ""Think Node"" process and character guidance for robust messaging.
Modify Toolsets: Easily add or remove tools for the AI agent to expand its capabilities.
Invoice Customization: Adapt the Edit Fields node and the Google Docs template to match your invoicing needs. You can extend it to parse invoice details directly from the Telegram command.
RAG Parameters: Experiment with chunk sizes, overlap, and different embedding models in the RAG pipeline.
Extend Commands: Add new commands and corresponding functionalities by expanding the main Switch node and building out new automation paths.
By setting up and dissecting this workflow, beginners can gain a practical understanding of building complex, AI-driven applications, integrating various services, and managing different data flows."
"Generate Qualified Leads from LinkedIn with Apify, GPT- 4, and Airtable",https://n8n.io/workflows/4340-generate-qualified-leads-from-linkedin-with-apify-gpt-4-and-airtable/,"Who is this for?
This workflow is perfect for sales teams, business development professionals, recruitment agencies, and fractional CFO service providers who need to identify and qualify companies actively hiring. Whether you're prospecting for new clients, building a database of potential customers, or researching market opportunities, this automated solution saves hours of manual research while delivering high-quality, AI-analyzed leads.
What problem is this workflow solving?
Finding qualified prospects in the finance sector is time-consuming and often inefficient. Traditional methods involve:
Manually browsing LinkedIn job postings for hours
Difficulty distinguishing between genuine opportunities and recruitment spam
Inconsistent lead categorization and qualification
Risk of contacting the same companies multiple times
Lack of structured data for sales team follow-up
This workflow automates the entire lead generation process, from data collection to AI-powered qualification, ensuring you focus only on the most promising opportunities.
What this workflow does
This comprehensive lead generation system performs six key functions:
Automated LinkedIn Job Scraping: Uses Apify's reliable LinkedIn Jobs Scraper to extract detailed job postings for finance positions, including company information, job descriptions, and contact details.
Smart Data Processing: Removes duplicates, filters companies by size, and structures data for consistent analysis across all leads.
Intelligent Lead Categorization: Compares new leads against your existing database to optimize processing and avoid duplicate work.
AI-Powered Qualification: Leverages OpenAI's GPT-4 Mini to analyze each lead and determine:
Company Category: Consumer companies, Fractional CFO services, Recruiting agencies, or Other
Finance Role Validation: Confirms the position is genuinely finance-related
Seniority Level: Entry, Mid, Senior, Director, or C-Level classification
Job Summary: Concise description for quick sales team review
Automated Database Management: Stores qualified leads in Airtable with comprehensive profiles, preventing duplicates while maintaining data integrity.
Lead Scoring & Routing: Prioritizes leads based on processing status and qualification results for efficient sales team follow-up.
Setup
Prerequisites
You'll need accounts for three services:
Airtable (Free tier supported) - For lead storage and management
Apify (14-day free trial available) - For LinkedIn job scraping
OpenAI (Pay-per-use) - For AI-powered lead analysis
Step 1: Create Required Credentials
Apify API Credential
Sign up for an Apify account at apify.com
Navigate to Settings ‚Üí Integrations ‚Üí API tokens
Create a new API token
In n8n, create a new Apify API credential with your token
OpenAI API Credential
Create an account at platform.openai.com
Generate an API key in the API section
In n8n, create a new OpenAI credential with your key
Airtable Personal Access Token
Go to airtable.com/create/tokens
Create a personal access token with the following scopes:
data.records:read
data.records:write
schema.bases:read
In n8n, create a new Airtable Personal Access Token credential
Step 2: Set Up Airtable Base
Create a new Airtable base with the following structure:
Table Name: Qualified Leads
Required Fields:
- Company Name (Single line text)
- Job Title (Single line text) 
- Is Finance Job (Checkbox)
- Seniority Level (Single select: Entry, Mid, Senior, Director, C-Level)
- Company Category (Single select: Consumer, Recruiting, Fractional CFO, Other)
- Job Summary (Long text)
- Company LinkedIn (URL)
- Job Link (URL)
- Posted Date (Date)
- Location (Single line text)
- Industry (Single line text)
- Company Employees (Number)
Step 3: Configure the Workflow
Import the Workflow: Copy the JSON and import it into your n8n instance
Update Credentials: Replace placeholder credential IDs with your actual credential IDs in:
""Scrape LinkedIn Jobs"" node (Apify credential)
""OpenAI GPT-4 Mini"" node (OpenAI credential)
""Save to Airtable"" and ""Get Existing Leads"" nodes (Airtable credential)
Configure Airtable Connection: Update the base ID and table ID in both Airtable nodes
Set Search Parameters: In the ""Edit Variables"" node, configure:
linkedinUrls: Your target LinkedIn job search URLs
maxEmployees: Maximum company size filter (default: 200)
batchSize: Processing batch size for API efficiency (default: 5)
Step 4: Test the Workflow
Start with a small test by setting count: 50 in the HTTP Request node
Use a specific LinkedIn job search URL (e.g., ""CFO jobs in New York"")
Execute the workflow manually and verify results in your Airtable base
Review the AI categorization accuracy and adjust prompts if needed
How to customize this workflow to your needs
Targeting Different Roles
Modify the LinkedIn search URLs in the ""Edit Variables"" node to target different positions:
- ""https://www.linkedin.com/jobs/search/?keywords=Controller""
- ""https://www.linkedin.com/jobs/search/?keywords=Finance%20Director""  
- ""https://www.linkedin.com/jobs/search/?keywords=VP%20Finance""
Adjusting Company Size Filters
Change the maxEmployees parameter to focus on different company segments:
Startups: 1-50 employees
SMBs: 51-500 employees
Enterprise: 500+ employees
Customizing AI Analysis
Enhance the GPT-4 prompt in the ""AI Lead Analyzer"" node to include:
Industry-specific criteria
Geographic preferences
Technology stack requirements
Company growth stage indicators
Integration Options
Extend the workflow by adding:
Slack notifications for new qualified leads
Email alerts for high-priority prospects
CRM integration (Salesforce, HubSpot, Pipedrive)
Lead enrichment with additional data sources
Scheduling Automation
Set up the workflow to run automatically:
Daily: For active prospecting campaigns
Weekly: For ongoing market research
Monthly: For periodic database updates
Performance & Cost Optimization
API Efficiency: The workflow processes leads in batches to optimize API usage
Smart Deduplication: Avoids re-processing existing leads to reduce costs
Configurable Limits: Adjust batch sizes and employee count filters based on your needs
Expected Costs: Approximately $0.05-$0.20 per 100 analysed leads (OpenAI costs)
Troubleshooting
Common Issues:
Rate Limiting: Increase delays between API calls if you encounter rate limits
Data Quality: Review LinkedIn search URLs for relevance to your target market
AI Accuracy: Adjust prompts if categorisation doesn't match your criteria
Airtable Errors: Verify field names match exactly between workflow and base structure
Support Resources:
Apify LinkedIn Scraper Documentation
OpenAI API Documentation
Airtable API Reference
Transform your lead generation process with this powerful, AI-driven workflow that delivers qualified prospects ready for immediate outreach."
"Automate Microsoft Teams Meeting Analysis with GPT-4.1, Outlook & Mem.ai",https://n8n.io/workflows/4512-automate-microsoft-teams-meeting-analysis-with-gpt-41-outlook-and-memai/,"Automate Microsoft Teams Meeting Analysis with GPT-4.1, Outlook & Mem.ai
Watch the YouTube video to get started
Follow along with the blog post
Template Overview
This advanced n8n template automates Microsoft Teams meeting analysis, knowledge base creation, and email drafting directly from meeting recordings and transcripts.
It provides end-to-end automation for the following use cases:
Meeting Analysis
Connects to Microsoft Graph to retrieve meeting recordings, chat messages, and transcripts.
Analyses meeting content using AI models.
Extracts key points, action items, questions, and themes from meetings automatically.
AI-Powered Web App
Automatically creates a detailed report from past meetings.
Stores meeting summaries, metadata, and insights into an easy-to-browse frontend.
New meetings are added dynamically without manual work.
**
Knowledge Base Indexing (via Mem.ai)**
Uploads extracted meeting data into a structured knowledge base.
Supports categorisation, search and chat functionality across meetings.
Draft Follow-Up Emails
Draft personalised follow-up emails for meeting participants.
Email drafts include:
Meeting summary
Key decisions
Action items
Emails can be sent manually with a human in the loop context via Microsoft Outlook integration.
Core Components
Microsoft Graph API for accessing meetings, chats, files, and user profiles.
SharePoint API for file storage and search.
n8n Webhooks to trigger processes dynamically.
Generic OAuth2 authentication for seamless Microsoft access.
JavaScript Code Nodes for flexible, intelligent parsing and structuring of meeting data.
AI/LLM nodes for summarisation and content creation.
Designed to be scalable, modular, and easily customisable for different organisation sizes and industries.
üõ°Ô∏è Note
Failure to correctly configure Azure permissions will prevent the template from functioning. Ensure admin approval is fully completed during setup.
Important Prerequisites
‚ö†Ô∏è Administrator consent is required
This template requires a Microsoft 365 Global Administrator or Application Administrator to grant admin consent to a set of Microsoft Graph and SharePoint API scopes.
Basic Azure App Registration knowledge is required:
You need to set up an Azure App Registration manually, configure OAuth2 authentication, and assign specific API permissions.
A setup guide is included, but familiarity with:
App registrations
API permissions
Client secrets
OAuth2 flows
is highly recommended.
Knowledge of Postgres is required
The template includes a SQL script to create the required Postgres table (see blog post).
You are responsible for hosting your own database (You can use Supabase with the Postgres connection string)."
"Automate SEO Blog Content Creation with GPT-4, Perplexity AI and WordPress",https://n8n.io/workflows/3874-automate-seo-blog-content-creation-with-gpt-4-perplexity-ai-and-wordpress/,"AI Blog Publisher ‚Äì Automated Blog Content Workflow
This workflow is designed for individuals and teams who regularly publish content on their blog and want to automate the entire process from start to finish. Its main goal is to generate long-form, SEO-optimized blog posts and publish them directly to WordPress ‚Äî without needing a copywriter, editor, or someone to handle CMS uploads. The workflow generates a topic, writes a full article based on your brand guidelines, pulls a featured image from Pexels, publishes the post to WordPress, and logs the publication details to Google Sheets, creating a complete archive of published content. This allows users to deliver high-quality, search-optimized content every day while saving significant time. The whole process ‚Äî from idea to publication ‚Äî runs fully automatically and can be scheduled to execute without any manual input. It works just as well for solo creators as for marketing teams or agencies producing content at scale.
How it works
When triggered, the workflow generates a new blog post idea and checks your Google Sheet to see if the topic has already been published. If the topic is unique, it calls a sub-workflow that contains your brand‚Äôs writing style, tone, and blog goals. It then uses Perplexity AI to gather supporting research and context. Based on all this input, a complete 2000‚Äì2500 word article is generated in clean HTML, ready for WordPress. The workflow then searches Pexels for a relevant image and sets it as the featured image. Finally, it publishes the post to WordPress, including the proper title, meta description, and category. All relevant data ‚Äî such as title, link, and publish date ‚Äî is logged in your Google Sheet.
How to set up
To get this workflow running, all you need to do is connect the required APIs ‚Äî OpenAI, WordPress, Google Sheets, and Pexels ‚Äî and make a few basic adjustments. Replace the placeholder URLs in the HTTP request nodes with your actual WordPress address. In the Google Sheets nodes, add your own spreadsheet ID and tab name. In the node that fetches brand data, insert the workflow ID of your brand brief sub-workflow. You can also personalize the AI prompts by entering your blog name, company name, and a call-to-action to give the content a more tailored voice. Once that‚Äôs done, you can trigger the workflow manually or schedule it using the Schedule Trigger node."
Generate & Auto-Post Social Videos to Multiple Platforms with GPT-4 and Kling AI,https://n8n.io/workflows/3501-generate-and-auto-post-social-videos-to-multiple-platforms-with-gpt-4-and-kling-ai/,"AI-Powered Social Video Generator with Auto-Posting to Instagram, TikTok, YouTube, Facebook, LinkedIn, Threads, Pinterest, Twitter (X), and Bluesky
Who is this workflow for?
This workflow is ideal for content creators, marketers, social media managers, and automation enthusiasts who want to generate, customize, and publish short-form videos across multiple platforms without manual editing or posting. If you use tools like ChatGPT, Kling, or Blotato and want to streamline your content creation process, this workflow is made for you.
What problem does this workflow solve?
Publishing regular video content on multiple platforms is time-consuming‚Äîespecially when adding voice-overs, captions, and managing distribution. This workflow solves that by:
Automating video generation using AI (Kling + GPT-4)
Adding realistic voice narration
Styling subtitles for social media
Creating titles and social captions
Auto-posting to Instagram, TikTok, YouTube, Facebook, Threads, Twitter (X), LinkedIn, Pinterest, and Bluesky
All of this is triggered by a simple message sent via Telegram.
How the workflow works
This end-to-end automation transforms a short Telegram message into a fully produced and published social video:
Receives a text prompt from Telegram
Transforms it into a detailed video scene using GPT-4
Generates a cinematic video with Kling
Creates a voice-over script and converts it to audio
Merges the video and the audio
Adds styled captions
Writes a social caption and an SEO-optimized title
Saves metadata to Google Sheets
Sends a preview via Telegram
Publishes the video to 9 social platforms using Blotato
Setup
Connect your Telegram bot to the ""Telegram Trigger"" node
Add your OpenAI API key to all GPT-related nodes
Configure Kling API access in the ""Generate Video"" node
Link your Cloudinary account for audio upload
Connect JSON2Video to handle video merging and captioning
Set up Google Sheets with your preferred spreadsheet ID
Connect your Blotato API key and fill in the platform IDs (Instagram, TikTok, etc.)
Test by sending a Telegram message like:
generate video A robot exploring Mars, Why AI will reshape humanity
Disclaimer: This workflow uses Community Nodes, which are only available on self-hosted instances of n8n.
How to customize this workflow to your needs
Change the visual style: Adjust the GPT-4 prompt formatting to reflect your brand tone
Edit voice style: Replace the TTS provider or tweak OpenAI's voice settings
Revise captions or titles: Fine-tune the system prompts in the ""Create Description"" or ""Create Title"" nodes
Target fewer platforms: Disable or remove nodes for platforms you don‚Äôt use
Add approval steps: Insert a Telegram confirmation step before auto-publishing
üìÑ Documentation: Notion Guide
Demo Video
üé• Watch the full tutorial here: YouTube Demo"
"Digest new YouTube videos to Slack with Google Sheets, RapidAPI & GPT-4o-mini",https://n8n.io/workflows/4801-digest-new-youtube-videos-to-slack-with-google-sheets-rapidapi-and-gpt-4o-mini/,"Who is this for?
Marketing, content, and enablement teams that need a quick, human-readable summary of every new video published by the YouTube channels they care about‚Äîwithout leaving Slack.
What problem does this workflow solve?
Manually checking multiple channels, skimming long videos, and pasting the highlights into Slack wastes time. This template automates the whole loop: detect a fresh upload from your selected channels ‚Üí pull subtitles ‚Üí distill the key take-aways with GPT-4o-mini ‚Üí drop a neatly-formatted digest in Slack.
What this workflow does
Schedule Trigger fires every 10 min, then grabs a list of YouTube RSS feeds from a Google Sheet.
HTTP + XML fetch & parse each feed; only brand-new videos continue.
YouTube API fetches title/description, RapidAPI grabs English subtitles.
Code nodes build an AI payload; OpenAI returns a JSON summary + article.
A formatter turns that JSON into Slack Block Kit, and Slack posts it.
Processed links are appended back to the ‚ÄúVideo Links‚Äù sheet to prevent dupes.
Setup
Make a copy of this Google Sheet and connect a Google Sheets OAuth2 credential with edit rights.
Slack App: create ‚Üí add chat:write, channels:read, app_mention; enable Event Subscriptions; install and store the Bot OAuth token in an n8n Slack credential.
RapidAPI key for https://yt-api.p.rapidapi.com/subtitles (300 free calls/mo) ‚Üí save as HTTP Header Auth.
OpenAI key ‚Üí save in an OpenAI credential.
Add your RSS feed URLs to the ‚ÄúRSS Feed URLs‚Äù tab; press Execute Workflow.
How to customise
Adjust the schedule interval or freshness window in ‚ÄúIf newly published‚Äù.
Swap the OpenAI model or prompt for shorter/longer digests.
Point the Slack node at a different channel or DM.
Extend the AI payload to include thumbnails or engagement stats.
Use-case ideas
Product marketing: Instantly brief sales & CS teams when a competitor uploads a feature demo.
Internal learning hub: Auto-summarise conference talks and share bullet-point notes with engineers.
Social media managers: Get ready-to-post captions and key moments for re-purposing across platforms."
Convert Documents to Markdown with MinerU API and GPT-4o-mini,https://n8n.io/workflows/4808-convert-documents-to-markdown-with-mineru-api-and-gpt-4o-mini/,"How it works
This workflow automates the conversion of various document formats (such as PDF, Word, and PPT) into Markdown. It connects to the MinerU API service, which leverages OCR, formula, and table recognition to produce high-quality output. Users can initiate the process by simply uploading a document through an n8n chat interface.
Set up steps
Ensure you have a local n8n instance running.
Set up and run the MinerU MCP (MinerU Computing Platform) server locally.
Import this workflow into your n8n instance.
Configure your AI model credentials (e.g., for OpenAI, add your API Key and Base URL).
Click the ""Write Files from Disk"" node and edit the file path to your desired local save location.
Click the ""MCP Client"" node and input your MinerU MCP server address (e.g., http://localhost:8000/sse).
Click the ""Open Chat"" button to upload a file, send a message, and test the workflow."
Build an MCP Server with Airtable,https://n8n.io/workflows/3879-build-an-mcp-server-with-airtable/,"Who is this for?
This template is designed for anyone who wants to integrate MCP with their AI Agents using Airtable. Whether you're a developer, a data analyst, or an automation enthusiast, if you're looking to leverage the power of MCP and Airtable in your n8n workflows, this template is for you.
What problem is this workflow solving?
This template caters to MCP beginners seeking a hands-on example and developers looking to integrate Airtable MCP service. When integrating MCP with Airtable, manually updating AI Agents after changes to Airtable data on the MCP Server is time-consuming and error-prone.
This template automates the process, enabling the AI Agent to instantly recognize changes made to Airtable on the MCP Server. In data management, for example, it ensures that record updates or additions in Airtable are automatically detected by the AI Agent. With detailed steps, it simplifies the integration process for all users.
What this workflow does
This workflow focuses on integrating MCP with Airtable within n8n. Specifically, it allows you to build an MCP Server and Client using Airtable nodes in n8n. Any changes made to the Airtable Base/Table on the MCP Server are automatically recognized by the MCP Client in the workflow. This means that you can make changes to your Airtable (such as adding, deleting, or modifying records) on the MCP Server, and the MCP Client in the n8n workflow will immediately detect these changes without any manual intervention.
Setup
Requirements
An active n8n account.
Access to Airtable API.
A sample base and rows in Airtable that you can use to test.
An API key from your preferred LLM to power the AI agent.
Step-by-step guide
Create a new workflow in n8n: Log in to your n8n account and create a new workflow.
Add Airtable nodes: Search for and add the Airtable nodes to your workflow that you wish the MCP client to have access to.
Set up the MCP Server and Client: Use the appropriate nodes in n8n to set up the MCP Server and Client. Connect the Airtable nodes to the MCP nodes as required.
Activate and test the workflow: Talk to the chat trigger once all credentials have been updated and table data synced and try adding some rows, deleting or finding and updating cells.
How to customize this workflow to your needs
If you want to customize this workflow, you can:
Modify the triggers: You can change the conditions under which the MCP Client detects changes. For example, you can set it to detect changes only in specific fields or based on certain record values in Airtable.
Integrate with other services: You can add more nodes to the workflow to integrate with other services, such as sending notifications to Slack or triggering further actions based on the detected Airtable changes.
Need help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
Extract Business Leads from Google Maps with Dumpling AI to Google Sheets,https://n8n.io/workflows/3593-extract-business-leads-from-google-maps-with-dumpling-ai-to-google-sheets/,"Who is this for?
This workflow is built for marketers, sales teams, agencies, virtual assistants, and anyone who regularly researches or contacts local businesses. It's ideal for building lead lists, tracking competitors, or creating location-specific outreach campaigns.
What problem is this workflow solving?
Instead of manually searching Google Maps and copying business info into spreadsheets, this automation pulls structured business data (e.g. restaurants, gyms, service providers) and logs it directly into Google Sheets. It saves hours of work and ensures cleaner, more usable data.
What this workflow does
The workflow takes a Google Maps search query (like ""best restaurants in New York"") and sends it to Dumpling AI. It returns a list of places including their name, address, website, phone number, rating, and more. Each result is split into a row and automatically added to a Google Sheet.
Setup
Dumpling AI
Sign up at Dumpling AI
Generate your API key
In the HTTP Request node, select Header Auth and paste your key in the Authorization field
Google Sheets
Create a sheet with tab name Leads
Add the following column headers to row 1:
Name, Address, Phone number, Website, Rating, Price Level, Type, Booking Link, Position
Connect your Google Sheets account and link this sheet in the node
Customize the Query
In the HTTP node, replace the query string (e.g., ""best+restaurants+in+New+York"") with your own search term
Run It
Use the manual trigger to test
Optionally swap in a Schedule or Webhook node to run it automatically
How to customize this workflow to your needs
Change the search query to target different cities or business types
Use filters to only save leads with a minimum rating or price level
Add GPT to summarize listings or qualify leads
Swap Google Sheets for Airtable or a CRM system for deeper integration"
"Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3",https://n8n.io/workflows/2878-host-your-own-ai-deep-research-agent-with-n8n-apify-and-openai-o3/,"This template attempts to replicate OpenAI's DeepResearch feature which, at time of writing, is only available to their pro subscribers.
An agent that uses reasoning to synthesize large amount of online information and complete multi-step research tasks for you.
Source
Though the inner workings of DeepResearch have not been made public, it is presumed the feature relies on the ability to deep search the web, scrape web content and invoking reasoning models to generate reports. All of which n8n is really good at!
Using this workflow, n8n users can enjoy a variation of the Deep Research experience for themselves and their teams at a fraction of the cost. Better yet, learn and customise this Deep Research template for their businesses and/or organisations.
Check out the generated reports here: https://jimleuk.notion.site/19486dd60c0c80da9cb7eb1468ea9afd?v=19486dd60c0c805c8e0c000ce8c87acf
How it works
A form is used to first capture the user's research query and how deep they'd like the researcher to go.
Once submitted, a blank Notion page is created which will later hold the final report and the researcher gets to work.
The user's query goes through a recursive series of web serches and web scraping to collect data on the research topic to generate partial learnings.
Once complete, all learnings are combined and given to a reasoning LLM to generate the final report.
The report is then written to the placeholder Notion page created earlier.
How to use
Duplicate this Notion database template and make sure all Notion related nodes point to it.
Sign-up for APIFY.com API Key for web search and scraping services.
Ensure you have access to OpenAI's o3-mini model. Alternatively, switch this out for o1 series.
You must publish this workflow and ensure the form url is publically accessible.
On depth & breadth configuration
For more detailed reports, increase depth and breadth but be warned the workflow will take exponentially longer and cost more to complete. The recommended defaults are usually good enough.
Depth=1 & Breadth=2 - will take about 5 - 10mins.
Depth=1 & Breadth=3 - will take about 15 - 20mins.
Dpeth=3 & Breadth=5 - will take about 2+ hours!
Customising this workflow
I deliberately chose not to use AI-powered scrapers like Firecrawl as I felt these were quite costly and quotas would be quickly exhausted. However, feel free to switch web search and scraping services which suit your environment.
Maybe you don't decide to source the web and instead, data collection comes from internal documents instead. This template gives you freedom to change this.
Experiment with different Reasoning/Thinking models such as Deepseek and Google's Gemini 2.0.
Finally, the LLM prompts could definitely be improved. Refine them to fit your use-case.
Credits
This template is largely based off the work by David Zhang (dzhng) and his open source implementation of Deep Research: https://github.com/dzhng/deep-research"
Binance SM 1day Indicators Tool,https://n8n.io/workflows/4746-binance-sm-1day-indicators-tool/,"This advanced agent analyzes long-term price action in the Binance Spot Market using 1-day candles. It calculates key macro indicators like RSI, MACD, BBANDS, EMA, SMA, and ADX to identify high-confidence trend setups and market momentum. Used by the Quant AI system for directional bias and macro-level signal validation.
üé• Live Demo:
üéØ Purpose
Detect major trend reversals, consolidation zones, and macro bias
Support long-term swing trading decisions
Provide reliable 1-day signals for downstream agents
üß† Core Features
Feature Description
üîÅ Trigger Called by parent workflows via Execute Workflow
üì• Input Format { ""message"": ""MATICUSDT"", ""sessionId"": ""telegram_id"" }
üì° Webhook Call Sends request to internal 1d indicators webhook
üßÆ Technical Indicators RSI, MACD, BBANDS, EMA, SMA, ADX (based on 40 daily candles)
üß† GPT (gpt-4.1-mini) Agent Interprets numerical data into human-readable trend signals
üí¨ Output Summary suitable for Telegram or further agent consumption
üîó External Tools Called
https://treasurium.app.n8n.cloud/webhook/1d-indicators
Sends:
{
  ""symbol"": ""SOLUSDT""
}
üìä Indicator Calculations
Indicator Purpose
RSI (14) Overbought / Oversold Signals
MACD (12,26,9) Trend Reversals / Momentum
BBANDS (20, 2) Volatility Expansion
EMA (20) Short-Term Trend Confirmation
SMA (20) Macro-Level Support/Resistance
ADX (14) Trend Strength + Directional DI
üì¶ Setup
Import the JSON into n8n.
Add your OpenAI API credentials.
Ensure webhook /1d-indicators is connected and working.
Use this agent as a sub-workflow in:
Binance SM Financial Analyst Tool
Binance Spot Market Quant AI Agent
üì§ Output Example
üìÖ 1D Overview ‚Äì MATICUSDT

‚Ä¢ RSI: 71 ‚Üí Overbought  
‚Ä¢ MACD: Bearish Cross forming  
‚Ä¢ BBANDS: Widening Volatility  
‚Ä¢ EMA &lt; SMA ‚Üí Downtrend Momentum  
‚Ä¢ ADX: 33 ‚Üí High Trend Strength
üìå Notes
Not user-facing ‚Äî outputs are structured JSON or Telegram-style summaries.
Pairs well with shorter timeframe tools (15m‚Äì4h) for confidence stacking.
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected.
No unauthorized rebranding permitted.
üîó Need help? Reach out on LinkedIn ‚Äì Don Jayamaha"
"Generate AI Media with ComfyUI: Images, Video, 3D & Audio Bridge",https://n8n.io/workflows/4468-generate-ai-media-with-comfyui-images-video-3d-and-audio-bridge/,"Unlock low-cost, high-control generative media workflows directly from n8n by integrating with ComfyUI. Ideal for indie creators, AI developers, or small teams seeking scalable media automation‚Äîfrom images to video, 3D, and even audio‚Äîthis workflow makes generative content production more flexible and programmable than ever.
How it works
Accept a media generation request via an n8n trigger (HTTP webhook, schedule, etc.)
Parse input and inject it into a ComfyUI payload
Send the payload to your local or remote ComfyUI instance
Wait for and collect the output media files (e.g., images, videos, 3D models, or audio)
Send the result to a destination like email, Telegram, S3, or upload it back to a CMS or client app
‚ú® The format and complexity of the media are entirely dependent on the ComfyUI workflow you use, meaning this n8n integration is as powerful and creative as your ComfyUI setups.
Set up steps
Set up and run a local or remote ComfyUI instance with API access enabled
Load or create a ComfyUI workflow that suits your content goals (image gen, video stitching, etc.)
Open this n8n template and set your ComfyUI server URL, input template, and output handling preferences
Connect additional services for input (e.g., Airtable, HTTP) and output (e.g., Notion, Slack, S3) depending on your use case"
"Auto-Generate Meeting Attendee Research with GPT-4o, Google Calendar, and Gmail",https://n8n.io/workflows/3796-auto-generate-meeting-attendee-research-with-gpt-4o-google-calendar-and-gmail/,"How it works:
Whenever a new event is scheduled on your Google Calendar, this workflow generates a Meeting Briefing email, giving an overview of each person on the call and the company they work for.
It makes use of the web search tool on the OpenAI Responses API to make lookups.
The workflow triggers when a new event is added to the calendar, loops over each attendee, generating reports on each person and their company, collates the results, and sends the briefing as an email.
Set up steps:
Add your credentials for Google Calendar (for viewing events) and Gmail (to send the email)
Add your OpenAI credentials as a Header Auth on the Company Search and Person Search nodes.
Name: Authorization
Value: Bearer {{ YOUR_API_KEY }}
Edit the ""Edit Fields"" node with the email that you want to send the briefing to, and a short bit of context about yourself."
AI-Powered Gmail Email Organization with Auto-Archiving and Priority Labels,https://n8n.io/workflows/3686-ai-powered-gmail-email-organization-with-auto-archiving-and-priority-labels/,"Who is this for?
This workflow is perfect for busy professionals, students, or anyone who struggles to keep their Gmail inbox organized and clutter-free.
What problem is this workflow solving?
It helps you avoid email overload by automating the process of organizing your Gmail inbox. Unnecessary emails are archived, while important emails are categorized into ""MustRead"" or ""NotNeed"" for better prioritization.
What this workflow does
Connects to your Gmail inbox.
Automatically archives emails that are unnecessary or irrelevant.
Sorts remaining emails into two categories:
MustRead: Emails that require immediate attention.
NotNeed: Less critical emails for review later.
Setup
Connect your Gmail account to the workflow.
Define the criteria for ""MustRead"" and ""NotNeed"" emails by updating the filter rules in the nodes.
Activate the workflow to start organizing your inbox.
How to customize this workflow to your needs
Adjust the filters for archiving emails based on your specific preferences.
Modify the sorting rules for ""MustRead"" and ""NotNeed"" categories to match your workflow.
Add additional actions, such as sending notifications for ""MustRead"" emails."
"Publish WordPress Posts to Social Media X, Facebook, LinkedIn, Instagram with AI",https://n8n.io/workflows/3086-publish-wordpress-posts-to-social-media-x-facebook-linkedin-instagram-with-ai/,"Workflow Description for n8n: Social Media Post from Ideas Copy
This workflow automates the process of creating and publishing social media posts across multiple platforms (Twitter/X, Facebook, LinkedIn, and Instagram) based on content from a WordPress post. It uses AI models to generate platform-specific captions and images, and integrates with Google Sheets, WordPress, and various social media APIs.
Is a powerful tool for automating social media post creation and publishing, saving time and ensuring consistent, platform-optimized content across multiple channels.
Below is a breakdown of the workflow:
1. How It Works
The workflow is designed to streamline the creation and publishing of social media posts. Here's how it works:
Trigger:
The workflow starts with a Manual Trigger node, which initiates the process when the user clicks ""Test workflow.""
Fetch Data:
The Google Sheets node retrieves the WordPress Post ID from a predefined Google Sheet.
The Get Post node fetches the corresponding WordPress post content using the retrieved Post ID.
Generate Social Media Content:
The Social Media Manager node uses an AI model (OpenRouter) to analyze the WordPress post and generate platform-specific captions for Twitter/X, Facebook, LinkedIn, and Instagram.
The AI ensures that each caption is tailored to the platform's audience, tone, and style, including hashtags and calls to action.
Generate Images:
The Image Instagram and Image Facebook e Linkedin nodes use OpenAI to generate platform-specific images for Instagram, Facebook, and LinkedIn posts.
Publish on Social Media:
The workflow publishes the generated content and images on the respective platforms:
Publish on X: Posts the caption on Twitter/X.
Publish on LinkedIn: Posts the caption and image on LinkedIn.
Publish on Facebook: Posts the caption and image on Facebook.
Publish on Instagram: Posts the caption and image on Instagram.
Update Google Sheets:
The workflow updates the Google Sheet to mark the posts as published (e.g., ""X OK,"" ""Facebook OK,"" ""LinkedIn OK,"" ""Instagram OK"").
2. Set Up Steps
To set up and use this workflow in n8n, follow these steps:
Google Sheets Setup:
Create (or clone) a Google Sheet with columns for POST ID, TEXT, TWITTER, FACEBOOK, INSTAGRAM, and LINKEDIN.
Link the Google Sheet to the Google Sheets node by providing the Document ID and Sheet Name.
WordPress Integration:
Set up WordPress credentials in n8n for the Get Post node.
Ensure the WordPress site is accessible via its REST API.
AI Model Configuration:
Configure the OpenRouter credentials in n8n for the Social Media Manager node.
Ensure the AI model is set up to generate platform-specific captions.
Image Generation:
Set up OpenAI credentials for the Image Instagram and Image Facebook e Linkedin nodes to generate images.
Social Media API Integration:
Set up credentials for each social media platform:
Twitter/X: Configure the Publish on X node with Twitter OAuth2 credentials.
LinkedIn: Configure the Publish on LinkedIn node with LinkedIn OAuth2 credentials.
Facebook: Configure the Publish on Facebook and Publish on Instagram nodes with Facebook Graph API credentials.
Test the Workflow:
Click the ""Test workflow"" button in n8n to trigger the workflow.
The workflow will:
Fetch the WordPress post content.
Generate platform-specific captions and images.
Publish the posts on Twitter/X, Facebook, LinkedIn, and Instagram.
Update the Google Sheet to mark the posts as published.
Optional Customization:
Modify the workflow to include additional platforms or customize the AI-generated content further.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants,https://n8n.io/workflows/2846-ai-voice-chatbot-with-elevenlabs-and-openai-for-customer-service-and-restaurants/,"The ""Voice RAG Chatbot with ElevenLabs and OpenAI"" workflow in n8n is designed to create an interactive voice-based chatbot system that leverages both text and voice inputs for providing information. Ideal for shops, commercial activities and restaurants
How it works:
Here's how it operates:
Webhook Activation: The process begins when a user interacts with the voice agent set up on ElevenLabs, triggering a webhook in n8n. This webhook sends a question from the user to the AI Agent node.
AI Agent Processing: Upon receiving the query, the AI Agent node processes the input using predefined prompts and tools. It extracts relevant information from the knowledge base stored within the Qdrant vector database.
Knowledge Base Retrieval: The Vector Store Tool node interfaces with the Qdrant Vector Store to retrieve pertinent documents or data segments matching the user‚Äôs query.
Text Generation: Using the retrieved information, the OpenAI Chat Model generates a coherent response tailored to the user‚Äôs question.
Response Delivery: The generated response is sent back through another webhook to ElevenLabs, where it is converted into speech and delivered audibly to the user.
Continuous Interaction: For ongoing conversations, the Window Buffer Memory ensures context retention by maintaining a history of interactions, enhancing the conversational flow.
Set up steps:
To configure this workflow effectively, follow these detailed setup instructions:
ElevenLabs Agent Creation:
Begin by creating an agent on ElevenLabs (e.g., named 'test_n8n').
Customize the first message and define the system prompt specific to your use case, such as portraying a character like a waiter at ""Pizzeria da Michele"".
Add a Webhook tool labeled 'test_chatbot_elevenlabs' configured to receive questions via POST requests.
Qdrant Collection Initialization:
Utilize the HTTP Request nodes ('Create collection' and 'Refresh collection') to initialize and clear existing collections in Qdrant. Ensure you update placeholders QDRANTURL and COLLECTION accordingly.
Document Vectorization:
Use Google Drive integration to fetch documents from a designated folder. These documents are then downloaded and processed for embedding.
Employ the Embeddings OpenAI node to generate embeddings for the downloaded files before storing them into Qdrant via the Qdrant Vector Store node.
AI Agent Configuration:
Define the system prompt for the AI Agent node which guides its behavior and responses based on the nature of queries expected (e.g., product details, troubleshooting tips).
Link necessary models and tools including OpenAI language models and memory buffers to enhance interaction quality.
Testing Workflow:
Execute test runs of the entire workflow by clicking 'Test workflow' in n8n alongside initiating tests on the ElevenLabs side to confirm all components interact seamlessly.
Monitor logs and outputs closely during testing phases to ensure accurate data flow between systems.
Integration with Website:
Finally, integrate the chatbot widget onto your business website replacing placeholder AGENT_ID with the actual identifier created earlier on ElevenLabs.
By adhering to these comprehensive guidelines, users can successfully deploy a sophisticated voice-driven chatbot capable of delivering precise answers utilizing advanced retrieval-augmented generation techniques powered by OpenAI and ElevenLabs technologies."
‚ö°AI-Powered YouTube Video Summarization & Analysis,https://n8n.io/workflows/2679-ai-powered-youtube-video-summarization-and-analysis/,"-- Disclaimer: This workflow uses a community node and therefore only works for self-hosted n8n users --
Transform YouTube videos into comprehensive summaries and structured analysis instantly. This n8n workflow automatically extracts, processes, and analyzes video transcripts to deliver clear, organized insights without watching the entire video.
Time-Saving Features
üöÄ Instant Processing
Simply provide a YouTube URL and receive a structured summary within seconds, eliminating the need to watch lengthy videos. Perfect for research, learning, or content analysis.
ü§ñ AI-Powered Analysis
Leverages GPT-4o-mini to analyze video transcripts, organizing key concepts and insights into a clear, hierarchical structure with main topics and essential points.
Smart Processing Pipeline
üìù Automated Transcript Extraction
Supports public YouTube video
Handles multiple URL formats
Extracts complete video transcripts automatically
üß† Intelligent Content Organization
Breaks down content into main topics
Highlights key concepts and terminology
Maintains technical accuracy while improving clarity
Structures information logically with markdown formatting
Perfect For
üìö Researchers & Students
Quick comprehension of educational content and lectures without watching entire videos.
üíº Business Professionals
Efficient analysis of industry talks, presentations, and training materials.
üéØ Content Creators
Rapid research and competitive analysis of video content in your niche.
Technical Implementation
üîÑ Workflow Components
Webhook endpoint for URL submission
YouTube API integration for video details
Transcript extraction system
GPT-4 powered analysis engine
Telegram notification system (optional)
Transform your video content consumption with an intelligent system that delivers structured, comprehensive summaries while saving hours of viewing time."
Automated Resume Job Matching Engine with Bright Data MCP & OpenAI 4o mini,https://n8n.io/workflows/4330-automated-resume-job-matching-engine-with-bright-data-mcp-and-openai-4o-mini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
The Automated Resume Job Matching Engine is an intelligent workflow designed for career platforms, HR tech startups, recruiting firms, and AI developers who want to streamline job-resume matching using real-time data from LinkedIn and job boards.
This workflow is tailored for:
HR Tech Founders - Building next-gen recruiting products
Recruiters & Talent Sourcers - Seeking automated candidate-job fit evaluation
Job Boards & Portals - Enriching user experience with AI-driven job recommendations
Career Coaches & Resume Writers - Offering personalized job fit analysis
AI Developers - Automating large-scale matching tasks using LinkedIn and job data
What problem is this workflow solving?
Manually matching a resume to job description is time-consuming, biased, and inefficient. Additionally, accessing live job postings and candidate profiles requires overcoming web scraping limitations.
This workflow solves:
Automated LinkedIn profile and job post data extraction using Bright Data MCP infrastructure
Semantic matching between job requirements and candidate resume using OpenAI 4o mini
Pagination handling for high-volume job data
End-to-end automation from scraping to delivery via webhook and persisting the job matched response to disk
What this workflow does
Bright Data MCP for Job Data Extraction
Uses Bright Data MCP Clients to extract multiple job listings (supports pagination)
Pulls job data from LinkedIn with the pre-defined filtering criteria's
OpenAI 4o mini LLM Matching Engine
Extracts paginated job data from the Bright Data MCP extracted info via the MCP scrape_as_html tool.
Extracts textual job description information via the scraped job information by leveraging the Bright Data MCP scrape_as_html tool.
AI Job Matching node handles the job description and the candidate resume compare to generate match scores with insights
Data Delivery
Sends final match report to a Webhook Notification endpoint
Persistence of AI matched job response to disk
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
In n8n, configure the OpenAi account credentials.
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.

Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>.
Update the Set input fields for candidate resume, keywords and other filtering criteria's.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
Update the file name and path to persist on disk.
How to customize this workflow to your needs
Target Different Job Boards
Set input fields with the sites like Indeed, ZipRecruiter, or Monster
Customize Matching Criteria
Adjust the prompt inside the AI Job Match node
Include scoring metrics like skills match %, experience relevance, or cultural fit
Automate Scheduling
Use a Cron Node to periodically check for new jobs matching a profile
Set triggers based on webhook or input form submissions
Output Customization
Add Markdown/PDF formatting for report summaries
Extend with Google Sheets export for internal analytics
Enhance Data Security
Mask personal info before sending to external endpoints"
Tesla Quant Trading AI Agent using Telegram + GPT-4.1 (Main InterFace),https://n8n.io/workflows/4092-tesla-quant-trading-ai-agent-using-telegram-gpt-41-main-interface/,"üìà Get daily and on-demand Tesla (TSLA) trading signals via Telegram‚Äîpowered by GPT-4.1 and real-time market data.
This is the central AI supervisor that orchestrates seven sub-agents for technical analysis, price pattern recognition, and news sentiment. Reports are delivered in structured Telegram-ready HTML, optimized for traders seeking fast, intelligent decision-making signals.
‚ö†Ô∏è This master agent requires 7 connected sub-workflows to function. One of them, the News & Sentiment Agent, also requires a DeepSeek Chat API key for language processing.
üîå Required Sub-Workflows
You must download and publish the following workflows:
Tesla Financial Market Data Analyst Tool
Tesla News and Sentiment Analyst Tool (Requires DeepSeek Chat API Key)
Tesla 15min Indicators Tool
Tesla 1hour Indicators Tool
Tesla 1day Indicators Tool
Tesla 1hour & 1day Klines Tool
Tesla Quant Technical Indicators Webhooks Tool (Requires Alpha Vantage Premium API Key)
üìç See all tools at:
üîó https://n8n.io/creators/don-the-gem-dealer/
üîç What This Agent Does
Listens to your trading query via Telegram
Calls the Financial Analyst and News & Sentiment Analyst
These agents aggregate:
RSI, MACD, BBANDS, SMA, EMA, ADX
Candlestick pattern + volume divergence analysis
News summaries and sentiment scoring via DeepSeek Chat
GPT-4.1 composes the final structured TSLA trade report with:
Spot and leverage setups
Signal rationale
Confidence score
Sentiment tag
News summary
üß† Output Example
TSLA Trading Report (Daily Summary)
Spot Trade
‚Ä¢ Action: Buy
‚Ä¢ Entry: 172.45
‚Ä¢ TP: 182.00
‚Ä¢ SL: 169.80
‚Ä¢ Signal: RSI bounce + Bullish Engulfing
‚Ä¢ Sentiment: Neutral
Leveraged Position
‚Ä¢ Position: Long
‚Ä¢ Leverage: 3x
‚Ä¢ TP: 186
‚Ä¢ SL: 170
‚Ä¢ Confidence: High (83/100)
üì∞ Top News
‚Ä¢ Tesla Model Y delivery surge ‚Äì Electrek
‚Ä¢ Options market pricing in upside ‚Äì Bloomberg
‚Ä¢ FSD delayed in Canada ‚Äì TeslaNorth
üõ†Ô∏è Setup Instructions
1. Import All 8 Workflows
Ensure all sub-agents above are published in your n8n instance.
2. Create Your Telegram Bot
Use @BotFather to generate the token and connect to the trigger/send nodes.
3. Connect OpenAI GPT-4.1
Add your OpenAI credentials for GPT-4.1 in the designated node.
4. Add DeepSeek Chat API Key
Sign up at https://deepseek.com and insert your DeepSeek Chat credentials in the News Agent.
5. Add Alpha Vantage Premium API Key
Sign up at https://www.alphavantage.co/premium/
Use HTTP Header Auth for webhook-based indicator fetchers.
6. Replace Telegram ID
Update the placeholder <<replace your ID here>> with your actual Telegram numeric ID in the auth node.
üìå Included Sticky Notes
‚úÖ Telegram Bot Setup
‚úÖ Agent Routing & Memory
‚úÖ Financial vs. Sentiment Trigger Flow
‚úÖ Report Formatting (HTML)
‚úÖ API Requirements (GPT-4.1, DeepSeek, Alpha Vantage)
‚úÖ Troubleshooting & Licensing
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade report structure are IP-protected. No unauthorized rebranding permitted.
üîó For support: LinkedIn ‚Äì Don Jayamaha
üöÄ Deploy the Tesla Quant Trading AI system with GPT-4.1, DeepSeek Chat, and Alpha Vantage Premium‚Äîright into Telegram.
All 8 workflows are required.
üé• Tesla Quant AI Agent ‚Äì Live Demo
Experience the power of the Tesla Quant Trading AI Agent in action."
"Generate Images and Convert to Video Using Flux, Kraken & Runway",https://n8n.io/workflows/4056-generate-images-and-convert-to-video-using-flux-kraken-and-runway/,"(Image Generation ‚Üí Hosting ‚Üí Video Generation)
This workflow is designed for creators, automation enthusiasts, and indie hackers who want to generate image-based videos automatically using AI tools ‚Äî at a low cost.
‚öôÔ∏è Workflow Overview
This automation performs the following steps:
Trigger (Schedule or manual)
Generate an image using Flux (choose between two APIs)
Upload the image to Kraken.io to get a public URL
Send the image to Runway ML (choose between two APIs) to generate a video
Receive the video as a URL ‚Äî ready for posting, download, or further automation
üõ†Ô∏è Step-by-Step Setup
üñºÔ∏è Flux (Image Generation)
You can use either of the following providers:
Option 1: Flux by BlackForest Labs (Direct API)
üîë Get your API key here: https://docs.bfl.ml/
Paste your API key in the HTTP Request node named Flux (Blackforest)
You can customize prompts or styles inside the JSON body
Option 2: Flux via RapidAPI
üîë Subscribe and get your key here:
https://rapidapi.com/poorav925/api/ai-text-to-image-generator-flux-free-api/playground/apiendpoint_e38039ee-1912-4ef9-b4d4-270d72fca851
Enter your RapidAPI key in the X-RapidAPI-Key header
Optional: tweak prompts, style, or resolution inside the JSON body
üêô Kraken.io (Hosting the Image Publicly)
Runway ML requires the image to be publicly accessible. We use Kraken.io to host the generated image and return a public URL.
üîë Get your API credentials: https://kraken.io/account/api-credentials
Setup:
Copy your API Key and API Secret
Open the Kraken Upload node in n8n
Replace placeholders with your credentials
The node uploads your image and gives back a public image URL for Runway to use
üé¨ RunwayML (Video Generation)
You also have two options here:
Option 1: Runway Official API
üîë Get your credentials at: https://dev.runwayml.com/
Use the public image URL from Kraken in the JSON body
Paste your Bearer token in the Authorization header
Customize other settings like video length, style, FPS, etc.
Option 2: Runway via RapidAPI
üîë Subscribe and get your key here:
https://rapidapi.com/fortunehoppers/api/runwayml/playground/apiendpoint_93c8554d-8097-40cd-8252-3d4dec9c0e68
Paste your RapidAPI key in the request header
Customize prompt and generation options in the body
Use the Kraken-generated image URL as the input source
üì§ What to Do with the Video
Once the video is generated, you‚Äôll get a direct video URL. You can:
Save it to Google Sheets or Notion
Send it via email
Trigger a YouTube upload automation
Or download manually for editing and reposting
üí° Optional Tips & Notes
You can schedule this workflow to generate AI videos daily or weekly
Combine it with a Google Sheet of prompts for bulk automation
Try using a consistent visual style or theme for better branding
This workflow is lightweight and affordable ‚Äî perfect for indie projects or experimental content generation
Great for shorts, quote visuals, music loops, AI art promos, etc.
üîó Resources
Flux (Blackforest) Docs
Flux on RapidAPI
RunwayML Official Docs
Runway on RapidAPI
Kraken.io API Dashboard
üôã Need Help?
Feel free to reach out:
üê¶ Twitter: @juppfy
üìß Email: joseph@uppfy.com
If you‚Äôd like to hire me for custom n8n workflows or product automations, don‚Äôt hesitate to get in touch."
Adaptive RAG with Google Gemini & Qdrant: Context-Aware Query Answering,https://n8n.io/workflows/4043-adaptive-rag-with-google-gemini-and-qdrant-context-aware-query-answering/,"Description
This workflow automatically classifies user queries and retrieves the most relevant information based on the query type. üåü It uses adaptive strategies like;
Factual, Analytical, Opinion, and Contextual to deliver more precise and meaningful responses by leveraging n8n's flexibility. Integrated with Qdrant vector store and Google Gemini, it processes each query faster and more effectively. üöÄ
How It Works?
Query Reception: A user query is triggered (e.g., through a chatbot interface). üí¨
Classification: The query is classified into one of four categories:
Factual: Queries seeking verifiable information.
Analytical: Queries that require in-depth analysis or explanation.
Opinion: Queries looking for different perspectives or subjective viewpoints.
Contextual: Queries specific to the user or certain contextual conditions.
Adaptive Strategy Application: Based on classification, the query is restructured using the relevant strategy for better results.
Response Generation**: The most relevant documents and context are used to generate a tailored response. üéØ
Set Up Steps
Estimated Time: ‚è≥ 10-15 minutes
Prerequisites: You need an n8n account and a Qdrant vector store connection.
Steps:
Import the n8n workflow: Load the workflow into your n8n instance.
Connect Google Gemini and Qdrant: Link these tools for query processing and data retrieval.
Connect the Trigger Interface: Integrate with a chatbot or API to trigger the workflow.
Customize: Adjust settings based on the query types you want to handle and the output format. üîß
For more detailed instructions, please check the sticky notes inside the workflow. üìå"
Automate YouTube Uploads with AI-Generated Metadata from Google Drive,https://n8n.io/workflows/3906-automate-youtube-uploads-with-ai-generated-metadata-from-google-drive/,"üë• Who Is This For?
Content creators, marketing teams, and channel managers who want a simple, hands‚Äëoff solution to upload videos and automatically generate optimized metadata from video transcripts.
üõ† What Problem Does This Solve?
Manual video uploads with proper metadata creation is time‚Äëconsuming and repetitive. This workflow fully automates:
Monitoring a specific Google Drive folder for new video uploads
Seamless YouTube upload processing
Transcript extraction for context understanding
AI‚Äëpowered generation of titles, descriptions, and tags
Metadata application to uploaded videos without manual intervention
üîÑ Node‚Äëby‚ÄëNode Breakdown
Step Node Purpose
1 New Video? (Trigger) ‚Äì Monitors specified Google Drive folder
2 Download New Video ‚Äì Retrieves the video file from Google Drive
3 Upload to YouTube ‚Äì Uploads the video to YouTube with initial settings
4 Get Transcript ‚Äì Extracts transcript from the uploaded video
5 Adjust Transcript Format ‚Äì Formats raw transcript for processing
6 Create Description ‚Äì Generates SEO‚Äëoptimized description
7 YT Tags (Message Model) ‚Äì Creates relevant tags based on content
8 YT Title (Message Model) ‚Äì Generates compelling title
9 Define File Path Upload Format (Optional) ‚Äì Structures data paths
10 Update Video‚Äôs Metadata ‚Äì Applies generated title, description, tags
‚öôÔ∏è Pre‚Äëconditions / Requirements
n8n with Google Drive and YouTube API credentials configured (stored as n8n credentials/variables; no hard‚Äëcoded IDs)
Dedicated Google Drive folder for video uploads
YouTube channel with proper upload permissions
AI service access for transcript processing and metadata generation
Sufficient storage for temporary video handling
‚öôÔ∏è Setup Instructions
Import this workflow into your n8n instance.
Configure Google Drive credentials; reference folder ID via n8n variable (do not hard‚Äëcode).
Set up YouTube API credentials with upload and edit permissions.
Specify the target Google Drive folder ID in the New Video? trigger node (via variable).
Configure AI service credentials for transcript and metadata generation.
Adjust message templates for title, description, and tag creation.
Test with a small video file before production use.
üé® How to Customize
Modify AI prompts to match your channel‚Äôs tone and style.
Add conditional logic based on video categories or naming conventions.
Implement notification systems to alert when uploads complete.
Create custom metadata templates for different content types.
Include timestamps or chapter markers based on transcript analysis.
Add social media sharing nodes to announce new uploads.
‚ö†Ô∏è Important Notes
Video quality is preserved through the upload process.
Consider YouTube API quotas when handling multiple uploads.
Transcript quality affects metadata generation results.
Videos are initially uploaded without visibility adjustments.
Processing time depends on video length and transcript complexity.
üîê Security and Privacy
Store API credentials and folder IDs as n8n Credentials/Variables‚Äîremove any hard‚Äëcoded tokens or IDs.
Video files are processed temporarily and not stored permanently.
Limit Google Drive folder access to authorized users only.
Manage YouTube upload permissions carefully (use OAuth/service accounts).
Ensure compliance with organizational data‚Äëhandling policies."
Parse and Extract Data from Documents/Images with Mistral OCR,https://n8n.io/workflows/3102-parse-and-extract-data-from-documentsimages-with-mistral-ocr/,"Mistral OCR is a super convenient way to parse and extract data from multi-page PDFs or single images using AI.
What makes it special and differs it from the competition is that Mistral OCR also performs document page splitting and markdown conversion. This helps reduce dependencies required for document parsing workflows where tools like StirlingPDF.
Read the official documentation on Mistral OCR API here: https://docs.mistral.ai/capabilities/document/#tag/ocr/operation/ocr_v1_ocr_post
How it works
To access Mistral-OCR, you'll need to use Mistral Cloud API via the HTTP request node
Mistral OCR can only accept 2 file types: PDF and Image. Here, we use 2 different request to the Mistral-OCR API to parse a bank statement PDF and an screenshot of a bank statement to extract the tables.
Next, we explore a more secure method of uploading documents to the Mistral OCR API by using Mistral's cloud storage. In example 2, we first store a copy of our documents to Mistral cloud and then generate a signed URL to retreive the file before sending it to Mistral OCR. This ensures the file is not accessible publicly and protects it from unauthorised access.
Finally, another way to use Mistral-OCR is via document understanding. This allows you to ask questions about the document rather than extract contents from it. In example 3, I demonstrate this use-case asking Mistral-small to tell me how many deposits are shown in the bank statement.
How to use
Ensure your documents are either publicly accessible for Mistral-OCR or upload them to Mistral Cloud. Alternatively, signed urls from AWS S3 or Cloudflare R2 should also work.
Requirements
Mistral Cloud account and API Key. You'll also need credit on your account to use Mistral-OCR.
Customising the workflow
Mistral-OCR also works for images such as charts and diagrams so try using it on Financial Reports.
Mistral-OCR is even cheaper with batching enabled. This returns your results within 24hrs but is half the price per page."
AI Automated HR Workflow for CV Analysis and Candidate Evaluation,https://n8n.io/workflows/2860-ai-automated-hr-workflow-for-cv-analysis-and-candidate-evaluation/,"How it Works
This workflow automates the process of handling job applications by extracting relevant information from submitted CVs, analyzing the candidate's qualifications against a predefined profile, and storing the results in a Google Sheet. Here‚Äôs how it operates:
Data Collection and Extraction:
The workflow begins with a form submission (On form submission node), which triggers the extraction of data from the uploaded CV file using the Extract from File node.
Two informationExtractor nodes (Qualifications and Personal Data) are used to parse specific details such as educational background, work history, skills, city, birthdate, and telephone number from the text content of the CV.
Processing and Evaluation:
A Merge node combines the extracted personal and qualification data into a single output.
This merged data is then passed through a Summarization Chain that generates a concise summary of the candidate‚Äôs profile.
An HR Expert chain evaluates the candidate against a desired profile (Profile Wanted), assigning a score and providing considerations for hiring.
Finally, all collected and processed data including the evaluation results are appended to a Google Sheets document via the Google Sheets node for further review or reporting purposes [[9]].
Set Up Steps
To replicate this workflow within your own n8n environment, follow these steps:
Configuration:
Begin by setting up an n8n instance if you haven't already; you can sign up directly on their website or self-host the application.
Import the provided JSON configuration into your n8n workspace. Ensure that all necessary credentials (e.g., Google Drive, Google Sheets, OpenAI API keys) are correctly configured under the Credentials section since some nodes require external service integrations like Google APIs and OpenAI for language processing tasks.
Customization:
Adjust the parameters of each node according to your specific requirements. For example, modify the fields in the formTrigger node to match what kind of information you wish to collect from applicants.
Customize the prompts given to AI models in nodes like Qualifications, Summarization Chain, and HR Expert so they align with the type of analyses you want performed on the candidates' profiles.
Update the destination settings in the Google Sheets node to point towards your own spreadsheet where you would like the final outputs recorded.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Automate Contact Enrichment from HubSpot to Salesforce using Explorium.ai,https://n8n.io/workflows/4839-automate-contact-enrichment-from-hubspot-to-salesforce-using-exploriumai/,"HubSpot Contact Enrichment with Explorium
Template
Download the following json file and import it to a new n8n workflow:
hubspot_flow.json
Overview
This n8n workflow monitors your HubSpot instance for newly created contacts and automatically enriches them with additional contact information. When a contact is created, the workflow:
Detects the new contact via HubSpot webhook trigger
Retrieves recent contact details from HubSpot
Matches the contact against Explorium's database using name, company, and email
Enriches the contact with professional emails and phone numbers
Updates the HubSpot contact record with discovered information
This automation ensures your sales and marketing teams have complete contact information, improving outreach success rates and data quality.
Key Features
Real-time Webhook Trigger: Instantly processes new contacts as they're created
Intelligent Matching: Uses multiple data points (name, company, email) for accurate matching
Comprehensive Enrichment: Adds both professional and work emails, plus phone numbers
Batch Processing: Efficiently handles multiple contacts to optimize API usage
Smart Data Mapping: Intelligently maps multiple emails and phone numbers
Profile Enrichment: Optional additional enrichment for deeper contact insights
Error Resilience: Continues processing other contacts if some fail to match
Prerequisites
Before setting up this workflow, ensure you have:
n8n instance (self-hosted or cloud)
HubSpot account with:
Developer API access (for webhooks)
Private App or OAuth2 app created
Contact object permissions (read/write)
Explorium API credentials (Bearer token) - Get explorium api key
Understanding of HubSpot contact properties
HubSpot Requirements
Required Contact Properties
The workflow uses these HubSpot contact properties:
firstname - Contact's first name
lastname - Contact's last name
company - Associated company name
email - Primary email (read and updated)
work_email - Work email (updated by workflow)
phone - Phone number (updated by workflow)
API Access Setup
Create a Private App in HubSpot:
Navigate to Settings ‚Üí Integrations ‚Üí Private Apps
Create new app with Contact read/write scopes
Copy the Access Token
Set up Webhooks (for Developer API):
Create app in HubSpot Developers portal
Configure webhook for contact.creation events
Note the App ID and Developer API Key
Custom Properties (Optional)
Consider creating custom properties for:
Multiple email addresses
Mobile vs. office phone numbers
Data enrichment timestamps
Match confidence scores
Installation & Setup
Step 1: Import the Workflow
Copy the workflow JSON from the template
In n8n: Navigate to Workflows ‚Üí Add Workflow ‚Üí Import from File
Paste the JSON and click Import
Step 2: Configure HubSpot Developer API (Webhook)
Click on the HubSpot Trigger node
Under Credentials, click Create New
Enter your HubSpot Developer credentials:
App ID: From your HubSpot app
Developer API Key: From your developer account
Client Secret: From your app settings
Save as ""HubSpot Developer account""
Step 3: Configure HubSpot App Token
Click on the HubSpot Recently Created node
Under Credentials, click Create New (App Token)
Enter your Private App access token
Save as ""HubSpot App Token account""
Apply the same credentials to the Update HubSpot node
Step 4: Configure Explorium API Credentials
Click on the Explorium Match Prospects node
Under Credentials, click Create New (HTTP Header Auth)
Configure the authentication:
Name: Authorization
Value: Bearer YOUR_EXPLORIUM_API_TOKEN
Save as ""Header Auth Connection""
Apply to all Explorium nodes:
Explorium Enrich Contacts Information
Explorium Enrich Profiles
Step 5: Configure Webhook Subscription
In HubSpot Developers portal:
Go to your app's webhook settings
Add subscription for contact.creation events
Set the target URL from the HubSpot Trigger node
Activate the subscription
Step 6: Activate the Workflow
Save the workflow
Toggle the Active switch to ON
The webhook is now listening for new contacts
Node Descriptions
HubSpot Trigger: Webhook that fires when new contacts are created
HubSpot Recently Created: Fetches details of recently created contacts
Loop Over Items: Processes contacts in batches of 6
Explorium Match Prospects: Finds matching person in Explorium database
Filter: Validates successful matches
Extract Prospect IDs: Collects matched prospect identifiers
Enrich Contacts Information: Fetches emails and phone numbers
Enrich Profiles: Gets additional profile data (optional)
Merge: Combines all enrichment results
Split Out: Separates individual enriched records
Update HubSpot: Updates contact with new information
Data Mapping Logic
The workflow maps Explorium data to HubSpot properties:
Explorium Data HubSpot Property Notes
professions_email email Primary professional email
emails[].address work_email All email addresses joined
phone_numbers[].phone_number phone All phones joined with commas
mobile_phone phone (fallback) Used if no other phones found
Data Processing
The workflow handles complex data scenarios:
Multiple emails: Joins all discovered emails with commas
Phone numbers: Combines all phone numbers into a single field
Missing data: Uses ""null"" as placeholder for empty fields
Name parsing: Cleans sample data and special characters
Usage & Operation
Automatic Processing
Once activated:
Every new contact triggers the webhook immediately
Contact is enriched within seconds
HubSpot record is updated automatically
Process repeats for each new contact
Manual Testing
To test the workflow:
Use the pinned test data in the HubSpot Trigger node, or
Create a test contact in HubSpot
Monitor the execution in n8n
Verify the contact was updated in HubSpot
Monitoring Performance
Track workflow health:
Go to Executions in n8n
Filter by this workflow
Monitor success rates
Review any failed executions
Check webhook delivery in HubSpot
Troubleshooting
Common Issues
Webhook not triggering
Verify webhook subscription is active in HubSpot
Check the webhook URL is correct and accessible
Ensure workflow is activated in n8n
Test webhook delivery in HubSpot developers portal
Contacts not matching
Verify contact has firstname, lastname, and company
Check for typos or abbreviations in company names
Some individuals may not be in Explorium's database
Email matching improves accuracy significantly
Updates failing in HubSpot
Check API token has contact write permissions
Verify property names exist in HubSpot
Ensure rate limits haven't been exceeded
Check for validation rules on properties
Missing enrichment data
Not all prospects have all data types
Phone numbers may be less available than emails
Profile enrichment is optional and may not always return data
Error Handling
Built-in error resilience:
Failed matches don't block other contacts
Each batch processes independently
Partial enrichment is possible
All errors are logged for review
Debugging Tips
Check webhook logs: HubSpot shows delivery attempts
Review executions: n8n logs show detailed error messages
Test with pinned data: Use the sample data for isolated testing
Verify API responses: Check Explorium API returns expected data
Best Practices
Data Quality
Complete contact records: Ensure name and company are populated
Standardize company names: Use official names, not abbreviations
Include existing emails: Improves match accuracy
Regular data hygiene: Clean up test and invalid contacts
Performance Optimization
Batch size: 6 is optimal for rate limits
Webhook reliability: Monitor delivery success
API quotas: Track usage in both platforms
Execution history: Regularly clean old executions
Compliance & Privacy
GDPR compliance: Ensure lawful basis for enrichment
Data minimization: Only enrich necessary fields
Access controls: Limit who can modify enriched data
Audit trail: Document enrichment for compliance
Customization Options
Additional Enrichment
Extend with more Explorium data:
Job titles and departments
Social media profiles
Professional experience
Skills and interests
Company information
Enhanced Processing
Add workflow logic for:
Lead scoring based on enrichment
Routing based on data quality
Notifications for high-value matches
Custom field mapping
Integration Extensions
Connect to other systems:
Sync enriched data to CRM
Trigger marketing automation
Update data warehouse
Send notifications to Slack
API Considerations
HubSpot Limits
API calls: Monitor daily limits
Webhook payload: Max 200 contacts per trigger
Rate limits: 100 requests per 10 seconds
Property limits: Max 1000 custom properties
Explorium Limits
Match API: Batched for efficiency
Enrichment calls: Two parallel enrichments
Rate limits: Based on your plan
Data freshness: Real-time matching
Architecture Considerations
This workflow integrates with:
HubSpot workflows and automation
Marketing campaigns and sequences
Sales engagement tools
Reporting and analytics
Other enrichment services
Security Best Practices
Webhook validation: Verify requests are from HubSpot
Token security: Rotate API tokens regularly
Access control: Limit workflow modifications
Data encryption: All API calls use HTTPS
Audit logging: Track all enrichments
Advanced Configuration
Custom Field Mapping
Modify the Update HubSpot node to map to custom properties:
// Example custom mapping
{
  ""custom_mobile"": ""{{ $json.data.mobile_phone }}"",
  ""custom_linkedin"": ""{{ $json.data.linkedin_url }}"",
  ""enrichment_date"": ""{{ $now.toISO() }}""
}
Conditional Processing
Add logic to process only certain contacts:
Filter by contact source
Check for specific properties
Validate email domains
Exclude test contacts
Support Resources
For assistance:
n8n issues: Check n8n documentation and forums
HubSpot API: Reference HubSpot developers documentation
Explorium API: Contact Explorium support
Webhook issues: Use HubSpot webhook testing tools"
"Search Google, Bing, Yandex & Extract Structured Results with Bright Data MCP & Google Gemini",https://n8n.io/workflows/4820-search-google-bing-yandex-and-extract-structured-results-with-bright-data-mcp-and-google-gemini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for?
The Search Engine Intelligence Extractor is a powerful n8n automation that leverages Bright Data‚Äôs MCP based AI Agents to simulate human-like searches across Google, Bing, and Yandex, and then distills clean, structured insights using Google Gemini.
This workflow is tailored for:
SEO analysts researching competitors or market trends
Market researchers needing real-time search visibility
Journalists & content writers gathering contextual insights
AI developers creating intelligent assistants
Digital marketers tracking brand mentions or news
What problem is this workflow solving?
Traditional scraping of search engines is often blocked, cluttered, or filled with irrelevant information. Manually analyzing and cleaning this data for insight is time-consuming.
This workflow solves the problem by:
Simulating real user search behavior via Bright Data MCP based AI Agent
Performing multi-platform search (Google, Bing, Yandex) in one unified flow
Extracting clean, human-readable results (stripping ads, navigation, etc.)
Structuring the content using Google Gemini LLM
Automating delivery via Webhook or saving to disk
What this workflow does
Input Fields Node:
Accepts the search query
Accepts action for example - Perform a google search. Replace the action with bing, yandex etc. for other search providers
Accepts Webhook notification URL
Bright Data MCP Agent Execution:
Triggers Bright Data‚Äôs intelligent search agent
Handles search navigation, result loading, pagination
Human Readable Data Extractor:
Cleanses HTML, removes ads, footers, irrelevant links
Produces a readable narrative of results
Final Output Handling:
Saves the processed response to disk
Sends the structured data to a Webhook for real-time use
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
Add Scheduled Execution
Add a Cron trigger to run this workflow on a set schedule (e.g., daily/weekly keyword tracking).
Push Results to Custom Destinations
Connect output to:
Google Sheets (for analytics or dashboards)
PostgreSQL or MySQL DBs (for structured storage)
Notion or Airtable (for content pipelines)
Slack or Email (for alerting teams)
Customize Webhook Notifications
Update the Webhook URL in the notification node to push processed results to external APIs, CRMs, or real-time dashboards."
Personalized CrunchBase Lead Outreach with AI-Generated Summaries and Gmail,https://n8n.io/workflows/4790-personalized-crunchbase-lead-outreach-with-ai-generated-summaries-and-gmail/,"Automated outreach system that identifies and contacts potential leads from CrunchBase with personalized, timely messages.
üöÄ What It Does
Identifies target companies and contacts
Personalizes email content
Schedules follow-ups
Tracks responses
Integrates with email providers
üéØ Perfect For
Sales development reps
Business development teams
Startup founders
Investment professionals
Partnership managers
‚öôÔ∏è Key Benefits
‚úÖ Automated lead generation
‚úÖ Personalized outreach at scale
‚úÖ Follow-up automation
‚úÖ Response tracking
‚úÖ Time-saving workflow
üîß What You Need
CrunchBase API access
Email service (e.g., Gmail, SendGrid)
n8n instance
CRM (optional)
üìä Features
Contact information extraction
Email template personalization
Send time optimization
Open/click tracking
Response handling
üõ†Ô∏è Setup & Support
Quick Setup
Start sending in 30 minutes with our step-by-step guide
üì∫ Watch Tutorial
üíº Get Expert Support
üìß Direct Help
Transform your outbound sales process with automated, personalized outreach to high-quality leads from CrunchBase."
Binance SM 4hour Indicators Tool,https://n8n.io/workflows/4745-binance-sm-4hour-indicators-tool/,"A medium-term trend analyzer for the Binance Spot Market that leverages core technical indicators across 4-hour candle data to provide human-readable swing-trade signals via AI.
üé• Live Demo:
üéØ What It Does
Accepts a Binance trading pair (e.g., AVAXUSDT)
Sends the symbol to an internal webhook for technical indicator calculation
Computes 4h RSI, MACD, Bollinger Bands, SMA, EMA, ADX
Returns structured, GPT-analyzed signals ready for Telegram delivery
üß† AI Agent Details
Model: GPT-4.1-mini (OpenAI Chat)
Agent Role: Translates raw indicator values into sentiment-labeled signals
Memory: Tracks session + symbol context for cleaner multi-turn logic
üîó Required Backend Workflow
To calculate indicators, this tool depends on:
POST https://treasurium.app.n8n.cloud/webhook/4h-indicators
{
  ""symbol"": ""AVAXUSDT""
}
Returns a JSON object with the latest 40√ó4h candle-based calculations.
üì• Input Format
{
  ""message"": ""AVAXUSDT"",
  ""sessionId"": ""telegram_chat_id""
}
üìä Sample Output
üïì 4h Technical Signals ‚Äì AVAXUSDT

‚Ä¢ RSI: 64 ‚Üí Slightly Bullish  
‚Ä¢ MACD: Bullish Cross above baseline  
‚Ä¢ BB: Upper band touch ‚Äì volatility expanding  
‚Ä¢ EMA &gt; SMA ‚Üí Confirmed Upside Momentum  
‚Ä¢ ADX: 31 ‚Üí Strengthening Trend
üìö Use Case Scenarios
Use Case Result
Swing trend confirmation Uses 4h indicators to validate or reject setups
Breakout signal confluence Helps assess if momentum is real or noise
Inputs to Quant AI or Analyst Supports higher-frame trade recommendation synthesis
üõ†Ô∏è Setup Instructions
Import the JSON template into your n8n workspace.
Set your OpenAI API credentials for the GPT node.
Ensure the /webhook/4h-indicators backend tool is live and accessible.
Connect this to your Binance Financial Analyst Tool or master Quant AI orchestrator.
ü§ñ Parent Workflows That Use This Tool
Binance SM Financial Analyst Tool
Binance Spot Market Quant AI Agent
üìé Sticky Notes & Annotations
This workflow includes internal sticky notes describing:
Node roles (GPT, webhook, memory)
System behavior (reasoning agent logic)
Telegram formatting guidance
üîê Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
All architecture, prompt logic, and signal formatting are proprietary. Redistribution or rebranding is prohibited.
üîó Connect with the creator: Don Jayamaha ‚Äì LinkedIn"
Generate Complete Business Plans with Customizable AI Models and Specialized Agents,https://n8n.io/workflows/4558-generate-complete-business-plans-with-customizable-ai-models-and-specialized-agents/,"üëî Who is this for?
Entrepreneurs and startup founders preparing for investors
Business consultants drafting complete client plans
Strategy teams building long-term business models
Accelerators, incubators, or pitch trainers
‚ùì What problem does this workflow solve?
Writing a full business plan takes days of work, multiple tools, and often gets stuck in messy docs or slides. This template automates every major section, generating a clean, detailed, and professional business plan with AI in just minutes.
‚öôÔ∏è What this workflow does
Starts with a chat message asking for your business idea or startup concept
Passes the idea through 83 intelligent agents, each handling a full business plan chapter:
Executive Summary
Problem & Solution
Product Description
Market Research
Competitor Analysis
Business Model
Marketing Strategy (includes guerrilla ideas)
Operational Plan
Financial Plan
Team & Advisors
Roadmap
Conclusion & Next Steps
Each section uses tailored prompts and business logic
Combines all outputs into a structured, professional Markdown file
Final result: a ready-to-export business plan in seconds
üõ†Ô∏è Setup
Import this template into your n8n instance
Replace the ‚ÄúLLM Chat Model‚Äù node with your preferred model (Ollama, GPT-4, etc.)
Start from the chat input node ‚Äî describe your startup or idea
Wait for all agents to finish
Download the final Business plan file
ü§ñ LLM Flexibility (Choose Your Model)
Supports:
OpenAI (GPT-4 / GPT-3.5)
Ollama (LLaMA 3.1, Mistral, DeepSeek, etc.)
Any compatible N8N chat model
To change the model, just replace the ‚ÄúLanguage Model‚Äù node ‚Äî no other updates required
üìå Notes
All nodes are clearly named by function (e.g., ‚ÄúMarket Research Generator‚Äù)
Sticky notes included for clarity
Generates high-quality plans suitable for VCs or accelerators
Modular: you can turn off or reorder any chapter
üì© Need help?
Email: sinamirshafiee@gmail.com
Happy to support setup, LLM switching, or custom section development."
"Auto Meeting Summarizer with Google Drive, OpenAI Whisper & GPT-4 to Sheets",https://n8n.io/workflows/4370-auto-meeting-summarizer-with-google-drive-openai-whisper-and-gpt-4-to-sheets/,"üé§ Audio-to-Insights: Auto Meeting Summarizer
Transform your meeting recordings into actionable insights automatically. This powerful n8n workflow monitors your Google Drive for new audio files, transcribes them using OpenAI's Whisper, generates intelligent summaries with ChatGPT, and logs everything in Google Sheets - all without lifting a finger.
üîÑ How It Works
This workflow operates as a seamless 6-step automation pipeline:
Step 1: Smart Detection
The workflow continuously monitors a designated Google Drive folder (polls every minute) for newly uploaded audio files.
Step 2: Secure Download
When a new audio file is detected, the system automatically downloads it from Google Drive for processing.
Step 3: AI Transcription
OpenAI's Whisper technology converts your audio recording into accurate text transcription, supporting multiple audio formats.
Step 4: Intelligent Summarization
ChatGPT processes the transcript using a specialized prompt that extracts:
Key discussion points and decisions
Action items with assigned persons and deadlines
Priority levels and follow-up tasks
Clean, professional formatting
Step 5: Timestamp Generation
The system automatically adds the current date and formats it consistently for tracking purposes.
Step 6: Automated Logging
The final summary is appended to your Google Sheets document with the date, creating a searchable archive of all meeting insights.
‚öôÔ∏è Setup Steps
Prerequisites
Before setting up the workflow, ensure you have:
Active Google Drive account
OpenAI API key with credits
Google Sheets access
n8n instance (cloud or self-hosted)
Configuration Steps
1. Credential Setup
Google Drive OAuth2: Required for folder monitoring and file downloads
OpenAI API Key: Needed for both transcription (Whisper) and summarization (ChatGPT)
Google Sheets OAuth2: Essential for writing summaries to your spreadsheet
2. Google Drive Configuration
Create a dedicated folder in Google Drive for meeting recordings
Copy the folder ID from the URL (the long string after /folders/)
Update the folderToWatch parameter in the workflow
3. Google Sheets Preparation
Create a new Google Sheet or use an existing one
Ensure it has columns: Date and Meeting Summary
Copy the spreadsheet ID from the URL
Update the documentId parameter in the workflow
4. Audio Requirements
Supported Formats: MP3, WAV, M4A, MP4
Recommended Size: Under 100MB for optimal processing
Language: Optimized for English (customizable for other languages)
Quality: Clear audio produces better transcriptions
5. Workflow Activation
Import the workflow JSON into your n8n instance
Configure all credential connections
Test with a sample audio file
Activate the workflow trigger
üöÄ Use Cases
Project Management
Team Standup Summaries: Convert daily standups into actionable task lists
Sprint Retrospectives: Extract improvement points and action items
Stakeholder Updates: Generate concise reports for leadership
Sales & Customer Success
Discovery Call Notes: Capture prospect pain points and requirements
Demo Follow-ups: Track questions, objections, and next steps
Customer Check-ins: Monitor satisfaction and expansion opportunities
Consulting & Professional Services
Client Strategy Sessions: Document recommendations and implementation plans
Requirements Gathering: Organize complex project specifications
Progress Reviews: Track deliverables and milestone achievements
HR & Training
Interview Debriefs: Standardize candidate evaluation notes
Training Sessions: Create searchable knowledge bases
Performance Reviews: Document development plans and goals
Research & Development
Brainstorming Sessions: Capture innovative ideas and concepts
Technical Reviews: Log decisions and architectural choices
User Research: Organize feedback and insights systematically
üí° Advanced Customization Options
Enhanced Summarization
Modify the ChatGPT prompt to focus on specific elements:
- Add speaker identification for multi-person meetings
- Include sentiment analysis for customer calls
- Generate department-specific summaries (technical, sales, legal)
- Extract financial figures and metrics automatically
Integration Expansions
Slack Integration: Auto-post summaries to relevant channels
Email Notifications: Send summaries to meeting participants
CRM Updates: Push action items directly to Salesforce/HubSpot
Calendar Integration: Schedule follow-up meetings based on action items
Quality Improvements
Audio Preprocessing: Add noise reduction before transcription
Multi-language Support: Configure for international teams
Custom Templates: Create industry-specific summary formats
Approval Workflows: Add human review before final storage
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues
Large File Processing: Split recordings over 100MB into smaller segments
Poor Audio Quality: Use noise reduction tools before uploading
API Rate Limits: Implement delay nodes for high-volume usage
Formatting Issues: Adjust ChatGPT prompts for consistent output
Optimization Tips
Upload files in supported formats only
Ensure stable internet connection for cloud processing
Monitor OpenAI API usage and costs
Regularly backup your Google Sheets data
Test workflow changes with sample files first
üìä Expected Outputs
Sample Summary Format:
**Meeting Summary - March 15, 2024**

**Key Discussion Points:**
- Q1 budget review and allocation decisions
- New product launch timeline and milestones
- Team restructuring and role assignments

**Action Items:**
- John: Finalize budget proposal by March 20th (High Priority)
- Sarah: Schedule product demo sessions for March 25th
- Team: Submit org chart feedback by March 18th

**Decisions Made:**
- Approved additional marketing budget of $50K
- Delayed product launch to April 15th for quality assurance
- Promoted Lisa to Senior Developer role
üìû Questions & Support
For any questions, customizations, or technical support regarding this workflow:
üìß Email Support
Primary Contact: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Best For: Setup questions, customization requests, troubleshooting
üé• Learning Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Step-by-step setup tutorials
Advanced customization guides
Workflow optimization tips
üîó Professional Network
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing support
Share your workflow success stories
Get updates on new automation ideas
üí° What to Include in Your Support Request
Describe your specific use case
Share any error messages or logs
Mention your n8n version and setup type
Include sample audio file characteristics (if relevant)
Ready to transform your meeting chaos into organized insights? Download the workflow and start automating your meeting summaries today!"
Monitor and Track brand Sentiment on Facebook Groups with Bright data,https://n8n.io/workflows/4235-monitor-and-track-brand-sentiment-on-facebook-groups-with-bright-data/,"Workflow documentation updated on 21 May 2025
This workflow keeps track of your brand mentions across different Facebook groups and provides an analysis of the posts as positive, negative or neutral and updates this to Googe sheets for further analysis
This is useful and relevants for brands looking to keep track of what people are saying about their brands and guage the customer satisfaction or disatisfaction based on what they are talking about
Who is this template for?
This workflow is for you if You
Need to keep track of your brand sentiments across different niche facebook groups
Own a saas and want to monitor it across different local facebook Groups
Are looking to do some competitor research to understand what others dont like about their products
Are testing the market on different market offerings and products to get best results
Are looking for sources other that review sites for product, software or service reviews
Need to keep track of your brand sentiments across different niche facebook groups
Are starting on market research and would like to get insights from differnt facebook groups on app usage, strngths weaknesses, features etc
How it works
You will set the desired schedule by which to monitor the groups
This gets the brand names and facebook Groups to monitor.
Setup Steps
Before you begin
You will need access to a Bright Data API to run this workflows
Make a copy of the sheet below and add the urls for the facebook groups to scrap and the brand names you wish to monitor.
Import the workflow json to your canvas
Make a copy of this Google sheet to get started easily
Set your APi key in the Set up KEYs node
Map out the Google sheet to your tables
You can use/update the current AI models to differnt models eg Gemini or anthropic
Run the workflow
Setup B
Bright Data provides an option to receive the results on an external webhook via a POST call. This can be collected via the recieve results webhook node and passed to a google sheet"
Automated daily workflow backup to GitHub,https://n8n.io/workflows/4064-automated-daily-workflow-backup-to-github/,"This workflow provides a robust solution for automatically backing up all your n8n workflows to a designated GitHub repository on a daily basis. By leveraging the n8n API and GitHub API, it ensures your workflows are version-controlled and securely stored, safeguarding against data loss and facilitating disaster recovery.
How it works
The automation follows these key steps:
Scheduled trigger: The workflow is initiated automatically every day at a pre-configured time.
List existing backups: It first connects to your GitHub repository to retrieve a list of already backed-up workflow files. This helps in determining whether a workflow's backup file needs to be created or updated.
Retrieve n8n workflows: The workflow then fetches all current workflows directly from your n8n instance using the n8n REST API.
Process and prepare: Each retrieved workflow is individually processed. Its data is converted into JSON format. This JSON content is then encoded to base64, a format suitable for GitHub API file operations.
Commit to GitHub: For each n8n workflow:
A standardized filename is generated (e.g., workflow-name-tag.json).
The workflow checks if a file with this name already exists in the GitHub repository (based on the list fetched in step 2).
If the file exists: It updates the existing file with the latest version of the workflow.
If it's a new workflow (file doesn't exist): A new file is created in the repository.
Each commit is timestamped for clarity.
This process ensures that you always have an up-to-date version of all your n8n workflows stored securely in your GitHub version control system, providing peace of mind and a reliable backup history.
Pre-requisites
Before you can use this template, please ensure you have the following:
An active n8n instance (self-hosted or cloud).
A GitHub account.
A GitHub repository created where you want to store the workflow backups.
A GitHub Personal Access Token with repo scope (or fine-grained token with read/write access to the specific backup repository). This token will be used for GitHub API authentication.
n8n API credentials (API key) for your n8n instance.
Set up steps
Setting up this workflow should take approximately 10-15 minutes if you have your credentials ready.
Import the template: Import this workflow into your n8n instance.
Configure n8n API credentials:
Locate the ""Retrieve workflows"" node.
In the ""Credentials"" section for ""n8n API"", create new credentials (or select existing ones).
Enter your n8n instance URL and your n8n API Key (you can create your n8n api key in the settings of your n8n instance)
Configure GitHub credentials:
Locate the ""List files from repo"" node (and subsequently ""Update file"" / ""Upload file"" nodes which will use the same credential).
In the ""Credentials"" section for ""GitHub API"", create new credentials.
Select OAuth2/Personal Access Token authentication method.
Enter the GitHub Personal Access Token you generated as per the pre-requisites.
Specify repository details:
In the ""List files from repo"", ""Update file"", and ""Upload file"" GitHub nodes:
Set the Owner: Your GitHub username or organization name.
Set the Repository: The name of your GitHub repository dedicated to backups.
Set the Branch (e.g., main or master) where backups should be stored.
(Optional) Specify a Path within the repository if you want backups in a specific folder (e.g., n8n_backups/). Leave blank to store in the root.
Adjust schedule (Optional):
Select the ""Schedule Trigger"" node.
Modify the trigger interval (e.g., change the time of day or frequency) as needed. By default, it's set for a daily run.
Activate the workflow: Save and activate the workflow.
Explanation of nodes
Here's a detailed breakdown of each node used in this workflow:
Schedule trigger
Type: n8n-nodes-base.scheduleTrigger
Purpose: This node automatically starts the workflow based on a defined schedule (e.g., daily at midnight).
List files from repo
Type: n8n-nodes-base.github
Purpose: Connects to your specified GitHub repository and lists all files, primarily to check for existing workflow backups.
Aggregate
Type: n8n-nodes-base.aggregate
Purpose: Consolidates the list of file names obtained from the ""List files from repo"" node into a single item for easier lookup later in the ""Check if file exists"" node.
Retrieve workflows
Type: n8n-nodes-base.n8n
Purpose: Uses the n8n API to fetch a list of all workflows currently present in your n8n instance.
Json file
Type: n8n-nodes-base.convertToFile
Purpose: Takes the data of each workflow (retrieved by the ""Retrieve workflows"" node) and converts it into a structured JSON file format.
To base64
Type: n8n-nodes-base.extractFromFile
Purpose: Converts the binary content of the JSON file (from the ""Json file"" node) into a base64 encoded string. This encoding is required by the GitHub API for file content.
Commit date & file name
Type: n8n-nodes-base.set
Purpose: Prepares metadata for the GitHub commit. It generates:
commitDate: The current date and time for the commit message.
fileName: A standardized file name for the workflow backup (e.g., my-workflow-vps-backups.json), typically using the workflow's name and its first tag.
Check if file exists
Type: n8n-nodes-base.if
Purpose: A conditional node. It checks if the fileName (generated by ""Commit date & file name"") is present in the list of files aggregated by the ""Aggregate"" node. This determines if the workflow backup already exists in GitHub.
Update file
Type: n8n-nodes-base.github
Purpose: If the ""Check if file exists"" node determines the file does exist, this node updates that existing file in your GitHub repository with the latest workflow content (base64 encoded) and a commit message.
Upload file
Type: n8n-nodes-base.github
Purpose: If the ""Check if file exists"" node determines the file does not exist, this node creates and uploads a new file to your GitHub repository with the workflow content and a commit message.
Customization
Here are a few ways you can customize this template to better fit your needs:
Backup path: In the GitHub nodes (""List files from repo"", ""Update file"", ""Upload file""), you can specify a Path parameter to store backups in a specific folder within your repository (e.g., workflows/ or daily_backups/).
Filename convention: Modify the ""Commit date & file name"" node (specifically the expression for fileName) to change how backup files are named. For example, you might want to include the workflow ID or a different date format.
Commit messages: Customize the commit messages in the ""Update file"" and ""Upload file"" GitHub nodes to include more specific information if needed.
Error handling: Consider adding error handling branches (e.g., using the ""Error Trigger"" node or checking for node execution failures) to notify you if a backup fails for any reason.
Filtering workflows: If you only want to back up specific workflows (e.g., those with a particular tag or name pattern), you can add a ""Filter"" node after ""Retrieve workflows"" to include only the desired workflows in the backup process.
Backup frequency: Adjust the ""Schedule Trigger"" node to change how often the backup runs (e.g., hourly, weekly, or on specific days).
Template was created in n8n v1.92.2"
Scrape LinkedIn Job Listings for Hiring Signals & Prospecting with Bright Data,https://n8n.io/workflows/3580-scrape-linkedin-job-listings-for-hiring-signals-and-prospecting-with-bright-data/,"LinkedIn Hiring Signal Scraper ‚Äî Jobs & Prospecting Using Bright Data
Purpose:
Discover recent job posts from LinkedIn using Bright Data's Dataset API, clean the results, and log them into Google Sheets ‚Äî for both job hunting and identifying high-intent B2B leads based on hiring activity.
Use Cases:
Job Seekers ‚Äì Spot relevant openings filtered by role, city, and country.
Sales & Prospecting ‚Äì Use job posts as buying signals.
If a company is hiring for a role you support (e.g. marketers, developers, ops) ‚Äî
it's the perfect time to reach out and offer your services.
Tools Needed:
n8n Nodes:
Form Trigger
HTTP Request
Wait
If
Code
Google Sheets
Sticky Notes (for embedded guidance)
External Services:
Bright Data (Dataset API)
Google Sheets
API Keys & Authentication Required:
Bright Data API Key
‚Üí Add in the HTTP Request headers:
Authorization: Bearer YOUR_BRIGHTDATA_API_KEY
Google Sheets OAuth2
‚Üí Connect your account in n8n to allow read/write access to the spreadsheet.
General Guidelines:
Use descriptive names for all nodes.
Include retry logic in polling to avoid infinite loops.
Flatten nested fields (like job_poster and base_salary).
Strip out HTML tags from job descriptions for clean output.
Things to be Aware Of:
Bright Data snapshots take ~1‚Äì3 minutes ‚Äî use a Wait node and polling.
Form filters affect output significantly:
üîç We recommend filtering by ""Last 7 days"" or ""Past 24 hours"" for fresher data.
Avoid hardcoding values in the form ‚Äî leave optional filters empty if unsure.
Post-Processing & Outreach:
After data lands in Google Sheets, you can use it to:
Personalize cold emails based on job titles, locations, and hiring signals.
Send thoughtful LinkedIn messages (e.g., ""Saw you're hiring a CMO..."")
Prioritize outreach to companies actively growing in your niche.
Additional Notes:
üìÑ Copy the Google Sheet Template:
Click here to make your copy
‚Üí Rename for each campaign or client.
Form fields include:
Job Location (city or region)
Keyword (e.g., CMO, Backend Developer)
Country (2-letter code, e.g., US, UK)
This workflow gives you a competitive edge ‚Äî
üìå For candidates: Be first to apply.
üìå For sellers: Be first to pitch.
All based on live hiring signals from LinkedIn.
STEP-BY-STEP WALKTHROUGH
Step 1: Set up your Google Sheet
Open this template
Go to File ‚Üí Make a copy
You'll use this copy as the destination for the scraped job posts
Step 2: Fill out the Input Form in n8n
The form allows you to define what kind of job posts you want to scrape.
Fields:
Job Location ‚Üí e.g. New York, Berlin, Remote
Keyword ‚Üí e.g. CMO, AI Architect, Ecommerce Manager
Country Code (2-letter) ‚Üí e.g. US, UK, IL
üí° Pro Tip:
For best results, set the filter inside the workflow to:
time_range = ""Past 24 hours"" or ""Last 7 days""
This keeps results relevant and fresh.
Step 3: Trigger Bright Data Snapshot
The workflow sends a request to Bright Data with your input.
Example API Call Body:
[
  {
    ""location"": ""New York"",
    ""keyword"": ""Marketing Manager"",
    ""country"": ""US"",
    ""time_range"": ""Past 24 hours"",
    ""job_type"": ""Part-time"",
    ""experience_level"": """",
    ""remote"": """",
    ""company"": """"
  }
]
Bright Data will start preparing the dataset in the background.
Step 4: Wait for the Snapshot to Complete
The workflow includes a Wait Node and Polling Loop that checks every few minutes until the data is ready.
You don't need to do anything here ‚Äî it's all automated.
Step 5: Clean Up the Results
Once Bright Data responds with the full job post list:
‚úîÔ∏è Nested fields like job_poster and base_salary are flattened
‚úîÔ∏è HTML in job descriptions is removed
‚úîÔ∏è Final data is formatted for export
Step 6: Export to Google Sheets
The final cleaned list is added to your Google Sheet (first tab).
Each row = one job post, with columns like:
job_title, company_name, location, salary_min, apply_link, job_description_plain
Step 7: Use the Data for Outreach or Research
Example for Job Seekers:
You search for:
Location: Berlin
Keyword: Product Designer
Country: DE
Time range: Past 7 days
Now you've got a live list of roles ‚Äî with salary, recruiter info, and apply links.
‚Üí Use it to apply faster than others.
Example for Prospecting (Sales / SDR):
You search for:
Location: London
Keyword: Growth Marketing
Country: UK
And find companies hiring growth marketers.
‚Üí That's your signal to offer help with media buying, SEO, CRO, or your relevant service.
Use the data to:
Write personalized cold emails (""Saw you're hiring a Growth Marketer‚Ä¶"")
Start warm LinkedIn outreach
Build lead lists of companies actively expanding in your niche
API Credentials Required:
Bright Data API Key
Used in HTTP headers: Authorization: Bearer YOUR_BRIGHTDATA_API_KEY
Google Sheets OAuth2
Allows n8n to read/write to your spreadsheet
Adjustments & Customization Tips:
Modify the HTTP Request body to add more filters (e.g. job_type, remote, company)
Increase or reduce polling wait time depending on Bright Data speed
Add scoring logic to prioritize listings based on title or location
Final Notes:
üìÑ Google Sheet Template:
Make your copy here
‚öôÔ∏è Bright Data Dataset API:
Visit BrightData.com
üì¨ Personalization works best when you act quickly.
Use the freshest data to reach out with context ‚Äî not generic pitches.
This workflow turns LinkedIn job posts into sales insights and job leads.
All in one click. Fully automated. Ready for your next move."
Scalable Multi-Agent Chat Using @mentions,https://n8n.io/workflows/3473-scalable-multi-agent-chat-using-mentions/,"Summary
Engage multiple, uniquely configured AI agents (using different models via OpenRouter) in a single conversation. Trigger specific agents with @mentions or let them all respond. Easily scalable by editing simple JSON settings.
Overview
This workflow is for users who want to experiment with or utilize multiple AI agents with distinct personalities, instructions, and underlying models within a single chat interface, without complex setup. It solves the problem of managing and interacting with diverse AI assistants simultaneously for tasks like brainstorming, comparative analysis, or role-playing scenarios.
It enables dynamic conversations with multiple AI assistants simultaneously within a single chat interface. You can:
Define multiple unique AI agents.
Configure each agent with its own name, system instructions, and LLM model (via OpenRouter).
Interact with specific agents using @AgentName mentions.
Have all agents respond (in random order) if no specific agents are mentioned.
Maintain conversation history across multiple turns.
It's designed for flexibility and scalability, allowing you to easily add or modify agents without complex workflow restructuring.
Key Features
Multi-Agent Interaction: Chat with several distinct AI personalities at once.
Individual Agent Configuration: Customize name, system prompt, and LLM for each agent.
OpenRouter Integration: Access a wide variety of LLMs compatible with OpenRouter.
Mention-Based Triggering: Direct messages to specific agents using @AgentName.
All-Agent Fallback: Engages all defined agents randomly if no mentions are used.
Scalable Setup: Agent configuration is centralized in a single Code node (as JSON).
Conversation Memory: Remembers previous interactions within the session.
How to Set Up
Configure Settings (Code Nodes):
Open the Define Global Settings Code node: Edit the JSON to set user details (name, location, notes) and add any system message instructions that all agents should follow.
Open the Define Agent Settings Code node: Edit the JSON to define your agents. Add or remove agent objects as needed. For each agent, specify:
""name"": The unique name for the agent (used for @mentions).
""model"": The OpenRouter model identifier (e.g., ""openai/gpt-4o"", ""anthropic/claude-3.7-sonnet"").
""systemMessage"": Specific instructions or persona for this agent.
Add OpenRouter Credentials:
Locate the AI Agent node.
Click the OpenRouter Chat Model node connected below it via the Language Model input.
In the 'Credential for OpenRouter API' field, select or create your OpenRouter API credentials.
How to Use
Start a conversation using the Chat Trigger input.
To address specific agents, include @AgentName in your message. Agents will respond sequentially in the order they are mentioned.
Example: ""@Gemma @Claude, please continue the count: 1"" will trigger Gemma first, followed by Claude.
If your message contains no @mentions, all agents defined in Define Agent Settings will respond in a randomized order.
Example: ""What are your thoughts on the future of AI?"" will trigger Chad, Claude, and Gemma (based on your default settings) in a random sequence.
The workflow will collect responses from all triggered agents and return them as a single, formatted message.
How It Works (Technical Details)
Settings Nodes: Define Global Settings and Define Agent Settings load your configurations.
Mention Extraction: The Extract mentions Code node parses the user's input (chatInput) from the When chat message received trigger. It looks for @AgentName patterns matching the names defined in Define Agent Settings.
Agent Selection:
If mentions are found, it creates a list of the corresponding agent configurations in the order they were mentioned.
If no mentions are found, it creates a list of all defined agent configurations and shuffles them randomly.
Looping: The Loop Over Items node iterates through the selected agent list.
Dynamic Agent Execution: Inside the loop:
An If node (First loop?) checks if it's the first agent responding. If yes (true path -> Set user message as input), it passes the original user message to the Agent. If no (false path -> Set last Assistant message as input), it passes the previous agent's formatted output (lastAssistantMessage) to the next agent, creating a sequential chain.
The AI Agent node receives the input message. Its System Message and the Model in the connected OpenRouter Chat Model node are dynamically populated using expressions referencing the current agent's data from the loop ({{ $('Loop Over Items').item.json.* }}).
The Simple Memory node provides conversation history to the AI Agent.
The agent's response is formatted (e.g., **AgentName**:\n\nResponse) in the Set lastAssistantMessage node.
Response Aggregation: After the loop finishes, the Combine and format responses Code node gathers all the lastAssistantMessage outputs and joins them into a single text block, separated by horizontal rules (---), ready to be sent back to the user.
Benefits
Scalability & Flexibility: Instead of complex branching logic, adding, removing, or modifying agents only requires editing simple JSON in the Define Agent Settings node, making setup and maintenance significantly easier, especially for those managing multiple assistants.
Model Choice: Use the best model for each agent's specific task or persona via OpenRouter.
Centralized Configuration: Keeps agent setup tidy and manageable.
Limitations
Sequential Responses: Agents respond one after another based on mention order (or randomly), not in parallel.
No Direct Agent-to-Agent Interaction (within a turn): Agents cannot directly call or reply to each other during the processing of a single user message. Agent B sees Agent A's response only because the workflow passes it as input in the next loop iteration.
Delayed Output: The user receives the combined response only after all triggered agents have completed their generation."
AI-Powered Research Assistant for Platform Questions with GPT-4o and MCP,https://n8n.io/workflows/3303-ai-powered-research-assistant-for-platform-questions-with-gpt-4o-and-mcp/,"Description
This workflow empowers you to effortlessly get answers to your n8n platform questions through an AI-powered assistant. Simply send your query, and the assistant will search documentation, forum posts, and example workflows to provide comprehensive, accurate responses tailored to your specific needs.
Note: This workflow uses community nodes (n8n-nodes-mcp.mcpClientTool) and will only work on self-hosted n8n instances. You'll need to install the required community nodes before importing this workflow.
!
What does this workflow do?
This workflow streamlines the information retrieval process by automatically researching n8n platform documentation, community forums, and example workflows, providing you with relevant answers to your questions.
Who is this for?
New n8n Users: Quickly get answers to basic platform questions and learn how to use n8n effectively
Experienced Developers: Find solutions to specific technical issues or discover advanced workflows
Teams: Boost productivity by automating the research process for n8n platform questions
Anyone looking to leverage AI for efficient and accurate n8n platform knowledge retrieval
Benefits
Effortless Research: Automate the research process across n8n documentation, forum posts, and example workflows
AI-Powered Intelligence: Leverage the power of LLMs to understand context and generate helpful responses
Increased Efficiency: Save time and resources by automating the research process
Quick Solutions: Get immediate answers to your n8n platform questions
Enhanced Learning: Discover new workflows, features, and best practices to improve your n8n experience
How It Works
Receive Request: The workflow starts when a chat message is received containing your n8n-related question
AI Processing: The AI agent powered by OpenAI GPT-4o analyzes your question
Research and Information Gathering: The system searches across multiple sources:
Official n8n documentation for general knowledge and how-to guides
Community forums for bug reports and specific issues
Example workflow repository for relevant implementations
Response Generation: The AI agent compiles the research and generates a clear, comprehensive answer
Output: The workflow provides you with the relevant information and step-by-step guidance when applicable
n8n Nodes Used
When chat message received (Chat Trigger)
OpenAI Chat Model (GPT-4o mini)
N8N AI Agent
n8n-assistant tools (MCP Client Tool - Community Node)
n8n-assistant execute (MCP Client Tool - Community Node)
Prerequisites
Self-hosted n8n instance
OpenAI API credentials
MCP client community node installed
MCP server configured to search n8n resources
Setup
Import the workflow JSON into your n8n instance
Configure the OpenAI credentials
Configure your MCP client API credentials
In the n8n-assistant execute node, ensure the parameter is set to ""specific"" (corrected from ""spesific"")
Test the workflow by sending a message with an n8n-related question
MCP Server Connection
To connect to the MCP server that powers this assistant's research capabilities, you need to use the following URL:
https://smithery.ai/server/@onurpolat05/n8n-assistant
This MCP server is specifically designed to search across three types of n8n resources:
Official documentation for general platform information and workflow creation guidance
Community forums for bug-related issues and troubleshooting
Example workflow repositories for reference implementations
Configure this URL in your MCP client credentials to enable the assistant to retrieve relevant information based on user queries.
This workflow combines the convenience of chat with the power of AI to provide a seamless n8n platform research experience. Start getting instant answers to your n8n questions today!"
üß† Empower Your AI Chatbot with Long-Term Memory and Dynamic Tool Routing,https://n8n.io/workflows/3025-empower-your-ai-chatbot-with-long-term-memory-and-dynamic-tool-routing/,"Empower Your AI Chatbot with Long-Term Memory and Dynamic Tool Routing
This n8n workflow equips your AI agent with long-term memory and a dynamic tools router, enabling it to provide intelligent, context-aware responses while managing tasks across multiple tools. By combining persistent memory and modular task routing, this workflow makes your AI smarter, more efficient, and highly adaptable.
üë• Who Is This For?
AI Developers & Automation Enthusiasts: Integrate advanced AI features like long-term memory and task routing without coding expertise.
Businesses & Teams: Automate tasks while maintaining personalized, context-aware interactions.
Customer Support Teams: Improve user experience with chatbots that remember past interactions.
Marketers & Content Creators: Streamline communication across platforms like Gmail and Telegram.
AI Researchers: Experiment with persistent memory and multi-tool integration.
üöÄ What Problem Does This Solve?
This workflow simplifies the creation of intelligent AI systems that retain memory, manage tasks dynamically, and automate notifications across tools like Gmail and Telegram‚Äîsaving time and improving efficiency.
üõ†Ô∏è What This Workflow Does
Save & Retrieve Memories: Uses Google Docs for long-term storage to recall past interactions or user preferences.
Dynamic Task Routing: Routes tasks to the right tools (e.g., saving/retrieving memories or sending notifications).
AI-Powered Context Understanding: Combines OpenAI GPT-based short-term memory with long-term memory for smarter responses.
Multi-Channel Notifications: Sends updates via Gmail or Telegram.
üîß Setup
API Credentials:
Connect to OpenAI (AI processing), Google Docs (memory storage), Gmail/Telegram (notifications).
Customize Parameters:
Adjust the AI agent's system message for your use case.
Define task-routing rules in the tools router node.
Test & Deploy:
Verify memory saving/retrieval, task routing, and notification delivery.
üí° How to Customize
Modify the system message in the OpenAI node to tailor your agent‚Äôs behavior.
Add or adjust routing rules for additional tools.
Update notification settings to match your communication preferences."
Binance SM Price-24hrStats-OrderBook-Kline Tool,https://n8n.io/workflows/4742-binance-sm-price-24hrstats-orderbook-kline-tool/,"A powerful sub-agent that collects real-time market structure data from Binance for any trading pair ‚Äî including price, volume, order book depth, and candlestick snapshots across multiple timeframes (15m, 1h, 4h, 1d).
üé• Live Demo:
üéØ Purpose
This workflow powers the Quant AI system with:
‚úÖ Real-time price feed (/ticker/price)
‚úÖ 24-hour stats (OHLC, % change, volume via /ticker/24hr)
‚úÖ Live order book depth (/depth)
‚úÖ Latest candlestick data (/klines) for all major intervals
All outputs are parsed and formatted using GPT and returned to the parent agent (e.g., Financial Analyst Tool) as a Telegram-optimized summary.
‚öôÔ∏è Workflow Architecture
Node Role
üîó Execute Workflow Trigger Accepts input from parent workflow
üß† Simple Memory Stores session + symbol info
ü§ñ Binance SM Market Agent Parses prompt, routes tool calls
üí° OpenAI Chat Model (gpt-4o-mini) Converts raw data into a clean, readable format for Telegram
üåê getCurrentPrice Gets latest price
üåê get24hrStats Gets OHLC/volume over past 24 hours
üåê getOrderBook Gets top 100 bids and asks
üåê getKlines Gets latest 15m, 1h, 4h, and 1d candles
üì• Input Requirements
This workflow is not called directly by the user. Instead, it is triggered by another workflow, such as:
{
  ""message"": ""BTCUSDT"",
  ""sessionId"": ""539847013""
}
üì§ Telegram Output Example
üìä BTCUSDT Market Overview

üí∞ Price: $63,220  
üìà 24h Change: +2.3% | Volume: 45,210 BTC  

üìâ Order Book  
‚Ä¢ Top Bid: $63,190  
‚Ä¢ Top Ask: $63,230  

üï∞Ô∏è Latest Candles  
‚Ä¢ 15m: O: $63,000 | C: $63,220 | Vol: 320 BTC  
‚Ä¢ 1h : O: $62,700 | C: $63,300 | Vol: 980 BTC  
‚Ä¢ 4h : O: $61,800 | C: $63,500 | Vol: 2,410 BTC  
‚Ä¢ 1d : O: $59,200 | C: $63,220 | Vol: 7,850 BTC
‚úÖ Use Cases
Scenario Output Provided
‚ÄúShow current BTC price and trend‚Äù Price, 24h stats, candles, and order book in one message
‚ÄúCandles for SOL‚Äù 15m, 1h, 4h, 1d candlesticks for SOLUSDT
Triggered by Quant AI system Clean Telegram-ready summary with all structure tools merged
üß© Toolchain Breakdown
Tool Name Endpoint Purpose
getCurrentPrice /api/v3/ticker/price Latest trade price
get24hrStats /api/v3/ticker/24hr 24h OHLC, % change, volume
getOrderBook /api/v3/depth Top 100 bids and asks
getKlines /api/v3/klines 1-candle snapshot across 4 TFs
üöÄ Installation Steps
Import the JSON into your n8n instance
Connect your OpenAI credentials for the Chat Model node
No Binance API key needed ‚Äî public endpoints
Trigger this tool only via:
Binance SM Financial Analyst Tool
Binance Spot Market Quant AI Agent
üîê Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and trade structure are IP-protected. No unauthorized rebranding permitted.
üîó For support: Don Jayamaha ‚Äì LinkedIn"
Automate Candidate Screening with LlamaIndex & GPT-4o-mini for Personalized Email Responses,https://n8n.io/workflows/4727-automate-candidate-screening-with-llamaindex-and-gpt-4o-mini-for-personalized-email-responses/,"This n8n workflow ‚Äî HRMate ‚Äî streamlines your entire recruitment process by automatically parsing incoming job applications, evaluating candidate fit using AI, and sending personalized acceptance or rejection emails ‚Äî all without manual intervention.
It triggers on every new email application, extracts candidate info and attachments (PDF resumes), leverages** LlamaIndex AI** for content parsing, then scores candidates against job requirements via OpenAI. Finally, it sends tailored emails and updates candidate status automatically.
üí° Why Use HRMate Fix?
Save hours of manual CV screening and email replies
Never ghost applicants again ‚Äî automatic, polite rejections with AI-generated growth suggestions
Increase hiring accuracy with AI-assisted candidate scoring
Centralize communication with auto-acceptance invites including calendly scheduling links
Fully customizable and scalable for multiple job openings
‚ö° Who Is This For?
HR teams overwhelmed with incoming applications
Small businesses wanting professional recruitment automation
Recruiters seeking data-driven candidate assessments
Companies aiming to improve applicant experience and speed
‚ùì What Problem Does It Solve?
Screening candidates manually is slow, inconsistent, and often leads to candidates being ignored (ghosted). HRMate Fix automates parsing, evaluation, and personalized email communication, drastically reducing workload and improving candidate engagement.
üîß What This Workflow Does
‚è± Trigger: On every new incoming email application via Gmail Trigger (polls every minute)
üìé Attachment Check: Detects and uploads PDF resumes to LlamaIndex AI for parsing
üîç Parsing: Waits and retrieves clean markdown text of candidate documents
ü§ñ AI Evaluation: Matches candidate profile against job criteria using GPT-4o-mini, scoring fit from 1 to 100
üíå Email Communication:
If score ‚â• 80 ‚Üí sends acceptance email with calendly interview scheduling link
If score < 80 ‚Üí sends rejection email with professional, motivational feedback generated by AI
üóÇ Status Update: Marks candidate as accepted/rejected and optionally updates Google Sheets
üîê Setup Instructions
Import .json workflow into n8n
Add credentials:
Gmail OAuth2 (trigger & email sending)
LlamaIndex API key (for PDF parsing)
OpenAI API key (GPT-4o-mini model for evaluation)
Customize job criteria in ""Set Job Criteria Text"" node
Update email templates and calendly links as needed
Test workflow with real incoming applications
üß© Pre-Requirements
n8n instance (self-hosted or cloud)
Gmail account with API credentials
LlamaIndex API access
OpenAI API key
Google Sheets (optional, for status tracking)
üõ†Ô∏è Customize It Further
Add new job positions by replicating job criteria and subject filters
Modify evaluation criteria or prompts in OpenAI node
Integrate Slack or other notification nodes
Adjust email content and branding for acceptance/rejection messages
üß† Nodes Used
Gmail Trigger
HTTP Request (LlamaIndex API)
Wait node (poll parsing status)
Code node (markdown cleaning)
OpenAI GPT-4o-mini node (candidate evaluation)
IF nodes (conditional email flow)
Gmail node (send acceptance/rejection emails)
Set nodes (data preparation & status marking)
Google Sheets (optional update)
Sticky Notes for documentation
üìû Support
Created by Khmuhtadin Automation Studio
Website: khmuhtadin.com"
Track Website Visibility in Google's AI Overview with SerpAPI and Google Sheets,https://n8n.io/workflows/4724-track-website-visibility-in-googles-ai-overview-with-serpapi-and-google-sheets/,"This template helps anyone track how often their website appears in Google‚Äôs AI Overview. a growing part of search results that can‚Äôt currently be tracked using traditional SEO tools.
With this workflow, users can:
Input a list of keywords (from Google Search Console or manual research).
Use the SerpApi to pull Google search results.
Extract AI Overview content and its list of sources.
Map that information into a structured Google Sheet, including whether your site is listed in those sources.
Setup is straightforward and fully automated, but you'll need:
A SerpApi key
A connected Google Sheets account
Who is this for?
This workflow is designed for SEO professionals, digital marketers, and site owners who want to track their website‚Äôs visibility in Google AI Overviews.
What problem does it solve?
AI Overviews are rapidly becoming more common in Google search results. However, there's no tool (yet) that tells you if your website is appearing in those answers. This is a blind spot for SEO. This workflow helps you check your site‚Äôs presence in AI Overviews manually, at scale.
What does the workflow do?
The workflow:
Takes a list of target keywords (exported from GSC or elsewhere)
Uses SerpApi to get search results from Google
Extracts the AI Overview block and its sources
Checks if your domain is among them
Saves all results into a Google Sheet
The final Google Sheet will contain: Keyword | AI Overview Exists | List of Sources | Is my domain listed
Setup
You‚Äôll need:
A SerpApi API key
A Google Sheet with your list of keywords
A connected Google Sheets account in n8n
How to customize this workflow
Change the list of keywords (pull from GSC or edit the sheet manually)
Replace the placeholder domain with your own
Adjust the Google Sheet column mapping as needed"
WhatsApp AI Customer Service Bot with GPT-4o-mini and Gmail Alerts,https://n8n.io/workflows/4456-whatsapp-ai-customer-service-bot-with-gpt-4o-mini-and-gmail-alerts/,"Who is this for?
This workflow is intended for WhatsApp admins, customer service, individual users or teams in businesses who want to automate their small business or medium to large businesses on WhatsApp practically by automatically replying about your business products or services using Open AI capabilities on AI Agent. Based on an automated system there is a task to answer many customers or clients on WhatsApp, in reality answering one by one is very time consuming and tiring, especially if we have to compose sentences first or click on the template and send them. This is also a form of devotion to the community at n8n and the n8n company, as well as devotion to small businesses so that the reality is no longer tiring and able to answer the problems of reality that exist.
How does it work?
Easy explanation:
Whatsapp trigger here the ""Input submissions"" node is responsible for inputting chats from customers in the form of data. Then, it is sent to the next node, namely the if node, here there are two if nodes, namely ""Signpost"" and ""Is text message?"" responsible for forwarding customer chat input data and selecting if correct then it will be forwarded to the AI Agent and if wrong then there is a ""Fallback Text"" node. Once correct, the AI Agent will analyze with the OpenAI model capability and store it in Simple Memory. The AI Agent here has been given directions and commands on what it should be like, in the parameters section, the prompt contains a special task, the knowledge base of your business, and several points and rules that must be there. After that, it will be sent via the WhatsApp reply node which is addressed to the customer. After that, the Gmail Set will process it again to be addressed to the Send Gmail Notifications Node, which contains email address parameters and also the contents of the email as a notification of success or failure. Then, there are no operations after that after the Gmail node. And enjoy this workflow that works for you.
Setup instructions
Complete what is in the node as stated in the notes column.
You need a ""Credential Account"" on each WhatsApp node, then the OpenAI node and the Gmail Node. If you already have an account, just connect it, and if not, create it first, you can register it by following the guide from n8n, it makes it very easy.
Because here it has been set up neatly and cleanly like in the if node. Next, you need to add your business to the AI Agent node, there are parameters and a prompt section, I have made a sample here to make it easier for you. You can immediately adjust it to your business or service or product, later what communication or sentence you want to convey to customers on WhatsApp. After that, don't forget to add an email address to the Gmail node for notification of success or failure of each customer from the running of this system.
Save and run, test the workflow and activate the workflow. This workflow is ready to use.
Requirements
As a reminder:
Must be set in nodes such as your business description, also according to the conditions of your business, services, products, so that the AI Agent matches your business knowledge base
Must have (if not, make sure you have registered) in each ""Credential Account"" by following the guide on how to create it n8n the guide is very complete
Don't forget to save, and make sure the workflow is active.
How to customize this workflow to your needs
You can immediately set up your business knowledge base in the nodes, so that the accuracy is also high when carrying out tasks and answering them."
Parse Incoming Invoices From Outlook using AI Document Understanding,https://n8n.io/workflows/3396-parse-incoming-invoices-from-outlook-using-ai-document-understanding/,"This n8n template monitors an Outlook mailbox for invoices, automatically parses/extracts data from them and then uploads the output to an Excel Workbook.
One of my top workflow requests, this template can save many hours of manual labour for you or your finance/accounts team.
How it works
A scheduled trigger is set to fetch recent Outlook messages to the Accounts receivable mailbox.
Each message is analysed to determine whether or not it from a supplier and is issuing/contains an invoice.
For each valid message, the attachments are downloaded and non-invoice documents are filtered out via AI Vision classification.
Invoices are then processed through a AI vision model again to extract the details.
The extracted data can then be used for reconciliation or otherwise. For this demonstration, we'll just append the row to an Excel sheet for now.
How to use
Ensure your Microsoft365 credential points to the correct mailbox. If a shared folder is used, toggle ""shared folder"" option to ""on"" and for the principal ID, use the email address.
If you receive lots of other types of messages such as replies and forwards, you may want to implement additional checks to prevent processing invoices twice. The ""remove duplicates"" node can help with this.
Requirements
Outlook for Mailbox
Google Gemini for Document Understanding and Invoice Extraction
Excel for Data Storage
Customising this workflow
Note the assumption for this template is that all invoices will come as a PDF attachment. In real life, this is rarely the case! Adding in document conversion to cover all invoice formats.
Human feedback is also an important factor in AI workflows. Try tagging emails as a way to notify team members that the invoice was processed."
"Resume Screening & Behavioral Interviews with Gemini, Elevenlabs, & Notion ATS",https://n8n.io/workflows/3765-resume-screening-and-behavioral-interviews-with-gemini-elevenlabs-and-notion-ats/,"Description
Candidate Engagement | Resume Screening | AI Voice Interviews | Applicant Insights
This intelligent n8n workflow automates the process of extracting and scoring resumes received through a company career page, populating a Notion database with AI insights where the recruiter or hiring manager can automatically invite the applicant to an instant interview with an Elevenlabs AI voice agent. After the agent conducts the behavior-based interview, the workflow scores the overall interview against customizable evaluation criteria and updates the Notion database with AI insights about the applicant.
AI Powered Resume Screening & Voice AI that interviews like a Recruiter!
AI Insights in Notion dashboard
Who is this for?
HR teams, recruiters, and talent acquisition professionals
This workflow is ideal for HR teams, recruiters, and talent acquisition professionals looking for a foundational, extensible framework to automate early stage recruiting. Whether you're exploring AI for the first time or scaling automation across your hiring process, this template provides a base for screening, interviewing, and tracking candidates‚Äîpowered entirely by n8n, Elevenlabs, Notion, and LLM integrations. Be sure to consult State and Country regulations with respect to AI Compliance, AI Bias Audits, AI Risk Assessment, and disclosure requirements.
What problem is this workflow solving?
Manually screening resumes and conducting initial interviews slows down hiring. This template automates:
Resume assessment against job description.
Scheduling first and second round interviews.
First-round AI-led behavioral interviews with AI scoring assessment.
Centralized tracking of AI assessments in Notion.
What this does
This customizable tool, configured to manage 3 requisitions in parallel, automates the application process, resume screen, and first round behavioral interviews.
Pre-screen Applicants with AI
Immediately screens and scores applicant‚Äôs resume against the job description. The AI Agent generates a score and an AI assessment, adding both to the applicant's profile in Notion. Notion automatically notifies hiring manager when a resume receives a score of 8 or higher.
Voice AI that Interviews like a Recruiter
AI Voice agent adapts probing questions based on applicant‚Äôs response and intelligently dives deeper into skill and experience to assess answers against a scoring rubric for each question.
AI Applicant Insights in Notion
Get detailed post-interview AI analysis, including interview recordings and question-by-question scoring breakdowns to help identify who you should advance to the next stage in the process.
AI insight provided in Notion ATS dashboard with drag and drop to advance top candidates to the next interview stage.
How it works
Link to Notion Template
Notion Career Page:
Notion Career Page published to web, can be integrated with your preferred job board posting system.
Notion Job Posting:
Gateway for applicants to apply to active requisitions with ‚ÄòClick to Apply‚Äô button.
Application Form:
N8N webform embedded into Notion job posting captures applicant information and routes for AI processing.
AI Agent evaluates resume against job description
AI Agent evaluates resume against the job description, stored in Notion, and scores the applicant on a scale of 1 to 10, providing rationale for score.
Creates ATS record in Notion with assessment and score
Workflow creates the applicant record in the Notion ATS where Recruiters and Hiring Managers see applicants in a filtered view, sorted by AI generated resume score. Users can automatically advance applicants to the next step in process (AI Conversation interview) with drag and drop functionality.
Invites applicant to an Instant AI Interview
Dragging the applicant to AI Interview step in the Notion ATS dashboard triggers Notion automation that sends the applicant an email with a link to the Elevenlabs Conversation AI Agent. The AI Conversation Agent is provided with instructions on how to conduct the behavior-based interview, including probing questions, for the specific role.
AI Conversation Agent Behavior Based Interview
The email link resolves to an ElevenLabs AI Conversation agent that has been instructed to interview applicants using pre-defined interview questions, scoring rubric, job description, and company profile. The Elevenlabs agent assesses the applicant on a scale of 1 to 5 for each interview question and provides an overall assessment of the interview based on established evaluation criteria.
Click to hear AI Voice Agent in action
Example:
Role: IT Support Analyst
Mark: Elevenlabs AI Agent instructed to interview applicants for specific role
Gemini: Google AI coached to answer questions as an IT Support Analyst being interviewed
Updates Notion record with Interview Assessment and Score
All results‚Äîincluding the conversation transcript, interview scores, and rationale for assessment are automatically added back to the applicant‚Äôs profile in Notion where the Hiring Manager can validate the AI assessment by skimming through the embedded audio file.
AI Interview Overall Score: 1 to 5 based on response to all questions and probes.
AI Agent confirms that it was able to evaluate the interview using the assigned rubric.
AI Interview Criteria Score: Success/Failure based on response to individual interview questions.
Invites applicant to second interview with Hiring Manager
Dragging the applicant to the ‚ÄòHiring Manager Interview‚Äô step in the Notion ATS dashboard triggers a Notion automation that sends an email with a link to the Hiring Manager‚Äôs calendar scheduling solution.
Configuration and Set Up
Accounts & API Keys
You wil need accounts and credentials for:
n8n (hosted or self-hosted)
Elevenlabs (for AI Conversation Agent)
Gemini (for LLM model access)
Google Drive (to back up applicant data)
Calendly (to automate interview scheduling)
Gmail (to automate interview scheduling)
Data / Documents to implement
Job Descriptions for each role
Interview questions for each role
Evaluation criteria for each interview question
Notion Set Up
Customize your Notion Career Page
Link to Free Notion Template that enables workflow:
Update Notion job description database
-update job description(s) for each role
-add interview questions to the job description database page in Notion
-add evaluation criteria to the job description database page in Notion
-edit each ‚ÄòClick to Apply‚Äô button in the job description template so it resolves to the corresponding N8N 'Application Form' webform production URL (detail provided below)
Notion Applicant Tracker
In the Applicant Tracker database, update position titles, tab headings, in the custom database view (Notion) so it reflects the title of the position you are posting. Edit the filter for each tab so it matches the position title.
Notion Email Automation
Update Notion automation templates used to invite applicants to the AI Interview and Hiring Manager interview. Note: Trigger email automation by dragging applicant profile to the next Applicant Comm Status in the Applicant Tracker.
AI Interview invite template: revise position title to reflect the title of the role you are posting; include the link to your Conversation AI Agent for that role in the email body. Note: each unique role will use an Elevenlabs AI conversation agent designed for that role.
Hiring Manager Interview invite template: revise position title to reflect the title of the role you are posting; include the link to your Calendly page or similar solution provider to automate interview scheduling.
N8N Configuration
Workflow 1
Application Forms (3 Nodes - one for each job)
Update the N8N form title and description to match the job description you configured in Notion.
Confirm Job Code in Applicant Form node matches Job Code in Notion for that position.
Edit the Form Response to customize the message you want displayed after applicant clicks submit.
Upload CV - Google Drive
Authenticate your Google Drive account and select the folder that will be used to store resumes
Get Job Description - Notion
Authenticate your Notion account and select your Career Page from the list of databases that contain your job descriptions.
Applicant Data Backup - Google Sheet
Create a Google Sheet where you will track applicant data for AI Compliance reporting requirements. Open the node in n8n and use the field names in the node as Google Sheet column headings.
Workflow 2
Elevenlabs Web Hook (Node 1)
Edit the Web Hook POST node and copy your production URL that is displayed in the Node. This URL is entered into the Elevenlabs AI Conversation Agent post-call webhook described below.
AI Agent
Authenticate your LLM model (Gemini in this example) and add your Notion database as a tool to pull the evaluation_criteria hosted in Notion for the specific role.
Extract Audio
Create an Elevenlabs API key for your conversation agent and enter that key as a json header for the Extract Audio node
Upload Audio to Drive - Google Drive
Authenticate your Google Drive account and select the folder that will be used to store the audio file.
Elevenlabs Configuration
Create an Elevenlabs account
Create Conversation AI Agent
Add First Message and Systems Prompt:
Design your ‚ÄòFirst Message‚Äô and ‚ÄòSystems Prompt‚Äô that guides the AI agent conducting the interview.
Tool Tip: provide instruction that limits the number of probes per interview question.
Knowledge Base:
Upload your role specific interview questions and job description, using the same text that is stored in your Notion Career page for the role. You can also add a document about your company and instruct the Elevenlabs agent to answer questions about culture, strategy, and company growth.
Analysis: Evaluation Criteria:
Add your evaluation criteria, less than 2000 characters, for each interview question / competency.
Analysis: Data Collection:
Add the following elements, using the exact character string represented below.
phone_number_AI_screen
""capture applicant's phone number provided at the start of the conversation and share this as a string, integers only.""
full_name
""capture applicant's full name provided at the start of the conversation.""
Advanced: Max Duration
Set the max duration for interview in seconds. The AI Agent will timeout at the max duration.
Conversation AI Widget:
Customize your AI Conversation Agent landing page, including the position tile and company name.
AI Conversation Agent URL:
Copy the AI Conversation Agent URL and add it to your Notion email template triggered by the AI Interview email automation. Use a custom AI Agent URL for each distinct job description.
Enable your Elevenlabs Post-Call Webhook for your Conversation Agent:
Log into your Elevenlabs account and go to Conversational AI Settings and click on Post-Call Web Hook. This is where you enter the production URL from the N8N Web Hook node (Workflow 2). This sends the AI Voice Agent output to your n8n workflow which feeds back to your Notion dashboard."
"Extract & Classify Invoices & Receipts with Gmail, OpenAI and Google Drive",https://n8n.io/workflows/3719-extract-and-classify-invoices-and-receipts-with-gmail-openai-and-google-drive/,"Who is it for?
Anyone who wants to automatically aggregate their invoices or receipts. Main beneficiaries: small business owners and freelancers.
How it works
Creates a folder in Google Drive for uploading invoices and receipts. Responds (Webhook response) with URL to the created folder.
Gets all emails with attachments from a Gmail mailbox.
(Optional) Filters emails, e.g. exclude emails sent to specific address.
Filters only PDF attachments.
Classifies all PDF attachment contents with an AI model (is it a receipt or an invoice?).
Uploads receipts and invoices to the created Google Drive folder and optionally sends an email with the attachments to, e.g., your accountant.
Pre-conditions/Requirements
Gmail and Google Drive accounts
A Google Cloud OAuth 2.0 Client ID or a service account with Google Drive and Gmail APIs enabled
OpenAI API account and API key
Set up steps
Provide credentials for the nodes: Gmail, Google Drive, OpenAI.
Configure parameters in the ""Configure"" node. Most importantly:
""sendInvoicesTo"" for the email address where invoices/receipts should be sent.
It uses a Webhook node trigger. It expects a body with a schema such as:
{
  ""name"": ""getInvoicesAndReceiptsFromEmails"",
  ""description"": ""Finds and uploads to Google Drive all receipts and invoices from emails within a specified date range."",
  ""parameters"": {
    ""type"": ""object"",
    ""properties"": {
      ""startDate"": {
        ""type"": ""string"",
        ""format"": ""date-time"",
        ""description"": ""The start date of the range to search for emails. Must be in ISO 8601 format.""
      },
      ""endDate"": {
        ""type"": ""string"",
        ""format"": ""date-time"",
        ""description"": ""The end date of the range to search for emails. Must be in ISO 8601 format.""
      },
      ""sendEmail"": {
        ""type"": ""boolean"",
        ""description"": ""Indicates whether to send an email with all receipts and invoices after processing. Must be true or false.""
      }
    },
    ""required"": [
      ""startDate"",
      ""endDate""
    ]
  }
}
Example body:
{
  ""startDate"": ""2025-03-01T00:00:00Z"",
  ""endDate"": ""2025-04-01T00:00:00Z"",
  ""sendEmail"": true
}
How to use with AI chat
You can trigger the workflow with an AI chat that supports tool use, such as BrowseWiz. For setup instructions, read the blog post."
Automated Content Generation & Publishing - Wordpress,https://n8n.io/workflows/3018-automated-content-generation-and-publishing-wordpress/,"Workflow Description: Automated Content Publishing for WordPress
This n8n workflow automates the entire process of content generation, image selection, and scheduled publishing to a self-hosted WordPress website. It is designed for bloggers, marketers, and businesses who want to streamline their content creation and posting workflow.
üåü Features
‚úÖ AI-Powered Content Generation
Uses ChatGPT to generate engaging, market-ready blog articles
Dynamically incorporates high-search volume keywords
‚úÖ Automated Image Selection
Searches for relevant stock images from Pexels
Embeds images directly into posts
(Optional) Supports Featured Image from URL (FIFU) plugin for WordPress
‚úÖ Scheduled & Randomized Posting
Automatically schedules posts at predefined intervals
Supports randomized delay (0-6 hours) for natural publishing
‚úÖ WordPress API Integration
Uses WordPress REST API to directly publish posts
Configures featured images, categories, and metadata
Supports SEO-friendly meta fields
‚úÖ Flexible & Customizable
Works with any WordPress website (self-hosted)
Can be modified for other CMS platforms
üîß How It Works
1Ô∏è‚É£ Trigger & Scheduling
Automatically runs at preset times or on-demand
Supports cron-like scheduling
2Ô∏è‚É£ AI Content Generation
Uses a well-crafted prompt to generate high-quality blog posts
Extracts relevant keywords for both SEO and image selection
3Ô∏è‚É£ Image Fetching from Pexels
Searches and retrieves high-quality images
Embeds image credits and ensures proper formatting
4Ô∏è‚É£ WordPress API Integration
Sends post title, content, image, and metadata via HTTP Request
Can include custom fields, categories, and tags
5Ô∏è‚É£ Randomized Delay Before Publishing
Ensures natural posting behavior
Avoids bulk publishing issues
üìå Requirements
Self-hosted WordPress website with REST API enabled
FIFU Plugin (optional) for external featured images
n8n Self-Hosted or Cloud Instance
üöÄ Who Is This For?
‚úÖ Bloggers who want to automate content publishing
‚úÖ Marketing teams looking to scale content production
‚úÖ Business owners who want to boost online presence
‚úÖ SEO professionals who need consistent, optimized content
üí° Ready to Automate?
üëâ Click here to get this workflow! (Replace with Purchase URL)"
Generate a Legal Website Accessibility Statement with AI and WAVE,https://n8n.io/workflows/4738-generate-a-legal-website-accessibility-statement-with-ai-and-wave/,"Who is this for?
This template is for any website owner, digital agency, or compliance officer operating within the European Union. It's designed for users who need to comply with the upcoming European Accessibility Act (EAA) but may not have deep technical or legal expertise.
Disclaimer
This workflow uses an npm package called ""cheerio"" to work with the specified URLs HTML code. Installing packages is only possible in self hosting.
What problem is this workflow solving? / Use Case
Starting June 28, 2025, the European Accessibility Act (EAA) mandates that most websites offering products or services in the EU must be accessible and publish a formal Accessibility Statement. Manually creating this legal document is complex, requiring both a technical site analysis and knowledge of specific legal requirements. This workflow automates the generation of a compliant first draft, saving significant time and effort.
What this workflow does
After you input your details (like website URL and API key) in a central configuration node, this workflow automatically:
Scans your live website for accessibility issues using the powerful WAVE API.
Processes the scan results to identify the main problem areas.
Instructs a Google Gemini AI agent with a specialized legal prompt based on the European Accessibility Act.
Generates a formal Accessibility Statement in your desired language.
Saves the statement as an .html file and sends it to you as an email attachment.
Setup
This workflow is designed for a quick setup:
Configure All Variables: Click the 'CHANGE THESE: dependencies' node. This is your central control panel. Fill in all the values, including your WAVE API Key, the URL to analyze, company details, and desired output language.
Set Up Credentials: You will need to connect your Google accounts for the workflow to run.
Gemini: Click the 'gemini 2.5 pro' node, click the gear icon (‚öôÔ∏è) next to the ""Credential"" field, and connect your Google Gemini API credentials.
Gmail: Click the 'Send report by email' node and connect your Gmail account to allow sending the final report.
Activate & Execute: Make sure the workflow is active in the top-right corner, then click 'Execute Workflow' to run your first analysis.
How to customize this workflow to your needs
This template is a great starting point for any EU country. Here's how to adapt it:
Localize for Your Country (Important!): The generated statement contains a placeholder for the ""Enforcement Procedure"". You must edit the prompt in the 'Accessibility Statement Generator' node to replace this placeholder with the name and link to your specific country's official enforcement body.
Change the AI: Swap the Google Gemini node for any other AI model, like OpenAI or Anthropic Claude, by replacing the node and connecting it to the agent.
Change the Trigger: Replace the 'When clicking ‚ÄòExecute workflow‚Äô' node with a Form Trigger or Webhook Trigger to run this workflow based on external inputs, for example, to offer this analysis as a service to your clients."
AI-Powered Local Event Finder with Multi-Tool Search,https://n8n.io/workflows/4816-ai-powered-local-event-finder-with-multi-tool-search/,"Summary
This n8n workflow implements an AI-powered ""Local Event Finder"" agent. It takes user criteria (like event type, city, date, and interests), uses a suite of search tools (Brave Web Search, Brave Local Search, Google Gemini Search) and a web scraper (Jina AI) to find relevant events, and returns formatted details. The entire agent is exposed as a single, easy-to-use MCP (Multi-Capability Peer) tool, making it simple to integrate into other workflows or applications.
This template cleverly combines the MCP server endpoint and the AI agent logic into a single n8n workflow file for ease of import and management.
Key Features
Intelligent Multi-Tool Search: Dynamically utilizes web search, precise local search, and advanced Gemini semantic search to find events.
Detailed Information via Web Scraping: Employs Jina AI to extract comprehensive details directly from event web pages.
Simplified MCP Tool Exposure: Makes the complex event-finding logic available as a single, callable tool for other MCP-compatible clients (e.g., Roo Code, Cline, other n8n workflows).
Customizable AI Behavior: The core AI agent's behavior, tool usage strategy, and output formatting can be tailored by modifying its System Prompt.
Modular Design: Uses distinct nodes for LLM, memory, and each external tool, allowing for easier modification or extension.
Benefits
Simplifies Client-Side Integration: Offloads the complexity of event searching and data extraction from client applications.
Provides Richer Event Data: Goes beyond simple search links to extract and format key event details.
Flexible & Adaptable: Can be adjusted to various event search needs and can incorporate new tools or data sources.
Efficient Processing: Leverages specialized tools for different aspects of the search process.
Nodes Used
MCP Trigger
Tool Workflow
Execute Workflow Trigger
AI Agent
Google Gemini Chat Model (ChatGoogleGenerativeAI)
Simple Memory (Window Buffer Memory)
MCP Client (for Brave Search tools via Smithery)
Google Gemini Search Tool
Jina AI Tool
Prerequisites
An active n8n instance.
Google AI API Key: For the Gemini LLM (Google Gemini Chat Model node) and the Google Gemini Search Tool. Ensure your key is enabled for these services.
Jina AI API Key: For the jina_ai_web_page_scraper node. A free tier is often available.
Access to a Brave Search MCP Provider (Optional but Recommended):
This template uses MCP Client nodes configured for Brave Search via a provider like Smithery. You'll need an account/API key for your chosen Brave Search MCP provider to configure the smithery brave search credential.
Alternatively, you could adapt these to call Brave Search API directly if you manage your own access, or replace them with other search tools.
Setup Instructions
Import Workflow: Download the JSON file for this template and import it into your n8n instance.
Configure Credentials:
Google Gemini LLM:
Locate the Google Gemini Chat Model node.
Select or create a ""Google Gemini API"" credential (named Google Gemini Context7 in the template) using your Google AI API Key.
Google Gemini Search Tool:
Locate the google_gemini_event_search node.
Select or create a ""Gemini API"" credential (named Gemini Credentials account in the template) using your Google AI API Key (ensure it's enabled for Search/Vertex AI).
Jina AI Web Scraper:
Locate the jina_ai_web_page_scraper node.
Select or create a ""Jina AI API"" credential (named Jina AI account in the template) using your Jina AI API Key.
Brave Search (via MCP):
You'll need an MCP Client HTTP API credential to connect to your Brave Search MCP provider (e.g., Smithery).
Create a new ""MCP Client HTTP API"" credential in n8n. Name it, for example, smithery brave search.
Configure it with the Base URL and any required authentication (e.g., API key in headers) for your Brave Search MCP provider.
Locate the brave_web_search and brave_local_search MCP Client nodes in the workflow.
Assign the smithery brave search (or your named credential) to both of these nodes.
Activate Workflow: Ensure the workflow is active.
Note MCP Trigger Path:
Locate the local_event_finder (MCP Trigger) node.
The Path field (e.g., 0ca88864-ec0a-4c27-a7ec-e28c5a900697) combined with your n8n webhook base URL forms the endpoint for client calls.
Example Endpoint: YOUR_N8N_INSTANCE_URL/webhooks/PATH-TO-MCP-SERVER
Customization
AI Behavior: Modify the ""System Message"" parameter within the event_finder_agent node to change the AI's persona, its strategy for using tools, or the desired output format.
LLM Model: Swap the Google Gemini Chat Model node with another compatible LLM node (e.g., OpenAI Chat Model) if desired. You'll need to adjust credentials and potentially the system prompt.
Tools: Add, remove, or replace tool nodes (e.g., use a different search provider, add a weather API tool) and update the event_finder_agent's system prompt and tool configuration accordingly.
Scraping Depth: Be mindful of the jina_ai_web_page_scraper's usage due to potential timeouts. The system prompt already guides the LLM on this, but you can adjust its usage instructions."
AI Client Onboarding Agent: Auto Welcome Email Generator,https://n8n.io/workflows/4448-ai-client-onboarding-agent-auto-welcome-email-generator/,"Transform every new client signup into an immediate, professional welcome experience.
This intelligent workflow monitors Google Forms submissions, generates personalized welcome emails with onboarding checklists using AI, and delivers professional first impressions instantly to every new client - ensuring no new client is ever missed while maintaining consistent, high-quality onboarding.
üöÄ What It Does
Smart Form Monitoring: Automatically detects new client submissions from Google Forms and triggers personalized onboarding sequences within minutes.
AI-Powered Personalization: Uses Google Gemini to create custom welcome emails featuring the client's name, company details, specific services, and tailored onboarding steps.
Professional Checklist Creation: Generates comprehensive 6-step onboarding checklists covering account setup through first milestone review.
Instant Email Delivery: Sends personalized welcome emails immediately to new clients, creating instant engagement and professional first impressions.
Error-Proof Reliability: Built-in error handling ensures no client falls through the cracks, with automatic failure detection and recovery.
üéØ Key Benefits
‚úÖ Never Miss a New Client: Automatic processing ensures 100% response rate to signups
‚úÖ Instant Professional Response: Welcome emails sent within 2 minutes of form submission
‚úÖ Consistent Experience: Every client receives the same high-quality welcome process
‚úÖ Save 5+ Hours Weekly: Eliminate manual follow-up and welcome email creation
‚úÖ Boost Client Satisfaction: Professional first impressions set positive expectations
‚úÖ Scale Without Stress: Handle unlimited new clients without additional effort
üè¢ Perfect For
Service-Based Businesses
Consultants and coaches welcoming new clients
Agencies onboarding new accounts and projects
Professional services establishing client relationships
Freelancers creating impressive first impressions
Business Applications
Client Acquisition: Convert form submissions into engaged clients
Relationship Building: Start strong professional relationships from day one
Process Automation: Streamline repetitive onboarding tasks
Team Efficiency: Free up staff for high-value client work
‚öôÔ∏è What's Included
Complete Workflow Setup: Ready-to-deploy n8n workflow with all integrations configured
Google Forms Integration: Automatically triggers from new form submissions
AI Email Generation: Google Gemini creates personalized, professional welcome content
Smart Data Processing: Extracts and formats client information intelligently
Gmail Integration: Professional email delivery with your branding
Error Handling: Robust failure detection and recovery systems
Setup Documentation: Step-by-step configuration and customization guide
üîß Technical Requirements
n8n Platform: Cloud or self-hosted instance
Google Workspace: For Forms, Sheets, and Gmail integration
Google Gemini API: For AI-powered email personalization (free tier available)
Basic Setup Skills: Follow included setup guide (30 minutes)
üìä Sample Client Experience
What Your New Client Receives:
Subject: Welcome to Our Service, Sarah Johnson

Hi Sarah,

Welcome to our service! We're excited to work with Digital Marketing Solutions on your upcoming project.

Here's your personalized onboarding checklist:

‚úÖ ONBOARDING STEPS:
1. Account Setup - We'll create your client portal access within 24 hours
2. Welcome Call Scheduled - Let's discuss your goals and timeline
3. Document Collection - We'll gather project requirements and materials
4. Service Configuration - Our team will customize our approach for SEO optimization
5. Onboarding Session - We'll walk you through our process
6. First Milestone Review - Schedule progress check and feedback session

Your project details:
‚Ä¢ Services Needed: SEO optimization and content strategy
‚Ä¢ Special Notes: Focus on local search rankings for healthcare practice

What's next? Our team will contact you within 24 hours to schedule your welcome call and begin account setup.

Best regards,
Your Digital Marketing Solutions Team
üé® Customization Options
Service-Specific Checklists: Tailor onboarding steps for different service offerings
Brand Personalization: Include your company voice, tone, and specific processes
Multi-Service Support: Different welcome sequences for various service types
Follow-Up Automation: Extend to multi-email onboarding sequences
Team Integration: Connect to Slack, project management, or CRM systems
Calendar Booking: Include automatic scheduling links for welcome calls
üîÑ How It Works
New client submits intake form with their details and service needs
Workflow automatically detects the new submission within 1 minute
Client data is extracted and formatted for personalization
AI generates custom welcome email with relevant onboarding steps
Professional email is sent instantly to the client's provided address
Error handling ensures reliability with automatic failure notifications
üí° Use Case Examples
Marketing Agency: Instantly welcome new clients with campaign-specific onboarding steps and account manager introductions
Business Consultant: Send personalized welcome emails with assessment schedules and document collection lists
Web Design Studio: Provide immediate project timelines, discovery session booking, and asset collection instructions
Coaching Practice: Welcome new clients with program overviews, session scheduling, and preparation materials
üìà Expected Results
100% response rate to new client signups (no one gets missed)
90% faster initial client communication vs manual processes
50% improvement in client satisfaction with onboarding experience
75% reduction in administrative time spent on welcome processes
Professional first impression for every single new client
üõ†Ô∏è Setup & Support
Quick Implementation: Complete setup in 30 minutes with step-by-step guide
Pre-Built Templates: Professional email templates ready to customize
Video Walkthrough: Complete setup tutorial available
Ongoing Support: Direct access to creator for help and customization
üìû Get Help & Learn More
üé• Free Video Tutorials
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
üíº Professional Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for implementation consulting and optimization
Share your client onboarding success stories
Access exclusive templates for different service types
üìß Direct Support
Email: Yaron@nofluff.online
Technical setup assistance and troubleshooting
Custom workflow modifications for your business
Integration help with existing systems
Response within 24 hours"
"Generate Daily E-Commerce Order Reports with Supabase, GPT-4.1 and Gmail",https://n8n.io/workflows/4406-generate-daily-e-commerce-order-reports-with-supabase-gpt-41-and-gmail/,"This n8n workflow automates the generation and delivery of a daily order summary via email. It leverages an AI Agent to fetch and summarize e-commerce order data from the last 24 hours stored in Supabase, providing a concise overview of the daily business operations.
How it works
Scheduled Trigger: The workflow is triggered every day at 8 AM.
Sender Email Configuration: A manual step allows you to set the sender's email address.
AI Agent: An AI Agent node acts as the central intelligence, interacting with various tools to gather and process data.
Supabase Data Fetching: The AI Agent calls ""Get Orders,"" ""Get Order Items,"" ""Get Clients,"" and ""Get Products"" tables to retrieve relevant e-commerce data from your Supabase database.
OpenAI Chat Model: An OpenAI Chat Model with the 4.1 model is integrated to help the AI Agent understand and summarize the fetched data into a human-readable format.
Gmail Summary: Finally, the workflow sends a summarized report to your specified email address using the ""Send Gmail Summary"" node.
Set up steps
This setup will take approximately 15-20 minutes.
Download the workflow: Download this workflow and import it into your n8n instance.
Configure the Daily 8am trigger: Ensure the ""Daily 8am"" trigger is active and set to your desired timezone.
Set Sender Email: In the ""Set Sender Email"" node, manually enter the email address you wish to use as the sender for the daily reports.
Configure AI Agent:
Chat Model: Connect your OpenAI Chat Model credential.
Memory & Tools: Ensure all the necessary nodes (""Get Orders"", ""Get Order Items"", ""Get Clients"", ""Get Products"", ""Send Gmail Summary"") are correctly linked to the AI Agent. In our workflow we call data from 4 tables in Supabase.
Configure Supabase Database Connections:
For each of the ""Get Orders,"" ""Get Order Items,"" ""Get Clients,"" and ""Get Products"" nodes, you will need to configure your Supabase credentials to access your e-commerce database.
Select the tables (e.g., orders, order_items, clients, products) that you want the AI agent to pull data from in your Supabase schema.
Configure Gmail Credentials: In the ""Send Gmail Summary"" node, connect your Gmail account credentials to allow n8n to send emails on your behalf.
Test the workflow: Run the workflow manually to ensure all connections are working correctly and the email summary is generated as expected.
Requirements
n8n instance: An active n8n instance (self-hosted or cloud).
Supabase Account: A Supabase account with your e-commerce order data accessible.
OpenAI API Key: An OpenAI API key for the Chat Model.
Gmail Account: A Gmail account credentials to send the daily summaries.
Need help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
Evaluation metric example: RAG document relevance,https://n8n.io/workflows/4273-evaluation-metric-example-rag-document-relevance/,"AI evaluation in n8n
This is a template for n8n's evaluation feature.
Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow.
By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't.
How it works
This template shows how to calculate a workflow evaluation metric: retrieved document relevance (i.e. whether the information retrieved from a vector store is relevant to the question).
The workflow takes a question and checks whether the information retrieved to answer it is relevant.
To run this workflow, you need to insert documents into a vector data store, so that they can be retrieved by the agent to answer questions. You can do this by running the top part of the workflow once.
The main workflow works as follows:
We use an evaluation trigger to read in our dataset
It is wired up in parallel with the regular trigger so that the workflow can be started from either one. More info
We make sure that the agent outputs the list data from the tools that it used
If we‚Äôre evaluating (i.e. the execution started from the evaluation trigger), we calculate the relevance metric using AI to compare the retrieved documents with the question
We pass this information back to n8n as a metric
If we‚Äôre not evaluating we avoid calculating the metric, to reduce cost"
"Turn YouTube Videos into Summaries, Transcripts, and Visual Insights",https://n8n.io/workflows/3188-turn-youtube-videos-into-summaries-transcripts-and-visual-insights/,"Who is this for?
This workflow is built for anyone who works with YouTube content, whether you're:
A learner looking to understand a video‚Äôs key points
A content creator repurposing video material
A YouTube manager looking to update titles, descriptions
A social media strategist searching for the most shareable clips

Don't just ask questions about what's said. Find out what's going on in a video too.
Video Overview: https://www.youtube.com/watch?v=Ovg_KfKxnC8
What problem does this solve?
YouTube videos hold valuable insights, but watching and processing them manually takes time. This workflow automates:
Quick content extraction: Summarize key ideas without watching full videos
Visual analysis: Understand what‚Äôs happening beyond spoken words
Clip discovery: Identify the best moments for social sharing
How the workflow works
This n8n-powered automation:
Uses Google‚Äôs Gemini 1.5 Flash AI for intelligent video analysis
Provides multiple content analysis templates tailored to different needs
What makes this workflow powerful?
The easiest place to start is by requesting a summary or transcript. From there, you can refine the prompts to match your specific use case and the type of video content you‚Äôre working with.
But what's even more amazing? You can ask questions about what‚Äôs happening in the video ‚Äî and get detailed insights about the people, objects, and scenes. It's jaw-dropping.
This workflow is versatile ‚Äî the actions adapt based on the values set. That means you can use a single workflow to:
Extract transcripts
Generate an extended YouTube description
Write a summary blog post
You can also modify the trigger based on how you want to run the workflow ‚Äî use a webhook, connect it to an event in Airtable, or leave it as-is for on-demand use. The output can then be sent anywhere: Notion, Airtable, CMS platforms, or even just stored for reference.
How to set it up
Connect your Google API key
Paste a YouTube video URL
Select an analysis method
Run the workflow and get structured results
Analysis Templates
Basic & Timestamped Transcripts: Extract spoken content
Summaries: Get concise takeaways
Visual Scene Analysis: Detect objects, settings, and people
Clip Finder: Locate shareable moments
Actionable Insights: Extract practical information
Customization Options
Modify templates to fit your needs
Connect with external platforms
Adjust formatting preferences
Advanced Configuration
This workflow is designed for use with gemini-1.5-flash. In the future, you can update the flow to work with different models or even modify the HTTP request node to define which API endpoint should be used.
It's also been designed so you can use this flow on it's own or add to a new / existing worflow.
This workflow helps you get the most out of YouTube content ‚Äî quickly and efficiently."
Simple Eval for Legal Benchmarking,https://n8n.io/workflows/4712-simple-eval-for-legal-benchmarking/,"This workflow demonstrates a simple way to run evals on a set of test cases stored in a Google Sheet.
The example we are using comes from an info extraction task dataset, where we tested 6 different LLMs on 18 different test cases.
You can see our sample data in this spreadsheet here to get started.
Once you have this working for our dataset, you can plug in your own test cases matching different LLMs to see how it works with your own data.
How it works:
It loads test cases from Google Sheets.
For each row in our Google Sheet, it grabs the source document, converting it to text.
Our ""LLM judge"" passes the input/output of each LLM to GPT-4.1 to evaluate each test case (Pass/Fail + Reason).
It logs the outcome to a Google Sheet.
A 0.5s pause between each request gets around OpenAI's API rate limits.
Set up steps:
Add your credentials for Google Sheets, Google Drive, and OpenRouter.
Make a copy of the original data spreadsheet so that you can edit it yourself. You will need to plug your version in the Update Results node to see the spreadsheet update on each run of the loop."
Slack-OpenAI Assistant Integration for Direct Messages & @mentions,https://n8n.io/workflows/4683-slack-openai-assistant-integration-for-direct-messages-and-mentions/,"Getting AI assistance in Slack requires switching between apps and losing conversation context. Teams waste time copying messages to ChatGPT, losing the flow of their work discussions. This workflow brings OpenAI Assistant directly into your Slack workspace, responding to direct messages and @mentions while maintaining conversation memory and context.
How it works?
This workflow creates an intelligent Slack bot powered by OpenAI Assistant that responds naturally to team communications. It handles both direct messages and @mentions, maintains conversation history, and provides contextual responses without leaving your Slack workspace.
Listen for Slack events - detects direct messages and @mentions while filtering out bot responses to prevent loops
Process message types - handles both assistant mode (DMs) and public channel @mentions
Set typing indicator - shows customizable ""thinking"" status for natural conversation flow
Generate AI response - uses your configured OpenAI Assistant with custom instructions and knowledge base
Maintain conversation memory - tracks context across up to 5 recent messages per session
Reply intelligently - responds in threads for @mentions, direct messages for DMs
Who is it for?
Teams and organizations wanting to integrate AI assistance directly into their Slack workflows. Perfect for customer support teams, internal help desks, or any group that needs quick access to AI-powered responses without leaving their communication platform.
Setup
Setup takes approximately 20-25 minutes and requires configuring Slack app permissions with proper bot user filtering, OpenAI Assistant credentials, and customizing response behavior. The workflow includes built-in loop prevention and conversation threading for organized discussions.
What's included?
Your purchase includes comprehensive implementation support: detailed step-by-step walkthrough with screen recording, complete Slack app configuration guide with required scopes and permissions, OpenAI Assistant setup tutorial, conversation memory configuration, and access to our tech support forum for troubleshooting any implementation challenges."
Automate Instagram Carousel Posts from Google Sheets with Gemini AI & Meta Graph API,https://n8n.io/workflows/4511-automate-instagram-carousel-posts-from-google-sheets-with-gemini-ai-and-meta-graph-api/,"üì∏ Google Sheets to Instagram Carousel Post Automation via Locally Hosted n8n + Facebook Graph API
Overview
This n8n workflow reads product data from a Google Sheet, generates a professional Instagram carousel post caption using AI, uploads up to 4 images, and publishes the post via the Facebook Graph API to your linked Instagram Business Account.
It also marks the post as ""uploaded"" in the sheet to avoid duplicates.
üîÅ Trigger
Schedule Trigger: Runs every 2 hours by default (can be changed to fit your posting frequency).
üìã Step-by-Step Flow
1. Google Sheets ‚Äì ""Is Uploaded?"" Filter
Connects to your product sheet.
Filters rows where the uploaded? column is set to ""no"".
Returns only 1 row at a time to prevent batch duplicates.
2. Gemini AI Agent
Uses Google‚Äôs Gemini 1.5 Flash (free) to generate professional captions.
Captions include:
Emojis
Hashtags
Pricing callout
Soft CTA prompting users to ""Check our bio for the link"" (no direct URLs in post)
3. Edit Fields
Prepares and formats:
Product title, description, and price
Image URLs from mainimage, carousel1, carousel2, carousel3
Gemini-generated caption
Instagram Page ID
4. Image Uploads (Flexible)
Supports 1 to 4 image URLs from the Google Sheet.
Each upload is handled by a separate Facebook Graph API node.
Upload is conditionally skipped if the image URL is empty.
This makes the workflow dynamic ‚Äî it will still post even if only 1 or 2 images are available.
Uploaded image container IDs are collected for use in the next step.
5. Merge + Prepare Carousel Media IDs
Merges successful image uploads into one dataset.
Outputs image media IDs in a children field like:
{
  ""children"": ""17932447635024904,17872489326361281""
}
6. Create Carousel Container
Sends the children media IDs + caption to the Graph API.
This creates the Instagram Carousel Container tied to your business account.
7. Post to Instagram
Final API call publishes the carousel post to your Instagram feed.
8. Google Sheets ‚Äì Mark as Uploaded
Updates the uploaded? column of the posted row to ""yes"" to prevent reposting.
üìÑ Google Sheets Format
The spreadsheet should look like this:
ID title description price sale_price mainimage carousel1 carousel2 carousel3 uploaded?
1 Product Name Short description 500 400 img1.jpg img2.jpg img3.jpg img4.jpg no
ü§ñ Gemini API (Free AI Caption Generator)
This project uses Google‚Äôs Gemini 1.5 Flash model ‚Äî a free, fast, and powerful AI caption generator.
To use it:
Visit the official Gemini API key page:
https://ai.google.dev/gemini-api/docs/api-key
Click ‚ÄúGet API Key‚Äù and sign in with your Google account.
Paste the generated API key into your n8n Gemini credential.
Captions are automatically created for each post!
üì∫ Facebook/Instagram API Setup (Video Guide)
If you're new to Meta‚Äôs Graph API, this guide is highly recommended:
Watch here:
https://www.youtube.com/watch?v=AGSyWdjN5A4&ab_channel=Let'sAutomateIt
It covers:
Creating a Meta Developer App
Setting up permissions
Access token generation
Linking Instagram and Facebook Business accounts
‚ö†Ô∏è Disclaimer
This was a quick fix for a specific workflow need.
It may not be the most scalable or universal solution ‚Äî but it works reliably for our use case.
Feel free to fork, improve, or adapt it to your setup.
üåê GitHub Repository
Direct links may not work if you're seeing this on certain platforms.
You can find the repo here:
User: Jackson0Wells
Repo: instagram-carousel-uploader-n8n
URL: https://github.com/Jackson0Wells/instagram-carousel-uploader-n8n/tree/main
‚≠ê Star the repo if this helped!"
Automated Video Analysis: AI-Powered Insight Generation from Google Drive,https://n8n.io/workflows/4621-automated-video-analysis-ai-powered-insight-generation-from-google-drive/,"This cutting-edge n8n automation is a sophisticated video intelligence tool designed to transform raw video content into actionable insights. By intelligently connecting Google Drive, AI analysis, and automated processing, this workflow:
Discovers Video Content:
Automatically retrieves videos from Google Drive
Supports scheduled or on-demand analysis
Eliminates manual content searching
Advanced AI Analysis:
Leverages Google Gemini AI
Provides comprehensive video insights
Extracts meaningful content summaries
Intelligent Processing:
Validates file status
Prepares content for AI analysis
Ensures high-quality insight generation
Seamless Workflow Integration:
Automated scheduling
Cross-platform content processing
Reduces manual intervention
Key Benefits
ü§ñ Full Automation: Zero-touch video intelligence
üí° AI-Powered Insights: Advanced content analysis
üìä Comprehensive Processing: Detailed video understanding
üåê Multi-Platform Synchronization: Seamless content flow
Workflow Architecture
üîπ Stage 1: Content Discovery
Scheduled Trigger: Automated workflow initiation
Google Drive Integration: Video file retrieval
Intelligent File Selection:
Identifies target videos
Prepares for AI analysis
üîπ Stage 2: Content Preparation
File Download
LLM Chain Processing
AI-Ready Content Formatting
üîπ Stage 3: AI Analysis
Gemini API Integration
Comprehensive Content Examination
Intelligent Insight Generation
üîπ Stage 4: Result Structuring
Analysis Result Formatting
Structured Insight Preparation
Ready-to-Use Intelligence
Potential Use Cases
Content Creators: Video content analysis
Marketing Teams: Content insight generation
Educational Institutions: Lecture and presentation review
Research Organizations: Automated video intelligence
Media Companies: Rapid content assessment
Setup Requirements
Google Drive
Connected Google account
Configured video folder
Appropriate sharing settings
Google Gemini API
API credentials
Configured analysis parameters
Access to Gemini Pro model
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Multi-model AI analysis
üìä Detailed insight scoring
üîî Automated reporting
üåê Cross-platform insight sharing
üß† Advanced content categorization
Technical Considerations
Implement robust error handling
Use secure API authentication
Maintain flexible content processing
Ensure compliance with AI usage guidelines
Ethical Guidelines
Respect content privacy
Maintain transparent analysis practices
Ensure appropriate content usage
Protect intellectual property rights
Hashtag Performance Boost üöÄ
#AIVideoAnalysis #ContentIntelligence #GeminiAI #VideoInsights #AutomatedLearning #AIWorkflow #MachineLearning #ContentAnalytics #TechInnovation #AIAutomation
Workflow Visualization
[Schedule Trigger]
    ‚¨áÔ∏è
[Download from Drive]
    ‚¨áÔ∏è
[LLM Chain Processing]
    ‚¨áÔ∏è
[Check File Status]
    ‚¨áÔ∏è
[Analyze Video]
    ‚¨áÔ∏è
[Format Analysis Result]
Connect With Me
Ready to revolutionize your video intelligence?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your video content analysis with intelligent, automated solutions!"
"Create AI Videos with Scripts, Images & HeyGen Avatars (üî• LIMITED-TIME OFFER)",https://n8n.io/workflows/4569-create-ai-videos-with-scripts-images-and-heygen-avatars-limited-time-offer/,"Short Content Automation üé¨ (AI Video System with Bulk Gen, Avatar & Music Customization)
üî• LIMITED-TIME OFFER: AI Video Automation (Previously $59)
Previously Template

üöÄ Transform Your Content Game in Minutes ‚Äì Save Hours of Work!
Introducing a powerful AI Video System to automate short video creation ‚Äî from scriptwriting to avatar + image generation, music, and assembly ‚Äî all with n8n. This is not a basic template. It's a complete, customizable system with step-by-step videos and my personal enhancements.
üéÅ Why $10 Now? (Regular Price: $59)
‚úÖ Over 4000 people viewed my original automation ‚Äî see it here ‚Äî and I receive many messages from people who want it.
‚úÖ I learned n8n from free templates and I want to give back.
‚úÖ People asking for a more accessible price ‚Äî this is my answer.
‚è≥ Limited-Time Deal: 7 Days Only!
After that, this price goes away and the blueprint will be unlisted.
üì¶ What You Get
‚úÖ Full n8n Workflow: From text prompt ‚Üí ready-to-publish short video
‚úÖ AI Tools Integration: OpenAI, Leonardo.AI, HeyGen, etc.
‚úÖ Customization Options: Avatar, Voice, Music, Style
‚úÖ Video Setup Guide: Step-by-step walkthrough
‚úÖ Clean UI, easy-to-use, even if you're not a developer
üí¨ Made With Love by an n8n Creator
This system is based on months of refinement and feedback from users like you.
Now it‚Äôs your turn to scale your content, automate your work, and stand out.
üíµ Only $10 ‚Äì Let‚Äôs Build Together.
üìå Get it now. Use it forever. No subscriptions. Just value."
"Automatic News Summarization & Email Digest with GPT-4, NewsAPI and Gmail",https://n8n.io/workflows/4375-automatic-news-summarization-and-email-digest-with-gpt-4-newsapi-and-gmail/,"üì∞ AI News Digest Agent: Auto News Summarizer & Email Newsletter
Create an intelligent news curation system that automatically fetches breaking headlines, generates AI-powered summaries, and delivers personalized news digests to your subscriber list. Perfect for newsletter creators, team leaders, and content curators who want to keep their audience informed without the manual effort of news monitoring and summarization.
üîÑ How It Works
This streamlined 5-step automation delivers fresh news insights around the clock:
Step 1: Automated News Collection
The workflow runs on a configurable schedule (default: every 10 minutes) to fetch the latest headlines from NewsAPI, ensuring your content stays current with breaking developments.
Step 2: Intelligent Content Curation
The system pulls top headlines from reliable news sources, filtering by country, category, and relevance to deliver the most important stories of the day.
Step 3: AI-Powered Summarization
GPT-4 processes the collected headlines and creates:
Concise 5-bullet point summaries
Key insights and implications
Easy-to-digest news overviews
Professional formatting for email distribution
Step 4: Subscriber Management
The workflow accesses your Google Sheets subscriber list, retrieving names and email addresses for personalized delivery.
Step 5: Automated Email Distribution
Personalized news digests are automatically sent to each subscriber via Gmail, with custom greetings and professionally formatted content.
‚öôÔ∏è Setup Steps
Prerequisites
NewsAPI account (free tier available)
OpenAI API access for content summarization
Google Sheets for subscriber management
Gmail account for email distribution
n8n instance (cloud or self-hosted)
Required Google Sheets Structure
Create a simple subscriber database:
Name Email
John Smith john@example.com
Sarah Johnson sarah@company.com
Mike Chen mike.chen@startup.co
Configuration Steps
Credential Setup
NewsAPI Key: Sign up at newsapi.org for free headline access
OpenAI API Key: Required for AI-powered news summarization
Google Sheets OAuth2: Access your subscriber spreadsheet
Gmail OAuth2: Enable automated email sending
News Source Configuration
Country Selection: Choose target region (US, UK, CA, AU, etc.)
Category Filters: Focus on specific topics (technology, business, health)
Source Selection: Prefer certain news outlets or avoid others
Language Settings: Configure for international audiences
AI Summarization Customization
Default prompt creates 5-bullet summaries, but can be tailored for:
Industry Focus: Technology, finance, healthcare, politics
Audience Type: General public, professionals, executives
Content Depth: Brief overviews vs detailed analysis
Tone & Style: Formal, conversational, or technical
Email Template Personalization
Subject Line Formatting: Include date, breaking news indicators
Greeting Customization: Use subscriber names for personal touch
Content Layout: Professional formatting with clear sections
Branding Elements: Add your organization's signature or logo
Delivery Schedule Optimization
Frequency Settings: Every 10 minutes, hourly, or daily
Time Zone Considerations: Optimize for subscriber locations
Breaking News Alerts: Immediate delivery for urgent stories
Digest Compilation: Collect multiple stories for periodic summaries
üöÄ Use Cases
Newsletter Publishers
Content Automation: Generate newsletter content without manual curation
Consistent Publishing: Maintain regular delivery schedules automatically
Audience Growth: Provide value that encourages subscriptions and shares
Time Savings: Eliminate hours of daily news monitoring and writing
Corporate Communications
Employee Updates: Keep teams informed about industry developments
Executive Briefings: Deliver curated news summaries to leadership
Client Communications: Share relevant industry insights with customers
Stakeholder Relations: Maintain informed investor and partner networks
Educational Institutions
Student Resources: Provide current events for academic discussions
Faculty Updates: Keep educators informed about relevant developments
Research Support: Deliver news related to specific academic fields
Parent Communications: Share educational policy and school-related news
Professional Services
Client Value Addition: Provide industry-specific news as a service benefit
Thought Leadership: Position your firm as an informed industry expert
Business Development: Share insights that demonstrate market knowledge
Team Knowledge Sharing: Keep entire organization current on industry trends
Community Organizations
Member Engagement: Keep community members informed and engaged
Local News Focus: Customize for regional or local news coverage
Event Planning: Stay informed about developments affecting your community
Advocacy Support: Monitor news relevant to your organization's mission
üîß Advanced Customization Options
Multi-Source News Aggregation
Expand beyond NewsAPI with additional sources:
RSS Feed Integration: Add specialized industry publications
Social Media Monitoring: Include trending topics from Twitter/LinkedIn
Government Sources: Official announcements and policy updates
International Coverage: Global perspectives on major stories
Intelligent Content Filtering
Implement smart curation features:
Sentiment Analysis: Filter positive, negative, or neutral news
Relevance Scoring: Prioritize stories based on subscriber interests
Duplicate Detection: Avoid sending repetitive story coverage
Quality Assessment: Ensure content meets editorial standards
Subscriber Segmentation
Create targeted news experiences:
Interest Categories: Technology, business, sports, entertainment
Geographic Preferences: Local, national, or international focus
Delivery Preferences: Frequency and format customization
Engagement Tracking: Monitor opens, clicks, and subscriber behavior
Enhanced Email Features
Professional newsletter capabilities:
HTML Templates: Rich formatting with images and links
Call-to-Action Buttons: Drive engagement with your content or services
Social Sharing: Enable easy sharing of newsletter content
Analytics Integration: Track email performance and subscriber engagement
üìä Content Generation Examples
Sample Email Output:
Subject: üì∞ Your Daily News Digest - March 15, 2024
Hi John,
Please find today's top news headlines summarized below:
üìà BUSINESS & TECHNOLOGY
Federal Reserve signals potential rate cuts following inflation data
Major tech companies announce AI partnership for healthcare applications
Renewable energy sector sees record investment levels in Q1 2024
Cryptocurrency markets stabilize after regulatory clarity announcement
Supply chain disruptions ease as global shipping routes normalize
üí° These developments suggest growing economic optimism and continued technology sector innovation. The healthcare AI partnership particularly signals significant advances in medical technology accessibility.
Stay informed and have a great day!
Powered by AI News Digest Agent
Unsubscribe | Update Preferences
Breaking News Alert Format:
Subject: üö® Breaking News Alert - Major Development
Hi Sarah,
BREAKING: [Headline]
Key Details:
[Critical point 1]
[Critical point 2]
[Impact analysis]
Full coverage in your next scheduled digest.
AI News Digest Agent
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
API Rate Limiting
Monitor NewsAPI quota usage and upgrade plan if needed
Implement intelligent caching to reduce redundant requests
Stagger requests during high-traffic periods
Set up alerts for approaching rate limits
Email Delivery Challenges
Monitor Gmail sending limits and implement delays if needed
Use professional email authentication (SPF, DKIM)
Maintain clean subscriber lists to avoid spam flags
Implement unsubscribe functionality for compliance
Content Quality Control
Review AI summaries periodically for accuracy and bias
Implement feedback loops for continuous prompt improvement
Create editorial guidelines for consistent tone and style
Monitor subscriber feedback and engagement metrics
Optimization Strategies
Performance Enhancement
Use parallel processing for multiple news sources
Implement intelligent caching for repeated content
Optimize AI prompts for faster processing and better results
Monitor workflow execution time and resource usage
Subscriber Growth
Create compelling value propositions for newsletter signups
Implement referral systems for organic growth
Share sample newsletters on social media and websites
Collect feedback to continuously improve content quality
Content Strategy
A/B test different summary formats and lengths
Analyze which news categories generate most engagement
Experiment with sending times for optimal open rates
Create themed newsletters for special events or topics
üìà Success Metrics
Engagement Indicators
Open Rates: Percentage of subscribers reading newsletters
Click-Through Rates: Engagement with linked news sources
Subscriber Growth: New signups and retention rates
Forward/Share Rates: Viral coefficient of your content
Content Quality Measurements
Relevance Scores: Subscriber feedback on content usefulness
Timeliness: How quickly breaking news reaches subscribers
Accuracy: Verification of AI-summarized content
Completeness: Coverage of important stories in your focus areas
üìû Questions & Support
Need assistance with your AI News Digest Agent setup or optimization?
üìß Technical Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Specialization: NewsAPI integration, AI content optimization, email deliverability
üé• Educational Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup and configuration tutorials
Advanced customization techniques for different industries
Email marketing best practices for automated newsletters
Troubleshooting common integration issues
Scaling strategies for growing subscriber lists
ü§ù Professional Community
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing newsletter automation support
Share your news curation success stories
Access exclusive templates and workflow variations
Join discussions about content automation trends
üí¨ Support Request Best Practices
Include in your support message:
Your target audience and newsletter focus
Current subscriber count and growth goals
Specific news categories or geographic regions of interest
Any technical errors or integration challenges
Current content creation workflow and pain points"
Automated Financial Tracker: Telegram Invoices to Notion with Gemini AI Reports,https://n8n.io/workflows/3960-automated-financial-tracker-telegram-invoices-to-notion-with-gemini-ai-reports/,"Automated Financial Tracker: Telegram Invoices to Notion with AI Summaries & Reports
Tired of manually logging every expense? Streamline your financial tracking with this powerful n8n workflow!
Snap a photo of your invoice in Telegram, and let AI (powered by Google Gemini) automatically extract the details, record them in your Notion database, and even send you a quick summary. Plus, get scheduled weekly reports with charts to visualize your spending. Automate your finances, save time, and gain better insights with this easy-to-use template!
Transform your expense tracking from a chore into an automated breeze. Try it out!
Overview:
This workflow revolutionizes how you track your finances by automating the entire process from invoice capture to reporting. Simply send a photo of an invoice or receipt to a designated Telegram chat, and this workflow will:
Extract Data with AI: Utilize Google Gemini's capabilities to perform OCR on the image, understand the content, and extract key details like item name, quantity, price, total, date, and even attempt to categorize the expense.
Store in Notion: Automatically log each extracted transaction into a structured Notion database.
Instant Feedback: Send a summary of the processed transaction back to your Telegram chat.
Scheduled Reporting: Generate and send a visual summary of your expenses (e.g., weekly spending by category) as a chart to your preferred Telegram chat or group.
This workflow is perfect for individuals, freelancers, or small teams looking to effortlessly manage their expenses without manual data entry.
Key Features & Benefits:
Effortless Expense Logging: Just send a picture ‚Äì no more typing!
AI-Powered Data Extraction: Leverages Google Gemini for intelligent invoice processing.
Centralized Data in Notion: Keep all your financial records neatly organized in a Notion database.
Automated Categorization: AI helps in categorizing your expenses (e.g., Food & Beverage, Transportation).
Instant Summaries: Get immediate confirmation and a summary of what was recorded.
Visual Reporting: Receive scheduled charts (e.g., bar charts of spending by category) directly in Telegram.
Customizable: Easily adapt the workflow to your specific needs, categories, and reporting preferences.
Time-Saving: Drastically reduces the time spent on manual financial administration.
How It Works (Workflow Breakdown):
The workflow is divided into two main parts:
Part 1: Real-time Invoice Processing & Logging (## Auto Notes Transaction with Telegram and Notion database)
Telegram Trigger (Telegram Trigger | When recive photo): Activates when a new photo is sent to the configured Telegram chat.
Get Photo Info (Get Info Photo from telegram chat): Retrieves the details of the received photo.
Get Image Info (Get Image Info): Prepares the image data.
AI Data Extraction (Google Gemini Chat Model & Basic LLM Chain):
The image data is sent to the Google Gemini Chat Model.
A specific prompt instructs the AI to extract details (date, ID, name, quantity, price, total, category, tax) in a JSON array format and provide a summary message. The categories include Food & Beverage, Transportation, Utilities, Shopping, Healthcare, Entertainment, Housing, and Education.
Parse AI Output (Parse To your object | Table): Structures the AI's JSON output for easier handling.
Split Transactions (Split Out | data transaction): If an invoice contains multiple items, this node splits them into individual records.
Record to Notion (Record To Notion Database): Each transaction item is added as a new page/entry in your specified Notion database, mapping fields like Name, Quantity, Price, Total, Category, Date, and Tax.
Send Telegram Summary (Sendback to chat and give summarize text): The summary message generated by the AI is sent back to the original Telegram chat.
Part 2: Scheduled Financial Reporting (## Schedule report to send on chanel or private message)
Schedule Trigger (Schedule Trigger | for send chart report): Runs at a predefined interval (e.g., every week) to generate reports.
Get Recent Data from Notion (Get Recent Data from Notions): Fetches transaction data from the Notion database for a specific period (e.g., the past week).
Summarize Data (Summarize Transaction Data): Aggregates the data, for example, by summing up the 'total' amount for each 'category'.
Prepare Chart Data (Convert Data to JSON chart payload): Transforms the summarized data into a JSON format suitable for generating a chart (e.g., labels for categories, data for spending amounts).
Generate Chart (Generate Chart): Uses the QuickChart node to create a visual chart (e.g., a bar chart) from the prepared data.
Send Chart to Telegram (Send Chart Image to Group or Private Chat): Sends the generated chart image to a specified Telegram chat ID or group.
Nodes Used (Key Nodes):
Telegram Trigger & Telegram Node: For receiving images and sending messages/images.
Google Gemini Chat Model (Langchain): For AI-powered OCR and data extraction from invoices.
Basic LLM Chain (Langchain): To interact with the language model using specific prompts.
Output Parser Structured (Langchain): To structure the output from the language model.
Notion Node: For reading from and writing to your Notion databases.
Schedule Trigger: To automate the reporting process.
Summarize Node: To aggregate data for reports.
Code Node: Used here to format data for the chart.
QuickChart Node: For generating charts.
SplitOut Node: To process multiple items from a single invoice.
Setup Instructions:
Credentials:
Telegram: Create a Telegram bot and get its API token. You'll also need the Chat ID where you'll send invoices and where reports should be sent.
Google Gemini (PaLM) API: You'll need an API key for Google Gemini.
Notion: Create a Notion integration and get the API key. Create a Notion database with properties corresponding to the data you want to save (e.g., Name (Title), Quantity (Number), Price (Number), Total (Number), Category (Select), Date (Text or Date), Tax (Number)). Share this database with your Notion integration.
Configure Telegram Trigger:
Add your Telegram Bot API token.
When you first activate the workflow or test the trigger, send /start to your bot in the chat you want to use for sending invoices. n8n will then capture the Chat ID.
Configure Google Gemini Node (Google Gemini Chat Model):
Select or add your Google Gemini API credentials.
Review the prompt in the Basic LLM Chain node and adjust if necessary (e.g., date format, categories).
Configure Notion Nodes:
Record To Notion Database:
Select or add your Notion API credentials.
Select your target Notion Database ID.
Map the properties from the workflow (e.g., ={{ $json.name }}) to your Notion database columns.
Get Recent Data from Notions:
Select or add your Notion API credentials.
Select your target Notion Database ID.
Adjust the filter if needed (default is ""past_week"").
Configure Telegram Node for Reports (Send Chart Image to Group or Private Chat):
Select or add your Telegram Bot API token.
Enter the Chat ID for the group or private chat where you want to receive the reports.
Configure Schedule Trigger (Schedule Trigger | for send chart report):
Set your desired schedule (e.g., every Monday at 9 AM).
Test: Send an image of an invoice to your Telegram bot and check if the data appears in Notion and if you receive a summary message. Wait for the scheduled report or manually trigger it to test the reporting functionality.
Sticky Note Text for Your n8n Template:
(These are suggestions. You would place these directly into the sticky notes within your n8n workflow editor.)
Existing High-Level Sticky Notes:
## Auto Notes Transaction with Telegram and Notion database
## Schedule report to send on chanel or private message
Specific Sticky Notes to Add:
On Telegram Trigger | When recive photo:
üì∏ INVOICE INPUT üì∏
Bot listens here for photos of your receipts/invoices.
Ensure your Telegram Bot API token is set in credentials.
Near Google Gemini Chat Model & Basic LLM Chain:
ü§ñ AI MAGIC HAPPENS HERE üß†
- Image is sent to Google Gemini for data extraction.
- Check 'Basic LLM Chain' to customize the AI prompt (e.g., categories, output format).
- Requires Google Gemini API credentials.
On Parse To your object | Table:
‚ú® STRUCTURING AI DATA ‚ú®
Converts the AI's text output into a usable JSON object.
Check the schema if you modify the AI prompt significantly.
On Record To Notion Database:
üìù SAVING TO NOTION üìù
- Extracted transaction data is saved here.
- Configure with your Notion API key & Database ID.
- Map fields correctly to your database columns!
On Sendback to chat and give summarize text:
üí¨ TRANSACTION SUMMARY üí¨
Sends a confirmation message back to the user in Telegram
with a summary of the recorded expense.
On Schedule Trigger | for send chart report:
üóìÔ∏è REPORTING SCHEDULE üóìÔ∏è
Set how often you want to receive your spending report (e.g., weekly, monthly).
On Get Recent Data from Notions:
üìä FETCHING DATA FOR REPORT üìä
- Retrieves transactions from Notion for the report period.
- Default: ""Past Week"". Adjust filter as needed.
- Requires Notion API credentials & Database ID.
On Summarize Transaction Data:
‚ûï SUMMARIZING SPENDING ‚ûï
Aggregates your expenses, usually by category,
to prepare for the chart.
On Convert Data to JSON chart payload (Code Node):
üé® PREPARING CHART DATA üé®
This Code node formats the summarized data
into the JSON structure needed by QuickChart.
On Generate Chart (QuickChart Node):
üìà GENERATING VISUAL REPORT üìà
Creates the actual chart image based on your spending data.
You can customize chart type (bar, pie, etc.) here.
On Send Chart Image to Group or Private Chat:
üì§ SENDING REPORT TO TELEGRAM üì§
- Delivers the generated chart to your chosen Telegram chat/group.
- Set the correct Chat ID and Bot API token.
General Sticky Note (Place where relevant):
üîë CREDENTIALS NEEDED üîë
Remember to set up API keys/tokens for:
- Telegram
- Google Gemini
- Notion
General Sticky Note (Place where relevant):
üí° CUSTOMIZE ME! üí°
- Adjust AI prompts for better accuracy.
- Change Notion database structure.
- Modify report frequency and content."
"ü§ñ Instagram MCP AI Agent ‚Äì Read, Reply & Manage Comments with GPT-4o",https://n8n.io/workflows/3896-instagram-mcp-ai-agent-read-reply-and-manage-comments-with-gpt-4o/,"ü§ñ Instagram AI Agent with MCP Server ‚Äì Built for Smart Engagement and Automation
Hi! I‚Äôm Amanda ü•∞
I build intelligent automations with n8n and Make.
This powerful workflow was designed to help teams automatically handle Instagram interactions with AI. Using Meta Graph API, LangChain, MCP Server, and GPT-4o, it allows your AI agent to search for posts, read captions, fetch comments, and even reply or message followers, all through structured tools.
üîß What the workflow does
Searches for recent media using Instagram ID and access token
Reads and extracts captions or media URLs
Fetches comments and specific replies from each post
Replies to comments automatically with GPT-generated responses
Sends direct messages to followers who commented
Maps user input and session to keep memory context via LangChain
Communicates via Server-Sent Events (SSE) using your MCP Server URL
üß∞ Nodes & Tech Used
LangChain Agent + Chat Model with GPT-4o
Memory Buffer for session memory
toolHttpRequest to search media, comments, and send replies
MCP Trigger and MCP Tool (custom SSE connection)
Set node for input and variable assignment
Webhook and JSON for Instagram API structure
‚öôÔ∏è Setup Instructions
Create your Instagram App in Meta Developer Portal
Add your Instagram ID and Access Token in the Set node
Update the MCP Server Tool URL in the MCP Instagram node
Use your n8n server URL (e.g. https://yourdomain.com/mcp/server/instagram/sse)
Trigger the workflow using the included LangChain Chat Trigger
Interact via text to ask the agent to:
‚ÄúGet latest posts‚Äù
‚ÄúReply to comment X with this message‚Äù
‚ÄúSend DM to this user about...‚Äù
üë• Who this is for
Social media teams managing multiple comments
Brands automating engagement with followers
Agencies creating smart, autonomous digital assistants
Developers building conversational Instagram bots
‚úÖ Requirements
Meta Graph API access
Instagram Business account
n8n instance (Cloud or Self-hosted)
MCP Server configured (SSE Endpoint enabled)
OpenAI API Key (for GPT-4o + LangChain)
üåê Want to use this workflow?
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda"
Automated Instagram Comment Replies using Gemini AI with Context-Aware Responses,https://n8n.io/workflows/3713-automated-instagram-comment-replies-using-gemini-ai-with-context-aware-responses/,"Instagram Auto-Comment Responder with AI Agent Integration
Version: 1.1.0 ‚Äß n8n Version: 1.88.0+ ‚Äß License: MIT
A fully automated workflow for managing and responding to Instagram comments using AI agents. Designed to improve engagement and save time, this system listens for new Instagram comments, verifies and filters them, fetches relevant post data, processes valid messages with a natural language AI, and posts context-aware replies directly on the original post.
Key Features
üí¨ AI-Driven Engagement: Intelligent responses to comments via a GPT-powered agent.
‚úÖ Webhook Verification: Handles Instagram webhook handshake to ensure secure integration.
üì¶ Data Extraction: Maps incoming payload fields (user ID, username, message text, media ID) for processing.
üö´ Self-Comment Filtering: Automatically skips comments made by the account owner to prevent loops.
üì° Post Data Retrieval: Fetches the media‚Äôs id and caption from the Graph API (v22.0) before generating a reply.
üß† Natural Language Processing: Uses a custom system prompt to maintain brand tone and context.
üîÅ Automated Replies: Posts the AI-generated message back to the comment thread using Instagram‚Äôs API.
üß© Modular Architecture: Clear separation of steps via sticky notes and dedicated HTTP Request and Agent nodes.
Use Cases
Social Media Automation: Keep followers engaged 24/7 with instant, relevant replies.
Community Building: Maintain a consistent voice and tone across all interactions.
Brand Reputation Management: Ensure no valid comment goes unanswered.
AI Customer Support: Triage simple questions and direct followers to resources or support.
Technical Implementation
Webhook Verification
Node: Webhook + Respond to Webhook
Echoes hub.challenge to confirm subscription and secure incoming events.
Data Extraction
Node: Set
Maps payload fields into structured variables: conta.id, usuario.id, usuario.name, usuario.message.id, usuario.message.text, usuario.media.id, endpoint.
User Validation
Node: Filter
Skips processing if conta.id equals usuario.id (self-comments).
Post Data Retrieval
Node: HTTP Request (Get post data)
GET https://graph.instagram.com/v22.0/{{ $json.usuario.media.id }}?fields=id,caption&access_token={{ credentials }}
Captures the media‚Äôs caption for richer context in replies.
AI Response Generation
Nodes: AI Agent + OpenRouter Chat Model
Uses a detailed system prompt with:
Profile persona (expert in AI & automations, friendly tone).
Input data (username, comment text, post caption).
Filtering logic (spam, praise, questions, vague comments).
Returns either the reply text or [IGNORE] for irrelevant content.
Posting the Reply
Node: HTTP Request (Post comment)
POST {{ $json.endpoint }}/{{ $json.usuario.message.id }}/replies with message={{ $json.output }}
Sends the AI answer back under the original comment.
Instructions for Setup
Import Workflow
In n8n > Workflows > Import from File, upload the provided .json template.
Configure Credentials
Instagram Graph API (Header Auth or FacebookGraphApi) with instagram_basic, instagram_manage_comments scopes.
OpenRouter/OpenAI API key for AI agent.
Customize System Prompt
Edit the AI Agent‚Äôs prompt to adjust brand tone, language (Brazilian Portuguese), length, or emoji usage.
Test & Activate
Publish a test comment on an Instagram post.
Verify each node‚Äôs execution, ensuring the webhook, filter, data extraction, HTTP requests, and AI Agent respond as expected.
Extend & Monitor
Add sentiment analysis or lead capture nodes as needed.
Monitor execution logs for errors or rate-limit events.
Tags
Social Media ‚Ä¢ Instagram Automation ‚Ä¢ Webhook Verification ‚Ä¢ AI Agent ‚Ä¢ HTTP Request ‚Ä¢ Auto Reply ‚Ä¢ Community Management"
"Automated LinkedIn Lead Generation, Scoring & Communication with AI-Agent",https://n8n.io/workflows/3490-automated-linkedin-lead-generation-scoring-and-communication-with-ai-agent/,"‚ö†Ô∏è DISCLAIMER: This workflow uses the HDW LinkedIn community node, which is only available on self-hosted n8n instances. It will not work on n8n.cloud.
Overview
This workflow automates the entire LinkedIn lead generation process from finding prospects that match your Ideal Customer Profile (ICP) to sending personalized messages. It uses AI to analyze lead data, score potential clients, and prioritize your outreach efforts.
Key Features
AI-Driven Lead Generation: Convert ICP descriptions into LinkedIn search parameters
Comprehensive Data Enrichment: Analyze company websites, LinkedIn posts, and news
Intelligent Lead Scoring: Prioritize leads based on AI analysis of intent signals
Automated Outreach: Connect with prospects and send personalized messages
Requirements
Self-hosted n8n instance with the HDW LinkedIn community node installed
OpenAI API access (for GPT-4o)
Google Sheets access
HDW API key (available at app.horizondatawave.ai)
LinkedIn account
Setup Instructions
1. Install Required Nodes
Ensure the HDW LinkedIn community node is installed on your n8n instance
Command: npm install n8n-nodes-hdw
(or use this instruction)
2. Configure Credentials
OpenAI: Add your OpenAI API key
Google Sheets: Set up Google account access
HDW LinkedIn: Configure your API key from horizondatawave.ai
3. Set Up Google Sheet
Create a new Google Sheet with the following columns (or copy template):
Name, URN, URL, Headline, Location, Current company, Industry, etc.
The workflow will populate these columns automatically
4. Customize Your ICP
Use chat to provide the AI Agent with your Ideal Customer Profile
Example: ""Target marketing directors at SaaS companies with 50-200 employees""
5. Adjust Scoring Criteria
Modify the lead scoring prompt in the ""Company Score Analysis"" node to match your specific product/service
Tune the evaluation criteria based on your unique business needs
6. Configure Message Templates
Update the HDW LinkedIn Send Message node with your custom message
How It Works
ICP Translation: AI converts your ICP description into LinkedIn search parameters
Lead Discovery: Workflow searches LinkedIn using these parameters
Data Collection: Results are saved to Google Sheets
Enrichment: System collects additional data about each lead:
Company website analysis
Lead's LinkedIn posts
Company's LinkedIn posts
Recent company news
Intent Analysis: AI analyzes all data to identify buying signals
Lead Scoring: Leads are scored on a 1-10 scale based on likelihood of interest
Connection Requests: Top-scoring leads receive connection requests
Follow-Up: When connections are accepted, automated messages are sent
Customization
Search Parameters: Adjust the AI Agent prompt to refine your target audience
Scoring Criteria: Modify scoring prompts to highlight indicators relevant to your product
Message Content: Update message templates for personalized outreach
Schedule: Configure when connection requests and messages are sent
Rate Limits & Best Practices
LinkedIn has connection request limits (approximately 100-200 per week)
The workflow includes safeguards to avoid exceeding these limits
Consider spacing your outreach for better response rates
Note: Always use automation tools responsibly and in accordance with LinkedIn's terms of service."
"AI Email Analyzer: Process PDFs, Images & Save to Google Drive + Telegram",https://n8n.io/workflows/3169-ai-email-analyzer-process-pdfs-images-and-save-to-google-drive-telegram/,"This workflow automates the process of analyzing emails and their attachments (PDFs and images) using AI models (DeepSeek, Gemini, and OpenRouter). It extracts and summarizes the content of emails and attachments, saves the summaries to Google Sheets, and sends a final consolidated summary via Telegram.
This is a powerful tool for automating email analysis and summarization, saving time and ensuring that important information is easily accessible and actionable.
Below is a breakdown of the workflow:
1. How It Works
The workflow is designed to process incoming emails, analyze their content and attachments, and generate summaries. Here's how it works:
Email Trigger:
The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new emails.
If an email contains attachments, the workflow processes them.
Check for Attachments:
The Contain Attachments? node checks if the email has attachments.
If attachments are present, the workflow proceeds to process them.
Process Attachments:
The Get PDF and Images Attachments node extracts PDF and image attachments from the email.
The Switch node separates PDFs and images for further processing:
PDFs: The Extract from PDF node extracts text from PDFs, and the PDF Analyzer node summarizes the content.
Images: The Analyze Image node uses AI to describe the content of images.
Summarize Email Content:
The Convert Text node converts the email's HTML content to plain text.
The Email Summarization Chain node uses AI to generate a summary of the email's text content.
Save Summaries:
The Save Summary PDF, Save Summary Image, and Save Summary Text nodes save the summaries of PDFs, images, and email text, respectively, to Google Sheets.
Consolidate Summaries:
The All Summaries node aggregates the summaries of the email text, PDFs, and images.
The Create Final Summary node uses AI to generate a unified summary of all the content.
Send Final Summary:
The Send Final Summary node sends the consolidated summary via Telegram to a specified chat ID.
2. Set Up Steps
To set up and use this workflow in n8n, follow these steps:
IMAP Configuration:
Set up IMAP credentials in n8n for the Email Trigger (IMAP) node.
Ensure the email account is accessible via IMAP.
AI Model Configuration:
Configure the DeepSeek, Gemini, and OpenRouter credentials in n8n for the Email Summarization Chain, PDF Analyzer, and Create Final Summary nodes.
Ensure the AI models are set up to generate summaries.
Google Sheets Integration:
Set up Google Sheets credentials in n8n for the Save Summary PDF, Save Summary Image, and Save Summary Text nodes.
Specify the Google Sheet and worksheet where the summaries will be saved.
Telegram Integration:
Set up Telegram credentials in n8n for the Send Final Summary node.
Insert your Chat ID in the Telegram node to receive the final summary.
Test the Workflow:
Send an email with attachments (PDFs and images) to the monitored email account.
The workflow will:
Extract and summarize the email content and attachments.
Save the summaries to Google Sheets.
Send a consolidated summary via Telegram.
Optional Customization:
replace IMAP trigger with Gmail or Outlook trigger
Modify the workflow to include additional features, such as:
Adding more AI models for different types of analysis.
Sending notifications via other channels (e.g., Slack, email).
Integrating with other storage services (e.g., Dropbox, AWS S3).
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Automate Content Generator for WordPress with DeepSeek R1,https://n8n.io/workflows/2813-automate-content-generator-for-wordpress-with-deepseek-r1/,"This workflow is designed to generate SEO-friendly content with DeepSeek R1 (or V3), publish it on WordPress, and update a Google Sheets document with the details of the created post. Below is a detailed analysis of what each node in the workflow does:
How It Works
Triggering the Workflow:
The workflow starts with a Manual Trigger node, which is activated when the user clicks ""Test workflow"" in the n8n interface.
Fetching Data:
The Get Ideas node retrieves data from a Google Sheets document. It reads a specific sheet and filters the data based on the ""ID POST"" column, returning the first matching row.
Setting the Prompt:
The Set your prompt node extracts the PROMPT field from the Google Sheets data and assigns it to a variable for use in subsequent steps.
Generating Content:
The Generate article node uses an AI model (DeepSeek) to create an SEO-friendly article based on the prompt. The article includes an introduction, 2-3 chapters, and a conclusion, formatted in HTML.
The Generate title node uses the same AI model to generate a concise, SEO-optimized title for the article.
Publishing on WordPress:
The Create post on WordPress node creates a new draft post on WordPress using the generated title and article content.
Generating and Uploading an Image:
The Generate Image node creates a photorealistic image based on the article title using an AI model (OpenAI).
The Upload image node uploads the generated image to WordPress as a media file.
The Set Image node assigns the uploaded image as the featured image for the WordPress post.
Updating Google Sheets:
The Update Sheet node updates the original Google Sheets document with the post details, including the title, post ID, creation date, and row number.
Set Up Steps
Configure Google Sheets Integration:
Set up the Google Sheets node to connect to your Google account and specify the document ID and sheet name to read from and update.
Set Up AI Models:
Configure the OpenAI nodes (for generating the article, title, and image) with the appropriate API credentials and model settings (e.g., deepseek-reasoner for text generation).
Configure WordPress Integration:
Set up the WordPress node with your WordPress site's API credentials to allow creating posts and uploading media.
Define the Prompt and Content Structure:
In the Set your prompt node, ensure the prompt variable is correctly mapped to the data from Google Sheets.
In the Generate article and Generate title nodes, define the instructions for the AI model to generate the desired content.
Set Up Image Generation:
Configure the Generate Image node with the appropriate prompt and image settings (e.g., size, quality, style).
Configure HTTP Requests for Media Upload:
Set up the Upload image and Set Image nodes to use the WordPress REST API for uploading and assigning the featured image.
Map Data for Google Sheets Update:
In the Update Sheet node, map the relevant fields (e.g., title, post ID, date) to the appropriate columns in the Google Sheets document.
Test and Activate the Workflow:
Run the workflow manually to ensure all steps execute correctly.
Once verified, activate the workflow for automated execution.
Overall purpose of the workflow
This workflow automates the creation of SEO-friendly content for a WordPress blog. Starting from a prompt extracted from a Google Sheets document, it generates an article, a title, and an image, publishes the post on WordPress, and updates the Google Sheets document with the details of the created post. This process is useful for blog managers who want to automate content creation and publishing.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Upwork Job Aggregator with OpenAI Summaries & Multi-channel Notifications,https://n8n.io/workflows/4793-upwork-job-aggregator-with-openai-summaries-and-multi-channel-notifications/,"Automated system that aggregates job postings from Upwork based on your criteria and delivers personalized notifications, helping you never miss relevant opportunities.
üöÄ What It Does
Custom job search based on keywords, skills, and categories
Real-time job posting monitoring
Smart filtering and prioritization
Multi-channel notifications (Email/Slack/Teams)
Duplicate detection
üéØ Perfect For
Freelancers seeking new opportunities
Agencies looking for projects
Recruiters sourcing talent
Business owners finding contractors
Digital nomads
‚öôÔ∏è Key Benefits
‚úÖ Save hours of manual searching
‚úÖ Get notified about relevant jobs first
‚úÖ Never miss perfect opportunities
‚úÖ Customizable search criteria
‚úÖ Time-zone friendly notifications
üîß What You Need
Upwork account
n8n instance
Preferred notification channels setup
üìä Features
Custom keyword alerts
Budget range filtering
Client history analysis
Job type filtering (hourly/fixed)
Experience level filtering
üõ†Ô∏è Setup & Support
Quick Setup
Be up and running in 15 minutes with our step-by-step guide
üì∫ Watch Tutorial
üíº Get Expert Support
üìß Direct Help
Transform your freelance career with automated job hunting. Spend less time searching and more time winning projects."
Automatic Gmail Message Categorization with GPT-4 Content Analysis & Labels,https://n8n.io/workflows/4680-automatic-gmail-message-categorization-with-gpt-4-content-analysis-and-labels/,"Who is this for?
This workflow is ideal for Gmail users and teams who receive a high volume of emails and want to streamline inbox management. It suits professionals seeking to organize messages automatically, including sales teams, project managers, support staff, and anyone who benefits from automated email categorization.
What problem is this workflow solving? / Use case
Manually labeling emails is time-consuming and can lead to inconsistent organization. This automated n8n workflow uses Gmail and OpenAI to analyze incoming messages and apply the appropriate labels, such as ""Quotation"", ""Inquiry"", ""Project progress"", and ""Notification"", based on content‚Äîimproving productivity and ensuring important messages are prioritized.
What this workflow does
The workflow retrieves new Gmail messages, analyzes their content with OpenAI, and automatically assigns pre-defined Gmail labels that match the email‚Äôs intent. This ensures emails are sorted efficiently using AI-powered content analysis and Gmail‚Äôs labeling system.
Setup
Ensure Gmail labels (e.g., ""Quotation"", ""Inquiry"") are created in your Gmail account.
Connect your Gmail and OpenAI accounts as credentials in n8n.
Import the workflow into your n8n instance and update node configurations to match your Gmail label names.
How to customize this workflow to your needs
Edit or add Gmail labels both in your Gmail account and within the workflow logic.
Adjust the prompt or parameters sent to OpenAI to better match your categorization style.
Expand or refine the list of label categories to fit your team‚Äôs or business‚Äôs requirements."
Automate VIRAL Youtube Titles & Thumbnails Creation (FLUX.1 + Apify),https://n8n.io/workflows/4504-automate-viral-youtube-titles-and-thumbnails-creation-flux1-apify/,"For Who?
Content Creators
Youtube Automation
Marketing Team
How it works?
1 - Enter your content idea in the Edit Fields node in a ""raw"" format. Ex : Boil Eggs Perfectly
2 - LLM create 3 keywords request based on the idea and Apify scrape the YTB Search
3 - Wait until the dataset is completed in Apify
4 - Retrieve Dataset from Apify, calculate approximation of CTR and filter top performing videos
5 - LLM analyze patterns of best performing titles and create a prompt based on it. Another LLM create 5 titles based on these criteria
6 - LLM analyze patterns of best performing thumbnails and create a prompt based on it. Another LLM create 1 thumbnail based on these criteria
7 - Return titles and thumbnail in a HTML Page
üì∫ YouTube Video Tutorial: https://youtu.be/Upuj9Pi94g0
SETUP
Setup Input Content Idea : Enter Keyword Related to the niche you want. Trigger can be replaced with anything as long as you retrieve a content idea. For example : Form submission, Database entry, etc ...
If you want to change the number of keywords, update the data accordingly in the ""Create Keywords"" LLM Chain node ‚û°Ô∏è Structured Output Parser AND in the ""YTB Search Scrape"" HTTP Request Node in Body ‚û°Ô∏è JSON ‚û°Ô∏è searchQueries.
If you want to change the number of scraped videos for each keyword, update the data accordingly in the ""Create Videos Dataset"" HTTP Request Node in Body ‚û°Ô∏è JSON ‚û°Ô∏è maxResults.
If you want to adjust the CTR Calculation feel free to update it in the Code Node ‚û°Ô∏è Follow the Comments (after ""//"") to find what you're looking for.
If you want to adjust the level of virality of the videos kept for analaysis go to Filter Node ‚û°Ô∏è Value.
Setup Output HTML Page : You can also replace this part with any type of storage. For example : Airtable Database, Google Drive/Google Sheet, Send to an email, etc ...
APIs : For the following third-party integrations, replace ==[YOUR_API_TOKEN]== with your API Token or connect your account via Client ID / Secret to your n8n instance :
Apify : https://docs.apify.com/api/v2/getting-started
OpenAI : https://platform.openai.com/docs/overview (base URL : https://api.openai.com/v1) OR OpenRouter : https://openrouter.ai/docs/quickstart (base URL : https://openrouter.ai/api/v1)
HuggingFace (FLUX.1) : https://huggingface.co/docs
üõ†Ô∏è Need Help with Your Workflows ? https://tally.so/r/wayeqB
üìß Contact me : contact.nassercc@gmail.com
üë®‚Äçüíª More Workflows : https://n8n.io/creators/nasser/"
"Automate LinkedIn Engagement with Phantombuster, OpenAI GPT & Google Sheets Tracking",https://n8n.io/workflows/4586-automate-linkedin-engagement-with-phantombuster-openai-gpt-and-google-sheets-tracking/,"Workflow Overview
This sophisticated n8n automation is a powerful LinkedIn engagement and networking tool designed to revolutionize professional social media interaction. By intelligently combining web scraping, AI, and automation technologies, this workflow:
Discovers Relevant Content:
Automatically scrapes LinkedIn posts
Identifies target profiles and recent content
Ensures consistent networking opportunities
Generates Intelligent Interactions:
Uses AI to craft contextual, professional comments
Ensures human-like, valuable engagement
Maintains professional tone and relevance
Automates Engagement Process:
Likes and comments on selected posts
Increases visibility and connection potential
Builds professional network systematically
Comprehensive Activity Tracking:
Logs all interactions in Google Sheets
Provides transparent engagement history
Enables performance analysis and optimization
Key Benefits
ü§ñ Full Automation: Consistent daily networking
üí° AI-Powered Interactions: Intelligent, context-aware engagement
üìä Detailed Tracking: Comprehensive interaction logging
üåê Professional Visibility: Strategic online presence management
Workflow Architecture
üîπ Stage 1: Content Discovery
Scheduled Trigger: Daily post scanning
Phantombuster Integration: LinkedIn post scraping
Targeted Profile Research:
Identifies recent posts
Extracts critical post metadata
üîπ Stage 2: AI-Powered Interaction
OpenAI GPT Model: Generates contextual comments
Intelligent Analysis:
Understands post content
Crafts personalized responses
Professional Tone Maintenance
üîπ Stage 3: Engagement Automation
Automated Liking: Increases post visibility
Intelligent Commenting:
Posts AI-generated comments
Ensures meaningful interaction
üîπ Stage 4: Performance Logging
Google Sheets Integration
Comprehensive Activity Tracking
Interaction History Preservation
Potential Use Cases
Sales Professionals: Lead generation and networking
Marketers: Increased brand visibility
Recruiters: Talent discovery and engagement
Entrepreneurs: Professional network expansion
Content Creators: Audience interaction and growth
Setup Requirements
Phantombuster Account
API key
Configured LinkedIn scraping agents
Profile URL list
OpenAI API
GPT model access
API key for comment generation
Preferred language model
Google Sheets
Connected Google account
Prepared tracking spreadsheet
Appropriate sharing settings
n8n Installation
Cloud or self-hosted instance
Workflow configuration
API credential management
Future Enhancement Suggestions
ü§ñ Advanced sentiment analysis
üìä Engagement performance metrics
üéØ Intelligent post targeting
üîç Machine learning optimization
üåê Multi-platform support
Technical Considerations
Implement robust error handling
Use exponential backoff for API calls
Maintain flexible engagement strategies
Ensure compliance with platform guidelines
Ethical Guidelines
Respect professional networking etiquette
Maintain genuine, value-adding interactions
Avoid spammy or repetitive engagement
Prioritize quality over quantity
Connect With Me
Ready to revolutionize your professional networking?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your LinkedIn strategy with intelligent, automated workflows!
#LinkedInAutomation #AINetworking #ProfessionalGrowth #CareerStrategy #NetworkingTech #AIMarketing #ProfessionalDevelopment #SocialMediaStrategy #ContentEngagement #BusinessIntelligence"
Automated YouTube Channel Lead Generation & Email Outreach with Apify and ZeroBounce,https://n8n.io/workflows/4582-automated-youtube-channel-lead-generation-and-email-outreach-with-apify-and-zerobounce/,"Workflow Overview
This sophisticated n8n automation is a powerful lead generation and outreach tool designed to transform YouTube channel research into actionable marketing opportunities. By intelligently connecting multiple services and APIs, this workflow:
Discovers Targeted Channels:
Scrapes YouTube channels based on specific keywords
Extracts comprehensive channel metadata
Identifies potential business opportunities
Intelligent Lead Qualification:
Filters channels with contact emails
Validates email authenticity
Ensures high-quality lead generation
Personalized Outreach:
Sends customized cold emails
Leverages channel-specific personalization
Automates initial contact process
Key Benefits
üïµÔ∏è Automated Lead Discovery: Find potential collaborators or clients
üß† Smart Filtering: Eliminate invalid or irrelevant leads
üìß Personalized Outreach: Contextual, channel-specific communication
‚è±Ô∏è Time-Saving: Eliminate manual research and email hunting
Workflow Architecture
üîç Stage 1: Channel Scraping
Apify Integration: Scrapes YouTube channels
Keyword-Based Search: Target specific niches
Metadata Extraction: Collect channel details, emails
üß© Stage 2: Lead Qualification
Email Existence Check: Filter channels with contact info
ZeroBounce Verification: Validate email authenticity
Quality Control: Ensure only valid leads proceed
üì¨ Stage 3: Personalized Outreach
Gmail Integration: Send customized cold emails
Dynamic Personalization: Use channel-specific details
Automated Communication: Streamline initial contact
Potential Use Cases
Marketing Agencies: Find potential clients
Influencer Marketers: Discover collaboration opportunities
Content Creators: Network and expand professional connections
Sales Teams: Generate targeted lead lists
Recruitment Specialists: Identify industry professionals
Setup Requirements
Apify Account
API token
YouTube Scraper Actor
Configured search keywords
ZeroBounce Account
Email verification API
Validation credits
Gmail Account
OAuth2 authentication
Configured sending profile
n8n Installation
Cloud or self-hosted instance
Import workflow configuration
Configure API credentials
Future Enhancement Suggestions
ü§ñ AI-powered email personalization
üìä Advanced lead scoring mechanisms
üîÑ Automated follow-up sequences
üìà Integration with CRM platforms
üåê Multi-platform lead generation
Ethical Considerations
Respect email communication guidelines
Comply with anti-spam regulations
Provide clear opt-out mechanisms
Maintain professional, value-driven outreach
Connect With Me
Ready to supercharge your lead generation?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your outreach strategy with intelligent, automated workflows!"
Automate LinkedIn Content from Twitter AI Posts with GPT-4 and Google Sheets,https://n8n.io/workflows/4487-automate-linkedin-content-from-twitter-ai-posts-with-gpt-4-and-google-sheets/,"How it works
Automates daily LinkedIn post creation from trending AI tweets.
Fetches latest tweets, processes content, checks for duplicates, converts to LinkedIn-ready format, schedules, and posts automatically.
Set up steps
Setup required: Google Sheets, LinkedIn, Telegram, RapidAPI, and OpenAI/OpenRouter API credentials.
Approximate setup time: 30-45 mins for all integrations and triggers.
Detailed credential/config instructions are in sticky notes inside the workflow."
Generate Enhanced AI Images via Telegram with DALL-E and GPT,https://n8n.io/workflows/4467-generate-enhanced-ai-images-via-telegram-with-dall-e-and-gpt/,"PromptCraft AI ‚Äì Telegram Image Generator
üöÄ How It Works
PromptCraft AI is an n8n automation that transforms simple image ideas sent through Telegram into stunning AI-generated images using OpenAI's DALL¬∑E (or other image models).
üîÅ Workflow Overview:
Telegram Trigger: Listens for messages from a user on Telegram.
Prompt Expansion: The message is transformed into a rich image description using GPT (OpenAI Chat Model).
Image Generation: The prompt is passed to OpenAI's image API to generate a high-quality image.
Send Image: The final image is sent back to the user on Telegram.
(Optional) Log image titles and links to Google Drive and Google Sheets.
‚öôÔ∏è Setup Instructions
üìã Prerequisites
[ ] n8n installed (Self-hosted or via n8n.cloud)
[ ] Telegram bot token (via @BotFather)
[ ] OpenAI API key (platform.openai.com)
[ ] Google Sheets & Drive OAuth2 credentials (optional)
üß† Step-by-Step Configuration
1. üì• Import the Workflow
Go to n8n ‚Üí click Import ‚Üí upload PromptCraft_AI_Template.json
2. üîê Set Up Credentials
In Credentials, add the following:
Telegram API ‚Üí Paste your bot token
OpenAI API ‚Üí Paste your OpenAI API key
(Optional) Google Sheets OAuth2, Google Drive OAuth2
3. üîÑ Replace Placeholders
Open each node that requires credentials:
Replace REPLACE_OPENAI_API_KEY with your actual OpenAI API key
Replace REPLACE_TELEGRAM_API_ID and credential names as needed
(Optional) Update Google Drive Folder ID & Sheet ID in respective nodes
4. ‚úÖ Activate the Workflow
Turn on the Telegram Trigger node.
Deploy and activate the full workflow.
5. ‚úâÔ∏è Test It Out
Send your Telegram bot a message like:
a knight riding a robotic horse in the future
Receive the generated image back in Telegram!
üí° Pro Tips
Use detailed or imaginative inputs for better outputs.
Fine-tune the GPT prompt for specific visual styles.
Extend with Google Vision, image upscaling, or watermarking.
üõü Support
For setup assistance or custom feature requests, feel free to contact me @dimejicole21@gmail.com
Happy Prompting! üñº‚ú®"
"Shopify Multi-Module Automation with GPT-4o, Langchain Agents & Integrations",https://n8n.io/workflows/4455-shopify-multi-module-automation-with-gpt-4o-langchain-agents-and-integrations/,"This n8n workflow orchestrates a powerful suite of AI Agents and automations to manage and optimize various aspects of an e-commerce operation, particularly for platforms like Shopify. It leverages Langchain AI Agents powered by OpenAI, along with a variety of integrated services, to handle customer support, product recommendations, abandoned cart recovery, inventory management, post-purchase engagement, review monitoring, marketing campaigns, and operational alerts.
Tools & Services Used
n8n: Core automation and orchestration platform.
Langchain AI Agents: For intelligent decision-making, task execution, and natural language processing across multiple modules.
OpenAI (e.g., GPT-4o Mini): Powers the AI Agents and direct LLM calls for tasks like sentiment analysis and content generation.
Shopify (Conceptual - requires specific API/Webhook setup): For triggers (abandoned cart, order delivered) and actions (fetching products/inventory, creating discounts).
Google Sheets: Used as a data store for FAQs, logging low stock, review databases, and campaign outcomes.
Slack: For internal team notifications (escalations, low stock, negative reviews).
Twilio (or similar SMS service): For sending SMS alerts (cart recovery, restock notifications).
Email (Generic SMTP/API): For sending various emails (AI responses, recovery emails, review requests, campaign emails).
Notion: For logging customer interactions.
HTTP Request Nodes: For generic API interactions (fetching customer info, product search, CRM, campaign tracking).
Webhook Nodes: For receiving external triggers (incoming messages, product inquiries, review submissions, restock signals).
Cron Nodes: For scheduled tasks (hourly inventory checks, daily campaigns).
Workflow Overview
This workflow is a collection of interconnected sub-workflows, each designed to automate a specific e-commerce process:
1. Customer Support & Escalation
Handles incoming customer messages, attempts to answer with FAQs or order status using an AI Agent, and escalates to a human agent via Slack if necessary. Logs interactions to Notion.
2. Product Inquiry & Recommendation
Processes product inquiries, searches a product catalog, filters/sorts results, uses an AI Agent for selection refinement, and can build a product carousel for user response.
3. Abandoned Cart Recovery
Detects abandoned carts, fetches customer data, uses an AI Agent to strategize recovery (e.g., offer discounts), and sends follow-up communications via email/SMS.
4. Inventory Management (Low Stock Alert)
Periodically checks inventory levels, identifies low-stock items, uses an AI Agent to format reports, and notifies relevant teams via Slack/email, logging to Google Sheets.
5. Post-Purchase Review Request
After a set period post-delivery, an AI Agent drafts and initiates sending a personalized product review request email to the customer.
6. Review Monitoring & Flagging
Listens for new product reviews, uses an AI model for sentiment analysis, alerts a support team for negative reviews via Slack, and logs all reviews.
7. Scheduled Marketing Campaigns
Fetches target audiences, segments them, uses AI Agents to generate tailored email content, sends campaigns, tracks basic metrics, and employs another AI Agent to suggest campaign adjustments.
8. Manual Restock Alert & Logging
Allows triggering a restock notification via webhook, sends an SMS alert, and uses an LLM to log the event.
Prerequisites
Ensure you have active accounts and necessary API keys/credentials for:
OpenAI
Shopify (or your e-commerce platform API)
Google Cloud (for Google Sheets API)
Slack (Bot Token)
Twilio (Account SID, Auth Token, From Number)
Your Email Sending Service (SMTP details or API key)
Notion (Integration Token)
Any other external APIs used (e.g., CRM, Product Search, Customer Info, Campaign Tracking)
How to Use This Template
Step 1: Import the Template
Open your n8n instance.
Go to ‚ÄúWorkflows‚Äù > ‚ÄúCreate Workflow‚Äù.
Click the three dots (‚Ä¶) > ‚ÄúImport from File‚Äù.
Upload the downloaded JSON file of this workflow.
Step 2: Configure Critical Nodes & Credentials
This is the most crucial step. Systematically go through each of the 8 sections outlined in the Sticky Notes within the workflow and configure the relevant nodes:
SECTION 1 (Customer Support):
Get FAQs Data (Google Sheets): Set sheetId to YOUR_SHEET_ID_HERE and configure Google API credentials.
Lookup Order API (HTTP Request): Set url to YOUR_ORDER_LOOKUP_API_ENDPOINT_HERE.
Notify Human Agent (Slack): Configure Slack API credentials and channel.
Send AI Response to Customer (HTTP Request): Set url to YOUR_CUSTOMER_RESPONSE_API_ENDPOINT_HERE.
Store to Notion (Notion): Set pageId (Database ID) to YOUR_NOTION_DATABASE_URL_OR_ID_HERE and Notion API credentials.
SECTION 2 (Product Inquiry):
Search Products API (HTTP Request): Set url to YOUR_PRODUCT_SEARCH_API_ENDPOINT_HERE.
Build Product Carousel (Function): Update YOUR_DEFAULT_PRODUCT_IMAGE_URL_HERE and YOUR_FALLBACK_PRODUCT_URL_HERE.
Respond to User with Carousel (HTTP Request): Set url for sending carousel data.
SECTION 3 (Abandoned Cart):
Detect Abandoned Cart (Shopify Trigger): Configure Shopify credentials and ensure the correct trigger event.
Fetch Customer Info (HTTP Request): Set url to YOUR_CUSTOMER_INFO_API_ENDPOINT_HERE.
Create Discount (if applicable) (Shopify): Configure Shopify credentials and review discount creation parameters.
Send Recovery Email (HTTP Request/Email Send): Set email API endpoint or SMTP credentials.
Send SMS Reminder (Twilio): Configure Twilio credentials and from number (YOUR_TWILIO_PHONE_NUMBER_HERE).
SECTION 4 (Low Stock Alerts):
Fetch Inventory (Shopify): Configure Shopify credentials.
Notify Slack (Low Stock) (Slack): Configure Slack API credentials and channel.
Generate Email Report (Low Stock) (Email Send): Configure SMTP/email credentials and to address.
Export to Sheets (Low Stock Log) (Google Sheets): Set sheetId to YOUR_INVENTORY_LOG_SHEET_ID_HERE and Google API credentials.
SECTION 5 (Review Request):
Order Delivered Trigger (Shopify Trigger): Configure Shopify credentials.
Fetch Customer Data (for Review) (HTTP Request): Set url to YOUR_CUSTOMER_INFO_API_ENDPOINT_HERE.
DraftReviewRequestEmail (Tool): Update YOUR_PRODUCT_PAGE_URL_BASE_HERE and YOUR_WEBSITE_URL_HERE.
Send Review Request Email (Email Send): Configure SMTP/email credentials.
SECTION 6 (Review Monitoring):
OpenAI node (currently Assistant Create): IMPORTANT: For sentiment analysis, change this node type to @n8n/n8n-nodes-langchain.lmChatOpenAi. Then configure its prompt for sentiment analysis (e.g., System: ""Analyze sentiment. Respond with positive, negative, or neutral."" User: ""Review: {{ $('Listen for Review Webhook').first().json.body.review_text }}""). Set OpenAI credentials.
Notify Support Team (Negative Review) (Slack): Configure Slack API credentials and channel.
Add Review to Database (Google Sheets): Set sheetId to YOUR_REVIEWS_DATABASE_SHEET_ID_HERE and Google API credentials.
SECTION 7 (Marketing Campaigns):
Fetch Target Audience API (HTTP Request): Set url to YOUR_CRM_API_FOR_AUDIENCE_HERE.
GenerateCampaignEmailVariant (Tool): Update YOUR_CAMPAIGN_LANDING_PAGE_HERE.
Send Campaign Email (Email Send): Configure SMTP/email credentials.
Track Campaign Metrics API (HTTP Request): Set url for YOUR_CAMPAIGN_TRACKING_API_ENDPOINT_HERE.
Log Campaign Outcome & Adjustments (Google Sheets): Set sheetId to YOUR_CAMPAIGN_LOG_SHEET_ID_HERE and Google API credentials.
SECTION 8 (Manual Restock):
Send SMS Alert (Restock) (Twilio): Configure Twilio credentials, from number (YOUR_TWILIO_PHONE_NUMBER_HERE), and to (WAREHOUSE_MANAGER_PHONE_NUMBER_HERE).
OpenAI Chat Model (for Basic LLM Chain): Configure OpenAI credentials.
Send restock Request Email1 (Email Send): Configure SMTP/email credentials and to address.
Step 3: Implement Tool Logic
Go into each @n8n/n8n-nodes-langchain.toolCode node (e.g., SearchFAQs, RefineProductSelection, DetermineDiscount, etc.).
The current JavaScript is placeholder. You MUST replace this with actual JavaScript logic that performs the intended action for each tool, often by making API calls or processing data based on the agent's input.
Step 4: Review AI Agent Prompts & Tool Connections
Carefully review the systemMessage for each AI Agent node to ensure it accurately reflects its role and how it should use its tools.
CRITICAL: Verify the index for tools connected to each AI Agent. Each tool connected to a single agent must have a unique, sequential index (0, 1, 2, ...).
AI Agent (ID cd71629d-... in your JSON): SearchFAQs tool should be index: 0, LookupOrderStatus tool should be index: 1. (Please double-check and correct the connection indices in your actual workflow file if they are still both pointing to index 0).
AI Agent6 (ID 4c24879b-... in your JSON): AnalyzeCampaignPerformance tool should be index: 0, SuggestCampaignAdjustments tool should be index: 1. (Please double-check and correct the connection indices in your actual workflow file if they are still both pointing to index 0).
Initial Test Run
Disable most of the workflow sections initially.
Focus on one section at a time (e.g., Section 1: Customer Support).
Manually trigger the first node (e.g., simulate an incoming webhook).
Execute node by node, checking the output and debugging configurations and tool logic.
For time-based triggers (Cron, Wait nodes), temporarily shorten the durations for faster testing.
Once a section is working, move to the next.
Use Cases
E-commerce Businesses (Shopify, etc.): To automate customer service, marketing, sales recovery, and operations.
Digital Marketing Agencies: To offer advanced automation services to e-commerce clients.
Freelancers & Consultants: To build and deploy sophisticated AI-driven e-commerce solutions.
n8n Developers: As a comprehensive template and learning resource for building complex multi-agent systems."
Automate Reddit Trend Analysis with GPT-4 and Slack/Gmail Distribution,https://n8n.io/workflows/4373-automate-reddit-trend-analysis-with-gpt-4-and-slackgmail-distribution/,"ü§ñ AI Reddit Scout Agent: Auto Post Analysis & Insights
Stay ahead of trends and conversations with this intelligent n8n workflow that automatically monitors Reddit, analyzes discussions using AI, and delivers actionable insights to your team via Slack and Gmail. Perfect for market research, content inspiration, and competitive intelligence.
üîÑ How It Works
This automated 4-step intelligence gathering system runs daily to keep you informed:
Step 1: Scheduled Monitoring
The workflow automatically triggers daily (default: 9 AM) to scan Reddit for fresh discussions and trending topics.
Step 2: Smart Content Retrieval
The Reddit API integration searches specified subreddits using customizable filters:
Target subreddits (e.g., r/ChatGPT, r/technology, r/startups)
Keyword matching for specific topics
Post ranking criteria (hot, top, new)
Engagement thresholds (upvotes, comments)
Step 3: AI-Powered Analysis
Each retrieved post is processed by GPT-4 through a specialized agent that:
Extracts key discussion points and themes
Classifies topics in 3-5 word categories
Generates concise one-sentence summaries
Identifies sentiment and engagement patterns
Highlights actionable insights and trends
Step 4: Multi-Channel Distribution
Analyzed insights are automatically delivered to:
Slack channels for immediate team visibility
Gmail inboxes for stakeholders and decision-makers
Formatted with links back to original Reddit discussions
‚öôÔ∏è Setup Steps
Prerequisites
Reddit account (for API access)
OpenAI API key with GPT-4 access
Slack workspace with posting permissions
Gmail account for email distribution
n8n instance (cloud or self-hosted)
Configuration Steps
Credential Setup
Reddit API: Configure Reddit node with API credentials
OpenAI API Key: Required for AI analysis and summarization
Slack OAuth2: Connect workspace for channel posting
Gmail OAuth2: Enable email distribution functionality
Reddit Configuration
Subreddit Selection: Define target communities to monitor
Professional: r/entrepreneur, r/marketing, r/sales
Technology: r/artificial, r/MachineLearning, r/programming
Industry-specific: r/fintech, r/biotech, r/startups
Search Parameters: Set keywords and filters
Keywords: ""AI trends"", ""market analysis"", ""customer feedback""
Time ranges: past day, week, or month
Minimum engagement thresholds
AI Analysis Customization
Default analysis prompt can be tailored for:
Market Research Focus: Extract business insights and opportunities
Competitive Intelligence: Identify competitor mentions and sentiment
Content Ideas: Find trending topics for content creation
Customer Research: Analyze user pain points and feedback
Distribution Setup
Slack Integration: Choose channels for different content types
#market-intelligence for business insights
#content-ideas for creative inspiration
#competitive-intel for competitor discussions
Email Configuration: Set recipient lists and formatting preferences
Schedule Optimization
Frequency: Daily, weekly, or custom intervals
Timing: Optimize for team availability and time zones
Volume Control: Limit posts per execution to avoid overwhelm
üöÄ Use Cases
Market Research & Intelligence
Trend Identification: Spot emerging technologies and methodologies
Customer Sentiment: Monitor brand mentions and user feedback
Competitive Analysis: Track competitor discussions and reputation
Industry Insights: Stay updated on regulatory changes and news
Content Marketing
Topic Discovery: Find trending subjects for blog posts and videos
Audience Research: Understand community questions and pain points
Content Validation: Test ideas before creating full content pieces
Engagement Strategies: Learn what resonates with target audiences
Product Development
Feature Requests: Identify commonly requested functionality
User Experience Feedback: Discover usability issues and improvements
Market Validation: Gauge interest in new product concepts
Beta Testing Insights: Monitor feedback on product releases
Sales & Business Development
Lead Generation: Find potential customers discussing relevant problems
Objection Handling: Understand common concerns and hesitations
Partnership Opportunities: Identify potential collaboration prospects
Market Education: Understand knowledge gaps in your target market
Investment & Strategy
Startup Monitoring: Track emerging companies and funding news
Technology Assessment: Evaluate new tools and platforms
Risk Assessment: Monitor potential threats and challenges
Opportunity Identification: Spot underserved markets and niches
üîß Advanced Customization Options
Multi-Subreddit Orchestration
Configure different analysis approaches:
Technical Communities: Focus on innovation and implementation details
Business Communities: Extract market opportunities and strategies
User Communities: Emphasize pain points and solution requests
News Communities: Highlight breaking developments and reactions
Sentiment & Engagement Scoring
Enhance analysis with quantitative metrics:
Sentiment Analysis: Positive, negative, neutral classification
Engagement Prediction: Likelihood of viral content
Influence Scoring: Identify high-authority contributors
Trend Momentum: Track discussion growth over time
Content Filtering & Prioritization
Implement smart content curation:
Quality Thresholds: Minimum upvotes, comments, or awards
Relevance Scoring: AI-based topic matching and ranking
Duplicate Detection: Avoid repetitive content in summaries
Language Processing: Handle multiple languages and slang
Extended Integration Options
Connect additional platforms and tools:
CRM Integration: Push leads to Salesforce, HubSpot, or Pipedrive
Documentation Tools: Save insights to Notion, Obsidian, or Confluence
Analytics Platforms: Feed data to Google Analytics or Mixpanel
Social Media: Cross-post insights to Twitter, LinkedIn, or Facebook
üìä Output Examples
Sample Slack Message:
üîç Reddit Intelligence Report - r/ChatGPT
üìä Top Discussion: ""Why I stopped using ChatGPT for coding""
üìù Topic: AI Tool Limitations & Alternatives
üìã Summary: Users discussing ChatGPT's declining code quality and exploring alternatives like Claude and GitHub Copilot for development work.
üìà Engagement: 847 upvotes, 234 comments
üí≠ Sentiment: Mixed (concerns about accuracy, praise for alternatives)
üîó View Discussion: [Reddit Link]
Key Insights:
Growing demand for specialized coding AI tools
Quality consistency remains a major user concern
Opportunity for developer-focused AI solutions
Generated at 9:15 AM ‚Ä¢ Next scan: Tomorrow 9:00 AM
Gmail Report Format:
Subject: Daily Reddit Intelligence - AI & Technology Trends
üì° REDDIT SCOUT REPORT
Date: March 15, 2024
üéØ TOP INSIGHTS TODAY:
AI Development Tools (r/programming)
Discussion: ""Best AI coding assistants in 2024""
Insight: Developers increasingly comparing specialized tools vs general AI
Engagement: High (500+ upvotes)
Startup Funding (r/entrepreneur)
Discussion: ""Raised $2M seed round - lessons learned""
Insight: Current funding landscape and investor priorities
Engagement: Moderate (150+ upvotes)
Customer Feedback (r/SaaS)
Discussion: ""Why our churn rate dropped 40%""
Insight: Onboarding and support strategy impacts
Engagement: High (300+ upvotes)
üîó View all discussions: [Links included]
Powered by AI Reddit Scout Agent
Next report: Tomorrow at 9:00 AM
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
API Rate Limiting
Implement exponential backoff delays
Distribute requests across multiple time periods
Cache results to minimize redundant calls
Monitor Reddit API status and limits
Content Quality Control
Set minimum engagement thresholds
Filter out low-quality or spam posts
Implement keyword blacklists for irrelevant content
Regular prompt tuning for better AI analysis
Information Overwhelm
Limit posts per subreddit per day
Prioritize by engagement and relevance scores
Create digest formats for high-volume periods
Allow team members to customize their alerts
Optimization Strategies
Performance Enhancement
Use parallel processing for multiple subreddits
Implement caching for repeated analysis
Optimize OpenAI prompt length and complexity
Monitor and optimize workflow execution time
Cost Management
Balance AI analysis depth with API costs
Use different models for different content types
Implement smart filtering before AI analysis
Track ROI through engagement metrics
Team Adoption
Start with pilot programs in specific departments
Provide training on interpreting AI insights
Create feedback loops for continuous improvement
Establish clear action protocols for different insight types
üìà Success Metrics
Intelligence Quality Indicators
Relevance Score: Percentage of insights that lead to action
Timeliness: How quickly trends are identified vs competitors
Accuracy: Validation of AI predictions against actual outcomes
Engagement: Team interaction with distributed insights
Business Impact Measurements
Content Performance: Improvement in content engagement rates
Market Timing: Success rate of trend-based decisions
Competitive Advantage: Early identification of opportunities/threats
Time Savings: Reduction in manual research and monitoring time
üìû Questions & Support
Need assistance with your AI Reddit Scout Agent setup or customization?
üìß Technical Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Expertise: Reddit API integration, AI prompt optimization, workflow scaling
üé• Learning Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup and configuration guides
Advanced filtering and analysis techniques
Integration tutorials for popular business tools
Troubleshooting common issues
ü§ù Professional Networking
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing automation support
Share your intelligence gathering success stories
Access to exclusive workflow templates and updates
üí¨ Support Request Guidelines
Include in your message:
Target subreddits and use cases
Current team size and distribution needs
Specific industries or topics of interest
Integration requirements with existing tools
Any error messages or unexpected behaviors"
Extract and Merge Twitter (X) Threads using TwitterAPI.io,https://n8n.io/workflows/4088-extract-and-merge-twitter-x-threads-using-twitterapiio/,"Twitter (X) Thread Fetcher: Extract and Merge Tweets from Threads
What it does
Thread Detection: Automatically detects whether the provided Twitter link is a single tweet or a thread.
Tweet Extraction: Fetches and returns the content of a single tweet, or gathers all tweets in a thread by retrieving the first tweet and all connected replies.
Thread Merging: Merges all tweets in the correct order to reconstruct the complete thread, filtering out any empty results.
Seamless Integration: Easily trigger this workflow from other n8n workflows to automate Twitter thread extraction from various sources.
How it works
Accepts a Twitter link as input-either a single tweet or a thread.
If the link is for a single tweet, fetches and returns the tweet content.
If the link is for a thread, fetches the first tweet, then iteratively retrieves all connected replies that form the thread, ensuring only relevant tweets are included.
Merges the first tweet and all subsequent thread tweets in order, filters out any empty results, and returns the complete thread.
Uses twitterapi.io for all Twitter API requests.
Set up steps
Setup typically takes just a few minutes. You‚Äôll need to configure your Twitter API credentials for twitterapi.io.
You can trigger this workflow manually for testing or call it from another workflow to automate thread fetching from sources like Notion, spreadsheets, or other platforms.
For best results, create a separate workflow to gather Twitter links from your preferred source, then trigger this workflow to fetch and return the full thread.
Detailed configuration instructions and node explanations are included as sticky notes within the workflow canvas.
Benefits
Light speed: Fetches a 15-tweet thread in just 3 seconds for rapid results.
Cost effective: Processes a 15-tweet thread for only $0.0027, making it highly affordable. (Cost may vary depending on the density of replies in the thread.)"
Interactive Knowledge Base Chat with Supabase RAG using AI üìöüí¨,https://n8n.io/workflows/3799-interactive-knowledge-base-chat-with-supabase-rag-using-ai/,"Google Drive File Ingestion to Supabase for Knowledge Base üìÇüíæ
Overview üåü
This n8n workflow automates the process of ingesting files from Google Drive into a Supabase database, preparing them for a knowledge base system. It supports text-based files (PDF, DOCX, TXT, etc.) and tabular data (XLSX, CSV, Google Sheets), extracting content, generating embeddings, and storing data in structured tables. This is a foundational workflow for building a company knowledge base that can be queried via a chat interface (e.g., using a RAG workflow). üöÄ
Problem Solved üéØ
Manually managing a knowledge base with files from Google Drive is time-consuming and error-prone. This workflow solves that by:
Automatically ingesting files from Google Drive as they are created or updated.
Extracting content from various file types (text and tabular).
Generating embeddings for text-based files to enable vector search.
Storing data in Supabase for efficient retrieval.
Handling duplicates and errors to ensure data consistency.
Target Audience:
Knowledge Managers: Build a centralized knowledge base from company files.
Data Teams: Automate the ingestion of spreadsheets and documents.
Developers: Integrate with other workflows (e.g., RAG for querying the knowledge base).
Workflow Description üîç
This workflow listens for new or updated files in Google Drive, processes them based on their type, and stores the extracted data in Supabase tables for later retrieval. Here‚Äôs how it works:
File Detection: Triggers when a file is created or updated in Google Drive.
File Processing: Loops through each file, extracts metadata, and validates the file type.
Duplicate Check: Ensures the file hasn‚Äôt been processed before.
Content Extraction:
Text-based Files: Downloads the file, extracts text, splits it into chunks, generates embeddings, and stores the chunks in Supabase.
Tabular Files: Extracts data from spreadsheets and stores it as rows in Supabase.
Metadata Storage: Stores file metadata and basic info in Supabase tables.
Error Handling: Logs errors for unsupported formats or duplicates.
Nodes Breakdown üõ†Ô∏è
1. Detect New File üîî
Type: Google Drive Trigger
Purpose: Triggers the workflow when a new file is created in Google Drive.
Configuration:
Credential: Google Drive OAuth2
Event: File Created
Customization:
Specify a folder to monitor specific directories.
2. Detect Updated File üîî
Type: Google Drive Trigger
Purpose: Triggers the workflow when a file is updated in Google Drive.
Configuration:
Credential: Google Drive OAuth2
Event: File Updated
Customization:
Currently disconnected; reconnect if updates need to be processed.
3. Process Each File üîÑ
Type: Loop Over Items
Purpose: Processes each file individually from the Google Drive trigger.
Configuration:
Input: {{ $json.files }}
Customization:
Adjust the batch size if processing multiple files at once.
4. Extract File Metadata üÜî
Type: Set
Purpose: Extracts metadata like file_id, file_name, mime_type, and web_view_link.
Configuration:
Fields:
file_id: {{ $json.id }}
file_name: {{ $json.name }}
mime_type: {{ $json.mimeType }}
web_view_link: {{ $json.webViewLink }}
Customization:
Add more metadata fields if needed (e.g., size, createdTime).
5. Check File Type ‚úÖ
Type: IF
Purpose: Validates the file type by checking the MIME type.
Configuration:
Condition: mime_type contains supported types (e.g., application/pdf, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet).
Customization:
Add more supported MIME types as needed.
6. Find Duplicates üîç
Type: Supabase
Purpose: Checks if the file has already been processed by querying knowledge_base.
Configuration:
Operation: Select
Table: knowledge_base
Filter: file_id = {{ $node['Extract File Metadata'].json.file_id }}
Customization:
Add additional duplicate checks (e.g., by file name).
7. Handle Duplicates üîÑ
Type: IF
Purpose: Routes the workflow based on whether a duplicate is found.
Configuration:
Condition: {{ $node['Find Duplicates'].json.length &gt; 0 }}
Customization:
Add notifications for duplicates if desired.
8. Remove Old Text Data üóëÔ∏è
Type: Supabase
Purpose: Deletes old text data from documents if the file is a duplicate.
Configuration:
Operation: Delete
Table: documents
Filter: metadata-&gt;&gt;'file_id' = {{ $node['Extract File Metadata'].json.file_id }}
Customization:
Add logging before deletion.
9. Remove Old Data üóëÔ∏è
Type: Supabase
Purpose: Deletes old tabular data from document_rows if the file is a duplicate.
Configuration:
Operation: Delete
Table: document_rows
Filter: dataset_id = {{ $node['Extract File Metadata'].json.file_id }}
Customization:
Add logging before deletion.
10. Route by File Type üîÄ
Type: Switch
Purpose: Routes the workflow based on the file‚Äôs MIME type (text-based or tabular).
Configuration:
Rules: Based on mime_type (e.g., application/pdf for text, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet for tabular).
Customization:
Add more routes for additional file types.
11. Download File Content üì•
Type: Google Drive
Purpose: Downloads the file content for text-based files.
Configuration:
Credential: Google Drive OAuth2
File ID: {{ $node['Extract File Metadata'].json.file_id }}
Customization:
Add error handling for download failures.
12. Extract PDF Text üìú
Type: Extract from File (PDF)
Purpose: Extracts text from PDF files.
Configuration:
File Content: {{ $node['Download File Content'].binary.data }}
Customization:
Adjust extraction settings for better accuracy.
13. Extract DOCX Text üìú
Type: Extract from File (DOCX)
Purpose: Extracts text from DOCX files.
Configuration:
File Content: {{ $node['Download File Content'].binary.data }}
Customization:
Add support for other text formats (e.g., TXT, RTF).
14. Extract XLSX Data üìä
Type: Extract from File (XLSX)
Purpose: Extracts tabular data from XLSX files.
Configuration:
File ID: {{ $node['Extract File Metadata'].json.file_id }}
Customization:
Add support for CSV or Google Sheets.
15. Split Text into Chunks ‚úÇÔ∏è
Type: Text Splitter
Purpose: Splits extracted text into manageable chunks for embedding.
Configuration:
Chunk Size: 1000
Chunk Overlap: 200
Customization:
Adjust chunk size and overlap based on document length.
16. Generate Text Embeddings üåê
Type: OpenAI
Purpose: Generates embeddings for text chunks using OpenAI.
Configuration:
Credential: OpenAI API key
Operation: Embedding
Model: text-embedding-ada-002
Customization:
Switch to a different embedding model if needed.
17. Store Text in Supabase üíæ
Type: Supabase Vector Store
Purpose: Stores text chunks and embeddings in the documents table.
Configuration:
Credential: Supabase credentials
Operation: Insert Documents
Table Name: documents
Customization:
Add metadata fields to store additional context.
18. Store Tabular Data üíæ
Type: Supabase
Purpose: Stores tabular data in the document_rows table.
Configuration:
Operation: Insert
Table: document_rows
Columns: dataset_id, row_data
Customization:
Add validation for tabular data structure.
19. Store File Metadata üìã
Type: Supabase
Purpose: Stores file metadata in the document_metadata table.
Configuration:
Operation: Insert
Table: document_metadata
Columns: file_id, file_name, file_type, file_url
Customization:
Add more metadata fields as needed.
20. Record in Knowledge Base üìö
Type: Supabase
Purpose: Stores basic file info in the knowledge_base table.
Configuration:
Operation: Insert
Table: knowledge_base
Columns: file_id, file_name, file_type, file_url, upload_date
Customization:
Add indexes for faster lookups.
21. Log File Errors ‚ö†Ô∏è
Type: Supabase
Purpose: Logs errors for unsupported file types.
Configuration:
Operation: Insert
Table: error_log
Columns: error_type, error_message
Customization:
Add notifications for errors.
22. Log Duplicate Errors ‚ö†Ô∏è
Type: Supabase
Purpose: Logs errors for duplicate files.
Configuration:
Operation: Insert
Table: error_log
Columns: error_type, error_message
Customization:
Add notifications for duplicates.
Interactive Knowledge Base Chat with Supabase RAG using GPT-4o-mini üìöüí¨
Introduction üåü
This n8n workflow creates an interactive chat interface that allows users to query a company knowledge base using Retrieval-Augmented Generation (RAG). It retrieves relevant information from text documents and tabular data stored in Supabase, then generates natural language responses using OpenAI‚Äôs GPT-4o-mini model. Designed for teams managing internal knowledge, this workflow enables users to ask questions like ‚ÄúWhat‚Äôs the remote work policy?‚Äù or ‚ÄúShow me the latest budget data‚Äù and receive accurate, context-aware responses in a conversational format. üöÄ
Problem Statement üéØ
Managing a company knowledge base can be a daunting task‚Äîemployees often struggle to find specific information buried in documents or spreadsheets, leading to wasted time and inefficiencies. Traditional search methods may not understand natural language queries or provide contextually relevant results. This workflow solves these issues by:
Offering a chat-based interface for natural language queries, making it easy for users to ask questions in their own words.
Leveraging RAG to retrieve relevant text and tabular data from Supabase, ensuring responses are accurate and context-aware.
Supporting diverse file types, including text-based files (e.g., PDFs, DOCX) and tabular data (e.g., XLSX, CSV), for comprehensive knowledge access.
Maintaining conversation history to provide context during interactions, improving the user experience.
Target Audience üë•
This workflow is ideal for:
HR Teams: Quickly access company policies, employee handbooks, or benefits documents.
Finance Teams: Retrieve budget data, expense reports, or financial summaries from spreadsheets.
Knowledge Managers: Build a centralized assistant for internal documentation, streamlining information access.
Developers: Extend the workflow with additional tools or integrations for custom use cases.
Workflow Description üîç
This workflow consists of a chat interface powered by n8n‚Äôs Chat Trigger node, an AI Agent node for RAG, and several tools to retrieve data from Supabase. Here‚Äôs how it works step-by-step:
User Initiates a Chat: The user interacts with a chat interface, sending queries like ‚ÄúSummarize our remote work policy‚Äù or ‚ÄúShow budget data for Q1 2025.‚Äù
Query Processing with RAG: The AI Agent processes the query using RAG, retrieving relevant data from Supabase tables and generating a response with OpenAI‚Äôs GPT-4o-mini model.
Data Retrieval and Response Generation: The workflow uses multiple tools to fetch data:
Retrieves text chunks from the documents table using vector search.
Fetches tabular data from the document_rows table based on file IDs.
Extracts full document text or lists available files as needed.
Generates a natural language response combining the retrieved data.
Conversation History Management: Stores the conversation history in Supabase to maintain context for follow-up questions.
Response Delivery: Formats and sends the response back to the chat interface for the user to view.
Nodes Breakdown üõ†Ô∏è
1. Start Chat Interface üí¨
Type: Chat Trigger
Purpose: Provides the interactive chat interface for users to input queries and receive responses.
Configuration:
Chat Title: Company Knowledge Base Assistant
Chat Subtitle: Ask me anything about company documents!
Welcome Message: Hello! I‚Äôm your Company Knowledge Base Assistant. How can I help you today?
Suggestions: What is the company policy on remote work?, Show me the latest budget data., List all policy documents.
Output Chat Session ID: true
Output User Message: true
Customization:
Update the title and welcome message to align with your company branding (e.g., HR Knowledge Assistant).
Add more suggestions relevant to your use case (e.g., What are the company benefits?).
2. Process Query with RAG üß†
Type: AI Agent
Purpose: Orchestrates the RAG process by retrieving relevant data using tools and generating responses with OpenAI‚Äôs GPT-4o-mini.
Configuration:
Credential: OpenAI API key
Model: gpt-4o-mini
System Prompt: You are a helpful assistant for a company knowledge base. Use the provided tools to retrieve relevant information from documents and tabular data. If the query involves tabular data, format it clearly in your response. If no relevant data is found, respond with ""I couldn‚Äôt find any relevant information. Can you provide more details?""
Input Field: {{ $node['Start Chat Interface'].json.message }}
Customization:
Switch to a different model (e.g., gpt-3.5-turbo) to adjust cost or performance.
Modify the system prompt to change the tone (e.g., more formal for HR use cases).
3. Retrieve Text Chunks üìÑ
Type: Supabase Vector Store (Tool)
Purpose: Retrieves relevant text chunks from the documents table using vector search.
Configuration:
Credential: Supabase credentials
Operation Mode: Retrieve Documents (As Tool for AI Agent)
Table Name: documents
Embedding Field: embedding
Content Field: content_text
Metadata Field: metadata
Embedding Model: OpenAI text-embedding-ada-002
Top K: 10
Customization:
Adjust Top K to retrieve more or fewer results (e.g., 5 for faster responses).
Ensure the match_documents function (see prerequisites) is defined in Supabase.
4. Fetch Tabular Data üìä
Type: Supabase (Tool, Execute Query)
Purpose: Retrieves tabular data from the document_rows table based on a file ID.
Configuration:
Credential: Supabase credentials
Operation: Execute Query
Query: SELECT row_data FROM document_rows WHERE dataset_id = $1 LIMIT 10
Tool Description: Run a SQL query - use this to query from the document_rows table once you know the file ID you are querying. dataset_id is the file_id and you are always using the row_data for filtering, which is a jsonb field that has all the keys from the file schema given in the document_metadata table.
Customization:
Modify the query to filter specific columns or add conditions (e.g., WHERE dataset_id = $1 AND row_data-&gt;&gt;'year' = '2025').
Increase the LIMIT for larger datasets.
5. Extract Full Document Text üìú
Type: Supabase (Tool, Execute Query)
Purpose: Fetches the full text of a document by concatenating all text chunks for a given file_id.
Configuration:
Credential: Supabase credentials
Operation: Execute Query
Query: SELECT string_agg(content_text, ' ') as document_text FROM documents WHERE metadata-&gt;&gt;'file_id' = $1 GROUP BY metadata-&gt;&gt;'file_id'
Tool Description: Given file id fetch the text from the documents
Customization:
Add filters to the query if needed (e.g., limit to specific metadata fields).
6. List Available Files üìã
Type: Supabase (Tool, Select)
Purpose: Lists all files in the knowledge base from the document_metadata table.
Configuration:
Credential: Supabase credentials
Operation: Select
Schema: public
Table: document_metadata
Tool Description: Use this tool to fetch all documents including the table schema if the file is csv, excel or xlsx
Customization:
Add filters to list specific file types (e.g., WHERE file_type = 'application/pdf').
Modify the columns selected to include additional metadata (e.g., file_size).
7. Manage Chat History üíæ
Type: Postgres Chat Memory (Tool)
Purpose: Stores and retrieves conversation history to maintain context.
Configuration:
Credential: Supabase credentials (Postgres-compatible)
Table Name: n8n_chat_history
Session ID Field: session_id
Session ID Value: {{ $node['Start Chat Interface'].json.sessionId }}
Message Field: message
Sender Field: sender
Timestamp Field: timestamp
Context Window Length: 5
Customization:
Increase the context window length for longer conversations (e.g., 10 messages).
Add indexes on session_id and timestamp in Supabase for better performance.
8. Format and Send Response üì§
Type: Set
Purpose: Formats the AI Agent‚Äôs response and sends it back to the chat interface.
Configuration:
Fields:
response: {{ $node['Process Query with RAG'].json.output }}
Customization:
Add additional formatting to the response if needed (e.g., prepend with a timestamp or apply markdown formatting).
Setup Instructions üõ†Ô∏è
Prerequisites üìã
n8n Setup:
Ensure you‚Äôre using n8n version 1.0 or higher.
Enable the AI features in n8n settings.
Supabase:
Create a Supabase project and set up the following tables:
documents: id (uuid), content_text (text), embedding (vector(1536)), metadata (jsonb)
document_rows: id (uuid), dataset_id (varchar), row_data (jsonb)
document_metadata: file_id (varchar), file_name (varchar), file_type (varchar), file_url (text)
knowledge_base: id (serial), file_id (varchar), file_name (varchar), file_type (varchar), file_url (text), upload_date (timestamp)
n8n_chat_history: id (serial), session_id (varchar), message (text), sender (varchar), timestamp (timestamp)
Add the match_documents function to Supabase to enable vector search:
CREATE OR REPLACE FUNCTION match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT 5,
  filter jsonb DEFAULT '{}'
) RETURNS TABLE (
  id uuid,
  content_text text,
  metadata jsonb,
  similarity float
) LANGUAGE plpgsql AS $$  
BEGIN
  RETURN QUERY
  SELECT
    documents.id,
    documents.content_text,
    documents.metadata,
    1 - (documents.embedding &lt;=&gt; query_embedding) as similarity
  FROM documents
  WHERE documents.metadata @&gt; filter
  ORDER BY similarity DESC
  LIMIT match_count;
END;
  $$;"
Automatically Classify and Label Gmail Emails with Google Gemini AI,https://n8n.io/workflows/3772-automatically-classify-and-label-gmail-emails-with-google-gemini-ai/,"Description
Quickly organize your inbox with AI!
This simple workflow automatically classifies incoming emails into different categories ‚Äî like High Priority, Work Related, or Promotions ‚Äî and applies Gmail labels accordingly.
Setup takes less than 2 minutes, and it runs 24/7, helping you stay focused on what matters most without manual sorting.
Tools/Services Needed
Gmail: To trigger the workflow and label emails.
Google Gemini (or any LLM Model): To intelligently classify email content.
How It Works
Gmail Trigger: Detects every new incoming email.
Text Classifier Node: Classifies the email content into predefined categories.
Google Gemini Chat Model: Provides the AI-powered understanding behind the classification.
Conditional Labeling:
If the email is High Priority, label it accordingly.
If it‚Äôs Work Related (e.g., internal emails), apply the work label.
If it‚Äôs a Promotion, sort it into the promotions label.
Gmail Labeling: Automatically adds the correct label to the email.
Setup Instructions
Connect your Gmail account to n8n.
Connect your Google Gemini (or other LLM) credentials.
Customize the categories and labels if needed.
Activate the workflow ‚Äî and that's it!
Notes
You can easily add more categories (like ""Finance,"" ""Newsletters,"" etc.) by adjusting the classification prompt.
Works best with a clean and minimal set of categories to avoid overlap.
Can be adapted to work with any other large language model (OpenAI, Claude, etc.) if preferred."
High-Level Service Page SEO Blueprint Report Generator,https://n8n.io/workflows/3583-high-level-service-page-seo-blueprint-report-generator/,"Introduction
The ""High-Level Service Page SEO Blueprint Report"" workflow is a powerful, AI-driven solution designed to generate comprehensive SEO content strategies for service-based businesses.
By analyzing competitor websites and user intent, this workflow creates a detailed blueprint that outlines the optimal structure, content, and conversion elements for a service page. The workflow leverages the JINA Reader API to extract content from competitor websites and uses Google Gemini AI to perform deep analysis across multiple dimensions: competitor content structure, user intent, strategic opportunities, and conversion optimization.
The final output is a professionally formatted Markdown document that provides actionable guidance for creating a high-performing service page that satisfies both user needs and search engine requirements. This workflow eliminates the time-consuming process of manually analyzing competitors and developing content strategies, providing a data-driven foundation for service page creation that would typically require hours of expert analysis.
Who is this for?
This workflow is designed for digital marketers, SEO specialists, content strategists, and web developers who need to create or optimize service pages for businesses. It's particularly valuable for marketing agencies and freelancers who regularly develop content strategies for clients across various industries.
Users should have a basic understanding of SEO concepts, content marketing, and website structure. While technical SEO knowledge is beneficial, the workflow is designed to provide comprehensive guidance even for those with intermediate-level expertise.
The ideal user is someone who wants to streamline their content planning process and ensure their service pages are built on data-driven insights rather than guesswork.
What problem is this workflow solving?
Creating effective service pages that rank well in search engines while converting visitors is a complex challenge that typically requires extensive competitive research, content planning, and conversion optimization expertise. This workflow addresses several key pain points:
Time-consuming competitor analysis: Manually analyzing multiple competitor websites to identify content patterns, heading structures, and meta tag strategies can take hours.
Difficulty identifying content gaps: Determining what topics competitors are missing that could provide a competitive advantage requires deep analysis and industry knowledge.
Balancing SEO and conversion elements: Creating content that satisfies both search engines and user needs while driving conversions is a delicate balance that many struggle to achieve.
Lack of structured approach: Many content creators work without a comprehensive blueprint, leading to inconsistent results and missed opportunities.
Difficulty translating analysis into actionable recommendations: Even when analysis is performed, turning those insights into a concrete content plan can be challenging.
This workflow automates these processes, providing a structured, data-driven approach to service page creation that saves hours of research and planning time.
What this workflow does
Overview
The workflow takes a list of competitor URLs and a target keyword as input, then performs a multi-stage analysis to generate a comprehensive service page blueprint. It extracts and analyzes competitor content, evaluates user intent, identifies strategic opportunities, and creates detailed recommendations for page structure, content, and conversion elements.
The final output is a professionally formatted Markdown document that serves as a complete roadmap for creating an effective service page.
Process
Data Collection: The workflow begins with a form that collects essential information: competitor URLs, target keyword, services offered, brand name, and whether the page is a homepage.
Competitor Content Extraction: The workflow processes each competitor URL, using the JINA Reader API to extract the HTML content from each site.
Content Structure Analysis: For each competitor site, the workflow extracts and analyzes heading structures, meta tags, schema markup, and recurring phrases (n-grams).
Competitor Analysis Report: The AI synthesizes the competitive data to identify patterns in meta titles/descriptions, common outline sections, key heading concepts, and structural elements.
User Intent Analysis: The workflow analyzes the target keyword to determine primary and secondary user intents, user personas, and their position in the buyer's journey.
Gap Analysis: The AI identifies content overlaps (""table stakes""), content gaps (opportunities), SEO keyword priorities, and potential UX/conversion advantages.
Page Outline Generation: Based on the previous analyses, the workflow creates an optimal page structure with H1, H2s, H3s, and potentially H4s, with justifications for each section.
UX & Conversion Recommendations: The workflow adds detailed recommendations for calls-to-action, trust signals, copywriting tone, visual elements, and risk reversal strategies.
Final Blueprint Creation: All analyses and recommendations are compiled into a comprehensive, well-structured Markdown document that serves as a complete service page blueprint.
Setup
Download or import the ""High-Level Service Page SEO Blueprint Report"" workflow JSON file into your n8n instance.
Create a JINA Reader API key by visiting https://jina.ai/api-dashboard/key-manager. You can claim a free API key that allows up to 1 million tokens.
Set up Google Gemini (PaLM) credentials by following the guide at https://docs.n8n.io/integrations/builtin/credentials/googleai/#using-geminipalm-api-key.
Update the ""Edit Fields"" node with:
Your JINA Reader API Key
Adjust the ""Waiting Time"" to 20 seconds if using the free Google Gemini API tier (which limits to 5 requests per minute)
Optionally change the Gemini model if needed
Activate the workflow and start the form trigger.
Complete the form with:
Competitors (up to 5 direct competitor URLs)
Target Keyword (the query related to your service)
Services Offered (details of your complete service offerings)
Brand Name (your company name)
Whether the page is a homepage
After processing, download the generated .txt file, which contains the blueprint in Markdown format.
How to customize this workflow to your needs
Adjust AI parameters: Modify the temperature settings in the Google Gemini Chat Model nodes to control creativity vs. precision in the AI outputs.
Customize extraction logic: Edit the ""Extract HTML Elements"" code node to focus on specific HTML elements that are most relevant to your industry or content type.
Modify analysis prompts: Customize the prompts in the various analysis nodes to focus on specific aspects of SEO or content strategy that are most important for your use case.
Add industry-specific guidance: Enhance the prompts with industry-specific instructions or examples to make the output more relevant to particular sectors.
Integrate with content management systems: Extend the workflow to automatically send the blueprint to content management systems, project management tools, or document storage platforms.
Add competitor scoring: Implement a scoring system to evaluate and rank competitors based on specific criteria relevant to your strategy.
Expand the analysis: Add additional analysis nodes to evaluate other aspects of competitor websites, such as page speed, mobile-friendliness, or backlink profiles."
Telegram AI Bot-to-Human Handoff for Sales Calls,https://n8n.io/workflows/3350-telegram-ai-bot-to-human-handoff-for-sales-calls/,"This n8n template demonstrates an approach to perform bot-to-human handoff using Human-in-the-loop functionality as a switch.
In this experiment, we play with the idea of states we want our agent to be in which controls it's interacton with the user.
First state - the agent is onboarding the user by collecting their details for a sales inquiry. After which, they are handed-off / transferred to a human to continue the call.
Second state - the agent is essentially ""deactivated"" as further messages to the bot will not reach it. Instead, a canned response is given to the user. The human agent must ""reactivate"" the bot by completing the human-in-the-loop form and give a summary of their conversation with the user.
Third state - the agent is ""reactivated"" with context of the human-to-user conversation and is set to provide after sales assistance. An tool is made available to the agent to again delegate back to the human agent when requested.
How it works
This template uses telegram to handle the interaction between the user and the agent.
Each user message is checked for a session state to ensure it is guided to the right stage of the conversation. For this, we can use Redis as a simple key-value store.
When no state is set, the user is directed through an onboarding step to attain their details. Once complete, the agent will ""transfer"" the user to a human agent - technically, all this involves is an update to the session state and a message to another chat forwarding the user's details.
During this ""human"" state, the agent cannot reply to the user and must wait until the human ""transfers"" the conversation back. The human can do this by replying to ""human-in-the-loop"" message with a summary of their conversation with the user. This session state now changes to ""bot"" and the context is implanted in the agent's memory so that the agent can respond to future questions.
At this stage of the conversation, the agent is now expected to handle and help the user with after-sales questions. The user can at anytime request transfer back to the human agent, repeating the previous steps as necessary.
How to use
Plan your user journey! Here is a very basic example of a sales inquiry with at most 3 states. More thought should be developed when many more states are involved.
You may want to better log and manage session states so no user is left in limbo. Try connecting the user and sessions to your CRM.
Note, the Onboarding agent and After-Sales agent have separate chat memories. When adding more agents, it is recommend to continue having separate chat memories to help focus between states.
Requirements
Telegram for chatbot & interface
Redis for session store and chat memory
OpenAI for AI agent
Customising this workflow
Not using Telegram? This template works with Whatsapp and other services with equivalent functionality."
WebSecScan: AI-Powered Website Security Auditor,https://n8n.io/workflows/3314-websecscan-ai-powered-website-security-auditor/,"WebSecScan: AI-Powered Website Security Auditor
This n8n workflow provides comprehensive website security analysis by leveraging OpenAI's models to detect vulnerabilities, configuration issues, and security misconfigurations. The workflow generates a professional HTML security report delivered directly via Gmail.
Key Features
Dual-Layer Security Analysis: Performs parallel security audits using specialized OpenAI agents:
Header Configuration Audit: Analyzes HTTP headers, CORS policies, CSP implementation, and cookie security
Vulnerability Assessment: Identifies XSS vectors, information disclosure, and client-side weaknesses
Detailed Security Grading: Automatically calculates a security grade (A+ to F) based on findings severity and quantity
Professional Report Generation: Creates a comprehensive HTML report with:
Security grade visualization
Color-coded vulnerability categories
Detailed recommendations with example configuration fixes
Header presence/absence indicators
Implementation guidance for remediation
Non-Invasive Testing: Performs analysis without active scanning or exploitation attempts
Technical Implementation
Multi-Agent Architecture: Utilizes two specialized OpenAI agents with custom prompts tailored for security analysis
Advanced Header Analysis: Detects presence and proper implementation of critical security headers:
Content-Security-Policy
Strict-Transport-Security
X-Content-Type-Options
X-Frame-Options
Referrer-Policy
Permissions-Policy
Intelligent Issue Detection: Uses JavaScript processing to analyze OpenAI outputs and count critical/warning issues
Responsive HTML Report: Dynamically generates a mobile-friendly report with detailed findings and recommendations
Setup Requirements
1. OpenAI API Configuration
Create an OpenAI API key at platform.openai.com
In n8n, go to Settings ‚Üí Credentials ‚Üí New ‚Üí OpenAI API
Enter your API key and save
2. Gmail Integration
Navigate to Settings ‚Üí Credentials ‚Üí New ‚Üí Gmail OAuth2 API
Complete the OAuth authentication flow
Configure recipient email in the ""Send Security Report"" node
3. Workflow Customization (Optional)
Modify the form title/description in the Landing Page node
Upgrade from gpt-4o-mini to gpt-4o for more comprehensive analysis
Add additional recipients to the email report
Usage Instructions
Activate the workflow and access the form via the generated URL
Enter any website URL to analyze (including the http:// or https:// prefix)
Receive a detailed security report via email within minutes
Share findings with your development team to implement fixes
This workflow represents a non-invasive security assessment tool. For production environments, complement with professional penetration testing services."
Invoices from Gmail to Drive and Google Sheets,https://n8n.io/workflows/3016-invoices-from-gmail-to-drive-and-google-sheets/,"Attachments Gmail to Drive and Google Sheets
Description
Automatically process invoice emails by saving attachments to Google Drive and extracting key invoice data to Google Sheets using AI. This workflow monitors your Gmail for unread emails with attachments, saves PDFs to a specified Google Drive folder, and uses OpenAI's GPT-4o to extract invoice details (date, description, amount) into a structured spreadsheet.
Use cases
Invoice Management: Automatically organize and track invoices received via email
Financial Record Keeping: Maintain a structured database of all invoice information
Document Organization: Keep digital copies of invoices organized in Google Drive
Automated Data Entry: Eliminate manual data entry for invoice processing
Resources
Gmail account
Google Drive account
Google Sheets account
OpenAI API key
Setup instructions
Prerequisites
Active Gmail, Google Drive, and Google Sheets accounts
OpenAI API key (GPT-4o model access)
n8n instance with credentials manager
Steps
Gmail and Google Drive Setup:
Connect your Gmail account in n8n credentials
Connect your Google Drive account with appropriate permissions
Create a destination folder in Google Drive for invoice storage
Google Sheets Setup:
Connect your Google Sheets account
Create a spreadsheet with columns: Invoice date, Invoice Description, Total price, and Fichero
Copy your spreadsheet ID for configuration
OpenAI Setup:
Add your OpenAI API key to n8n credentials
Configure Email Filter:
Update the email filter node to match your specific sender requirements
Benefits
Time Saving: Eliminates manual downloading, filing, and data entry
Accuracy: AI-powered data extraction reduces human error
Organization: Consistent file naming and storage structure
Searchability: Creates a searchable database of all invoice information
Automation: Runs every minute to process new emails as they arrive
Related templates
Email Parser to CRM
Document Processing Workflow
Financial Data Automation"
ü§ñ Telegram Messaging Agent for Text/Audio/Images,https://n8n.io/workflows/2751-telegram-messaging-agent-for-textaudioimages/,"ü§ñ This n8n workflow creates an intelligent Telegram bot that processes multiple types of messages and provides automated responses using AI capabilities. The bot serves as a personal assistant that can handle text, voice messages, and images through a sophisticated processing pipeline.
Core Components
Message Reception and Validation üì•
üîÑ Implements webhook-based message reception for real-time processing
üîê Features a robust user validation system that verifies sender credentials
üîÄ Supports both testing and production webhook endpoints for development flexibility
Message Processing Pipeline ‚ö°
üîÑ Uses a smart router to detect and categorize incoming message types
üìù Processes three main message formats:
üí¨ Text messages
üé§ Voice recordings
üì∏ Images with captions
AI Integration üß†
ü§ñ Leverages OpenAI's GPT-4 for message classification and processing
üó£Ô∏è Incorporates voice transcription capabilities for audio messages
üëÅÔ∏è Features image analysis using GPT-4 Vision API for processing visual content
Technical Architecture
Webhook Management üîå
üåê Maintains separate endpoints for testing and production environments
üìä Implements automatic webhook status monitoring
‚ö° Provides real-time webhook configuration updates
Error Handling ‚ö†Ô∏è
üîç Features comprehensive error detection and reporting
üîÑ Implements fallback mechanisms for unprocessable messages
üí¨ Provides user feedback for failed operations
Message Classification System üìã
üè∑Ô∏è Categorizes incoming messages into tasks and general conversation
üîÄ Implements separate processing paths for different message types
üß© Maintains context awareness across message processing
Security Features
User Authentication üîí
‚úÖ Validates user credentials against predefined parameters
üë§ Implements first name, last name, and user ID verification
üö´ Restricts access to authorized users only
Response System
Intelligent Responses üí°
ü§ñ Generates contextual responses based on message classification"
Facebook Ads Competitive Analysis using Gemini and Open AI,https://n8n.io/workflows/4716-facebook-ads-competitive-analysis-using-gemini-and-open-ai/,"How it works
User submits a keyword through a form to trigger the workflow.
Ads matching the keyword are scraped from Facebook Ads Library.
Ads are filtered by media type (image or video).
Images are analyzed using an AI model to describe visuals and text.
Videos are downloaded, processed, and sent to Gemini for visual and spoken content analysis.
Text, URLs, and media descriptions are compiled.
All ad insights are saved into a Google Sheet for easy viewing.
Set up steps
Requires Apify and Gemini API credentials, Google Sheets access, and OpenAI key.
Import the workflow into n8n and connect accounts to form, HTTP, and Sheets nodes.
Provide form trigger URL for user input; no advanced config needed.
Estimated setup time: ~15‚Äì20 minutes for a user familiar with n8n.
Sticky notes inside the workflow provide helpful guidance per section."
Automate Service Ticket Triage with GPT-4o & Taiga,https://n8n.io/workflows/4665-automate-service-ticket-triage-with-gpt-4o-and-taiga/,"Usecase: When a new service ticket is created in Taiga, it's often unclear whether it contains sufficient details to begin work. This workflow automates the triage process by:
Using an AI model to extract key information from the ticket description.
Automatically assigning values for:
Type (Bug, Enhancement, Onboarding, Question)
Severity (Wishlist, Minor, Normal, Important, Critical)
Priority (Low, Normal, High)
Status (New, Needs More Info, etc.)
Detecting missing critical data and blocking the ticket if incomplete.
Setup instructions here:
https://github.com/emooney/Service_Ticket_Triage_Helper"
Auto-Generate LinkedIn Posts from Articles with Dumpling AI and GPT-4o,https://n8n.io/workflows/4631-auto-generate-linkedin-posts-from-articles-with-dumpling-ai-and-gpt-4o/,"üë§ Who is this for?
This workflow is ideal for social media managers, personal brand strategists, ghostwriters, and founders who want to post regularly on LinkedIn without spending hours writing from scratch. It‚Äôs also useful for marketing agencies and assistants looking to automate consistent post creation using curated articles as source material.
üß© What problem does this workflow solve?
Manually reading multiple articles, extracting key insights, and writing a clean, professional LinkedIn post is a time-consuming process. This workflow automates everything: from pulling topics, finding related articles, summarizing them using AI, and even generating a matching image to accompany the post. It ensures faster content turnaround, more consistency, and less manual effort.
üîÅ What this workflow does
This workflow starts manually and retrieves one topic marked as ‚ÄúTo do‚Äù from a Google Sheet. That topic is used as a search term for Dumpling AI‚Äôs search endpoint, which scrapes and returns the top three article contents related to the topic. These articles are sent to a LangChain agent powered by GPT-4o, which analyzes and summarizes the content into a LinkedIn post in a friendly, insightful tone. It also generates an image prompt for the post.
After generating the post and image prompt, the data is extracted using a Set node. The prompt is sent to Dumpling AI‚Äôs image generation endpoint, which returns an image URL. Finally, the post text, image prompt, image URL, and status update (‚Äúcreated‚Äù) are saved back to the original row in Google Sheets.
üõ†Ô∏è Workflow Breakdown
Manual Trigger ‚Äì Starts the automation.
Google Sheets (Get Topic) ‚Äì Searches for the first row in your content pipeline sheet where the ‚Äústatus‚Äù is ‚ÄúTo do‚Äù.
HTTP Request (Dumpling AI Search) ‚Äì Uses the topic as a search query to pull 3 article contents using Dumpling AI‚Äôs API.
Set LangChain GPT Model ‚Äì Defines GPT-4o as the LLM for the LangChain Agent.
LangChain Agent (Summarize & Generate) ‚Äì Summarizes all 3 articles and generates a LinkedIn post and a related image prompt.
Set (Extract Data) ‚Äì Extracts postText and imagePrompt from the LangChain agent output.
HTTP Request (Dumpling Image Gen) ‚Äì Sends imagePrompt to Dumpling AI‚Äôs image generation endpoint.
Update Google Sheets ‚Äì Writes the post, image prompt, and image URL back to the sheet and changes the row status to ‚Äúcreated‚Äù.
‚öôÔ∏è Setup Instructions
Dumpling AI
Sign up at Dumpling AI
Get your API key and connect it in the HTTP Request nodes (Search and Image endpoints)
Use the /search endpoint to retrieve article content
Use the /generate-image endpoint to create the image
Google Sheets
Create a spreadsheet with columns: topic, status, postText, imagePrompt, imageURL
Add sample topics and set their status to To do
LangChain (GPT-4o)
Connect your OpenAI credentials to n8n
Make sure GPT-4o is available in your OpenAI account
Use the LangChain node to process multi-input summarization and generate a social media caption
Customize the Prompt (Optional)
Adjust the Set node to tweak the input format sent to the LangChain agent
Add constraints like tone, hashtags, or emojis to fit your brand style
üß† How to Customize This Workflow
Change the content source (RSS feed, Notion DB, etc.) instead of Google Sheets
Add a scheduler node to run this automatically every morning or weekly
Use Airtable instead of Google Sheets for more control and filtering
Send the final post to LinkedIn using the Buffer or LinkedIn API
Add a Telegram or Slack notification when new content is ready for approval"
Wikipedia Podcast Generator - AI-Powered Voice Content Creator via Telegram,https://n8n.io/workflows/4496-wikipedia-podcast-generator-ai-powered-voice-content-creator-via-telegram/,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.
Welcome to my Wikipedia Podcast Telegram Bot Workflow!
This workflow creates an intelligent Telegram bot that transforms Wikipedia articles into engaging 5-minute podcast episodes using natural language queries and voice messages.
What this workflow does
This workflow processes incoming Telegram messages (text or voice, e.g. ""Berlin"") and generates professional podcast content about any Wikipedia topic (e.g. ""Berlin"", ""Shakespeare"", etc.). The AI agent researches the requested subject, creates a structured podcast script, and delivers it as high-quality audio directly through Telegram.
Key Features:
Voice message support (speech-to-text and text-to-speech)
Wikipedia research integration for accurate content
Professional podcast structure (intro, main content, outro)
Natural-sounding AI voice synthesis
Conversational and educational tone optimized for audio consumption
This workflow has the following sequence:
Telegram Trigger - Receives incoming messages (text or voice) from users via Telegram bot
Text or Voice Switch - Routes the message based on input type (text message vs. voice message)
Voice Message Processing (if voice input):
Retrieval of voice file from Telegram
Transcription of voice message to text using OpenAI Whisper
Text Message Preparation (if text input) - Prepares the text message for the AI agent
Wikipedia Podcast Agent - Core AI agent that:
Researches the requested topic using Wikipedia tool
Creates a professional 5-minute podcast script (600-750 words)
Follows structured format: intro, main content, outro
Uses conversational, accessible, and enthusiastic tone
ElevenLabs Text to Speech - Converts the podcast script into natural-sounding audio using AI voice synthesis
Send Voice Response - Delivers the generated podcast audio back to the user via Telegram
Requirements:
Telegram Bot API: Documentation
Create a bot via @BotFather on Telegram
Get bot token and configure webhook
Anthropic API (Claude 4 Sonnet): Documentation
Used for AI agent processing and podcast script generation
Provides Wikipedia research capabilities
OpenAI API: Documentation
Used for speech transcription (Whisper model)
ElevenLabs API: Documentation
Used for high-quality text-to-speech generation
Provides natural-sounding voice synthesis
Important: The workflow uses the Wikipedia tool integrated with Claude 4 Sonnet to ensure accurate and comprehensive research. The AI agent is specifically prompted to create engaging, educational podcast content suitable for audio consumption.
Configuration Notes:
Update the Telegram chat ID in the trigger for your specific bot
Modify the voice selection in ElevenLabs for different narrator styles
The system prompt can be customized for different podcast formats or target audiences
Supports both individual users and can be extended for group chats
Feel free to contact me via LinkedIn, if you have any questions!"
Auto-Generate Blog & AI Image from YouTube Videos with Dumpling AI & GPT-4o,https://n8n.io/workflows/4327-auto-generate-blog-and-ai-image-from-youtube-videos-with-dumpling-ai-and-gpt-4o/,"Who is this for?
This template is designed for content creators, marketing teams, educators, or media managers who want to repurpose video content into written blog posts with visuals. It's ideal for anyone looking to automate the process of transforming YouTube videos into professional blog articles and custom images.
What problem is this workflow solving?
Creating written content from video material is time-consuming and manual. This workflow solves that by automating the entire pipeline: from detecting new YouTube video uploads to transcribing the audio, turning it into an engaging blog post, generating a matching visual, and saving both in Airtable. It saves hours of work while keeping your blog or social feed active and consistent.
What this workflow does
This automation listens for new YouTube videos added to a Google Drive folder, extracts the full transcript using Dumpling AI, and sends it to GPT-4o to generate a blog post and image prompt. Dumpling AI then turns the prompt into a 16:9 visual. The blog and visual are saved into Airtable for easy publishing or curation.
Setup
Google Drive Trigger
Create a folder in Google Drive and upload your YouTube videos there.
Link this folder in the ""Watch Folder for New YouTube Videos"" node.
Enable polling every minute or adjust as needed.
Download & Prepare the Video
The video is downloaded and converted into base64 format by the next two nodes:
Download Video File and Convert Downloaded Video to Base64.
Transcription with Dumpling AI
The base64 video is sent to Dumpling AI‚Äôs extract-video endpoint.
You must have a Dumpling AI account and an API key with access to this endpoint: Dumpling AI Docs
Generate Blog Content with GPT-4o
GPT-4o takes the transcript and generates:
A human-like blog post
A descriptive prompt for AI image generation
Make sure your OpenAI credentials are configured.
Generate the Visual
The prompt is passed to Dumpling AI‚Äôs generate-ai-image endpoint using model FLUX.1-pro.
The result is a clean 1024x576 image.
Save to Airtable
Blog content is stored under the Content field in Airtable.
The image prompt is also added to the Attachments column as a visual reference.
Ensure Airtable base and table are preconfigured with the correct field names.
How to customize this workflow to your needs
Change the GPT prompt to alter the tone or format of the blog post (e.g., add bullet points or SEO tags).
Modify the Dumpling AI prompt to generate different image styles.
Add a scheduler or webhook trigger to run at different intervals or through other integrations.
Connect this output to Ghost, Notion, or your CMS using additional nodes.
üß† Sticky Note Summary
Part 1: Transcription & Blog Prompt
Watches a Google Drive folder for new video uploads.
Downloads and encodes the video.
Transcribes full audio with Dumpling AI.
GPT-4o writes a blog post and descriptive image prompt.
Part 2: Image Generation & Airtable Save
Dumpling AI generates a visual from the image prompt.
Blog content is saved to Airtable.
The image prompt is patched into the Attachments field in the same record.
‚úÖ Use this if you want to automate repurposing YouTube videos into blog content with zero manual work."
"AI Personal Assistant with GPT-4o, RAG & Voice for WhatsApp using Supabase",https://n8n.io/workflows/3947-ai-personal-assistant-with-gpt-4o-rag-and-voice-for-whatsapp-using-supabase/,"üß† Intelligent AI Assistant with RAG & Voice for WhatsApp ‚Äì Built with GPT-4o & Supabase
üìå About this workflow and its creator
Hi! I‚Äôm Amanda, a creator of intelligent automations using n8n and Make. I‚Äôve been building AI-powered workflows for over 2 years, always focused on usability and innovation. This one here is very special to me ‚Äì a truly advanced AI assistant that reads, listens, interprets and responds like a real human ü§ñ‚ú®
This ready-to-use workflow acts as a powerful AI personal assistant capable of understanding messages via voice, text, documents, or even images. It supports full multi-channel operation (WhatsApp via Evolution API, Instagram, Facebook, and more), and includes advanced RAG capabilities using Supabase + GPT-4o. It‚Äôs designed to be highly extensible, with memory, prompt update tools, and knowledge base management.
‚öôÔ∏è What this workflow does
üí¨ Understands user input via text, document, audio or image (voice, OCR, PDF)
üé§ Transcribes and interprets voice messages using OpenAI Whisper
üß† Understands prompts and user commands using GPT-4o via LangChain agent
üóÇÔ∏è Searches knowledge base using RAG + Supabase vector DB
üìÑ Accepts documents and automatically indexes them for future questions
üßæ Summarizes documents and stores metadata in Supabase
üóÉÔ∏è Offers memory support (PostgreSQL chat memory per user session)
üìß Sends replies through WhatsApp (Evolution API), Instagram, Facebook, etc.
üìÖ Manages schedules (via tool integration with Google Calendar)
üì¨ Sends and searches emails (with support tools)
üõ† Modular and expandable structure (tools for saving knowledge, deleting, updating prompt)
üîß Setup Instructions
n8n Hosting
This workflow requires n8n self-hosted (or n8n Cloud with custom credentials + community nodes enabled).
Create required databases
Use the provided SQL queries inside the setar_supabase_tabelas_vectoriais, criar_cerebro, and criar_rag_controle nodes to initialize:
documents table for RAG
cerebro table for prompt
memoria_chat for session memory
rag_controle for summaries and indexing
Credentials needed
OpenAI API (for chat, embeddings and Whisper transcription)
Redis (for managing message buffer)
Supabase (for vector store + metadata)
Postgres (for memory and prompts)
Evolution API (or other messaging platforms)
Webhook
Set the webhook path to receive messages from your Evolution or WhatsApp API provider.
Configure ‚ÄòSet‚Äô node
In the config node, adjust:
adminNumero: your personal WhatsApp or admin number
evolutionApiKey: your private API key
utilizacaoApenasViaAdmin: toggle if this should only respond to admin numbers
Tool connections
Ensure the supporting workflows are also imported and connected for:
Emails
Knowledge management
Calendar events
üìé Notes
This workflow uses LangChain agents, OpenAI GPT-4o, Supabase, Redis, and PostgreSQL.
It includes multiple ‚Äústicky notes‚Äù inside the workflow with explanations.
Ideal for businesses, consultants, and developers looking to offer an intelligent and extendable AI chatbot experience.
üõç Want to use this on your system?
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Use n8n Cloud with my partner link: https://n8n.partnerlinks.io/amanda"
Comprehensive SEO Keyword Research with OpenAI & DataForSEO Analytics to NocoDB,https://n8n.io/workflows/3908-comprehensive-seo-keyword-research-with-openai-and-dataforseo-analytics-to-nocodb/,"AI-Powered SEO Keyword Research Workflow with n8n
automates comprehensive keyword research for content creation
Table of Contents
Introduction
Workflow Architecture
NocoDB Integration
Data Flow
Core Components
Setup Requirements
Possible Improvements
Introduction
This n8n workflow automates SEO keyword research using AI and data-driven analytics. It combines OpenAI's language models with DataForSEO's analytics to generate comprehensive keyword strategies for content creation. The workflow is triggered by a webhook from NocoDB, processes the input data through multiple stages, and returns a detailed content brief with optimized keywords.
Workflow Architecture
The workflow follows a structured process:
Input Collection: Receives data via webhook from NocoDB
Topic Expansion: Generates keywords using AI
Keyword Metrics Analysis: Gathers search volume, CPC, and difficulty metrics
Competitor Analysis: Analyzes competitor content for ranking keywords
Final Strategy Creation: Combines all data to generate a comprehensive keyword strategy
Output Storage: Saves results back to NocoDB and sends notifications
NocoDB Integration
Database Structure
The workflow integrates with two tables in NocoDB:
Input Table Schema
This table collects the input parameters for the keyword research:
Field Name Type Description
ID Auto Number Unique identifier
Primary Topic Text The main keyword/topic to research
Competitor URLs Text Comma-separated list of competitor websites
Target Audience Single Select Description of the target audience (Solopreneurs, Marketing Managers, etc.)
Content Type Single Select Type of content (Blog, Product page, etc.)
Location Single Select Target geographic location
Language Single Select Target language for keywords
Status Single Select Workflow status (Pending, Started, Done)
Start Research Checkbox Active Workflow when you set this to true
Output Table Schema
This table stores the generated keyword strategy:
Field Name Type Description
ID Auto Number Unique identifier
primary_topic_used Text The topic that was researched
report_content Long Text The complete keyword strategy in Markdown format
generatedAt Datetime Automatically generated by NocoDb
Webhook Settings
NocoDB Webhook Settings
Data Flow
The workflow handles data in the following sequence:
Webhook Trigger: Receives input from NocoDB when a new keyword research request is created
Field Extraction: Extracts primary topic, competitor URLs, audience, and other parameters
AI Topic Expansion: Uses OpenAI to generate related keywords, categorized by type and intent
Keyword Analysis: Sends primary keywords to DataForSEO to get search volume, CPC, and difficulty
Competitor Research: Analyzes competitor pages to identify their keyword rankings
Strategy Generation: Combines all data to create a comprehensive keyword strategy
Storage & Notification: Saves the strategy to NocoDB and sends a notification to Slack
Core Components
1. Topic Expansion
This component uses OpenAI and a structured output parser to generate:
20 primary keywords
30 long-tail keywords with search intent
15 question-based keywords
10 related topics
2. DataForSEO Integration
Two API endpoints are used:
Search Volume & CPC: Gets monthly search volume and cost-per-click data
Keyword Difficulty: Evaluates how difficult it would be to rank for each keyword
3. Competitor Analysis
This component:
Analyzes competitor URLs to identify which keywords they rank for
Identifies content gaps or opportunities
Determines the search intent their content targets
4. Final Keyword Strategy
The AI-generated strategy includes:
Top 10 primary keywords with metrics
15 long-tail opportunities with low competition
5 question-based keywords to address in content
Content structure recommendations
3 potential content titles optimized for SEO
Setup Requirements
To use this workflow, you'll need:
n8n Instance: Either cloud or self-hosted
NocoDB Account: For data input and storage
API Keys:
OpenAI API key
DataForSEO API credentials
Slack API token (for notifications)
Database Setup: Create the required tables in NocoDB as described above
Possible Improvements
The workflow could be enhanced with the following improvements:
Enhanced Keyword Strategy
Add topic clustering to group related keywords
Enhance the final output with more specific content structure suggestions
Include word count recommendations for each content section
Additional Data Sources
Integrate Google Search Console data for existing content optimization
Add Google Trends data to identify rising topics
Include sentiment analysis for different keyword groups
Improved Competitor Analysis
Analyze content length and structure from top-ranking pages
Identify common backlink sources for competitor content
Extract content headings to better understand content organization
Automation Enhancements
Add scheduling capabilities to run updates on existing content
Implement content performance tracking over time
Create alert thresholds for changes in keyword difficulty or search volume
Example Output
Here is an example Output the Workflow generated based on the following inputs.
Inputs:
Primary Topic: AI Automation
Competitor URLs: n8n.io, zapier.com, make.com
Target Audience: Small Business Owners
Content Type: Landing Page
Location: United States
Language: English
Output: Final Keyword Strategy
The workflow provides a powerful automation for content marketers and SEO specialists to develop data-driven keyword strategies with minimal manual effort.
Original Workflow: AI-Powered SEO Keyword Research Automation - The vibe Marketer"
"SEO Blog Generator with GPT-4o, Perplexity, and Telegram Integration",https://n8n.io/workflows/3672-seo-blog-generator-with-gpt-4o-perplexity-and-telegram-integration/,"SEO Blog Generator with GPT-4o, Perplexity, and Telegram Integration
This workflow helps you automatically generate SEO-optimized blog posts using Perplexity.ai, OpenAI GPT-4o, and optionally Telegram for interaction.
üöÄ Features
üß† Topic research via Perplexity sub-workflow
‚úçÔ∏è AI-written blog post generated with GPT-4o
üìä Structured output with metadata: title, slug, meta description
üì© Integration with Telegram to trigger workflows or receive outputs (optional)
‚öôÔ∏è Requirements
‚úÖ OpenAI API Key (GPT-4o or GPT-3.5)
‚úÖ Perplexity API Key (with access to /chat/completions)
‚úÖ (Optional) Telegram Bot Token and webhook setup
üõ† Setup Instructions
Credentials:
Add your OpenAI credentials (openAiApi)
Add your Perplexity credentials under httpHeaderAuth
Optional: Setup Telegram credentials under telegramApi
Inputs:
Use the Form Trigger or Telegram input node to send a Research Query
Subworkflow:
Make sure to import and activate the subworkflow Perplexity_Searcher to fetch recent search results
Customization:
Edit prompt texts inside the Blog Content Generator and Metadata Generator to change writing style or target industry
Add or remove output nodes like Google Sheets, Notion, etc.
üì¶ Output Format
The final blog post includes:
‚úÖ Blog content (1500-2000 words)
‚úÖ Metadata: title, slug, and meta description
‚úÖ Extracted summary in JSON
‚úÖ Delivered to Telegram (if connected)
Need help? Reach out on the n8n community forum"
üé® AI Design Team - Generate and Review AI Images with Ideogram and OpenAI,https://n8n.io/workflows/3460-ai-design-team-generate-and-review-ai-images-with-ideogram-and-openai/,"üé® AI Graphic Design Team - Generate and Review AI Images with Ideogram and OpenAI
Description
Who is this for?
This workflow is perfect for graphic designers, creative agencies, marketing teams, or freelancers who regularly use AI-generated images in their projects. It's specifically beneficial for teams that want to automate the generation, review, and management of AI-created graphics efficiently.
What problem does this workflow solve?
Design teams often face time-consuming manual reviews and inconsistent quality checks for AI-generated images. This workflow addresses these challenges by automating image generation and introducing a systematic, AI-driven vetting process. This ensures only high-quality, relevant images reach your team's assets, saving valuable time and enhancing workflow efficiency.
What this workflow does
AI Image Generation: Integrates Ideogram via HTTP Request to automatically create AI-generated images based on creative briefs.
Automated Image Review: Uses OpenAI to automatically evaluate and approve images, ensuring they meet your predefined quality standards.
Efficient Asset Management: Automatically creates structured Google Drive folders and compiles key metadata (including creation dates, prompts, and image links) into a CSV file and Google Sheet.
Immediate Email Notifications: Delivers a setup confirmation and provides easy access to Google Drive folders and assets via automated email notifications.
Final Approved Images: Outputs vetted, ready-to-use images for your creative projects, removing the burden of manual reviews.
Setup
Initial Email Configuration
Update your email details in both the ""Setup Gmail"" node and the ""Gmail"" notification node.
Run the initial setup workflow to automatically create the Google Drive folders ""Graphic_Design_Team"" and ""Image_Generations,"" and upload your CSV file (n8n-Graphic_Design_Team.csv).
Review Email & Set Up Google Sheets
Check your inbox for an automated email containing folder IDs and direct links.
Create and set up a Google Sheet by importing the provided CSV data from your email.
Update Workflow Nodes
Select your newly created Google Sheet in both Google Sheets nodes.
Update your Creative Brief node with the Google Drive folder IDs provided in the email.
Run Workflow for AI Image Generation & Review
Execute the workflow. Your generated images will be automatically vetted, organized, and ready for creative use.
How to Customize This Workflow
Tailor Image Generation Prompts: Adjust prompts and settings in the Ideogram HTTP Request node to better fit your project's creative requirements.
Set Quality Standards: Modify the criteria used by the OpenAI node to reflect your specific standards and preferences for image approval.
Customize Asset Organization: Adapt Google Drive folder structures, CSV headers, or Google Sheets integrations to match your team's organizational preferences.
Dependencies & Requirements
Nodes Used:
HTTP Request (Ideogram API integration)
OpenAI (Image review and quality assessment)
Gmail (Automated notifications)
Google Drive (File and asset management)
Google Sheets (Metadata organization)
Credentials:
Ensure Gmail, Google Drive, Google Sheets, and OpenAI credentials are properly configured in your n8n account. No custom or community nodes are needed.
Final Outcome
Upon completion, your workflow efficiently provides vetted, high-quality AI-generated images, organized in Google Drive and accessible via easy-to-use metadata in Google Sheets, drastically reducing manual intervention and accelerating your creative processes."
"Automate Blog Content Creation with Notion MCP, DeepSeek AI, and WordPress",https://n8n.io/workflows/3348-automate-blog-content-creation-with-notion-mcp-deepseek-ai-and-wordpress/,"Who Is This For
This workflow is ideal for content creators, bloggers, marketers, and professionals seeking to automate the creation and publication of SEO-optimized articles. It's particularly beneficial for those utilizing Notion for content management and WordPress for publishing.
What Problem Does This Workflow Solve
Manually creating SEO-friendly articles is time-consuming and requires consistent effort. This workflow streamlines the entire process‚Äîfrom detecting updates in Notion to publishing on WordPress‚Äîby leveraging AI for content generation, thereby reducing the time and effort involved.
What This Workflow Does
Monitor Notion Updates: Detects changes in a specified Notion database.
AI Content Generation: Utilizes an AI model to produce an SEO-optimized article based on Notion data.
Publish to WordPress: Automatically posts the generated article to a WordPress site.
Email Notification: Sends an email containing the article's title and URL.
Update Notion Database: Updates the corresponding entry in the Notion database with the article details.
Setup Guide
Prerequisites
WordPress account with API access.
API key for the AI model used.
Notion integration with the relevant database ID.
Credentials for the email service used (e.g., Gmail).
Community Node Requirement: This workflow utilizes the n8n-nodes-mcp community node, which is only compatible with self-hosted instances of n8n. For more information on installing and managing community nodes, refer to the n8n documentation.
n8n Docs
Steps
Import the workflow into your self-hosted n8n instance.
Install the required community node (n8n-nodes-mcp).
Configure API credentials for WordPress, the AI service, Notion, and the email service.
Define necessary variables, such as the notification email address and Notion database IDs.
Activate the workflow to automate the process.
How to Customize This Workflow
AI Prompt: Adjust the prompt used for content generation to align with your preferred tone and style.
Article Structure: Modify the structure of the generated article by tweaking settings in the content generation node.
Notifications: Customize the content and recipients of the emails sent post-publication.
Notion Updates: Tailor the fields updated in Notion to suit your specific requirements."
"Upload to Instagram, Tiktok & Youtube from Google Drive",https://n8n.io/workflows/2894-upload-to-instagram-tiktok-and-youtube-from-google-drive/,"Description
This automation template is designed for content creators, digital marketers, and social media managers looking to simplify their video posting workflow. It automates the process of generating engaging video descriptions and uploading content to both Instagram and TikTok, making your social media management more efficient and error-free.
Who Is This For?
Content Creators & Influencers: Streamline your video uploads and focus more on creating content.
Digital Marketers: Ensure consistent posting across multiple platforms with minimal manual intervention.
Social Media Managers: Automate repetitive tasks and maintain a steady online presence.
What Problem Does This Workflow Solve?
Manually creating descriptions and uploading videos to different platforms can be time-consuming and error-prone. This workflow addresses these challenges by:
Automating Video Uploads: Monitors a designated Google Drive folder for new videos.
Generating Descriptions: Uses OpenAI to transcribe video audio and generate engaging, customized social media descriptions.
Ensuring Multi-Platform Consistency: Simultaneously posts your video with the generated description to Instagram and TikTok.
Error Notifications: Optional Telegram integration sends alerts in case of issues, ensuring smooth operations.
How It Works
Video Upload: Place your video in the designated Google Drive folder.
Description Generation: The automation triggers OpenAI to transcribe your video‚Äôs audio and generate a captivating description.
Content Distribution: Automatically uploads the video and description to both Instagram and TikTok.
Error Handling: Sends Telegram notifications if any issues arise during the process.
Setup
Generate an API token at upload-post.com and configure it in both the Upload to TikTok and Upload to Instagram nodes.
Google Cloud Project: Create a project in Google Cloud Platform, enable the Google Drive API, and generate the necessary OAuth credentials to connect to your Google Drive account.
Set up your Google Drive folder in the Google Drive Trigger node.
Customize the OpenAI prompt in the Generate Social Description node to match your brand‚Äôs tone.
(Optional) Configure Telegram credentials for error notifications.
Requirements
Accounts: upload-post.com, Google Drive, and (optionally) Telegram.
API Keys & Credentials: Upload-post.com API token, OpenAI API key, and (optional) Telegram bot token.
Google Cloud: A project with the Google Drive API enabled and valid OAuth credentials.
Use this template to enhance your productivity, maintain consistency across your social media channels, and engage your audience with high-quality video content."
Automate Sleep Meditation Content Creation with ElevenLabs V3 & DeepSeek AI,https://n8n.io/workflows/4762-automate-sleep-meditation-content-creation-with-elevenlabs-v3-and-deepseek-ai/,"Guided Meditation Voiceover Creation Workflow
Automate the production of ad-friendly meditation content‚Äîfrom trend research to AI voiceover generation.
üåü Key Features
Style-Specific Content Generation

    Input a meditation style (e.g., ""calming,"" ""motivational"") and generate tailored scripts.

    Customize voice profiles using ElevenLabs.

Trend-Based Title Research

    Scrape YouTube/Brave Search for popular video titles in your niche.

    Remove duplicates, filter explicit content, and aggregate results.

AI-Powered Content Creation

    Generate fresh video titles using aggregated trends.

    Create 2500-character voiceover scripts with pacing cues (e.g., [sighs], ... pauses).

Structured Output Validation

    Auto-fix malformed AI outputs with recursive parsing.

    Enforce script constraints (no line breaks/comments).

Audio Production & Storage

    Convert scripts to lifelike speech via ElevenLabs.

    Auto-save audio to Google Drive with timestamps.
üõ†Ô∏è Nodes Used
Data Prep: Brave Search ‚Üí SplitOut ‚Üí Remove Duplicates ‚Üí Text Classifier (explicit filter)

AI Models: DeepSeek (title/script generation), Output Parsers (schema validation)

TTS: ElevenLabs (voice synthesis) ‚Üí Google Drive (storage)

Utilities: Webhook trigger, Code node (input validation), Sticky Notes (instructions)
üí° Use Cases
Create ASMR/sleep meditation content at scale.

Repurpose trending topics into new videos.

Generate voiceovers for faceless YouTube channels.
üîó Setup Notes
Required APIs:

    ElevenLabs (voice synthesis)

    Brave Search (video trends)

    Google Drive (storage)

Input: Send style (e.g., ""sleep meditation"") and voice_id to the webhook.

Output: MP3 audio + metadata saved to Google Drive.
üìå Pro Tips
Use output parsers to enforce JSON schemas and avoid malformed AI responses.

Attach royalty-free visuals using the included background video.
Extend with video composition orthumbnail generation workflows.
‚úÖ Ad-Friendly: Filters explicit titles and enforces YouTube-safe content guidelines."
Send Daily Weather Forecasts from OpenWeatherMap to Telegram with Smart Formatting,https://n8n.io/workflows/4608-send-daily-weather-forecasts-from-openweathermap-to-telegram-with-smart-formatting/,"üå§Ô∏è Daily Weather Forecast Bot
A comprehensive n8n workflow that fetches detailed weather forecasts from OpenWeatherMap and sends beautifully formatted daily summaries to Telegram.
üìã Features
üìä Daily Overview: Complete temperature range, rainfall totals, and wind conditions
‚è∞ Hourly Forecast: Weather predictions at key times (9AM, 12PM, 3PM, 6PM, 9PM)
üå°Ô∏è Smart Emojis: Context-aware weather icons and temperature indicators
üí° Smart Recommendations: Contextual advice (umbrella alerts, clothing suggestions, sun protection)
üå™Ô∏è Enhanced Details: Feels-like temperature, humidity levels, wind speed, UV warnings
üì± Rich Formatting: HTML-formatted messages with emojis for excellent readability
üïê Timezone-Aware: Proper handling of Luxembourg timezone (CET/CEST)
üõ†Ô∏è What This Workflow Does
Triggers daily at 7:50 AM to send morning weather updates
Fetches 5-day forecast from OpenWeatherMap API with 3-hour intervals
Processes and analyzes weather data with smart algorithms
Formats comprehensive report with HTML styling and emojis
Sends to Telegram with professional formatting and actionable insights
‚öôÔ∏è Setup Instructions
1. OpenWeatherMap API
Sign up at OpenWeatherMap
Get your free API key (1000 calls/day included)
Replace API_KEY in the HTTP Request node URL
2. Telegram Bot
Message @BotFather on Telegram
Send /newbot command and follow instructions
Copy the bot token to n8n credentials
Get your chat ID by messaging the bot, then visiting:
https://api.telegram.org/bot&lt;YOUR_BOT_TOKEN&gt;/getUpdates
Update the chatId parameter in the Telegram node
3. Location Configuration
Default location: Strassen, Luxembourg
To change: modify q=Strassen in the HTTP Request URL
Format: q=CityName,CountryCode (e.g., q=Paris,FR)
üéØ Technical Details
API Source: OpenWeatherMap 5-day forecast
Schedule: Daily at 7:50 AM (configurable)
Format: HTML with rich emoji formatting
Error Handling: 3 retry attempts with 5-second delays
Rate Limits: Uses only 1 API call per day
Timezone: Europe/Luxembourg (handles CET/CEST automatically)
üìä Weather Data Analyzed
Temperature ranges and ""feels like"" temperatures
Precipitation forecasts and accumulation
Wind speed and conditions
Humidity levels and comfort indicators
Cloud coverage and visibility
UV index recommendations
Time-specific weather patterns
üí° Smart Features
Conditional Recommendations: Only shows relevant advice
Night/Day Awareness: Different emojis for time of day
Temperature Context: Color-coded temperature indicators
Weather Severity: Appropriate icons for weather intensity
Humidity Comfort: Comfort level indicators
Wind Analysis: Descriptive wind condition text
üîß Customization Options
Schedule: Modify trigger time in the Schedule node
Location: Change city in HTTP Request URL
Forecast Hours: Adjust desiredHours array in the code
Temperature Thresholds: Modify emoji temperature ranges
Recommendation Logic: Customize advice triggers
üì± Sample Output
üå§Ô∏è Weather Forecast for Strassen, LU
üìÖ Monday, 2 June 2025
üìä Daily Overview
üå°Ô∏è Range: 12¬∞C - 22¬∞C
üíß Comfortable (65%)
‚è∞ Hourly Forecast
üïí 09:00 ‚òÄÔ∏è 15¬∞C
üïí 12:00 üå§Ô∏è 20¬∞C
üïí 15:00 ‚òÄÔ∏è 22¬∞C (feels 24¬∞C)
üïí 18:00 ‚õÖ 19¬∞C
üïí 21:00 üåô 16¬∞C
üì° Data from OpenWeatherMap | Updated: 07:50 CET
üöÄ Getting Started
Import this workflow to your n8n instance
Add your OpenWeatherMap API key
Set up Telegram bot credentials
Test manually first
Activate for daily automated runs
üìã Requirements
n8n instance (cloud or self-hosted)
Free OpenWeatherMap API account
Telegram bot token
Basic understanding of n8n workflows
Perfect for: Daily weather updates, team notifications, personal weather tracking, smart home automation triggers."
Resume Data Extraction and Storage in Supabase from Email Attachments,https://n8n.io/workflows/4106-resume-data-extraction-and-storage-in-supabase-from-email-attachments/,"Description
What Problem Does This Solve? üõ†Ô∏è
This workflow automates the process of extracting key information from resumes received as email attachments and storing that data in a structured format within a Supabase database. It eliminates the manual effort of reviewing each resume, identifying relevant details, and entering them into a database. This streamlines the hiring process, making it faster and more efficient for recruiters and HR professionals.
Target audience: Recruiters, HR departments, and talent acquisition teams.
What Does It Do? üåü
Monitors a designated email inbox for new messages with resume attachments.
Extracts key information such as name, contact details, education, work experience, and skills from the attached resumes.
Cleans and formats the extracted data.
Stores the processed data securely in a Supabase database.
Key Features üìã
Automatic email monitoring for resume attachments.
Intelligent data extraction from various resume formats (e.g., PDF, DOC, DOCX).
Customizable data fields to capture specific information.
Seamless integration with Supabase for data storage.
Uses OpenRouter to streamline API key management for services such as AI-powered parsing.
Setup Instructions
Prerequisites ‚öôÔ∏è
n8n Instance: Self-hosted or cloud instance of n8n.
Email Account: Gmail account with Gmail API access for receiving resumes.
Supabase Account: A Supabase project with a database/table ready to store extracted resume data. You'll need the Supabase URL and API key.
OpenRouter Account: For managing AI model API keys centrally when using LLM-based resume parsing.
Installation Steps üì¶
1. Import the Workflow:
Copy the exported workflow JSON.
Import it into your n8n instance via ‚ÄúImport from File‚Äù or ‚ÄúImport from URL‚Äù.
2. Configure Credentials:
In n8n > Credentials, add credentials for:
Email account (Gmail API): Provide Client ID and Client Secret from the Google Cloud Platform.
Supabase: Provide the Supabase URL and the anon public API key.
OpenRouter (Optional): Add your OpenRouter API key for use with any AI-powered resume parsing nodes.
Assign these credentials to their respective nodes:
Gmail Trigger ‚Üí Email credentials.
Supabase Insert ‚Üí Supabase credentials.
AI Parsing Node ‚Üí OpenRouter credentials.
3. Set Up Supabase Table:
Create a table in Supabase with columns such as:
name, email, phone, education, experience, skills, received_date, etc.
Make sure the field names align with the structure used in your workflow.
4. Customize Nodes:
Parsing Node(s): Modify the workflow to use an OpenAI model directly for field extraction, replacing the Basic LLM Chain node that utilizes OpenRouter.
5. Test the Workflow:
Send a test email with a resume attachment.
Check n8n's execution log to confirm the workflow triggered, parsed the data, and inserted it into Supabase.
Verify data integrity in your Supabase table.
How It Works
High-Level Workflow üîç
Email Monitoring: Triggered when a new email with an attachment is received (via Gmail API).
Attachment Check: Verifies the email contains at least one attachment.
Prepare Data: Extracts the attachment and prepares it for analysis.
Data Extraction: Uses OpenRouter-powered LLM (if configured) to extract structured information from the resume.
Data Storage: The structured information is saved into the Supabase database.
Node Names and Actions (Example)
Gmail Trigger: Triggers when a new email is received.
IF: Checks whether the received email includes any attachments.
Get Attachments: Retrieves attachments from the triggering email.
Prepare Data: Prepares the attachment content for processing.
Basic LLM Chain: Uses an AI model via OpenRouter to extract relevant resume data and returns it as structured fields.
Supabase-Insert: Inserts the structured resume data into your Supabase database."
Summarise MS Teams Channel Activity for Weekly Reports with AI,https://n8n.io/workflows/3971-summarise-ms-teams-channel-activity-for-weekly-reports-with-ai/,"This n8n template lets you summarize individual team member activity on MS Teams for the past week and generates a report.
For remote teams, chat is a crucial communication tool to ensure work gets done but with so many conversations happening at once and in multiple threads, ideas, information and decisions usually live in the moment and get lost just as quickly - and all together forgotten by the weekend!
Using this template, this doesn't have to be the case. Have AI crawl through last week's activity, summarize all messages and replies and generate a casual and snappy report to bring the team back into focus for the current week. A project manager's dream!
How it works
A scheduled trigger is set to run every Monday at 6am to gather all team channel messages within the last week.
Messages are grouped by user.
AI analyses the raw messages and replies to pull out interesting observations and highlights. This is referred to as the individual reports.
All individual reports are then combined and summarized together into what becomes the team weekly report. This allows understanding of group and similar activities.
Finally, the team weekly report is posted back to the channel. The timing is important as it should be the first message of the week and ready for the team to glance over coffee.
How to use
Ideally works best per project and where most of the comms happens on a single channel. Avoid combining channels and instead duplicate this workflow for more channels.
You may need to filter for specific team members if you want specific team updates.
Customise the report to suit your organisation, team or the channel. You may prefer to be more formal if clients or external stakeholders are also present.
Requirements
MS Teams for chat platform
OpenAI for LLM
Customising this workflow
If the teams channel is busy enough already, consider posting the final report to email.
Pull in project metrics to include in your report. As extra context, it may be interesting to tie the messages to production performance.
Use an AI Agent to query for knowledgebase or tickets relevant to the messages. This may be useful for attaching links or references to add context."
"Automate New Customer Onboarding with HubSpot, Google Calendar, and AI-Powered Gmail",https://n8n.io/workflows/3958-automate-new-customer-onboarding-with-hubspot-google-calendar-and-ai-powered-gmail/,"This n8n workflow streamlines the onboarding process for new customers by automating personalized email communication, calendar scheduling, and contact assignment in HubSpot. It is perfect for businesses looking to ensure a smooth and personalized onboarding experience for new clients.
üßë‚Äçüíº Who is this for?
Customer success teams who need to onboard new clients efficiently.
Sales teams who want to ensure smooth transitions from prospect to customer.
Small businesses that want to automate customer onboarding without complex systems.
üß© What problem is this workflow solving?
This workflow reduces the manual effort involved in onboarding new customers by:
Automatically sending personalized welcome emails.
Scheduling a welcome meeting using a calendar tool.
Assigning the customer to a Customer Success Manager (CSM) in HubSpot.
‚öôÔ∏è What this workflow does
Trigger via Webhook or HubSpot:
The workflow can be triggered either by a webhook (direct API call) or a HubSpot trigger (e.g., when a new contact is created).
HubSpot Connection:
Retrieves the list of HubSpot owners (users with contact access).
Identifies the owner of the new contact.
Calendar Management:
Utilizes a Calendar Agent to schedule a welcome meeting with the new customer.
The Calendar Agent can create, update, or delete events as needed.
Personalized Email Creation:
Uses an AI-powered Email Writer (OpenAI) to generate a personalized welcome email.
Transforms the email text into HTML for a polished format.
Email Sending via Gmail:
Sends the personalized email to the customer using Gmail.
Sets the new contact‚Äôs owner in HubSpot for further communication tracking.
üõ†Ô∏è Setup
Webhook Setup in n8n:
Create a new workflow and add a Webhook node.
Set the Webhook URL path (e.g., /webhook-customer-onboarding).
Make sure the workflow is active.
Webhook Setup in HubSpot:
Go to HubSpot Developer Account.
Navigate to Settings > Integrations > Webhooks.
Create a new webhook and set the URL as the n8n Webhook URL.
Choose POST as the request method.
Test the webhook to ensure it triggers the workflow in n8n.
Calendar Agent Configuration:
The Calendar Agent can be configured to create, update, or delete events.
Connect it to your calendar tool (Google Calendar, Outlook, etc.).
Customize the calendar event details (title, description, time).
Email Writer Setup:
Customize the AI prompt in the Email Writer node to match your brand‚Äôs voice.
Adjust the email text format for your specific needs.
Gmail Integration:
Connect your Gmail account in n8n.
Set the recipient email to the new customer‚Äôs email address.
‚úèÔ∏è How to customize this workflow to your needs
Modify the AI-Powered Email:
Adjust the email prompt for the AI model to create a different welcome message.
Change the email format or add custom variables (e.g., customer name, service details).
Customize Calendar Settings:
Set default time slots for welcome meetings.
Specify which calendar to use for scheduling.
Add Additional Steps:
Extend the workflow to automatically assign the customer to a specific HubSpot list.
Add a follow-up email or survey after the welcome meeting.
This workflow is perfect for businesses seeking an efficient and personalized onboarding process, ensuring new customers feel welcomed and supported from day one."
"Automate Instagram Posts with Google Drive, AI Captions & Facebook API",https://n8n.io/workflows/3478-automate-instagram-posts-with-google-drive-ai-captions-and-facebook-api/,"This template streamlines your Instagram content posting workflow by connecting Google Drive for image storage, using OpenAI for AI-generated captions, and leveraging Facebook Graph API for automated publishing.
Pre-requisites
Before setting up this workflow, ensure you have:
A Google account with access to Google Drive
An OpenAI API key for AI caption generation
A Facebook Developer account with Instagram Graph API access
An Instagram Business or Creator account connected to a Facebook Page
n8n.io account with workflow access
Setup Instructions
Configure Data Source
Create a Google Sheet with the following columns:
Name: Filename of your image in Google Drive
Caption: Optional custom caption (leave empty for AI-generated captions)
URL: your Video Reel or Image in Google Drive
Connect Google Drive
Add your Google Drive credentials in the ""Google Drive"" node
Specify the folder path where your Instagram image/Video are stored
Configure the node to retrieve image files based on filenames from your Google Sheet
Set Up OpenAI Integration
Add your OpenAI API key to the credentials
Configure the OpenAI node to generate engaging captions based on image content
Adjust temperature and model parameters for desired creativity level
Configure Facebook Graph API
Connect your Facebook account with Instagram access
Set up the Facebook Graph API node to post to your Instagram Business/Creator account
Ensure proper image formatting (1:1, 4:5, or 16:9 aspect ratios supported by Instagram)
Workflow Automation Setup
Configure the scheduler node to run at your preferred frequency
Set up error handling to notify you of any posting failures
Add conditional nodes to use either custom or AI-generated captions
Execution Instructions
After completing all connections, test the workflow with a single image
Monitor the execution in the n8n dashboard to ensure proper functioning
View the ""Executions"" tab to track successful posts and troubleshoot any errors
Adjust posting frequency and scheduling as needed
This template saves hours of manual Instagram posting work while maintaining an authentic presence. Perfect for social media managers, content creators, and businesses looking to maintain consistent Instagram activity without the daily manual effort.
The workflow handles image retrieval, caption generation or customization, proper Instagram API formatting, scheduled posting, and execution tracking - all in one automated solution."
"üì¢ Multi-Platform Video Publisher ‚Äì YouTube, Instagram & TikTok",https://n8n.io/workflows/3895-multi-platform-video-publisher-youtube-instagram-and-tiktok/,"Hi! I'm Amanda ‚ù§Ô∏è
I build intelligent automation flows with n8n and Make. This one is for all content creators, marketing teams, and agencies who want to publish once and post everywhere.
With this workflow, you can upload a single video to YouTube, Instagram Reels, and TikTok ‚Äî simultaneously and automatically.
‚úÖ What the workflow does
Downloads a video from a provided URL
Uploads the video to your YouTube channel with title and description
Publishes it as a Reel on Instagram via the Meta Graph API
Sends the same video to TikTok using their official API
Supports credential input via Set node (tokens, titles, descriptions)
‚öôÔ∏è Nodes & Tech Used
HTTP Request ‚Äì Download video and handle uploads to Instagram & TikTok
YouTube node ‚Äì Official n8n integration for video upload
Set node ‚Äì For handling user inputs (tokens, titles, video URLs)
Switch, Wait, Merge ‚Äì Logic to control publishing status
Manual or webhook start available
üõ†Ô∏è Setup Instructions
Open the workflow in your n8n (Cloud or self-hosted) instance
Edit the Set node called Credentials and fill in:
Token Instagram
Token Tiktok
YouTube title, description, and video URL
Instagram account ID
Connect your YouTube OAuth credentials in the YouTube node
Optionally, trigger via webhook to automate from other apps (Typebot, CRM, Drive)
Hit ""Execute Workflow"" or schedule via cron
üë• Who this is for
Content creators who want to post everywhere at once
Agencies managing video distribution across platforms
Social media managers and freelancers
Anyone wanting a one-click multi-platform publishing workflow
üåê Explore more workflows
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda"
Analyze Landing Page with OpenAI and Get Optimization Tips,https://n8n.io/workflows/3100-analyze-landing-page-with-openai-and-get-optimization-tips/,"Your Landing Page is Leaking Sales‚ÄîHere‚Äôs How to Fix It in Seconds
Visitors land on your page. But instead of converting, they bounce.
Why? Something‚Äôs broken. Something‚Äôs missing. But what?
‚ùå Is your CTA too weak?
‚ùå Is your messaging unclear?
‚ùå Is your design creating friction?
You know something is off, but don‚Äôt know what. What if you could get an instant, expert-level report on exactly what to fix?
This workflow will do an AI Analysis of your landing page, provide a CRO Audit, so you can optimize your landing page.
Who is This For?
SaaS Founders & Startups: Stop leaving money on the table. Make every visitor count.
Marketers & Growth Experts: Turn landing pages into high-converting assets.
E-commerce & Lead Gen Businesses: More conversions = more revenue.
How It Works
Paste your URL
Get an instant roast + fix list
Implement changes & watch conversions jump
The workflow scrapes the url you input, gets the htlm source code of the landing page, and sends it to OpenAI AI Agent.
The Agent makes a deep analysis, roasts the landing page, and provides 10 Conversion Rate Optimization Tips to improve your landing page.
Setup Guide
You will need OpenAI Credentials with an API Key to run the workflow.
The workflow is using the OpenAI-o1 model to deliver the best results. It costs between $0.20/0.30 per run.
You can adjust the prompt to your wish in the AI Agent parameters.
Once the workflow has been completed, select Logs to get a readable version.
Below is an example."
All-in-One Telegram/Baserow AI Assistant ü§ñüß† Voice/Photo/Save Notes/Long Term Mem,https://n8n.io/workflows/2986-all-in-one-telegrambaserow-ai-assistant-voicephotosave-noteslong-term-mem/,"Telegram Personal Assistant with Long-Term Memory & Note-Taking
This n8n workflow transforms your Telegram bot into a powerful personal assistant that handles voice, photo, and text messages. The assistant uses AI to interpret messages, save important details as long-term memories or notes in a Baserow database, and recall information for future interactions.
üåü How It Works
Message Reception & Routing
Telegram Integration: The workflow is triggered by incoming messages on your Telegram bot.
Dynamic Routing: A switch node inspects the message to determine whether it's voice, text, or photo (with captions) and routes it for the appropriate processing.
Content Processing
Voice Messages: Audio files are retrieved and sent to an AI transcription node to convert spoken words into text.
Text Messages: Text is directly captured and prepared for analysis.
Photos: If an image is received, the bot fetches the file (and caption, if provided) and uses an AI-powered image analysis node to extract relevant details.
AI-Powered Agent & Memory Management
The core AI agent (powered by GPT-4o-mini) processes the incoming message along with any previous conversation history stored in PostgreSQL memory buffers.
Long-Term Memory: When a message contains personal or noteworthy information, the assistant uses a dedicated tool to save this data as a long-term memory in Baserow.
Note-Taking: For specific instructions or reminders, the assistant saves concise notes in a separate Baserow table.
The AI agent follows defined rules to decide which details are saved as memories and which are saved as notes.
Response Generation
After processing the message and updating memory/notes as needed, the AI agent crafts a contextual and personalized response.
The response is sent back to the user via Telegram, ensuring smooth and natural conversation flow.
üöÄ Key Features
Multimodal Input:
Seamlessly handles voice, photo (with captions), and text messages.
Long-Term Memory & Note-Taking:
Uses a Baserow database to store personal details and notes, enhancing conversational context over time.
AI-Driven Contextual Responses:
Leverages an AI agent to generate personalized, context-aware replies based on current input and past interactions.
User Security & Validation:
Incorporates validation steps to verify the user's Telegram ID before processing, ensuring secure and personalized interactions.
Easy Baserow Setup:
Comes with a clear setup guide and sample configurations to quickly integrate Baserow for managing memories and notes.
üîß Setup Guide
Telegram Bot Setup:
Create your bot via BotFather and obtain the Bot Token.
Configure the Telegram webhook in n8n with your bot's token and URL.
Baserow Database Configuration:
Memory Table:
Create a workspace titled ""Memories and Notes"".
Set up a table (e.g., ""Memory Table"") with at least two fields:
Memory (long text)
Date Added (US date format with time)
Notes Table:
Duplicate the Memory Table and rename it to ""Notes Table"".
Change the first field's name from ""Memory"" to ""Notes"".
n8n Workflow Import & Configuration:
Import the workflow JSON into your n8n instance.
Update credentials for Telegram, Baserow, OpenAI, and PostgreSQL (for memory buffering) as needed.
Adjust node settings if you need to customize AI agent prompts or memory management rules.
Testing & Deployment:
Test your bot by sending various message types (text, voice, photo) to confirm that the workflow processes them correctly, updates Baserow, and returns the appropriate response.
Monitor logs to ensure that memory and note entries are correctly stored and retrieved.
‚ú® Example Interactions
Voice Message Processing:
User sends a voice note requesting a reminder.
Bot Response: ""Thanks for your message! I've noted your reminder and saved it for future reference.""
Photo with Caption:
User sends a photo with the caption ""Save this recipe for dinner ideas.""
Bot Response: ""Got it! I've saved this recipe along with the caption for you.""
Text Message for Memory Saving:
User: ""I love hiking on weekends.""
Bot Response: ""Noted! I‚Äôll remember your interest in hiking.""
Retrieving Information:
User asks: ""What notes do I have?""
Bot Response: ""Here are your latest notes: [list of saved notes].""
üõ†Ô∏è Resources & Next Steps
Telegram Bot Configuration: Telegram BotFather Guide
n8n Documentation: n8n Docs
Community Forums: Join discussions and share your customizations!
This workflow not only streamlines message processing but also empowers users with a personal AI assistant that remembers details over time. Customize the rules and responses further to fit your unique requirements and enjoy a more engaging, intelligent conversation experience on Telegram!"
Complete business WhatsApp AI-Powered RAG Chatbot using OpenAI,https://n8n.io/workflows/2845-complete-business-whatsapp-ai-powered-rag-chatbot-using-openai/,"The provided workflow in n8n is designed to create a Business WhatsApp AI RAG (Retrieval-Augmented Generation) Chatbot.
How it works:
Webhook Setup: The workflow begins by setting up webhooks for verification and response. The Verify webhook receives GET requests and sends back a verification code, while the Respond webhook handles incoming POST requests from Meta regarding WhatsApp messages.
Message Handling: Once a message is received, the workflow checks if the incoming JSON contains a user message. If it does, the message is processed further; otherwise, a generic response is sent.
AI Agent Interaction: The user's message is passed to the AI Agent node, which uses a conversational agent with a predefined system message tailored for an electronics store. This ensures that the AI provides accurate and professional responses based on the knowledge base.
Knowledge Base Utilization: The AI Agent references a knowledge base stored in Qdrant, a vector database. Documents from Google Drive are downloaded, vectorized using OpenAI embeddings, and stored in Qdrant for retrieval during conversations.
Response Generation: The AI Agent generates a response using the OpenAI chat model (gpt-4o-mini) and sends it back to the user via WhatsApp.
Set up steps:
Create Qdrant Collection:
Update the QDRANTURL and COLLECTION variables in the workflow.
Use the Create collection HTTP request node to initialize the collection in Qdrant.
Vectorize Documents:
Configure the Get folder and Download Files nodes to fetch documents from a specified Google Drive folder.
Use the Embeddings OpenAI node to generate embeddings for the downloaded files.
Store the vectorized documents in Qdrant using the Qdrant Vector Store node.
Configure Webhooks:
Ensure both Verify and Respond webhooks have the same URL.
Set the Verify webhook to use the GET HTTP method and the Respond webhook to use the POST HTTP method.
Set Up AI Agent:
Define the system prompt for the AI Agent, specifying guidelines for product information, technical support, customer service, and knowledge base usage.
Link the AI Agent to the OpenAI chat model and configure any additional tools as needed.
Test Workflow:
Trigger the workflow manually using the When clicking ‚ÄòTest workflow‚Äô node to ensure all components are functioning correctly.
Monitor the flow of data through the nodes and verify that responses are being generated and sent accurately.
By following these steps, the workflow will be fully operational, enabling a robust AI-powered chatbot capable of handling customer inquiries via WhatsApp.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
üîêü¶ôü§ñ Private & Local Ollama Self-Hosted AI Assistant,https://n8n.io/workflows/2729-private-and-local-ollama-self-hosted-ai-assistant/,"Transform your local N8N instance into a powerful chat interface using any local & private Ollama model, with zero cloud dependencies ‚òÅÔ∏è. This workflow creates a structured chat experience that processes messages locally through a language model chain and returns formatted responses üí¨.
How it works üîÑ
üí≠ Chat messages trigger the workflow
üß† Messages are processed through Llama 3.2 via Ollama (or any other Ollama compatible model)
üìä Responses are formatted as structured JSON
‚ö° Error handling ensures robust operation
Set up steps üõ†Ô∏è
üì• Install N8N and Ollama
‚öôÔ∏è Download Ollama 3.2 model (or other model)
üîë Configure Ollama API credentials
‚ú® Import and activate workflow
This template provides a foundation for building AI-powered chat applications while maintaining full control over your data and infrastructure üöÄ."
AI Agent To Chat With Files In Supabase Storage,https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/,"Video Guide
I prepared a detailed guide explaining how to set up and implement this scenario, enabling you to chat with your documents stored in Supabase using n8n.
Youtube Link
Who is this for?
This workflow is ideal for researchers, analysts, business owners, or anyone managing a large collection of documents. It's particularly beneficial for those who need quick contextual information retrieval from text-heavy files stored in Supabase, without needing additional services like Google Drive.
What problem does this workflow solve?
Manually retrieving and analyzing specific information from large document repositories is time-consuming and inefficient. This workflow automates the process by vectorizing documents and enabling AI-powered interactions, making it easy to query and retrieve context-based information from uploaded files.
What this workflow does
The workflow integrates Supabase with an AI-powered chatbot to process, store, and query text and PDF files. The steps include:
Fetching and comparing files to avoid duplicate processing.
Handling file downloads and extracting content based on the file type.
Converting documents into vectorized data for contextual information retrieval.
Storing and querying vectorized data from a Supabase vector store.
File Extraction and Processing: Automates handling of multiple file formats (e.g., PDFs, text files), and extracts document content.
Vectorized Embeddings Creation: Generates embeddings for processed data to enable AI-driven interactions.
Dynamic Data Querying: Allows users to query their document repository conversationally using a chatbot.
Setup
N8N Workflow
Fetch File List from Supabase:
Use Supabase to retrieve the stored file list from a specified bucket.
Add logic to manage empty folder placeholders returned by Supabase, avoiding incorrect processing.
Compare and Filter Files:
Aggregate the files retrieved from storage and compare them to the existing list in the Supabase files table.
Exclude duplicates and skip placeholder files to ensure only unprocessed files are handled.
Handle File Downloads:
Download new files using detailed storage configurations for public/private access.
Adjust the storage settings and GET requests to match your Supabase setup.
File Type Processing:
Use a Switch node to target specific file types (e.g., PDFs or text files).
Employ relevant tools to process the content:
For PDFs, extract embedded content.
For text files, directly process the text data.
Content Chunking:
Break large text data into smaller chunks using the Text Splitter node.
Define chunk size (default: 500 tokens) and overlap to retain necessary context across chunks.
Vector Embedding Creation:
Generate vectorized embeddings for the processed content using OpenAI's embedding tools.
Ensure metadata, such as file ID, is included for easy data retrieval.
Store Vectorized Data:
Save the vectorized information into a dedicated Supabase vector store.
Use the default schema and table provided by Supabase for seamless setup.
AI Chatbot Integration:
Add a chatbot node to handle user input and retrieve relevant document chunks.
Use metadata like file ID for targeted queries, especially when multiple documents are involved.
Testing
Upload sample files to your Supabase bucket.
Verify if files are processed and stored successfully in the vector store.
Ask simple conversational questions about your documents using the chatbot (e.g., ""What does Chapter 1 say about the Roman Empire?"").
Test for accuracy and contextual relevance of retrieved results."
"YouTube Video Summary to Discord with GPT-4o, Slack Approval, and Google Sheets",https://n8n.io/workflows/4584-youtube-video-summary-to-discord-with-gpt-4o-slack-approval-and-google-sheets/,"Workflow Overview
This advanced n8n automation is a sophisticated content intelligence tool that transforms YouTube video discovery into a seamless, multi-platform content distribution system. By leveraging RSS, AI, and multiple communication platforms, this workflow:
Discovers New Content:
Monitors YouTube channels via RSS feed
Captures new video uploads automatically
Extracts critical video metadata
Generates Intelligent Summaries:
Leverages OpenAI's GPT models to analyze video descriptions
Creates concise, engaging video summaries
Ensures high-quality, contextually accurate content
Collaborative Approval Process:
Sends summaries to Slack for human review
Allows team members to approve or reject content
Maintains rigorous quality control
Multi-Platform Distribution:
Logs summaries in Google Sheets for internal tracking
Posts approved summaries to Discord
Extends content reach with minimal manual effort
Key Benefits
ü§ñ Full Automation: From video upload to Discord post
üí° Smart Summarization: AI-powered content distillation
üîç Human Oversight: Slack approval ensures quality
üìä Comprehensive Tracking: Google Sheets documentation
üåê Multi-Platform Sharing: Seamless content distribution
Workflow Architecture
üîπ Stage 1: Content Discovery
RSS Trigger: Monitors YouTube channel for new videos
Metadata Extraction: Parses video URLs and IDs
YouTube API Integration: Retrieves detailed video information
üîπ Stage 2: AI-Powered Summarization
GPT Model: Generates concise, relevant summaries
Contextual Understanding: Analyzes video descriptions
Adaptive Summarization: Handles various content types
üîπ Stage 3: Collaborative Approval
Slack Notification: Sends summary for human review
Interactive Approval: Team can approve or reject content
Quality Control Mechanism: Prevents inappropriate or low-quality posts
üîπ Stage 4: Multi-Platform Distribution
Google Sheets Logging: Maintains comprehensive content archive
Discord Posting: Shares approved summaries with wider audience
Potential Use Cases
Content Creators tracking channel performance
Marketing teams automating content distribution
Social media managers expanding online presence
Community managers engaging across platforms
Researchers monitoring specific YouTube channels
Setup Requirements
YouTube Data API Credentials
Google Cloud API key
Channel RSS feed URL
OpenAI API Access
OpenAI account
API key for GPT model
Preferred GPT model (GPT-4o, GPT-3.5)
Slack Workspace
Slack app with appropriate permissions
Designated approval channel
Discord Server
Discord application credentials
Target channel for posting summaries
n8n Installation
n8n platform (cloud or self-hosted)
Import workflow configuration
Configure API credentials
Future Enhancements
Multi-channel support
Advanced filtering mechanisms
Sentiment analysis integration
Expanded platform distribution
Customizable summarization parameters
Technical Considerations
Implement robust error handling
Use exponential backoff for API calls
Ensure secure credential management
Maintain flexible parsing strategies
Ethical Guidelines
Respect content creator's intellectual property
Provide proper attribution
Ensure summaries add value
Maintain transparency in content distribution
Connect With Me
Want to revolutionize your content workflow?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your content strategy with intelligent, automated workflows!"
MCP Server with AI Agent as a Tool Context Reducer,https://n8n.io/workflows/4475-mcp-server-with-ai-agent-as-a-tool-context-reducer/,"Overview
Transform your LLM into a powerful GitHub automation specialist with this n8n workflow template. In a world where multiple MCP servers can overwhelm LLMs with context, this streamlined solution provides a dedicated GitHub Agent that handles all GitHub API operations through a single, specialized tool.
When you need GitHub operations like creating repositories, managing issues, or handling pull requests, your LLM can make one simple call to the GitHub Agent. This agent specializes exclusively in GitHub MCP server operations, offloading all contextual complexity and providing clean, efficient GitHub automation.
‚ú® Features
Single MCP Server Trigger - One tool and one parameter to handle all GitHub API interactions
Specialized GitHub Agent - Dedicated AI agent with direct GitHub MCP Server connection
Self-Executing Workflow - ""When Executed by Another Workflow"" trigger enables seamless workflow chaining
Scalable Architecture - Ready to integrate with unlimited GitHub tools and operations
Context Optimization - Reduces LLM token usage by delegating GitHub complexity to a specialized agent
Flexible Request Processing - Handles any GitHub operation through natural language requests
üéØ Use Cases
Repository Management - Create, clone, and manage repositories programmatically
Issue Tracking - Automate issue creation, updates, and management workflows
Pull Request Automation - Streamline code review and merge processes
GitHub Actions Integration - Trigger and monitor CI/CD workflows
Team Collaboration - Automate notifications and team management tasks
Documentation Updates - Automatically update README files and documentation
üèóÔ∏è Workflow Architecture
Node Breakdown:
MCP Server Trigger - Receives requests with GitHub operation parameters
Set GitHub Username - Configures GitHub user context for API calls
OpenAI Chat Model - Powers the intelligent GitHub agent with contextual understanding
Simple Memory - Maintains conversation context and operation history
GitHub AI Agent - Specialized Tools Agent with direct GitHub MCP Server access
[MCP Server Trigger] ‚Üí [Set GitHub Username] ‚Üí [GitHub AI Agent]
                                ‚Üì
[OpenAI Chat Model] ‚Üê [Simple Memory] ‚Üê [GitHub API Operations]
üìã Requirements
Essential Prerequisites:
‚úÖ OpenAI API Key - For AI Agent and Chat Model functionality
‚úÖ GitHub Username Configuration - Edit the ""Set GitHub Username"" node with your GitHub username for API calls
‚úÖ n8n Version - Compatible with n8n 2024+ releases
‚úÖ MCP Server Setup - Existing GitHub MCP server configuration
Recommended Setup:
GitHub Personal Access Token with appropriate permissions
Basic understanding of n8n workflow configuration
Familiarity with GitHub API operations
üöÄ Setup Instructions
Step 1: Import and Configure
Import the workflow template into your n8n instance
Navigate to the Set GitHub Username node
Replace the placeholder with your actual GitHub username
Step 2: API Keys Setup
Configure your OpenAI API key in the Chat Model node
Ensure your GitHub credentials are properly configured in n8n
Test the connection to verify API access
Step 3: MCP Server Integration
Connect your existing GitHub MCP server to the workflow
Verify the MCP Server Trigger is properly configured
Test with a simple GitHub operation (e.g., ""List my repositories"")
Step 4: Deploy and Test
Activate the workflow in your n8n instance
Test with various GitHub operations to ensure functionality
Monitor execution logs for any configuration issues
üîß Customization Options
Agent Behavior
Modify the Chat Model prompt to adjust agent personality and response style
Configure memory settings to control conversation context retention
Adjust timeout settings for long-running GitHub operations
GitHub Operations
Extend supported operations by adding new GitHub API endpoints
Configure repository filters to limit scope of operations
Set up notification preferences for important GitHub events
Integration Points
Webhook triggers for real-time GitHub event processing
Scheduled operations for regular repository maintenance
Cross-workflow triggers for complex automation chains
üí° Pro Tips
Start Simple: Begin with basic operations like repository listing before attempting complex workflows
Monitor Token Usage: The specialized agent approach significantly reduces OpenAI API costs
Batch Operations: Group related GitHub operations in single requests for efficiency
Error Handling: The agent provides detailed error messages for troubleshooting
ü§ù Support and Community
Documentation: Official n8n Documentation
Community Forum: n8n Community
Issues & Contributions: Feel free to suggest improvements or report issues
üìÑ License
This workflow template is provided under the MIT License. You're free to use, modify, and redistribute with attribution.
Created by: William Lettieri
Version: 1.0
Last Updated: May 28, 2025
Compatibility: n8n 2024+"
Smart Shopify Agent: AI-Powered Abandoned Cart Recovery,https://n8n.io/workflows/4396-smart-shopify-agent-ai-powered-abandoned-cart-recovery/,"ü§ñ AI Cart Recovery Agent: Smart Abandoned Checkout Assistant
Transform abandoned carts into recovered sales with intelligent automation. This sophisticated n8n workflow monitors checkout abandonment, implements smart waiting periods, and sends AI-generated personalized recovery emails only when needed - maximizing conversions while respecting customer experience.
üîÑ How It Works
This intelligent 7-step recovery system recovers lost sales automatically:
Step 1: Initial Abandonment Detection
The workflow fetches current abandoned checkout data from your e-commerce platform (Shopify, WooCommerce, etc.), identifying customers who added items but didn't complete their purchase.
Step 2: Strategic Grace Period
Instead of immediately sending recovery emails, the system waits 1 hour (customizable), giving customers natural time to complete their purchase without pressure or interruption.
Step 3: Smart Re-verification
After the waiting period, the workflow rechecks the abandonment status by fetching updated checkout data, ensuring accuracy before taking action.
Step 4: Intelligent Decision Logic
Advanced conditional logic compares initial and updated abandonment lists, determining if customers are still abandoned or have completed their purchase during the grace period.
Step 5: AI-Powered Email Generation
For customers still showing abandonment, GPT generates personalized recovery emails featuring:
Customer's actual name for personal connection
Specific products left in their cart
Friendly, non-pushy messaging tone
Optional discount incentives
Compelling call-to-action to complete purchase
Step 6: Automated Email Delivery
Personalized recovery emails are sent directly to abandoned customers via Gmail or your preferred email service, maintaining professional branding and deliverability.
Step 7: Comprehensive Activity Logging
All recovery attempts are logged in Google Sheets for tracking, including customer details, email content, and campaign performance analytics.
‚öôÔ∏è Setup Steps
Prerequisites
E-commerce platform with API access (Shopify, WooCommerce, BigCommerce)
OpenAI API key for personalized email generation
Gmail or SMTP email service for delivery
Google Sheets for activity tracking and analytics
n8n instance (cloud or self-hosted)
E-commerce Platform Configuration
Shopify Setup:
API Endpoint: https://your-store.myshopify.com/admin/api/2023-10/checkouts.json
Authentication: X-Shopify-Access-Token header
Required Permissions: Read checkouts, Read customers
Parameters: status=abandoned
WooCommerce Setup:
API Endpoint: https://your-site.com/wp-json/wc/v3/orders
Authentication: Consumer Key/Secret or JWT
Parameters: status=pending, status=failed
Required Plugins: WooCommerce REST API
Configuration Steps
1. Credential Setup
E-commerce API: Store admin API access tokens or keys
OpenAI API Key: GPT-4 access for intelligent email generation
Gmail OAuth2: Professional email delivery service
Google Sheets OAuth2: Activity logging and performance tracking
2. Abandonment Detection Configuration
Monitoring Frequency: Set workflow trigger schedule (hourly, daily)
Grace Period Duration: Customize wait time (default: 1 hour)
Platform Integration: Configure API endpoints for your specific platform
Data Filtering: Set criteria for what constitutes abandonment
3. AI Email Customization
Default email generation includes:
Personalization Level: Customer name, product specifics, cart value
Tone Customization: Friendly, urgent, helpful, or premium
Discount Integration: Optional percentage or fixed amount offers
Brand Voice: Maintain consistent company messaging and style
4. Recovery Campaign Settings
Email Timing: Optimal sending times based on customer time zones
Frequency Limits: Prevent over-emailing with cooldown periods
Segmentation Rules: Different approaches for high-value vs standard carts
Follow-up Sequences: Multi-email recovery campaigns with escalating incentives
5. Performance Tracking Setup
Analytics Dashboard: Google Sheets with recovery metrics and ROI
Success Tracking: Monitor completion rates and revenue recovered
A/B Testing: Compare different email approaches and timing
Customer Journey: Track from abandonment through recovery completion
üöÄ Use Cases
E-commerce Retailers
Fashion & Apparel: Recover high-value clothing and accessory purchases
Electronics: Target abandoned tech purchases with technical support offers
Home & Garden: Remind customers about seasonal or home improvement items
Beauty & Cosmetics: Recover abandoned skincare and makeup purchases
Subscription & SaaS Businesses
Software Trials: Convert abandoned trial signups into paid subscriptions
Membership Sites: Recover incomplete membership purchases
Online Courses: Re-engage learners who abandoned course purchases
Digital Services: Follow up on abandoned service bookings or consultations
B2B E-commerce
Office Supplies: Recover bulk order abandonments with volume discounts
Industrial Equipment: Follow up on high-value equipment quote requests
Professional Services: Re-engage businesses that abandoned service bookings
Software Licenses: Recover enterprise software purchase abandonments
Specialty Retailers
Luxury Goods: Provide white-glove service for high-value abandoned purchases
Custom Products: Follow up on personalized or custom order abandonments
Seasonal Items: Time-sensitive recovery for holiday or event-specific products
Limited Edition: Create urgency for exclusive or limited availability items
Service-Based Businesses
Travel & Hospitality: Recover abandoned hotel, flight, or package bookings
Event Tickets: Re-engage customers who abandoned concert or event purchases
Professional Services: Follow up on abandoned consultation or service bookings
Fitness & Wellness: Recover abandoned membership or class package purchases
üîß Advanced Customization Options
Multi-Platform Integration
Extend beyond single platform monitoring:
- Shopify Plus: Advanced checkout analytics and customer segmentation
- WooCommerce: Custom post-purchase and abandonment tracking
- Magento: Enterprise-level cart recovery with customer journey mapping
- BigCommerce: API-driven recovery with advanced personalization
- Custom Platforms: Webhook-based abandonment detection and recovery
Intelligent Email Sequencing
Create sophisticated recovery campaigns:
Progressive Incentives: Escalating discounts over multiple touchpoints
Behavioral Triggers: Different emails based on cart value, customer history
Seasonal Campaigns: Holiday-specific recovery messaging and offers
Win-Back Sequences: Long-term customer re-engagement beyond immediate recovery
Advanced Personalization
Enhance AI-generated content with:
Purchase History Analysis: Reference previous purchases and preferences
Browsing Behavior: Include recently viewed items and categories
Geographic Personalization: Local offers, shipping options, or store locations
Demographic Targeting: Age, gender, or interest-based messaging customization
Performance Optimization
Implement advanced tracking and optimization:
Revenue Attribution: Track exact recovery amounts and ROI calculations
Customer Lifetime Value: Prioritize high-value customer recovery efforts
Conversion Funnel Analysis: Identify optimal timing and messaging strategies
Predictive Analytics: Use ML to predict recovery likelihood and optimize approaches
üìä Recovery Email Examples
Fashion Retailer Example:
Subject: You left something stylish behind, Sarah!

Hi Sarah,

I noticed you were checking out those gorgeous items in your cart earlier - the Bohemian Summer Dress and Classic Leather Handbag have been waiting for you!

I completely understand if you got busy or needed time to think it over. These pieces are still available and ready to ship to you today.

Since you showed such great taste in selecting these items, I'd love to offer you 10% off your order to make the decision easier. Just use code WELCOME10 at checkout.

Your cart includes:
‚Ä¢ Bohemian Summer Dress (Size M) - $89.99
‚Ä¢ Classic Leather Handbag (Brown) - $156.99

Complete your purchase now and get free shipping to your door!

[Complete My Purchase] 

Best regards,
The StyleHub Team

P.S. These items are popular and inventory is limited - don't wait too long!
Software/SaaS Example:
Subject: Your ProductivityPro trial is waiting, Mike

Hi Mike,

You were just one step away from unlocking the full power of ProductivityPro for your team at TechStartup Inc.

I noticed you explored our Premium Plan features - the advanced reporting and team collaboration tools that could streamline your workflow and boost productivity by up to 40%.

Since you invested time exploring our platform, I'd like to offer you an exclusive 25% discount on your first year. This offer is valid for the next 48 hours.

Your selected plan:
‚Ä¢ ProductivityPro Premium (5 users) - $99/month
‚Ä¢ With 25% discount: $74/month (Save $300/year!)

Ready to transform your team's productivity?

[Activate My Account]

Questions? Reply to this email or schedule a quick 15-minute demo call.

Best regards,
David Chen
Customer Success Manager, ProductivityPro
High-Value B2B Example:
Subject: Your equipment quote is ready for approval, Jennifer

Hi Jennifer,

Thank you for your interest in our Industrial Packaging System for ManuCorp's new facility expansion.

I understand that equipment investments of this scale require careful consideration and stakeholder alignment. Your configured system includes:

‚Ä¢ Model X5000 Packaging Line - $45,000
‚Ä¢ Installation & Training Package - $8,000  
‚Ä¢ Extended 3-Year Warranty - $3,500
Total Investment: $56,500

Given the scope of your project, I'd like to extend our Q1 promotion pricing, which provides:
- 15% discount on equipment ($6,750 savings)
- Free installation supervision ($2,000 value)
- Expedited 6-week delivery

This brings your total to $48,750 - a savings of $7,750.

I'm available for a brief call to address any technical questions or help facilitate internal approvals. 

[Accept Quote & Proceed]

Best regards,
Robert Martinez
Senior Sales Engineer
Industrial Solutions Inc.
Direct: (555) 123-4567
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
API Rate Limiting
Implement exponential backoff for API requests
Stagger workflow execution times across different stores
Monitor API usage and upgrade plans as needed
Cache frequently accessed data to reduce API calls
Email Deliverability Challenges
Use authenticated SMTP services with proper SPF/DKIM setup
Monitor sender reputation and email engagement metrics
Implement opt-out mechanisms and respect unsubscribe requests
Segment email lists and avoid over-emailing customers
False Positive Recoveries
Extend grace periods for complex checkout processes
Implement more sophisticated abandonment detection logic
Add customer behavior analysis before triggering recovery
Create exception rules for technical checkout failures
Optimization Strategies
Recovery Timing Optimization
A/B test different grace period durations (30 min, 1 hour, 3 hours)
Analyze customer behavior patterns to optimize sending times
Consider time zone differences for global customer bases
Implement seasonal timing adjustments for holidays and events
Content Personalization Enhancement
Continuously refine AI prompts based on successful recoveries
Implement dynamic discount strategies based on cart value
Create customer segment-specific messaging approaches
Add urgency elements for time-sensitive or limited inventory items
Performance Measurement
Track recovery rates, revenue impact, and customer satisfaction
Implement cohort analysis for long-term customer value impact
Monitor email engagement metrics and optimize accordingly
Calculate true ROI including customer acquisition costs and lifetime value
üìà Success Metrics
Recovery Performance Indicators
Recovery Rate: Percentage of abandoned carts successfully recovered
Revenue Recovery: Total dollar amount recovered from abandoned purchases
Email Engagement: Open rates, click rates, and conversion rates
Time to Recovery: Average time from abandonment to completed purchase
Business Impact Measurements
ROI Calculation: Revenue recovered vs workflow operational costs
Customer Retention: Impact on long-term customer relationships
Average Order Value: Effect on overall purchase values post-recovery
Operational Efficiency: Automation savings vs manual recovery efforts
üìû Questions & Support
Need help implementing your AI Cart Recovery Agent?
üìß E-commerce Automation Expert Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Specialization: E-commerce automation, cart recovery optimization, AI email personalization
üé• Comprehensive Implementation Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup guides for major e-commerce platforms
Advanced AI email personalization techniques
Recovery campaign optimization strategies
Integration tutorials for Shopify, WooCommerce, and custom platforms
Performance tracking and analytics implementation
ü§ù E-commerce Automation Community
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing e-commerce automation support and consulting
Share your cart recovery success stories and ROI achievements
Access exclusive templates for different industry verticals
Join discussions about e-commerce automation trends and innovations
üí¨ Support Request Guidelines
Include in your support message:
Your e-commerce platform and current cart abandonment rates
Average order values and customer segments you serve
Current recovery processes and conversion challenges
Integration requirements with existing marketing tools
Specific technical errors or workflow execution issues"
Generate & Edit Images with OpenAI GPT-Image-1 and Share via Telegram,https://n8n.io/workflows/4391-generate-and-edit-images-with-openai-gpt-image-1-and-share-via-telegram/,"AI Image Generator & Editor with GPT-4 Vision - Complete Workflow
Template Description
Transform text prompts into stunning images or edit existing visuals using OpenAI's latest GPT-4 Vision model through an intuitive web form interface.
This comprehensive n8n automation provides three powerful image generation modes:
üé® Text-to-Image Generation
Simply enter a descriptive prompt and generate high-quality images from scratch using OpenAI's gpt-image-1 model. Perfect for creating original artwork, concepts, or visual content.
üñºÔ∏è Image-to-Image Editing
Upload an existing image file and transform it based on your text prompt. The AI analyzes your input image and applies modifications while maintaining the original structure and context.
üîó URL-Based Image Editing
Provide a direct URL to any online image and edit it with AI. Great for quick modifications of web images or collaborative workflows.
Key Features
Smart Input Processing
Flexible Form Interface: User-friendly web form with authentication
Multiple Input Methods: File upload, URL input, or text-only generation
Quality Control: Selectable quality levels (low, medium, high)
Format Support: Accepts PNG, JPG, and JPEG formats
Advanced AI Integration
Latest GPT-4 Vision Model: Uses gpt-image-1 for superior results
Intelligent Switching: Automatically detects input type and routes accordingly
Context-Aware Editing: Maintains image coherence during modifications
Customizable Parameters: Control size (1024x1024), quality, and generation settings
Dual Storage Options
Google Drive Integration: Automatic upload with public sharing permissions
ImgBB Hosting: Alternative cloud storage for instant public URLs
File Management: Organized storage with timestamp-based naming
Instant Telegram Delivery
Real-time Notifications: Results sent directly to your Telegram chat
Rich Media Messages: Includes generated image with prompt details
Quick Access Links: Direct links to view and download results
Markdown Formatting: Clean, professional message presentation
Technical Workflow
Form Submission ‚Üí User submits prompt and optional image
Smart Routing ‚Üí System detects input type (text/file/URL)
AI Processing ‚Üí OpenAI generates or edits image based on mode
Binary Conversion ‚Üí Converts base64 response to downloadable file
Cloud Upload ‚Üí Stores in Google Drive or ImgBB with public access
Telegram Delivery ‚Üí Sends result with viewing links and metadata
Perfect For
Content Creators: Generate unique visuals for social media and marketing
Designers: Quick concept development and image variations
Developers: Automated image processing for applications
Teams: Collaborative image editing and sharing workflows
Personal Use: Transform ideas into visual content effortlessly
Setup Requirements
OpenAI API Key: Access to GPT-4 Vision model
Google Drive API (optional): For Google Drive storage
ImgBB API Key (optional): For alternative image hosting
Telegram Bot: For result delivery
Basic Auth Credentials: For form security
What You Get
‚úÖ Complete image generation and editing pipeline
‚úÖ Secure web form with authentication
‚úÖ Dual cloud storage options
‚úÖ Instant Telegram notifications
‚úÖ Professional result formatting
‚úÖ Flexible input methods
‚úÖ Quality control settings
‚úÖ Automated file management
Start creating AI-powered images in minutes with this production-ready template!
Tags: #AI #ImageGeneration #OpenAI #GPT4 #ImageEditing #Telegram #GoogleDrive #Automation #ComputerVision #ContentCreation"
"AI-Optimized Content Posting to X, Discord & LinkedIn with OpenRouter",https://n8n.io/workflows/4022-ai-optimized-content-posting-to-x-discord-and-linkedin-with-openrouter/,"Amplify your social media presence with BuzzBlast, an n8n workflow designed to make your content go viral across X, Discord, and LinkedIn. By sending a single chat message, BuzzBlast leverages OpenRouter's AI to optimize your input for each platform‚Äôs unique audience‚Äîcrafting punchy tweets for X, engaging messages for Discord, and professional posts for LinkedIn. With smart language detection, it ensures the output matches your input‚Äôs language for authentic engagement.
Key Features
üöÄ Multi-Platform Posting: Shares optimized content to X, Discord, and LinkedIn simultaneously.
üß† AI Optimization: Uses OpenRouter‚Äôs AI to tailor content for virality on each platform.
üåê Language Detection: Matches output to your input language for seamless engagement.
üîÑ Smart Routing: Automatically directs content to the right platform using a switch node.
üì± Chat Trigger: Initiates posts via a simple chat message.
‚ö° Zero Hassle: No manual reformatting‚ÄîBuzzBlast handles it all.
Ideal For
Social media managers looking to streamline cross-platform posting.
Content creators aiming to boost engagement with minimal effort.
Businesses seeking to maximize reach across diverse audiences.
Pre-Requirements
n8n Instance: A running n8n instance (cloud or self-hosted).
Credentials:
X account with OAuth2 API access.
Discord Webhook API setup for your server.
LinkedIn account with OAuth2 API access.
OpenRouter account for AI language model access.
Chat Trigger Setup: A configured chat platform (e.g., Slack, Telegram) to send input messages to the workflow.
Setup Instructions
Import the Workflow:
Copy the provided workflow JSON and import it into your n8n instance via the ""Import Workflow"" option in the n8n editor.
Configure Credentials:
In the Post to X node, set up OAuth2 credentials for your X account.
In the Post to Discord node, configure a Discord Webhook for your server.
In the Post to LinkedIn node, add OAuth2 credentials for your LinkedIn account.
In the OpenRouter AI Model node, provide API credentials for your OpenRouter account.
Set Up Chat Trigger:
In the Chat Input Trigger node, configure your preferred chat platform (e.g., Slack, Telegram) to send trigger messages.
Ensure the webhook is active and correctly linked to your chat platform.
Test the Workflow:
Send a test message via your chat platform (e.g., ""Announcing our new product launch!"").
Verify that the AI optimizes the content and posts it to X, Discord, and LinkedIn as expected.
Activate the Workflow:
Once tested, toggle the workflow to ""Active"" in n8n to enable automatic execution when chat messages are received.
Customization Guidance
Changes Chat Trigger: Adjust the chat trigger using your preference platform like telegram, discord, etc.
Modify AI Prompt: Adjust the prompt in the AI Content Optimizer node to change the tone or style (e.g., more professional for LinkedIn or conversational for Discord).
Add New Platforms: Extend the Route to Platforms node by adding conditions for additional platforms (e.g., Instagram or Facebook) and corresponding posting nodes.
Change AI Model: In the OpenRouter AI Model node, select a different OpenRouter model to optimize content quality or manage costs.
Enhance Output Format: Update the JSON schema in the Parse AI Output node to include additional fields like hashtags, emojis, or links for specific platforms.
Add Error Handling: Include an error-handling node after the Route to Platforms node to log failed posts or retry them automatically.
Why Choose BuzzBlast?
BuzzBlast saves time, maximizes reach, and lets AI craft platform-perfect posts that resonate with your audience. Whether you're an influencer, marketer, or business, this workflow makes cross-platform posting effortless. Ready to make waves online? Grab BuzzBlast and start buzzing!"
Extract and Save Invoice Data from Google Drive to Sheets with Dumpling AI,https://n8n.io/workflows/4059-extract-and-save-invoice-data-from-google-drive-to-sheets-with-dumpling-ai/,"Who is this for?
This workflow is perfect for operations teams, accountants, e-commerce businesses, or finance managers who regularly process digital invoices and need to automate data extraction and record-keeping.
What problem is this workflow solving?
Manually reading invoice PDFs, extracting relevant data, and entering it into spreadsheets is time-consuming and error-prone. This workflow automates that process‚Äîwatching a Google Drive folder, extracting structured invoice data using Dumpling AI, and saving the results into Google Sheets.
What this workflow does
Watches a specific Google Drive folder for new invoices.
Downloads the uploaded invoice file.
Converts the file into a Base64 format.
Sends the file to Dumpling AI‚Äôs extract-document endpoint with a detailed parsing prompt.
Parses Dumpling AI‚Äôs JSON response using a Code node.
Splits the items array into individual rows using the Split Out node.
Appends each invoice item to a preformatted Google Sheet along with the full header metadata (order number, PO, addresses, etc.).
Setup
Google Drive Setup
Create or select a folder in Google Drive and place the folder ID in the trigger node.
Make sure your n8n Google Drive credentials are authorized for access.
Google Sheets
Create a Google Sheet with the following headers:
Order number, Document Date, Po_number, Sold to name, Sold to address, Ship to name, Ship to address, Model, Description, Quantity, Unity price, Total price
Paste the Sheet ID and sheet name (Sheet1) into the Google Sheets node.
Dumpling AI
Sign up at Dumpling AI
Go to your account settings and generate your API key.
Paste this key into the HTTP header of the Dumpling AI request node.
The endpoint used is: https://app.dumplingai.com/api/v1/extract-document
Prompt (already included)
This prompt extracts: order number, document date, PO number, shipping/billing details, and detailed line items (model, quantity, unit price, total).
How to customize this workflow to your needs
Adjust the Google Sheet fields to fit your invoice structure.
Modify the Dumpling AI prompt if your invoices have additional or different data points.
Add filtering logic if you want to handle different invoice types differently.
Replace Google Sheets with Airtable or a database if preferred.
Use a different trigger like an email attachment if invoices come via email."
"Create & Approve POV Videos with AI, ElevenLabs & Multi-Posting (TikTok/IG/YT)",https://n8n.io/workflows/4029-create-and-approve-pov-videos-with-ai-elevenlabs-and-multi-posting-tiktokigyt/,"POV Video Creator: Automating TikTok-Style Instagram Video Automation, Approval, and Multi-Platform Posting Using AI, ElevenLabs, Google Sheets, and Social Media APIs
Description
What Problem Does This Solve? üé•
This workflow automates the creation, rendering, approval, and posting of TikTok-style POV (Point of View) videos to Instagram, with cross-posting to Facebook and YouTube. It eliminates manual video production, approval delays, and inconsistent posting schedules. It ensures high-quality content creation and distribution for social media managers and content creators
Target audience: Social media managers, content creators, small to medium-sized businesses, and n8n users familiar with AI tools, Google Sheets, and social media APIs
What Does It Do? üåü
Generates daily POV video ideas using OpenAI
Creates images, videos, and audio with PIAPI.ai and ElevenLabs
Renders final videos with Creatomate
Manages approvals via email and Google Sheets
Posts approved videos to Instagram, Facebook, and YouTube
Tracks progress in a Google Sheet for transparency
Key Features
AI-driven idea generation and script creation
Automated media production with image, video, and audio synthesis
Email-based approval system for quality control
Cross-platform posting to Instagram, Facebook, and YouTube
Real-time tracking in Google Sheets and Google Drive
Error handling for rendering and posting failures
Setup Instructions
Prerequisites
n8n Instance: Self-hosted or cloud n8n instance
API Credentials:
OpenAI API: API key for idea generation, stored in n8n credentials
PIAPI.ai API: API key for image and video generation, stored in n8n credentials
ElevenLabs API: API key for audio generation, stored in n8n credentials
Creatomate API: API key for video rendering, stored in n8n credentials
Google Sheets/Drive API: OAuth2 credentials from Google Cloud Console with Sheets and Drive scopes
Gmail API: OAuth2 credentials from Google Cloud Console with Gmail scope
Instagram Graph API: User Access Token with instagram_content_publish permission from a Facebook App
Facebook Graph API: Access Token from the same Facebook App
YouTube API: OAuth2 credentials for YouTube uploads
Google Sheet: A sheet named ""POV Videos"" with a tab ""Instagram"" and columns: Timestamp, ID, Subject, Topic, Caption, POV_Status, Prompt, Publish_Status, Link, Final Video, Approval, row_number
Creatomate Template: A pre-configured template with video, audio, and text elements
Installation Steps
Import the Workflow:
Copy the workflow JSON from the ‚ÄúTemplate Code‚Äù section (to be provided)
Import it into n8n via ‚ÄúImport from File‚Äù or ‚ÄúImport from URL‚Äù
Configure Credentials:
Add API credentials in n8n‚Äôs Credentials section for OpenAI, PIAPI.ai, ElevenLabs, Creatomate, Google Sheets/Drive, Gmail, Instagram Graph, Facebook Graph, and YouTube
Assign credentials to respective nodes. For example:
In ""Text-to-Image"", use PIAPI.ai credentials: {{ $credentials.PIAPI }}
In ""Render with Creatomate"", use Creatomate credentials: {{ $credentials.Creatomate }}
In ""Send Approval Request"", use Gmail credentials
Set Up Nodes:
Schedule Trigger: Configure to run daily
Approval Email (Send Approval Request): Customize the HTML email template with approval/rejection links
Post to Social Media Nodes (Instagram Container, Facebook Posts, Post YouTube): Configure with your Instagram Business Account ID, Facebook Page ID, and YouTube channel details
Configure Google Sheet and Drive:
Create ""POV Videos"" Google Sheet with ""Instagram"" tab and specified columns
Share the sheet with your Google Sheets credential email
Create ""Audio"" and ""Video"" folders in Google Drive, noting their IDs
Test the Workflow:
Run manually to verify idea generation, media creation, and posting
Check email notifications, Google Sheet updates, and social media posts
Schedule the Workflow:
Enable ""Schedule Trigger"" and ""Schedule Trigger1"" for daily runs
Enable ""Get Latest Approved Video"" to poll at 7 PM daily
How It Works
High-Level Steps
Generate Video Ideas: Creates daily POV video concepts with OpenAI
Create Media: Produces images, videos, and audio using AI tools
Render Video: Combines media into a final video with Creatomate
Manage Approvals: Sends approval emails and processes decisions
Post to Platforms: Publishes approved videos to Instagram, Facebook, and YouTube
Detailed Descriptions
Detailed node descriptions are available in the sticky notes within the workflow (to be provided). Below is a summary of key actions
Node Names and Actions
Video Idea Generation and Script Creation
Schedule Trigger: Initiates daily workflow
Get Title: Fetches pending video ideas from Google Sheet
Generate Topics: Uses OpenAI to create a new video idea
Format Row: Structures the idea into a Google Sheet row
Insert new Prompt, Caption and Title/Topic: Adds the idea to Google Sheet
Generate Ideas: Produces 3 POV sequences
Generate Script: Expands a sequence into a detailed script
Set Topics: Stores the script for media creation
Media Creation
Text-to-Image: Generates an image with PIAPI.ai
Get Image: Retrieves the generated image
Generate Video Prompt: Creates a video prompt from the image
Generate Video: Produces a 5-second video with PIAPI.ai
Access Videos: Retrieves the video URL
Store Video: Updates Google Sheet with video URL
Generate Sound Prompt: Creates an audio prompt
Text-to-Sound: Generates a 20-second audio clip with ElevenLabs
Store Sound: Uploads audio to Google Drive
Allow Access: Sets audio file permissions
Video Rendering
Merge: Combines script, video, and audio data
List Elements: Formats data for Creatomate
Render with Creatomate: Renders the final video
Check Video Status: Routes based on render success/failure
Storage and Notification
Google Drive: Uploads the rendered video
New Render Video Alert: Sends success email
Failed Render: Sends failure email
Render Video Link: Updates Google Sheet with final video URL
Approval Process
Approval Email: Sends approval request email
Handle Approval/Rejection1: Processes approval/rejection via webhook
Video Update1: Updates Google Sheet with approval status
Social Media Posting
Get Latest Approved Video: Polls for approved videos
Check Approval: Routes based on approval status
Instagram Container: Creates Instagram media container
Post to Instagram: Publishes to Instagram
Facebook Posts: Posts to Facebook
Download Video: Downloads video for YouTube
Post YouTube: Uploads to YouTube
Mark Rejected: Updates status for rejected videos
Update Google Sheet: Updates publish status
Customization Tips
Expand Platforms: Add nodes to post to other platforms
Modify Approval Email: Update the Send Approval Request node to customize the HTML template
Alternative Notifications: Add nodes for Slack or Telegram alerts
Adjust Video Duration: Modify Generate Video node to change duration (default: 5 seconds)"
"Automated PR Code Reviews with GitHub, GPT-4, and Google Sheets Best Practices",https://n8n.io/workflows/3804-automated-pr-code-reviews-with-github-gpt-4-and-google-sheets-best-practices/,"AI-Agent Code Review for GitHub Pull Requests
Description:
This n8n workflow automates the process of reviewing code changes in GitHub pull requests using an OpenAI-powered agent.
It connects your GitHub repo, extracts modified files, analyzes diffs, and uses an AI agent to generate a code review based on your internal code best practices (fed from a Google Sheet).
It ends by posting the review as a comment on the PR and tagging it with a visual label like ‚úÖ Reviewed by AI.
üîß What It Does
Triggered on PR creation
Extracts code diffs from the PR
Formats and feeds them into an OpenAI prompt
Enriches the prompt using a Google Sheet of Swift best practices
Posts an AI-generated review as a comment on the PR
Applies a PR label to visually mark reviewed PRs
‚úÖ Prerequisites
Before deploying this workflow, ensure you have the following:
n8n Instance (Self-hosted or Cloud)
GitHub Repository with PR activity
OpenAI API Key for GPT-4o, GPT-4-turbo, or GPT-3.5
GitHub OAuth App (or PAT) connected to n8n to post comments and access PR diffs
(Optional) Google Sheets API credentials if using the code best practices lookup node.
‚öôÔ∏è Setup Instructions
1. Import the Workflow in n8n, click on Workflows ‚Üí Import from file or JSON
Paste or upload the JSON code of this template
2. Configure Triggers and Connections
üîÅ GitHub Trigger
Node: PR Trigger
Repository: Select the GitHub repo(s) to monitor
Events: Set to pull_request
Auth: Use GitHub OAuth2 credentials
üì• HTTP Request
Node: Get file's Diffs from PR
No authentication needed; it uses dynamic path from trigger
üß† OpenAI Model
Node: OpenAI Chat Model
Model: Select gpt-4o, gpt-4-turbo, or gpt-3.5-turbo
Credential: Provide your OpenAI API Key
üßë‚Äçüíª Code Review Agent
Node : Code Review Agent
Connected to OpenAI and optionally to tools like Google Sheets
üí¨ GitHub Comment Poster
Uses GitHub API to post review comments back on PR
Node: GitHub Robot
Credential: Use the agent Github account (OAuth or PAT)
Repo : Pick your owen Github Repository
üè∑Ô∏è PR Labeler (optional)
Adds label ReviewedByAI after successful comment
Node: Add Label to PR
Label : you ca customize the label text of your owen tag.
üìä Google Sheet Best Practices config (optional)
Connects to a Google Sheet for coding guideline lookups, we can replace Google sheet by another tool or data base
First prepare your best practices list with the clear description and the code bad/good examples
Add al the best practices in your Google Sheet
Configure the Code Best Practices node in the template :
Credential : Use your Google Sheet account by OAuth2
URL : Add your Google Sheet document URL
Sheet : Add the name of the best practices sheet"
Smart Sales Support Chatbot with GPT-4o and Google Sheets,https://n8n.io/workflows/3433-smart-sales-support-chatbot-with-gpt-4o-and-google-sheets/,"Who is this tempate for?
This workflow powers a simple yet effective customer and sales support chatbot for your webshop. It's perfect for solopreneurs who want to automate customer interactions without relying on expensive or complex support tools.
How it works?
The chatbot listens to user requests‚Äîsuch as checking product availability‚Äîand automatically handles the following
Fetches product information from a Google Sheet
Answers customer queries
Places an order
Updates the stock after a successful purchase
Everything runs through a single Google Sheet used for both stock tracking and order management.
Setup Instructions
Before you begin, connect your Google Sheets credentials by following this guide: This will be used to connect all the tools to Google Sheets
üëâ Setup Google sheets credentials
Get Stock
Open ""Get Stock"" tool node and select the Google sheet credentials you created.
Choose the correct google sheet document and sheet name and you are done.
Place order
Go to your ""Place Order"" tool node and select the Google sheet credentials you have created.
Choose the correct google sheet document and sheet name.
Update Stock
Open your ""Update Stock"" tool node and select the Google sheet credentials you have created.
Choose the correct google sheet document and sheet name.
In ""Mapping Column Mode"" section select map each column manually.
In ""Column to match on"" select the column with a unique identifier (e.g., Product ID) to match stock items.
In values to update section, add only the column(s) that need to be updated‚Äîusually the stock count.
AI Agent node
Adjust the prompt according to your use case and customize what you need.
Google Sheet Template
Stock sheet
Case ID Phone Model Case Name Case Type Image URL Quantity Avaialble Initital Inventory Sold
1023 Iphone 14 pro Black Leather Magsafe https://example.com/url 90 100 10
Order sheet
Case ID Phone Model Case Name Name Phone Number Address
1023 Black Leather Iphone 14 pro Fernando Torres 9998898888 Paris, France"
5 Ways to Process Images & PDFs with Gemini AI in n8n,https://n8n.io/workflows/3078-5-ways-to-process-images-and-pdfs-with-gemini-ai-in-n8n/,"How it works
Many users have asked in the support forum about different methods to analyze images and PDF documents with Google Gemini AI in n8n. This workflow answers that question by demonstrating five different approaches:
Single image with auto binary passthrough - The simplest approach using AI Agent's automatic binary handling
Multiple images with predefined prompts - For customized analysis with different instructions per image
Native n8n item-by-item processing - For handling multiple items using n8n's standard workflow paradigm
PDF analysis via direct API - For document analysis and text extraction
Image analysis via direct API - For direct control over API parameters
Each method has advantages depending on your specific use case, data volume, and customization needs.
Set up steps
Setup time: ~5-10 minutes
You'll need:
A Google Gemini API key
n8n with HTTP Request and AI Agent nodes
Important: For the HTTP Request nodes making direct API calls to Gemini (Methods 3, 4, and 5), you'll need to set up Query Authentication with your Gemini API key. Add a parameter named ""key"" with your API key value in the Query Auth section of these nodes.
I'll updated this if I find better ways. Also let me know if you know other ways. Eager to learn :)"
üêãü§ñ DeepSeek AI Agent + Telegram + LONG TERM Memory üß†,https://n8n.io/workflows/2864-deepseek-ai-agent-telegram-long-term-memory/,"This n8n workflow template is designed to integrate a DeepSeek AI agent with Telegram, incorporating long-term memory capabilities for personalized and context-aware responses. Here's a detailed breakdown:
Core Features
Telegram Integration
Uses a webhook to receive messages from Telegram users.
Validates user identity and message content before processing.
AI-Powered Responses
Employs DeepSeek's AI models for conversational interactions.
Includes memory capabilities to personalize responses based on past interactions.
Error Handling
Sends an error message if the input cannot be processed.
Model Options üß†
DeepSeek-V3 Chat: Handles general conversational tasks.
DeepSeek-R1 Reasoning: Provides advanced reasoning capabilities for complex queries.
Memory Buffer Window: Maintains session context for ongoing conversations.
Quick Setup üõ†Ô∏è
Telegram Webhook Configuration
Set up a webhook using the Telegram Bot API:
https://api.telegram.org/bot{my_bot_token}/setWebhook?url={url_to_send_updates_to}
Replace {my_bot_token} with your bot's token and {url_to_send_updates_to} with your n8n webhook URL.
Verify the webhook setup using:
https://api.telegram.org/bot{my_bot_token}/getWebhookInfo
DeepSeek API Configuration
Base URL: https://api.deepseek.com
Obtain your API key from the DeepSeek platform.
Implementation Details üîß
User Validation
The workflow validates the user's first name, last name, and ID using data from incoming Telegram messages.
Only authorized users proceed to the next steps.
Message Routing
Routes messages based on their type (text, audio, or image) using a switch node.
Ensures appropriate handling for each message format.
AI Agent Interaction
Processes text input using DeepSeek-V3 or DeepSeek-R1 models.
Customizable system prompts define the AI's behavior and rules, ensuring user-centric and context-aware responses.
Memory Management
Retrieves long-term memories stored in Google Docs to enhance personalization.
Saves new memories based on user interactions, ensuring continuity across sessions."
üêãDeepSeek V3 Chat & R1 Reasoning Quick Start,https://n8n.io/workflows/2777-deepseek-v3-chat-and-r1-reasoning-quick-start/,"This n8n workflow demonstrates multiple ways to harness DeepSeek's AI models in your automation pipeline! üåü
Core Features
Multiple Integration Methods üîå
Local deployment using Ollama for DeepSeek-R1
Direct API integration with DeepSeek Chat V3
Conversational agent with memory buffer
HTTP request implementation with both raw and JSON formats
Model Options üß†
DeepSeek Chat V3 for general conversation
DeepSeek-R1 for advanced reasoning
Memory-enabled agent for persistent context
Quick Setup üõ†Ô∏è
API Configuration
Base URL: https://api.deepseek.com
Get your API key from platform.deepseek.com/api_keys
Local Setup üíª
Install Ollama for local deployment
Set up DeepSeek-R1 via Ollama
Configure local credentials in n8n
Implementation Details üîß
Conversational Agent
Window Buffer Memory for context
Customizable system messages
Built-in error handling with retries
API Endpoints üåê
Chat completions for V3 and R1 models
OpenAI API format compatibles"
AI Agent : Google calendar assistant using OpenAI,https://n8n.io/workflows/2703-ai-agent-google-calendar-assistant-using-openai/,"This template is a simple AI Agent that acts as a Google Calendar Assistant.
It is designed for beginners to have their ""first AI Agent"" performing common tasks and to help them understand how it works.
For new users of n8n, AI Agents, and OpenAI:
This template involves using an OpenAI API Key. If you are new to AI Agents, make sure to research and understand key concepts such as:
""Tokens"" (used for API requests),
""Tool calling"" (how the AI interacts with external tools),
OpenAI's usage costs (how you will be billed for API usage).
Functionality
It has two main functionalities:
Create events in a calendar
Retrieve events from a calendar
How you can use it
Everything is explained with sticky notes in the workflow.
It is ready-to-use: all you need to do is connect your OpenAI credentials, and you can start using the workflow."
Google Sheets Duplication & Enrichment Automation,https://n8n.io/workflows/4865-google-sheets-duplication-and-enrichment-automation/,"How it Works
This workflow reads sheet details from a source Google Spreadsheet, creates a new spreadsheet, replicates the sheet structure, enriches the content by reading data, and writes it into the corresponding sheets in the new spreadsheet. The process is looped for every sheet, providing an automated way to duplicate and transform structured data.
üéØ Use Case
Automate duplication and data enrichment for multi-sheet Google Spreadsheets
Replicate templates across new documents with consistent formatting
Data team workflows requiring repetitive structured Google Sheets setup
Setup Instructions
1. Required Google Sheets
You must have a source spreadsheet with multiple sheets.
The destination spreadsheet will be created automatically.
2. API Credentials
Google Sheets OAuth2 ‚Äì connect to both read and write spreadsheets.
HTTP Request Auth ‚Äì if external API headers are needed.
3. Configure Fields in Write Sheet
Ensure you define appropriate columns and mapping for the destination sheet.
üîÅ Workflow Logic
Manual Trigger: Starts the flow on user demand.
Create New Spreadsheet: Generates a blank spreadsheet.
HTTP Request: Retrieves all sheet names from the source spreadsheet.
JavaScript Code: Extracts titles and metadata from the HTTP response.
Loop Over Sheets: Iterates through each sheet retrieved.
Delete Default Sheet: Removes the placeholder 'Sheet1'.
Create Sheets: Replicates each original sheet in the new document.
Read Spreadsheet1: Pulls data from the matching original sheet.
Write Sheet: Appends the data to the newly created sheets.
üß© Node Descriptions
Node Name Description
Manual Trigger Starts the workflow manually by user test.
Create New Spreadsheet Creates a new Google Spreadsheet for output.
HTTP Request Fetches metadata from the source spreadsheet including sheet names.
Code Processes sheet metadata into a list for iteration.
Loop Over Items Loops over each sheet to replicate and populate.
Google Sheets2 Deletes the default 'Sheet1' from the new spreadsheet.
Create Sheets Creates a new sheet matching each source sheet.
Read Spreadsheet1 Reads data from the source sheet.
Write sheet Writes the data into the corresponding new sheet.
üõ†Ô∏è Customization Tips
Adjust the Google Sheet title to be dynamic or user-input driven
Add filtering logic before writing data
Append custom audit columns like 'Timestamp' or 'Processed By'
Enable logging or Slack alerts after each sheet is created
üìé Required Files
File Name Purpose
My_workflow_4.json Main workflow JSON file for sheet duplication and enrichment
üß™ Testing Tips
Test with a spreadsheet containing 2‚Äì3 simple sheets
Validate whether all sheets are duplicated
Check if columns and data structure remain intact
Watch for authentication issues in Google Sheets nodes
üè∑ Suggested Tags & Categories
#GoogleSheets #Automation #DataEnrichment #Workflow #Spreadsheet"
Scrape Google Places via Dumpling AI and Auto-Save to Google Sheets,https://n8n.io/workflows/4632-scrape-google-places-via-dumpling-ai-and-auto-save-to-google-sheets/,"Who is this for?
This workflow is perfect for lead generation experts, digital marketers, SEO professionals, and virtual assistants who need to quickly collect local business information based on specific search terms without manually navigating Google Places.
What problem is this workflow solving?
Manually searching Google Places for business leads is time-consuming and inconsistent. This workflow automates the entire process using Dumpling AI‚Äôs Google Places search endpoint, helping users collect accurate and structured business data and log it into a Google Sheet automatically.
What this workflow does
This workflow runs daily at 1 PM. It starts by reading a list of business-related search terms from a Google Sheet (for example, ‚Äúdentists in Dallas‚Äù). Each term is sent to Dumpling AI‚Äôs search-places endpoint, which returns local business listings from Google Places. The data is split, structured, and logged row-by-row in a connected Google Sheet.
Nodes Overview
Run Every Day at 1 PM
A scheduled trigger that executes the workflow daily.
Google Sheets (Input) ‚Äì Fetch Search Terms from Sheet
Pulls a list of search terms from a Google Sheet. Each term should describe a business category and location (e.g., ‚Äúcoffee shops in Atlanta‚Äù).
HTTP Request ‚Äì Scrape Google Places via Dumpling AI
Sends each search term to Dumpling AI‚Äôs /search-places endpoint, returning data like business names, phone numbers, websites, ratings, and categories.
Split In Batches ‚Äì Split Places Result
Breaks the list of businesses returned for each search term into individual items for processing.
Google Sheets (Output) ‚Äì Save Each Business to Sheet
Saves the scraped data into a second Google Sheet. Each row contains:
title
address
rating
category
phoneNumber
website
üìù Notes
You must set up Dumpling AI and generate your API key from: Dumpling AI
You can change the run schedule in the schedule node to fit your needs (e.g., weekly or hourly)."
"Build an AI IT Support Agent with Azure Search, Entra ID & Jira",https://n8n.io/workflows/4560-build-an-ai-it-support-agent-with-azure-search-entra-id-and-jira/,"An intelligent IT support agent that uses Azure AI Search for knowledge retrieval, Microsoft Entra ID integration for user management, and Jira for ticket creation. The agent can answer questions using internal documentation and perform administrative tasks like password resets.
How It Works
The workflow operates in three main sections:
Agent Chat Interface: A chat trigger receives user messages and routes them to an AI agent powered by Google Gemini. The agent maintains conversation context using buffer memory and has access to multiple tools for different tasks.
Knowledge Management: Users can upload documentation files (.txt, .md) through a form trigger. These documents are processed, converted to embeddings using OpenAI's API, and stored in an Azure AI Search index with vector search capabilities.
Administrative Tools: The agent can query Microsoft Entra ID to find users, reset passwords, and create Jira tickets when issues need escalation. It uses semantic search to find relevant internal documentation before responding to user queries.
The workflow includes a separate setup section that creates the Azure AI Search service and index with proper vector search configuration, semantic search capabilities, and the required field schema.
Prerequisites
To use this template, you'll need:
n8n cloud or self-hosted instance
Azure subscription with permissions to create AI Search services
Microsoft Entra ID (Azure AD) access with user management permissions
OpenAI API account for embeddings
Google Gemini API access
Jira Software Cloud instance
Basic understanding of Azure resource management
Setup Instructions
Import the template into n8n.
Configure credentials:
Add Google Gemini API credentials
Add OpenAI API credentials for embeddings
Add Microsoft Azure OAuth2 credentials with appropriate permissions
Add Microsoft Entra ID OAuth2 credentials
Add Jira Software Cloud API credentials
Update workflow parameters:
Open the ""Set Common Fields"" nodes
Replace &lt;azure subscription id&gt; with your Azure subscription ID
Replace &lt;azure resource group&gt; with your target resource group name
Replace &lt;azure region&gt; with your preferred Azure region
Replace &lt;azure ai search service name&gt; with your desired service name
Replace &lt;azure ai search index name&gt; with your desired index name
Update the Jira project ID in the ""Create Jira Ticket"" node
Set up Azure infrastructure:
Run the manual trigger ""When clicking 'Test workflow'"" to create the Azure AI Search service and index
This creates the vector search index with semantic search configuration
Configure the vector store webhook:
Update the ""Invoke Query Vector Store Webhook"" node URL with your actual webhook endpoint
The webhook URL should point to the ""Semantic Search"" webhook in the same workflow
Upload knowledge base:
Use the ""On Knowledge Upload"" form to upload your internal documentation
Supported formats: .txt and .md files
Documents will be automatically embedded and indexed
Test the setup:
Use the chat interface to verify the agent responds appropriately
Test knowledge retrieval with questions about uploaded documentation
Verify Entra ID integration and Jira ticket creation
Security Considerations
Use least-privilege access for all API credentials
Microsoft Entra ID credentials should have limited user management permissions
Azure credentials need Search Service Contributor and Search Index Data Contributor roles
OpenAI API key should have usage limits configured
Jira credentials should be restricted to specific projects
Consider implementing rate limiting on the chat interface
Review password reset policies and ensure force password change is enabled
Validate all user inputs before processing administrative requests
Extending the Template
You could enhance this template by:
Adding support for additional file formats (PDF, DOCX) in the knowledge upload
Implementing role-based access control for different administrative functions
Adding integration with other ITSM tools beyond Jira
Creating automated escalation rules based on query complexity
Adding analytics and reporting for support interactions
Implementing multi-language support for international organizations
Adding approval workflows for sensitive administrative actions
Integrating with Microsoft Teams or Slack for notifications"
Automate WooCommerce Image Product Background Removal using API and Google Sheet,https://n8n.io/workflows/4488-automate-woocommerce-image-product-background-removal-using-api-and-google-sheet/,"This workflow automates the process of removing backgrounds from WooCommerce product images using the BackgroundCut API, and then updates the product images in both WooCommerce and a Google Sheet.
Once set up, the workflow processes product images in bulk, removing backgrounds and updating WooCommerce seamlessly.
This workflow is perfect for online stores that sell:
Clothing and fashion items
Jewelry and accessories
General consumer products
Any product that benefits from clean, background-free images for a professional storefront presentation will see improved visual appeal and potentially higher conversions.
Benefits
‚è± Time-saving: Automates what would otherwise be a manual and repetitive task of editing images and updating product listings.
üîÑ Fully Integrated: Connects Google Sheets, BackgroundCut API, FTP server, and WooCommerce in a seamless loop.
üì¶ Scalable: Supports batch processing, making it suitable for stores with hundreds of products.
üìÅ Organized Tracking: Updates the Google Sheet with the new image and a ‚ÄúDONE‚Äù flag for easy monitoring.
üîß Customizable: You can change the image processing API, storage server, or eCommerce platform if needed.
How It Works
Data Retrieval:
The workflow starts by fetching product data (ID and IMAGE URL) from a Google Sheets document.
Only rows without a ""DONE"" marker are processed to avoid duplicates.
Background Removal:
Each product image URL is sent to the BackgroundCut API, which removes the background and returns the edited image.
File Handling:
The processed image is uploaded to an FTP server with the original filename preserved.
A new URL for the edited image is generated and assigned to the product.
WooCommerce Update:
The product in WooCommerce is updated with the new image URL.
Sheet Update:
The Google Sheet is marked as ""DONE"" for the processed row, and the new image URL is recorded.
Batch Processing:
The workflow loops through all rows in the sheet until all products are processed.
Set Up Steps
Prepare the Google Sheet:
Clone the provided Google Sheet template.
Fill in the ID (product ID) and IMAGE (original image URL) columns.
API & Credentials Setup:
Get an API key from BackgroundCut.co.
Configure the HTTP Request node (""Remove from Image URL"") with:
Header Auth: Authorization = API_KEY.
Set up WooCommerce API credentials in the ""Update product"" node.
FTP Configuration:
Replace YOUR_FTP_URL in the ""New Image Url"" node with your FTP/CDN base URL.
Ensure FTP credentials are correctly set in the FTP node.
Execution:
Run the workflow manually via ""When clicking ‚ÄòExecute workflow‚Äô"".
The process automatically handles background removal, file upload, and WooCommerce updates.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Claude 3.7 Sonnet AI Chatbot Agent with Anthropic Web Search and Think Functions,https://n8n.io/workflows/4036-claude-37-sonnet-ai-chatbot-agent-with-anthropic-web-search-and-think-functions/,"This workflow builds a conversational AI chatbot agent using Claude 3.7 Sonnet model with the new . It enhances standard LLM capabilities with Anthropic‚Äôs features: Web Search and Think:
Real-time web search, to answer up-to-date factual queries.
A ‚ÄúThink‚Äù function, to support internal reasoning and memory-like behavior by Anthropic.
A memory buffer, allowing the agent to maintain conversation history.
A system prompt defining clear ethical, functional, and formatting rules for interaction.
When a user sends a message (trigger), the chatbot evaluates the query, optionally performs a web search if needed, processes the result using Claude, and responds accordingly.
‚úÖ Advantages
üß† Enhanced Reasoning Abilities
The Think tool allows the agent to simulate deep thought processes or contextual memory storage, improving conversational intelligence.
üåê Real-Time Knowledge via Web Search
The integrated web_search tool enables the agent to fetch the latest information from the internet, making it ideal for dynamic or news-driven use cases.
üßæ Contextual Responses with Memory Buffer
The inclusion of a memory buffer allows the agent to maintain state across messages, improving dialogue flow and continuity.
üõ°Ô∏è Built-in Ethical Guidelines
The system prompt enforces privacy, factual integrity, neutrality, and ethical response generation, making the agent safe for public or enterprise use.
How It Works
Chat Trigger: The workflow begins when a chat message is received via a webhook. This triggers the AI Agent to process the user's query.
AI Agent Processing: The AI Agent analyzes the query to determine if it requires information from the website or external sources. It follows a structured approach:
For website-related queries, it uses the provided context.
For external information, it employs the web_search tool to fetch up-to-date data from the internet.
The Think tool is used for internal reasoning or caching thoughts without altering data.
Language Model: The Anthropic Chat Model (Claude 3.7 Sonnet) generates responses based on the analyzed query, incorporating website context or web search results.
Memory: A simple memory buffer retains context from previous interactions to maintain continuity in conversations.
Output: The final response is delivered to the user, excluding internal processes like web searches or reasoning steps.
Set Up Steps
Configure Nodes:
Chat Trigger: Set up the webhook to receive user messages.
AI Agent: Define the system message and rules for handling queries.
Anthropic Chat Model: Select the Claude 3.7 Sonnet model and configure parameters like maxTokensToSample.
Memory: Initialize the memory buffer to store conversation context.
Tools:
web_search: Configure the HTTP request to the Anthropic API for web searches, including headers and authentication.
Think: Set up the tool for internal reasoning.
Connect Nodes:
Link the Chat Trigger to the AI Agent.
Connect the Anthropic Chat Model, Memory, and Tools (web_search and Think) to the AI Agent.
Credentials:
Ensure the Anthropic API credentials are correctly configured for both the chat model and the web_search tool.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Voice-to-Email Response System with Telegram, OpenAI Whisper & Gmail",https://n8n.io/workflows/3930-voice-to-email-response-system-with-telegram-openai-whisper-and-gmail/,"This workflow gives you the ability to reply to a long email with a voice note, rather than having to type everything out.
ChatGPT will format your audio response and create an email draft for you.
How it works
When a new email arrives in your inbox, the workflow checks if it needs a response, and it it does, it sends a message to you on Telegram via a VoiceEmailer bot.
When you reply to that message with an audio message, the second part of this workflow is triggered. It checks if the message is in the right format, transcribes the audio, and creates a draft response that shows up in the same email thread.
Set up steps
Add your credentials for Gmail and OpenAI
Create an Telegram bot following the instructions here.
Connect your telegram credentials so the workflow will use your bot.
Turn on the workflow, and message the bot from your telegram. Find the Chat ID from the Executions tab of your workflow, and enter it in as a variable."
Real Estate Lead Generation with BatchData Skip Tracing & CRM Integration,https://n8n.io/workflows/3666-real-estate-lead-generation-with-batchdata-skip-tracing-and-crm-integration/,"How It Works
This workflow automates the entire property lead generation process in a few simple steps:
Property Search: Connects to BatchData's Property Search API with customizable parameters (location, property type, value range, equity percentage, etc.)
Lead Filtering & Scoring: Processes results to identify the most promising leads based on criteria like absentee ownership, years owned, equity percentage, and tax status. Each property receives a lead score to prioritize follow-up.
Skip Tracing: Automatically retrieves owner contact information (phone, email, mailing address) for each qualified property.
Data Formatting: Structures all property and owner data into a clean, organized format ready for your systems.
Multi-Channel Output:
Generates an Excel spreadsheet with all lead details
Pushes leads directly to your CRM (configurable for HubSpot, Salesforce, etc.)
Sends a summary email with the spreadsheet attached
The workflow can run on a daily schedule or be triggered manually as needed. All parameters are easily configurable through dedicated nodes, requiring no coding knowledge.
Who's It For
This workflow is perfect for:
Real Estate Investors looking to find off-market properties with motivated sellers
Real Estate Agents who want to generate listing leads from distressed or high-equity properties
Investment Companies that need regular lead flow for acquisitions
Real Estate Marketers who run targeted campaigns to property owners
Wholesalers seeking to build a pipeline of potential deals
Property Service Providers (roof repair, renovation contractors, etc.) who target specific property types
Anyone who needs reliable, consistent lead generation for real estate without the manual work of searching, filtering, and organizing property data will benefit from this automation.
About BatchData
BatchData is a comprehensive property data provider that offers access to nationwide property information, owner details, and skip tracing services. Key features include:
Extensive Database: Covers 150+ million properties across all 50 states
Rich Property Data: Includes ownership information, tax records, sales history, valuation estimates, equity positions, and more
Skip Tracing Services: Provides owner contact information including phone numbers, email addresses, and mailing addresses
Distressed Property Indicators: Flags for pre-foreclosure, tax delinquency, vacancy, and other motivation factors
RESTful API: Professional API for programmatic access to all property data services
Regular Updates: Continuously refreshed data for accurate information
BatchData's services are designed for real estate professionals who need reliable property and owner information to power their marketing and acquisition strategies. Their API-first approach makes it ideal for workflow automation tools like N8N."
"Daily Newsletter Service using Excel, Outlook and AI",https://n8n.io/workflows/3446-daily-newsletter-service-using-excel-outlook-and-ai/,"This n8n template builds a newsletter (""daily digest"") delivery service which pulls and summarises the latest n8n.io template in select categories defined by subscribers.
It's scheduled to run once a day and sends the newsletter directly to subscriber via a nicely formatted email. If you've had trouble keeping up with the latest and greatest templates beign published daily, this workflow can save you a lot of time!
How it works
A scheduled trigger pulls a list of subscribers (email and category preferences) from an Excel workbook.
We work out unique categories amongst all subscribers and only fetch the latest n8n website templates from these categories to save on resources and optimise the number of API calls we make.
The fetched templates are summarised via AI to produce a short description which is more suitable for our email format.
For each subscriber, we filter and collect only the templates relevant to their category preferences (as defined in the Excel) and ensure that duplicate templates or those which have been ""seen before"" are omitted.
A HTML node is then used to generate the email newsletter. HTML emails are the perfect format since we can add links back to the template.
Finally, we use the Outlook node to send the email digest to the subscriber.
How to use
Populate your Excel sheet with 3 columns: name, email and categories. Categories is a comma-delimited list of categories which match the n8n template website. The available categories are AI, SecOps, Sales, IT Ops, Marketing, Engineering, DevOps, Building Blocks, Design, Finance, HR, Other, Product and Support.
To subscribe a new user, simply add their email to the Excel sheet with at least one category.
To unsubscribe a user, remove them from the sheet.
If you're not interested in paid templates, you may want to filter them out after fetching.
Requirements
Microsoft Excel for subscriber list
Microsoft Outlook for delivering emails
OpenAI for AI-generated descriptions
Customising the workflow
Use AI to summarise the week's trend of templates types and use-cases
This template can be the basis for other similar newsletters - just pull in a list of things from anywhere!"
"Social Media Content Generator And Publisher | X, Linkedin",https://n8n.io/workflows/3082-social-media-content-generator-and-publisher-or-x-linkedin/,"Generate and Publish AI Content to LinkedIn and X (Twitter) with n8n
Overview
This n8n workflow automates the generation and publishing of AI-powered social media content across LinkedIn and X (formerly Twitter). By leveraging AI, this workflow helps social media managers, marketers, and content creators streamline their posting process.
Who is this for?
Social media managers
Content creators
Digital marketers
Businesses looking to automate content generation
Features
AI-powered content creation tailored for LinkedIn and X (Twitter)
Automated publishing to both platforms
Structured output parsing to ensure consistency
OAuth2 authentication for secure posting
Merge and confirmation steps to track successful postings
Setup Instructions
Prerequisites
Before using this workflow, ensure you have:
An n8n instance set up
API credentials for:
Google Gemini AI (for content generation)
X Developer Account with OAuth2 authentication
LinkedIn Developer Account with OAuth2 authentication
A form submission service integrated with n8n
Workflow Breakdown
1. Trigger: Form Submission
A user submits a form containing the post title.
The form is secured with Basic Authentication.
The submitted title is passed to the AI Agent.
2. AI Content Generation
The Google Gemini Chat Model processes the title and generates:
LinkedIn post content
Twitter (X) post content
Hashtags
Call-to-action (LinkedIn)
Character limit check (Twitter)
3. Parsing AI Output
A structured output parser converts the AI-generated content into a JSON format.
Ensures correct formatting for LinkedIn and Twitter (X).
4. Publishing to Social Media
X (Twitter) Posting
Extracts the Twitter post from the AI output.
Publishes it via an OAuth2-authenticated X (Twitter) account.
LinkedIn Posting
Extracts the LinkedIn post from the AI output.
Publishes it via an OAuth2-authenticated LinkedIn account.
5. Merging Post Results
Merges the response data from both LinkedIn and Twitter after publishing.
6. Confirmation Step
Displays a final confirmation form once the posts are successfully published.
Benefits
Save time by automating content creation and publishing.
Ensure consistency across platforms with structured AI-generated posts.
Secure authentication using OAuth2 for LinkedIn and Twitter.
Increase engagement with AI-optimized hashtags and CTAs.
This workflow enables seamless social media automation, helping professionals post engaging AI-powered content effortlessly. üöÄ"
AI agent chat,https://n8n.io/workflows/1954-ai-agent-chat/,"This workflow employs OpenAI's language models and SerpAPI to create a responsive, intelligent conversational agent. It comes equipped with manual chat triggers and memory buffer capabilities to ensure seamless interactions.
To use this template, you need to be on n8n version 1.50.0 or later."
Enrich Company Firmographic Data in Google Sheets with Explorium MCP,https://n8n.io/workflows/4835-enrich-company-firmographic-data-in-google-sheets-with-explorium-mcp/,"Google Sheets Company Enrichment with Explorium MCP
Template
Download the following json file and import it to a new n8n workflow:
google_sheets_enrichment.json
<br />
Overview
This n8n workflow template enables automatic enrichment of company information in your Google Sheets. When you add a new company or update existing company details (name or website), the workflow automatically fetches additional business intelligence data using Explorium MCP and updates your sheet with:
Business ID
NAICS industry code
Number of employees (range)
Annual revenue (range)
Key Features
Automatic Triggering: Monitors your Google Sheet for new rows or updates to company name/website fields
Smart Processing: Only processes new or modified rows, not the entire sheet
Data Validation: Ensures both company name and website are present before processing
Error Handling: Processes each row individually to prevent one failure from affecting others
Powered by AI: Uses Claude Sonnet 4 with Explorium MCP for intelligent data enrichment
Prerequisites
Before setting up this workflow, ensure you have:
n8n instance (self-hosted or cloud)
Google account with access to Google Sheets
Anthropic API key for Claude
Explorium MCP Bearer token (Get explorium api key)
Required Google Sheet Structure
Your Google Sheet must have the following columns (exact names):
name - Company name
website - Company website URL
business_id - Will be populated by the workflow
naics - Will be populated by the workflow
number_of_employees_range - Will be populated by the workflow
yearly_revenue_range - Will be populated by the workflow
Installation & Setup
Step 1: Import the Workflow
Copy the workflow JSON from the template
In your n8n instance, go to Workflows ‚Üí Add Workflow ‚Üí Import from File
Paste the JSON and click Import
Step 2: Configure Google Sheets Credentials
You'll need to set up two Google credentials:
Google Sheets Trigger Credentials:
Click on the Google Sheets Trigger node
Under Credentials, click Create New
Follow the OAuth2 authentication process
Grant permissions to read and monitor your Google Sheets
Google Sheets Update Credentials:
Click on the Update Company Row node
Under Credentials, select the same credentials or create new ones
Ensure permissions include write access to your sheets
Step 3: Configure Anthropic Credentials
Click on the Anthropic Chat Model node
Under Credentials, click Create New
Enter your Anthropic API key
Save the credentials
Step 4: Configure Explorium MCP Credentials
Click on the MCP Client node
Under Credentials, click Create New (Bearer Auth)
Enter your Explorium Bearer token
Save the credentials
Step 5: Link Your Google Sheet
In the Google Sheets Trigger node:
Select your Google Sheet from the dropdown
Select the worksheet (usually ""Sheet1"")
Verify the columns to watch are set to name and website
In the Update Company Row node:
Select the same Google Sheet and worksheet
Ensure the matching column is set to name
Step 6: Activate the Workflow
Click the Active toggle in the top right to activate the workflow
The workflow will now monitor your sheet every minute for changes
How It Works
Workflow Process Flow
Google Sheets Trigger: Polls your sheet every minute for new rows or changes to name/website fields
Filter Valid Rows: Validates that both company name and website are present
Loop Over Items: Processes each company individually
AI Agent: Uses Explorium MCP to:
Find the company's business ID
Retrieve firmographic data (revenue, employees, NAICS code)
Format Output: Structures the data for Google Sheets
Update Company Row: Writes the enriched data back to the original row
Trigger Behavior
First Activation: May process all existing rows to establish a baseline
Ongoing Operation: Only processes new rows or rows where name/website fields change
Polling Frequency: Checks for changes every minute
Usage
Adding New Companies
Add a new row to your Google Sheet
Fill in the name and website columns
Within 1 minute, the workflow will automatically:
Detect the new row
Enrich the company data
Update the remaining columns
Updating Existing Companies
Modify the name or website field of an existing row
The workflow will re-process that row with the updated information
All enrichment data will be refreshed
Monitoring Executions
In n8n, go to Executions to see workflow runs
Each execution shows:
Which rows were processed
Success/failure status
Detailed logs for troubleshooting
Troubleshooting
Common Issues
All rows are processed instead of just new/updated ones
Ensure the workflow is activated, not just run manually
Manual test runs will process all rows
First activation may process all rows once
No data is returned for a company
Verify the company name and website are correct
Check if the company exists in Explorium's database
Some smaller or newer companies may not have data available
Workflow isn't triggering
Confirm the workflow is activated (Active toggle is ON)
Check that changes are made to the name or website columns
Verify Google Sheets credentials have proper permissions
Authentication errors
Re-authenticate Google Sheets credentials
Verify Anthropic API key is valid and has credits
Check Explorium Bearer token is correct and active
Error Handling
The workflow processes each row individually, so if one company fails to enrich:
Other rows will still be processed
The failed row will retain its original data
Check the execution logs for specific error details
Best Practices
Data Quality: Ensure company names and websites are accurate for best results
Website Format: Include full URLs (https://example.com) rather than just domain names
Batch Processing: The workflow handles multiple updates efficiently, so you can add several companies at once
Regular Monitoring: Periodically check execution logs to ensure smooth operation
API Limits & Considerations
Google Sheets API: Subject to Google's API quotas
Anthropic API: Each enrichment uses Claude Sonnet 4 tokens
Explorium MCP: Rate limits may apply based on your subscription
Support
For issues specific to:
n8n platform: Consult n8n documentation or community
Google Sheets integration: Check n8n's Google Sheets node documentation
Explorium MCP: Contact Explorium support for API-related issues
Anthropic/Claude: Refer to Anthropic's documentation for API issues
Example Use Cases
Sales Prospecting: Automatically enrich lead lists with company size and revenue data
Market Research: Build comprehensive databases of companies in specific industries
Competitive Analysis: Track and monitor competitor information
Investment Research: Gather firmographic data for potential investment targets"
Generate Visual Summary & Knowledge Graph Insights for Your Email,https://n8n.io/workflows/4619-generate-visual-summary-and-knowledge-graph-insights-for-your-email/,"The Ultimate Gmail Analysis and Visual Summarization Template
This workflow showcases various useful Gmail search, filter, and AI categorization operations and generates a knowledge graph for your mail using the InfraNodus GraphRAG API, which you can use to reveal the main topics and blind spots in your correspondence.
InfraNodus will then target those blind spots to generate interesting research questions for you and send the topical summary and insights via Telegram. You can also click the generated graph and explore the blind spots inside InfraNodus using the interactive visual interface:
What is it useful for?
Learn about advanced Gmail search, filtering, and AI categorization functions that can be useful for your other workflows
Analyze all your personal messages for the last week to get an overview of the main topics
Analyze all your Sent messages to find recurrent topics and gaps and generate ideas based. on those gaps
Generate ideas based on specific message filters (Personal, Promos, from a specific person, AI-defined criteria, e.g. urgency)
Get an overview of an interaction with a specific person / company
Get an overview of your notes
Generate new ideas based on your correspondence on a certain topic (e.g. ""business"")
Learn about various n8n nodes useful for email processing, filtering, and data conversion
Never miss important topics, use AI filter to get notified of the urgent and important emails via Telegram
How it works
This template can be triggered in multiple ways:
automatically in regular intervals (daily, weekly),
manually in n8n, or
via a private password-protected URL form where you can specify your search and filtering criteria
When you start the workflow, you specify:
your Gmail search filters (can be combined, e.g. after:2025/06/01 label:personal business to search for all emails received after 1 June 2025, filed in the Personal category containing the word ""business"". (optional, if empty, will retrieve all the emails or limited to the number you set in the Gmail node)
Additional Gmail labels (e.g. SENT or CATEGORY_PERSONAL or your custom categories). Use the search filter for faster processing (e.g. prefer label:person to CATEGORY_PERSONAL, but labels can be useful for additional filtering for your search queries) (optional, if empty, will retrieve all the emails)
AI filtering criteria ‚Äî set an additional classification criteria used to filter out the emails, e.g. ""Only the urgent, personal emails"" ‚Äî in that case, AI classification node working with Google's Gemini AI will be activated and will only pass through the email based on the criteria you specify.
Whether you want to build a text graph or a social graph ‚Äî see the workflow for detailed explanation of each
Use snippets of emails (default) or full text (for thorough analysis). We prefer snippets as it's faster and your graph context doesn't get biased towards longer emails this way.
Once you set up your search parameters in Steps 1 and 2, the template will follow the following steps:
Step 3 ‚Äî retrieve Google emails that satisfy your filter criteria. Filter them by additional labels provided if applicable.
Step 4 - if the user chooses to analyze full text, use additional Gmail node that retrieves the full text of the email message
Step 5 ‚Äî if AI filter rule is provided, use the AI Classifier node with Google Gemini Pro 2.5 model to classify the email based on the rule provided. Bypass if empty.
Step 6 - format the text or the email snippets to add the sender meta-data and category and to prepare to submit to InfraNodus
Step 7 - submit the data to the InfraNodus HTTP graphAndEntries endpoint and generate a knowledge graph
Step 8 - access this graph via the graphAndAdvice endpoint and generate a topical summary based on the GraphRAG representation and insight questions bridging the gaps identified. Send the results via a Telegram bot.
We use Telegram, because it takes only 30 seconds to set up a bot with an API, unlike Discord or Slack, which is long and cumbersome to set up. You can also attach a Gmail send node and generate an email instead.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Add this Authorization code in Steps 7 and 8 of the workflow.
Come up with the name of the graph and change it in the HTTP InfraNodus nodes in the steps 7 and 8 and also in the Telegram nodes that send a link to the graph.
For additional settings you can use in the HTTP InfraNodus nodes, see the InfraNodus access points page.
Authorize your Gmail account for Steps 2 and 3 Gmail nodes. The easiest way to set it up is to open a free Google Console API account and to create an OAuth access point for n8n. You can then reuse it with other Google services like Google Sheets, Drive, etc. So it's a useful thing to have in general.
Set up the Gemini AI API key using the instructions in the Step 5 Gemini AI node.
Set up the Telegram node bot for the Step 8. It takes only 30 seconds: just go to @botfather and type in /newbot and you'll have an API key ready. To get the conversation ID, follow the n8n / Telegram instructions in the node itself.
Once everything is ready, try to run the default automated workflow to test if everything works well, then use the Form for playing around with specific filters that you may find useful.
Requirements
An InfraNodus account and API key
An Google Cloud API OAuth client and key for Gmail access
A Gemini AI API key
A Telegram bot API key
FAQ
1. What's the best search query to use?
I personally like starting with analyzing the messages Gmail tags as ""personal"" from the last week (using the after:2025/05/28 label:personal search query) using the social graph settings. It helps me see who I interacted with, what it was about, and gives me a good bird's eye view into my last week's interactions, helping me see if I didn't miss anything.
I also find it useful to analyze the sent messages (using the after:2025/05/28 label:sent search filter or SENT category filter) as it helps me see what I was writing about recently and understand some recurrent topics and gaps in my interactions.
Finally, I also like to analyze notes (label:notes) or specific correspondence (from:your_friend@gmail.com) to get an overview and find gaps in the conversations.
2. Why use InfraNodus and not an AI summarization module?
You probably get a lot of spam, so your AI will get overwhelmed with the content that's not really useful. The InfraNodus graph helps you see the important patterns and discover what's missing by focusing on the gaps. You can use the interactive graph to quickly remove the stuff you don't need and to focus on the most relevant topics and conversations.
Customizing this workflow
You can connect a Gmail node instead of the Telegram one if you prefer to receive notifications directly by email. I don't like using Slack and Discord because their bots are too difficult to set up and take too long.
Check out the complete setup guide for this workflow at https://support.noduslabs.com/hc/en-us/articles/20394884531996-Build-a-Knowledge-Graph-and-Extract-Insights-from-Gmail-Emails-with-n8n-and-InfraNodus with a video tutorial coming soon and the links to other n8n workflows.
Check our other n8n workflows at https://n8n.io/creators/infranodus/ for useful content gap analysis, expert panel, and marketing, and research workflows that utilize GraphRAG for better AI generation.
Finally, check out https://infranodus.com to learn more about our network analysis technology used to build knowledge graphs from text."
"Jobs Newsletter Automation System (N8N, Bolt.new, RapidAPI, Mails.so & ChatGPT)",https://n8n.io/workflows/4470-jobs-newsletter-automation-system-n8n-boltnew-rapidapi-mailsso-and-chatgpt/,"Watch on Youtube‚ñ∂Ô∏è
Welcome to this complete step-by-step guide on how to build your own newsletter automation system using n8n, Bolt.new, and RapidAPI. Whether you're a solo founder, indie hacker, or community builder, this setup will allow you to collect subscribers, send them curated job updates, and manage unsubscriptions ‚Äî all with full control and zero reliance on third-party newsletter tools.
üöÄ Goal of This Guide
By the end of this guide, you will have a fully working system that allows you to:
Collect user subscriptions from a modern frontend interface
Send welcome or rejection emails (using your own SMTP)
Automatically scrape jobs via an API and send them to subscribers weekly or daily
Manage unsubscriptions with confirmation and webhook logic
Customize and manage all this using n8n workflows with no-code/low-code skills
This system is perfect for niche job boards, community newsletters, or any project that needs automated content delivery to subscribers.
üß± Tools You'll Be Using
n8n ‚Äì for automation workflows and acting as your backend
Bolt.new ‚Äì to build your newsletter landing page and subscription interface
Google Sheets ‚Äì to act as your lightweight subscriber/job database
RapidAPI ‚Äì to pull job listings from the Jobs Search API
Custom SMTP Email (Optional) ‚Äì to send branded emails using your own domain
üìÑ Step 1: Set Up Your Google Sheets Database
Make a copy of this Google Sheets template that will serve as your database:
üîó Click here to copy the Google Sheet template](https://docs.google.com/spreadsheets/d/11vxYkjfwIrnNHN6PIdAOa_HZdTvMXI0lm_Jecac4YO0/edit?gid=0#gid=0)
This includes:
A Subscribers sheet to store new signups
An Unsubscribers sheet to prevent duplicates
A Jobs sheet to store scraped job listings
‚öô Step 2: Get Your API Key for Jobs Scraping
We use this API from RapidAPI to pull job listings programmatically:
üîó Jobs Search API on RapidAPI
Sign up or log into RapidAPI
Subscribe to the Jobs Search API
Copy your API key ‚Äî you'll need this in n8n
‚öô Step 3: Get Your API Key for Email Validation
We use this API from Mails.so to confirm email's validity before adding them to our database:
üîó Mails.so API
Sign up or log into mails dot so
Visit the dashboard, then click on API
Copy the cURL command and import on http request node
üåê Step 4: Set Up Your Frontend with Bolt.new
You'll be building a beautiful, modern newsletter landing page using Bolt.new.
Use this link for prompts to generate:
Your landing page
Email templates (welcome, already subscribed, unsubscribe confirmation)
Terms & Privacy Policy pages
Unsubscribe confirmation page
üîó Access the Bolt.new Prompt Document
This includes:
A homepage form with input fields (Name, Email) and consent checkbox
Logic to send data to n8n webhook using fetch()
UI logic for showing webhook response (Success, Already Exists, Invalid Email)
Unsubscribe page handling
(Make your own copy so that you can edit it while we format the prompts)
üì§ Step 5: Set Up Email Sending With Your Custom Domain (Optional but Recommended)
To send branded HTML emails from your own domain, follow this tutorial to configure SMTP properly on n8n with your cPanel email account:
üîó Guide: How to Set Up SMTP with cPanel Email on n8n
This setup helps:
Improve deliverability
Avoid Gmail spam filters
Send beautiful HTML emails you can customize fully
üîÑ Step 6: Create n8n Workflows for Subscription Management
In n8n, you'll need to build these workflows:
‚úÖ 1. Handle Subscriptions
Receives webhook from frontend with name and email
Validates email (using mails.so)
Checks if already subscribed
Sends appropriate HTML email (Welcome, Already Exists, Invalid Email)
Adds to Google Sheet database
‚úÖ 2. Scrape Jobs and Email Subscribers
Use Cron node to run daily/weekly
Use RapidAPI to fetch new jobs
Format jobs into readable HTML
Send jobs to all active subscribers via SMTP
‚úÖ 3. Handle Unsubscriptions
Expose a webhook for /unsubscribe
Confirm email, show a button
On confirmation, add email to Unsubscribers sheet
Show feedback and redirect user back to homepage after 2 seconds
üß† What You're Learning Along the Way
How to use n8n as a backend service (reliable, scalable, visual)
How to use webhooks to connect frontend and backend logic
How to scrape APIs, format JSON data, and convert it to HTML emails
How to use Function nodes for data processing
How to build logic with IF and Switch nodes
How to design a minimal, clean frontend with Bolt.new
How to control the entire newsletter system without external platforms
Follow me on twitter @juppfy | or check out my agency website."
Notion AI Summary & Tags,https://n8n.io/workflows/4431-notion-ai-summary-and-tags/,"What This Workflow Does:
This n8n workflow automatically generates an AI-powered summary and relevant tags whenever a new row is added to your Notion database.
Simply save any URL to your Notion database using the [Notion Web Clipper] Chrome extension or [Save to Notion]‚Äîon both desktop and mobile.
This keeps all your saved content organized in one place instead of scattered across different platforms.
How it works:
The workflow is triggered when a new row is added to your Notion database (it checks for updates every minute).
It retrieves the content from the saved URL.
An AI agent analyzes the content to generate a summary and relevant tags.
The AI output is then formatted properly.
Finally, the formatted summary and tags are saved into the appropriate columns in your Notion database.
Notes:
Make sure your Notion database includes the following columns:
URL ‚Äì Stores the content URL you want to summarize.
AI Summary ‚Äì Where the AI-generated summary will be added.
Tags ‚Äì Where the AI-generated tags will be saved."
Scrape TikTok Profile & Transcript with Dumpling AI and Save to Google Sheets,https://n8n.io/workflows/4328-scrape-tiktok-profile-and-transcript-with-dumpling-ai-and-save-to-google-sheets/,"Who is this for?
This workflow is built for marketers, researchers, and content analysts who need to monitor TikTok content, analyze user data, or track trends across influencers. It's useful for agencies that manage creators or want to keep an organized record of profile performance and video content for reporting or outreach.
What problem is this workflow solving?
Instead of manually checking TikTok profiles or watching videos to understand performance or content, this workflow automates everything. It extracts both profile statistics and full video transcripts, then logs them in Google Sheets for easy access, filtering, and segmentation.
What this workflow does
The automation watches for new TikTok video URLs added to a Google Sheet. When a new row is detected:
It extracts the username from the URL.
Sends the username to Dumpling AI to get full profile data (followers, likes, videos).
Sends the video URL to Dumpling AI to extract the full transcript.
Appends all this information back into the same sheet.
Everything happens automatically after a new URL is added to the sheet.
Setup
Google Sheets Trigger
Connect your Google account and select the spreadsheet where TikTok links will be added.
The workflow will trigger on each new row.
Example sheet column: USERNAME Video
Extract Username
This Set node uses RegEx to extract the username (handle) from the TikTok video URL.
No need to change anything unless TikTok URL formatting changes.
Dumpling AI Profile Scraper
Go to Dumpling AI
Sign in and retrieve your API key
Create an agent using the get-tiktok-profile endpoint
Paste your API key into the httpHeaderAuth field in n8n
Dumpling AI Transcript Scraper
Also uses Dumpling AI
Make sure the endpoint get-tiktok-transcript is enabled in your Dumpling account
Connect using the same API key
Save to Google Sheets
The final node appends data back to your original Google Sheet
Required columns: USERNAME Video, Username, Follower count, Following Count, heart count, Video Count, Transcript
How to customize this workflow to your needs
Add a filter node to only save profiles with a minimum follower count
Add sentiment analysis for the transcript using OpenAI
Connect Airtable or Notion instead of Google Sheets
Use GPT to summarize or classify transcripts for research
‚ö†Ô∏è Notes
Requires a Dumpling AI account and API key
Make sure Google Sheets API is connected and has the correct permissions
TikTok usernames must start with @ for RegEx to work"
Create OpenAI-Compatible API Using GitHub Models for Free AI Access,https://n8n.io/workflows/4217-create-openai-compatible-api-using-github-models-for-free-ai-access/,"This n8n template shows you how to connect Github's Free Models to your existing n8n AI workflows.
Whilst it is possible to use HTTP nodes to access Github Models, The aim of this template is to use it with existing n8n LLM nodes - saves the trouble of refactoring!
Please note, Github states their model APIs are not intended for production usage! If you need higher rate limits, you'll need to use a paid service.
How it works
The approach builds a custom OpenAI compatible API around the Github Models API - all done in n8n!
First, we attach an OpenAI subnode to our LLM node and configure a new OpenAI credential.
Within this new OpenAI credential, we change the ""Base URL"" to point at a n8n webhook we've prepared as part of this template.
Next, we create 2 webhooks which the LLM node will now attempt to connect with: ""models"" and ""chat completion"".
The ""models"" webhook simply calls the Github Model's ""list all models"" endpoint and remaps the response to be compatible with our LLM node.
The ""Chat Completion"" webhook does a similar task with Github's Chat Completion endpoint.
How to use
Once connected, just open chat and ask away!
Any LLM or AI agent node connected with this custom LLM subnode will send requests to the Github Models API. Allowing your to try out a range of SOTA models for free.
Requirements
Github account and credentials for access to Models. If you've used the Github node previously, you can reuse this credential for this template.
Customising this workflow
This template is just an example. Use the custom OpenAI credential for your other workflows to test Github models.
References
https://docs.github.com/en/github-models/prototyping-with-ai-models
https://docs.github.com/en/github-models"
"Extract and Organize Colombian Invoices with Gmail, GPT-4o & Google Workspace",https://n8n.io/workflows/3951-extract-and-organize-colombian-invoices-with-gmail-gpt-4o-and-google-workspace/,"üßæ Personal Invoice Processor
This N8N workflow automates the extraction and organization of personal invoices in Colombia received via Gmail. It includes the following key steps:
üîÅ Flow Summary
Email Trigger
Polls Gmail every 30 minutes for emails with .zip attachments (assumed to contain invoices).
Expects ZIP file following DIAN standards.
ZIP File Handling
Extracts all files.
Filters only PDF and XML files for processing.
Data Extraction & Processing
Uses LangChain Agent + OpenAI (GPT-4o-mini) to extract:
Tipo de documento (Factura / Nota Cr√©dito)
N√∫mero de factura
Fecha de emisi√≥n (YYYY-MM-DD)
NIT emisor y receptor (sin d√≠gito de verificaci√≥n)
Raz√≥n social del emisor
Subtotal, IVA, Total
CUFE
Resumen de compra (max 20 words, formatted sentence)
Validation
Ensures Total = Subtotal + IVA using a calculator node.
Storage
Uploads the original PDF to Google Drive.
Renames the file to: YYYY-MM-DD-NUMERO_FACTURA.pdf.
Inserts or updates invoice details in Google Sheets using a unique Key (NIT_Emisor + Numero_Factura) to prevent duplication.
‚öôÔ∏è Designed for personal use with minimal latency tolerance and high automation reliability."
Generate Lessons Learned Reports from Jira Epics with AI and Google Docs,https://n8n.io/workflows/3934-generate-lessons-learned-reports-from-jira-epics-with-ai-and-google-docs/,"Who is this for?
Jira users who want to automate the generation of a Lessons Learned or Retrospective report after an Epic is Done.
What problem is this workflow solving? / use case
Lessons Learned / Retrospective reports are often omitted in Agile teams because they take time to write. With the use of n8n and AI this process can be automated.
What is this workflow doing
Triggers automatically upon an Epic reaching the ""Done"" status in Jira.
Collects all related tasks and comments associated with the completed Epic.
Intelligently filters the gathered data to provide the LLM with the most relevant information.
Utilizes an LLM with a structured System Message to generate insightful reports.
Delivers the finalized report directly to your specified Google Docs document.
Setup
Create a Jira API key and follow the Credentials Setup in the Jira trigger node.
Create credentials for Google Docs and paste your document ID into the Node.
How to customize this workflow to your needs
Change the System Message in the AI Agent to fit your needs."
Dynamically switch between LLMs for AI Agents using LangChain Code,https://n8n.io/workflows/3820-dynamically-switch-between-llms-for-ai-agents-using-langchain-code/,"Dynamically switch between LLMs for AI Agents using LangChain Code
Purpose
This example workflow demonstrates a way to connect multiple LLMs to a single AI Agent/LangChain Node and programmatically use one ‚Äì or in this case loop through them.
What it does
This AI workflow takes in customer complaints and generates a response that is being validated before returned. If the answer was not satisfactory, the response will be generated again with a more capable model.
How it works
A LangChain Code Node allows multiple LLMs to be connected to a single Basic LLM Chain. On every call only one LLM is actually being connected to the Basic LLM Chain, which is determined by the index defined in a previous Node.
The AI output is later validated by a Sentiment Analysis Node
If the result was not satisfactory, it loops back to the beginning and executes the same query with the next available LLM
The loop ends either when the result passed the requirements or when all LLMs have been used before.
Setup
Clone the workflow and select the belonging credentials. You'll need an OpenAI Account, alternatively you can swap the LLM nodes with ones from a different provider like Anthropic after the import.
How to use
Beware that the order of the used LLMs is determined by the order they have been added to the workflow, not by the position on the canvas.
After cloning this workflow into your environment, open the chat and send this example message:
I really love waiting two weeks just to get a keyboard that doesn‚Äôt even work. Great job. Any chance I could actually use the thing I paid for sometime this month?
Most likely you will see that the first validation fails, causing it to loop back to the generation node and try again with the next available LLM.
Since AI responses are unpredictable, the results and number of tries will differ for each run.
Disclaimer
Please note, that this workflow can only run on self-hosted n8n instances, since it requires the LangChain Code Node."
Perform SEO Keyword Research & Insights with Ahrefs API and Gemini 1.5 Flash,https://n8n.io/workflows/3769-perform-seo-keyword-research-and-insights-with-ahrefs-api-and-gemini-15-flash/,"This n8n workflow automates SEO keyword research by querying the Ahrefs API for keyword data and related keyword insights. The enriched data is then processed by an AI agent to format a response and provide valuable SEO recommendations.
Perfect for SEO specialists, content marketers, digital agencies, and anyone looking to gain valuable insights into keyword opportunities to boost their rankings.
‚öôÔ∏è How This Workflow Works
This workflow guides you through the entire SEO keyword research process, from entering the initial keyword to receiving detailed insights and related keyword suggestions.
1. üó£Ô∏è User Input (Keyword Query)
The user enters a keyword they want to research.
This input is captured by the Chat Input Node, ready for analysis.
2. ü§ñ AI Agent (Input Verification)
The AI Agent reviews the keyword input for any grammatical errors or extra commentary.
If necessary, it cleans the input to ensure a seamless query to the API.
3. üîë Ahrefs API (Keyword Data Retrieval)
The cleaned keyword is sent to the Ahrefs Keyword Tool API.
This retrieves a detailed report including metrics like search volume, keyword difficulty, and CPC.
4. üí° Related Keywords Extraction (Using JavaScript Function)
The workflow uses a JavaScript function to extract main keyword data and 10 related keywords data from the Ahrefs response.
You can tweak the script to adjust the number of related keywords or the level of detail you want.
5. üß† AI Agent (Text Formatting)
The aggregated data, including both the main keyword and related keywords, is sent to an AI agent.
The AI agent formats the data into a concise, readable format that can be shared with the user.
6. üì® Final Response
The formatted text is delivered to the user with keyword insights, recommendations, and related keyword suggestions.
‚úÖ Smart Retry & Error Handling
Each subworkflow includes a fail-safe mechanism to ensure:
‚úÖ Proper error handling for any issues with the API request.
üïí Failed API requests are retried after a customizable period (e.g., 2 hours or 1 day).
üí¨ User input validation prevents any incorrect or malformed queries from being processed.
üìã Ahrefs API Setup
To use this workflow, you‚Äôll need to set up your Ahrefs API credentials:
üîë Ahrefs API
Sign up for an Ahrefs account and get your key here: Ahrefs Keyword Tool API
Once signed up, you'll receive an API key, which you‚Äôll use in the x-rapidapi-key header in n8n.
Ensure you check the Ahrefs Keyword Tool API documentation for more details on available parameters.
üì• How to Import This Workflow
Copy the json code.
Open your n8n instance.
Open a new workflow.
Paste anywhere inside the workflow.
Voila.
üõ†Ô∏è Customization Options
Adjust the number of related keywords extracted (default is 10).
Customize the AI agent response formatting or add specific recommendations for users.
Modify the JavaScript function to extract different metrics from the Ahrefs API.
üß™ Use Case Example
Trying to optimize your blog post around a specific keyword?
Query a broad keyword, like ‚ÄúSEO tips‚Äù.
Get related keyword data and search volume insights.
Use the AI agent to provide keyword recommendations and additional topics to target.
üí• Boost your content strategy with fresh keywords and relevant search data!"
Daily AI News Translation & Summary with GPT-4 and Telegram Delivery,https://n8n.io/workflows/3596-daily-ai-news-translation-and-summary-with-gpt-4-and-telegram-delivery/,"üìù What this workflow does
Every morning at 8 a.m., this workflow fetches the latest AI-related articles from both GNews and NewsAPI. It merges up to 40 new articles daily, selects the 15 most relevant ones on AI technology and applications, and uses GPT-4.1 to generate concise summaries in accurate Traditional Chinese (while preserving essential English technical terms). Each summary also includes the article link for easy referral. The compiled digest is then posted to your designated Telegram account or group.
üë• Who is this for?
AI enthusiasts, professionals, and anyone interested in artificial intelligence news
Individuals and teams wanting a concise daily digest of AI developments in Traditional Chinese
Telegram users who prefer automated information delivery
üéØ What problem does this workflow solve?
With the rapid evolution of AI technology, it can be overwhelming to keep up with new developments. This workflow addresses information overload by automatically collecting, summarizing, and translating the most important AI news each morning ‚Äî all delivered conveniently to your chosen Telegram channel or group.
‚öôÔ∏è Setup
üîë Add NewsAPI and GNews API Keys
Register for accounts on NewsAPI.org and GNews to obtain your API keys.
Input your NewsAPI key directly into the Fetch NewsAPI articles node.
Input your GNews API key into the Fetch GNews articles node.
ü§ñ Set up your Telegram Bot
Create a Telegram Bot via BotFather and copy the generated Bot Token.
In n8n, create Telegram Bot credentials using this token.
In the Send summary to Telegram node, enter the chat ID of your target user, group, or channel to receive the messages.
üß† Configure OpenAI Credentials
In n8n, create a new credential using your OpenAI API key.
Assign this credential to the GPT-4.1 Model node (or equivalent OpenAI/AI nodes).
After completing these steps, your workflow is fully configured to fetch, summarize, and deliver daily AI news to your selected Telegram chat automatically.
üõ†Ô∏è How to customize this workflow
üîç Change the topic: Update the keywords in the NewsAPI and GNews nodes for other subjects (e.g., ""blockchain"", ""quantum computing"").
‚è∞ Adjust delivery time: Modify the scheduled trigger to your preferred hour.
‚úçÔ∏è Tweak summary style or language: Refine the prompt in the AI summarizer node for different tones or translate into other languages as needed.
üì¶ Dependencies
NewsAPI account
GNews account
Telegram Bot
OpenAI API access (for GPT-4.1) or compatible AI model for Langchain agent"
"ü§ñ AI Restaurant Assistant for WhatsApp, Instagram & Messenger",https://n8n.io/workflows/3585-ai-restaurant-assistant-for-whatsapp-instagram-and-messenger/,"Hi, I‚Äôm Amanda! üíå
This workflow was created with so much love, care, and attention‚Ä¶ especially for you, who runs a restaurant, a cozy little burger place, or a delivery business full of heart. ü•∞
I know how busy your days can be, so I made this sweet AI assistant to help you take care of your customers on WhatsApp, Instagram, Messenger (or Evolution API). It sends your beautiful menu, checks ZIP codes, creates payment links, and even notifies the kitchen when the order is ready. All gentle, all automatic, all with love. üíõ
üí° What this workflow does
Replies to customers via WhatsApp API, Instagram Direct, Messenger, and Evolution API
Checks ZIP codes to see if delivery is available using Google Maps
Sends your menu as images, because food should look as good as it tastes üçï
Collects item selections and offers lovely upsells like drinks or extras
Creates payment links with the Asaas API
Confirms when the payment is complete and sends the order to the kitchen
Stores all messages and session data safely in Supabase
Uses OpenAI GPT-4o to talk naturally and kindly with your customers
‚öôÔ∏è How to set it up (I‚Äôll guide you with care üß∏)
Connect your webhook from WhatsApp, Instagram, Messenger, or Evolution API
Create a Supabase table called n8n_workflow_followup
You can use this ready-made template here:
üëâ Supabase Sheet Template
Add your API keys (OpenAI, Supabase, Google Maps, and Asaas) securely in n8n
Customize the AI prompt with your brand‚Äôs voice and sweet style üí´
Set your delivery radius (default is 10km, but you can change it!)
Upload your menu images (from Google Drive, your website, or any link)
That‚Äôs it! Your assistant is now ready to serve with kindness and automation üíï
üçØ Works with:
‚úÖ n8n Cloud and Self-Hosted n8n
üîê All API credentials are safely stored using n8n‚Äôs secure credential manager
Want something customized just for you?
Chat with me, I‚Äôd love to help üíªüíõ Chat via WhatsApp (+55 17 99155-7874)
.
.
.
Tradu√ß√£o em Portugu√™s:
Oi, eu sou a Amanda! üíå
Esse workflow foi feito com muito carinho, dedica√ß√£o e cuidado... pensando especialmente em voc√™, que tem um restaurante, lanchonete ou delivery cheio de amor pelo que faz. ü•∞
Eu sei como o dia a dia pode ser corrido, e foi por isso que eu criei esse atendente com IA: pra te ajudar a responder clientes no WhatsApp, Instagram, Messenger (ou Evolution API), enviar card√°pio com imagens lindas, calcular entregas, gerar links de pagamento e at√© avisar a cozinha. Tudo com jeitinho, sem complica√ß√£o, e com muito cora√ß√£o. üíõ
üí° O que esse fluxo faz
Atende clientes pelo WhatsApp API, Instagram Direct, Messenger e Evolution API
Valida CEP e calcula se o cliente est√° dentro da √°rea de entrega (usando Google Maps)
Envia card√°pio com imagens, porque comer come√ßa pelos olhos üçï
Coleta os pedidos e tamb√©m oferece bebidas e adicionais
Gera link de pagamento automaticamente com a API do Asaas
Confirma o pagamento e avisa a cozinha quando estiver tudo certo
Armazena mensagens, hor√°rios e hist√≥rico no Supabase
Usa o GPT-4o da OpenAI pra conversar de forma educada e natural com seus clientes
‚öôÔ∏è Como configurar (com meu passo a passo cheio de cuidado üß∏)
Conecte seu webhook do WhatsApp, Instagram, Messenger ou Evolution API
Crie uma tabela no Supabase chamada n8n_workflow_followup
Voc√™ pode usar esse modelo aqui:
üëâ Planilha modelo Supabase
Adicione suas chaves de API do OpenAI, Google Maps, Supabase e Asaas no gerenciador do n8n
Personalize o prompt da IA com o nome do seu restaurante, estilo de fala e sua magia üí´
Defina a dist√¢ncia m√°xima de entrega (padr√£o: 10km)
Coloque seus pr√≥prios links de imagens do card√°pio (pode ser do Drive, site ou CDN)
Prontinho! Agora o seu restaurante tem um atendente inteligente, gentil e muito eficiente üíï
üçØ Funciona com:
‚úÖ n8n Cloud e n8n auto-hospedado
üîê E suas credenciais ficam guardadinhas com seguran√ßa no pr√≥prio n8n, t√° bom?
Quer algo feito especialmente pra voc√™?
Fala comigo com todo carinho üíªüíõ Falar no WhatsApp (+55 17 99155-7874)"
ü¶ú‚ú®Use OpenAI to Transcribe Audio + Summarize with AI + Save to Google Drive,https://n8n.io/workflows/3076-use-openai-to-transcribe-audio-summarize-with-ai-save-to-google-drive/,"Automate Audio Transcription, AI Summarization, and Google Drive Storage
Who is this for?
Content Teams, Researchers, and Administrators who need to automatically process voice memos, meeting recordings, or interview audio into structured, searchable documents.
What problem does this solve?
Eliminates manual transcription work by automatically converting audio files into organized text documents with AI analysis, while maintaining human oversight through approval workflows.
What this workflow does
Smart Audio Processing:
Triggers when new .m4a files appear in Google Drive
Uses OpenAI's Whisper for accurate transcription
Implements dual-format reporting (JSON + Markdown)
Human Oversight (optional):
Requires email approval before processing
45-minute response window with escalation options
AI-Powered Analysis:
Generates structured JSON reports with:
Key points & action items
Sentiment analysis
Technical terminology glossary
Creates Markdown versions for easy reading
Document Management:
Stores raw transcripts + reports in Google Drive
Automatic file naming with timestamps
Sends completion alerts via Email/Telegram
Workflow visualization showing audio file processing path
Setup
Credentials Needed:
Google Drive API access
OpenAI API key (GPT-4o-mini)
Gmail & Telegram integrations
Configuration:
Set your Google Drive folder ID in 3 nodes
Update email addresses in Gmail nodes
Customize approval timeout in ""Gmail User for Approval""
Customization Points:
File extension filters (.m4a)
AI report templates and prompts
Notification channels (Email/Telegram)
How to customize
Approval Process: Add SMS/Teams notifications via additional nodes
File Types: Modify filter node for .mp3/.wav support
Analysis Depth: Adjust GPT-4 prompts in ""Summarize to JSON"" nodes
Storage: Connect to Notion/Airtable instead of Google Drive"
Effortless Email Management with AI-Powered Summarization & Review,https://n8n.io/workflows/2862-effortless-email-management-with-ai-powered-summarization-and-review/,"How it Works
This workflow automates the handling of incoming emails, summarizes their content, generates appropriate responses using a retrieval-augmented generation (RAG) approach, and obtains approval or suggestions before sending replies. Below is an explanation of its functionality divided into two main sections:
Email Handling and Summarization:
The process begins with the Email Trigger (IMAP) node which listens for new emails in a specified inbox.
Once an email is received, the Markdown node converts its HTML content into plain text if necessary, followed by the Email Summarization Chain that uses AI to create a concise summary of up to 100 words.
Response Generation and Approval:
A Write email node generates a professional response based on the summarized content, ensuring brevity and professionalism while keeping within the word limit.
Before sending out any automated replies, the system sends these drafts via Gmail for human review and approval through the Gmail node configured with free-text response options. If approved, the finalized email is sent back to the original sender using the Send Email node; otherwise, it loops back for further edits or manual intervention.
Additionally, there's a Text Classifier node designed to categorize feedback from humans as either ""Approved"" or ""Declined"", guiding whether the email should proceed directly to being sent or require additional editing.
Set Up Steps
To replicate this workflow within your own n8n environment, follow these essential configuration steps:
Configuration:
Begin by setting up an n8n instance either locally or via cloud services offered directly from their official site.
Import the provided JSON configuration file into your workspace, making sure all required credentials such as IMAP, SMTP, OpenAI API keys, etc., are properly set up under Credentials since multiple nodes rely heavily on external integrations for functionalities like reading emails, generating summaries, crafting replies, and managing approvals.
Customization:
Adjust parameters according to specific business needs, including but not limited to adjusting the conditions used during conditional checks performed by nodes like Approve?.
Modify the template messages given to AI models so they align closely with organizational tone & style preferences while maintaining professionalism expected in business communications.
Ensure correct mappings between fields when appending data to external systems where records might need tracking post-interaction completion, such as Google Sheets or similar platforms."
HR & IT Helpdesk Chatbot with Audio Transcription,https://n8n.io/workflows/2752-hr-and-it-helpdesk-chatbot-with-audio-transcription/,"An intelligent chatbot that assists employees by answering common HR or IT questions, supporting both text and audio messages. This unique feature ensures employees can conveniently ask questions via voice messages, which are transcribed and processed just like text queries.
How It Works
Message Capture: When an employee sends a message to the chatbot in WhatsApp or Telegram (text or audio), the chatbot captures the input.
Audio Transcription: For audio messages, the chatbot transcribes the content into text using an AI-powered transcription service (e.g., Whisper, Google Cloud Speech-to-Text).
Query Processing:
The transcribed text (or directly entered text) is sent to an AI service (e.g., OpenAI) to generate embeddings.
These embeddings are used to search a vector database (e.g., Supabase or Qdrant) containing the company‚Äôs internal HR and IT documentation.
The most relevant data is retrieved and sent back to the AI service to compose a concise and helpful response.
Response Delivery: The chatbot sends the final response back to the employee, whether the input was text or audio.
Set Up Steps
Estimated Time: 20‚Äì25 minutes
Prerequisites:
Create an account with an AI provider (e.g., OpenAI).
Connect WhatsApp or Telegram credentials in n8n.
Set up a transcription service (e.g., Whisper or Google Cloud Speech-to-Text).
Configure a vector database (e.g., Supabase or Qdrant) and add your internal HR and IT documentation.
Import the workflow template into n8n and update environment variables for your credentials."
Email Summary Agent,https://n8n.io/workflows/2722-email-summary-agent/,"Problem
Teams often struggle with email overload, leading to missed actions and inefficient meeting preparation.
Solution
This workflow automates email management using n8n and AI. It fetches emails, summarizes key points and actions, and sends two concise updates‚Äîone in the morning and one at night.
How It Works
Triggers at 7 AM and 9 PM: Automates the process to summarize emails received during specific time blocks.
Fetches Emails: Retrieves emails from the last 24 hours or after a specific time.
Summarizes with AI: Uses OpenAI to process the email content into actionable summaries.
Sends Team Updates: Compiles the summaries into a concise, formatted email and sends it to the team.
Expected Results
Significant reduction in missed actions and follow-ups.
Customizations
Adjust timings, filters, and recipients to suit your team‚Äôs needs."
AI-Powered Interview Preparation System using Local LLM for Campus Placements,https://n8n.io/workflows/4761-ai-powered-interview-preparation-system-using-local-llm-for-campus-placements/,"Overview
An AI-powered, end-to-end interview preparation and mentoring automation system for campus placements. It enables placement cells to generate hyper-personalized 4-page interview preparation PDFs for shortlisted students, by combining job descriptions (JDs), candidate data, and LLMs via LangChain and Ollama.
Note: This template requires self-hosted n8n to run community nodes like LangChain and Ollama.
What This Workflow Does
Accepts a CSV of shortlisted students and a JD via form upload
Analyzes student profile vs JD using Ollama LLM via LangChain
Generates personalized interview preparation PDFs
Sends the PDF to each student via email
Logs all data in Google Sheets and prevents duplicate processing
Workflow Preview
üì∑ Please add a workflow screenshot here showing the main nodes and flow
Step-by-Step Flow
1. Form Submission
CSV of shortlisted students + JD + company name is submitted via HTTP Request form trigger.
2. Data Parsing and Google Sheet Logging
CSV parsed ‚Üí structured rows added to Google Sheet named with company + batch.
3. Candidate Filtering
Only students with N8N_Agent = Not Generated are selected to avoid reprocessing.
4. AI-Powered Report Generation
LangChain agent (via Ollama + Gemini Search Tool) generates a 4-page Markdown report:
Page 1: Profile Summary, Skill Gap Analysis, Company Insights
Page 2: 15‚Äì20 Personalized Interview Questions
Page 3: 5 Group Discussion Topics + Strategy
Page 4: Custom Preparation Plan + Suggested Resources
5. PDF Creation
Markdown ‚Üí Stylish PDF via APITemplate.io
6. Email Delivery
Each student receives a personalized email with the attached report.
7. Google Sheet Status Update
Marks the student‚Äôs row as ‚ÄúGenerated‚Äù in N8N_Agent column.
Prerequisites
Self-hosted n8n with Community Nodes enabled
Local or Docker-hosted Ollama with LLaMA3.2 or equivalent model
Activated LangChain and Gemini Search Tool nodes
APITemplate.io API Key
Connected Google Sheets account
SMTP setup or Gmail node for email delivery
Customization Tips
Replace the LLM prompt in the LangChain node with your own tone/style
Modify the PDF template on APITemplate.io to reflect your institution branding
Update the email copy for formal or informal tones
Add new filters (e.g., minimum CGPA, branch) for student selection"
"Anonymize & Reformat CVs with Gemini AI, Google Sheets & Apps Script",https://n8n.io/workflows/4708-anonymize-and-reformat-cvs-with-gemini-ai-google-sheets-and-apps-script/,"üßæ CV Anonymization & Reformatting Automation
An advanced N8N workflow to anonymize and reformat resumes for internal or external distribution
üîç Overview
This template automates the anonymization, structuring, and reformatting of CVs/resumes using a combination of AI, PDF parsing, and Google Apps Script. It‚Äôs ideal for companies or teams that need to showcase their workforce‚Äîwhile ensuring candidate privacy and presenting the information in a standardized format.
üë• Who it's for
HR departments
Consulting agencies
Project managers needing anonymized candidate profiles
Any enterprise team required to share CVs in a consistent, brand-compliant, and privacy-conscious way
‚öôÔ∏è What it does
Watches a Folder for new PDFs (CVs)
Extracts Content from the PDF
2.Uses Gemini Flash 2.0 to intelligently structure the resume content
Stores the structured data in a Google Sheet
Generates a New CV from a customizable template via Google Apps Script
Anonymizes Names (e.g. ‚ÄúJohn Doe‚Äù ‚Üí ‚ÄúJ. D.‚Äù)
(Optional) Sends or downloads the final anonymized version
üõ†Ô∏è Setup Requirements
To use this workflow, you‚Äôll need to configure:
Your input/output folders
Field mappings (e.g. what to extract)
A Google Apps Script endpoint for PDF generation
A Google Sheet acting as your structured data database
A Gemini API key and access (or any other LLM)
üí° Templates are customizable: you can define your own layout as long as the placeholders match your database field names.
‚ú® Customization Ideas
Automatically email the anonymized CV to a recipient
Use conditional logic to switch templates (e.g. for different roles or departments)
Add a second formatting layer for internal vs external audiences"
Automated Fiverr UGC Market Research: Track Gigs with Google Sheets,https://n8n.io/workflows/4583-automated-fiverr-ugc-market-research-track-gigs-with-google-sheets/,"This cutting-edge n8n automation is a powerful market research tool designed to continuously monitor and capture User-Generated Content (UGC) opportunities on Fiverr. By intelligently scraping, parsing, and logging gig data, this workflow provides:
Automated Market Scanning:
Daily scrapes of Fiverr UGC gigs
Real-time market intelligence
Consistent, hands-off data collection
Intelligent Data Extraction:
Parses complex HTML structures
Captures key gig details
Transforms unstructured web data into actionable insights
Seamless Data Logging:
Automatic Google Sheets integration
Comprehensive gig marketplace tracking
Historical data preservation
Key Benefits
ü§ñ Full Automation: Continuous market research
üí° Smart Filtering: Detailed UGC gig insights
üìä Instant Reporting: Real-time market trends
‚è±Ô∏è Time-Saving: Eliminate manual research
Workflow Architecture
üîç Stage 1: Automated Triggering
Scheduled Scraping: Daily gig discovery
Precise Timing: Configurable run intervals
Consistent Monitoring: Always-on market intelligence
üåê Stage 2: Web Scraping
HTTP Request: Fetch Fiverr search results
Dynamic Headers: Bypass potential scraping restrictions
Targeted Search: UGC-specific gig discovery
üß© Stage 3: Data Extraction
HTML Parsing: Extract critical gig information
Structured Data Collection:
Gig Prices
Seller Names
Gig Titles
Direct Gig URLs
üìã Stage 4: Data Logging
Google Sheets Integration: Automatic data storage
Historical Tracking: Build comprehensive gig databases
Easy Analysis: Spreadsheet-ready format
Potential Use Cases
Content Creators: Market rate research
Freelance Platforms: Competitive intelligence
Marketing Agencies: UGC trend analysis
Recruitment Specialists: Talent pool mapping
Business Strategists: Market opportunity identification
Setup Requirements
Fiverr Search Configuration
Targeted search keywords
Specific UGC categories
Web Scraping Preparation
User-agent rotation strategy
Potential proxy configuration
Robust error handling
Google Sheets Setup
Connected Google account
Prepared spreadsheet
Appropriate sharing permissions
n8n Installation
Cloud or self-hosted instance
Import workflow configuration
Configure API credentials
Future Enhancement Suggestions
ü§ñ AI-powered gig trend analysis
üìä Advanced data visualization
üîî Real-time price change alerts
üß† Machine learning market predictions
üåê Multi-platform gig tracking
Ethical Considerations
Respect Fiverr's Terms of Service
Implement responsible scraping practices
Avoid overwhelming target websites
Use data for legitimate research purposes
Technical Recommendations
Implement exponential backoff for requests
Use randomized delays between scrapes
Maintain flexible CSS selector strategies
Consider rate limiting and IP rotation
Connect With Me
Ready to unlock market insights?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your market research with intelligent, automated workflows!"
Documentation Lookup AI Agent using Context7 and Gemini,https://n8n.io/workflows/4547-documentation-lookup-ai-agent-using-context7-and-gemini/,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.
This workflow demonstrates how to build and expose a sophisticated n8n AI Agent as a single, callable tool using the Multi-Agent Collaboration Protocol (MCP). It allows external clients or other AI systems to easily query software library documentation via Context7, without needing to manage the underlying tool orchestration or complex conversational logic.
Core Idea:
Instead of building complex agentic loops on the client-side (e.g., in Python, a VS Code extension, or another AI development environment), this workflow offloads the entire agent's reasoning and tool-use process to n8n. The client simply sends a natural language query (like ""How do I use Flexbox in Tailwind CSS?"") to an SSE endpoint, and the n8n agent handles the rest.
Key Features & How It Works:
Public MCP Endpoint:
The main workflow uses the Context7 MCP Server Trigger node to create an SSE endpoint. This makes the agent accessible to any MCP-compatible client.
The path for the endpoint is kept long and random for basic 'security by obscurity'.
Tool Workflow as an Interface:
A Tool Workflow node (named call_context7_ai_agent in this example) is connected to the MCP Server Trigger. This node defines the single ""tool"" that external clients will see and call.
Dedicated AI Agent Sub-Workflow:
The call_context7_ai_agent tool invokes a separate sub-workflow which contains the actual AI logic.
This sub-workflow starts with a Context7 Workflow Start node to receive the user's query.
A Context7 AI Agent node (using Google Gemini in this example) is the brain, equipped with:
A system prompt to guide its behavior.
Simple Memory to retain context for each execution (using {{ $execution.id }} as the session key).
Two specialized Context7 MCP client tools:
context7-resolve-library-id: To convert library names (e.g., 'Next.js') into Context7-specific IDs.
context7-get-library-docs: To fetch documentation using the resolved ID, with options for specific topics and token limits.
Seamless Tool Use: The AI Agent autonomously decides when and how to use the resolve-library-id and get-library-docs tools based on the user's query, handling the multi-step process internally.
Benefits of This Approach:
Simplified Client Integration: Clients interact with a single, powerful tool, sending a simple query.
Reduced Client-Side Token Consumption: The detailed prompts, tool descriptions, and conversational turns are managed server-side by n8n, saving tokens on the client (especially useful if the client is another LLM).
Centralized Agent Management: Update your agent's capabilities, tools, or LLM model within n8n without any changes needed on the client side.
Modularity for Agentic Systems: Perfect for building complex, multi-agent systems where this n8n workflow can act as a specialized ""expert"" agent callable by others (e.g., from environments like Smithery).
Cost-Effective: By using a potentially less expensive model (like Gemini Flash) for the agent's orchestration and leveraging the free tier or efficient pricing of services like Context7, you can build powerful solutions economically.
Use Cases:
Providing an intelligent documentation lookup service for coding assistants or IDE extensions.
Creating specialized AI ""micro-agents"" that can be consumed by larger AI applications.
Building internal knowledge base query systems accessible via a simple API-like interface.
Setup:
Ensure you have the necessary n8n credentials for Google Gemini (or your chosen LLM) and the Context7 MCP client tools.
The Path in the Context7 MCP Server Trigger node should be unique and secure.
Clients connect to the ""Production URL"" (SSE endpoint) provided by the trigger node.
This workflow is a great example of how n8n can serve as a powerful backend for building and deploying modular AI agents.
I've made a video to try and explain this a bit too https://www.youtube.com/watch?v=dudvmyp7Pyg"
Create PDF from Images for free via Google Slides and Google Drive,https://n8n.io/workflows/4540-create-pdf-from-images-for-free-via-google-slides-and-google-drive/,"This n8n template offers a free and automated way to convert images from a Google Drive folder into a single PDF document. It uses Google Slides as an intermediary, allowing you to control the final PDF's page size and orientation.
If you're looking for a no-cost solution to batch convert images to PDF and need flexibility over the output dimensions (like A4, landscape, or portrait), this template is for you! It's especially handy for creating photo albums, visual reports, or simple portfolios directly from your Google Drive.
How it works
The workflow first copies a Google Slides template you specify. The page setup of this template (e.g., A4 Portrait) dictates your final PDF's dimensions.
It then retrieves all images from a designated Google Drive folder, sorts them by creation date.
Each image is added to a new slide in the copied presentation.
Finally, the entire Google Slides presentation is converted into a PDF and saved back to your Google Drive.
How to use
Connect your Google Drive and Google Slides accounts in the relevant nodes.
In the ""Set Pdf File Name"" node, define the name for your output PDF.
In the ""CopyPdfTemplate"" node:
Select your Google Slides template file (this sets the PDF page size/orientation).
Choose the Google Drive folder containing your source images.
Ensure your images are in the specified folder. For best results, images should have an aspect ratio similar to your chosen Slides template.
Run the workflow to generate your PDF by clicking 'Test Workflow'
Requirements
Google Drive account.
Google Slides account.
Google Slides Template stored on your Google Drive
Customising this workflow
Adjust the ""Filter: Only Images"" node if you use image formats other than PNG (e.g., image/jpeg for JPGs).
Modify the image sorting logic in the ""Sort by Created Date"" node if needed."
Score Contact Form Leads with GPT-4 and Send Slack Notifications,https://n8n.io/workflows/4458-score-contact-form-leads-with-gpt-4-and-send-slack-notifications/,"üî• AI Lead Scoring Agent: Smart Contact Form Triager
Automatically score every contact form lead as Hot/Warm/Cold and alert your sales team instantly.
This intelligent workflow captures contact form submissions, uses GPT-4 to analyze message content and score lead quality, then sends formatted alerts to Slack - ensuring your sales team always focuses on the hottest prospects first.
üöÄ What It Does
Instant Lead Capture: Automatically receives contact form submissions via webhook endpoint
AI-Powered Scoring: GPT-4 analyzes message content and classifies leads as Hot üî•, Warm üå§, or Cold ‚ùÑÔ∏è
Smart Data Extraction: Cleanly extracts name, email, and message from form submissions
Real-Time Slack Alerts: Sends formatted notifications to your sales team with lead details and AI scoring
üéØ Key Benefits
‚úÖ Never Miss Hot Prospects: AI identifies urgent leads automatically
‚úÖ Save Sales Time: Focus effort on highest-probability leads first
‚úÖ Instant Team Alerts: Real-time notifications in Slack channels
‚úÖ Smart Prioritization: AI scoring eliminates guesswork in lead quality
‚úÖ Zero Manual Work: Complete automation from form to sales alert
‚úÖ Universal Integration: Works with any contact form or landing page
üè¢ Perfect For
Sales & Marketing Teams
SaaS companies managing inbound leads
Service businesses qualifying prospects
E-commerce stores identifying serious buyers
Agencies prioritizing client inquiries
Business Applications
Lead Qualification: Identify purchase-ready prospects instantly
Sales Efficiency: Focus team effort on highest-value opportunities
Response Prioritization: Handle urgent inquiries first
Team Coordination: Keep entire sales team informed of new leads
‚öôÔ∏è What's Included
Complete Workflow: Ready-to-deploy lead scoring automation
Webhook Endpoint: Receives submissions from any contact form
AI Classification: GPT-4 powered lead interest analysis
Slack Integration: Professional team notifications with emojis and formatting
Data Processing: Clean extraction and formatting of lead information
üîß Quick Setup Requirements
n8n Platform: Cloud or self-hosted instance
OpenAI API: GPT-4 access for lead scoring
Slack Workspace: Team channel for lead notifications
Contact Form: Any form that can POST to webhook endpoint
üì± Sample Slack Alert
üî• New Lead: Sarah Johnson (sarah@techstartup.com)

Message: ""We're looking for a project management solution for our 50-person team. Need to implement ASAP as we're scaling fast. Can we schedule a demo this week?""

Triage: üî• Hot
‚ùÑÔ∏è New Lead: John Smith (john@email.com)

Message: ""Just browsing your website. Might be interested in learning more someday.""

Triage: ‚ùÑÔ∏è Cold
üé® Customization Options
Scoring Criteria: Adjust AI prompts for industry-specific lead qualification
Team Channels: Route different lead types to specific Slack channels
Additional Fields: Capture company size, budget, timeline data
CRM Integration: Connect to Salesforce, HubSpot, or Pipedrive
Follow-up Automation: Trigger email sequences based on lead temperature
Analytics Tracking: Monitor lead quality trends and conversion rates
üè∑Ô∏è Tags & Categories
#lead-scoring #sales-automation #contact-form-processing #ai-qualification #slack-integration #prospect-management #inbound-marketing #sales-productivity #lead-generation #openai-integration #webhook-automation #crm-automation #sales-alerts #lead-triage #ai-agent
üí° Use Case Examples
SaaS Company: Score demo requests based on company size and urgency mentions
Consulting Firm: Identify clients ready to start projects vs those still researching
E-commerce Store: Spot bulk buyers and wholesale inquiries vs casual browsers
Marketing Agency: Prioritize clients with specific budgets and timelines mentioned
üìà Expected Results
70% faster lead response times through smart prioritization
3x higher conversion rates focusing on Hot leads first
50% time savings on manual lead qualification
100% lead coverage - never miss or ignore a prospect again
üõ†Ô∏è Setup & Support
5-Minute Setup: Simple webhook configuration with any contact form
Universal Integration: Works with WordPress, Webflow, custom forms, landing pages
Team Training: Clear Slack notification format anyone can understand
Scalable: Handles unlimited form submissions automatically
üìû Get Help & Resources
YouTube: https://www.youtube.com/@YaronBeen/videos
üíº Sales Automation Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
üìß Direct Help
Email: Yaron@nofluff.online - Response within 24 hours
Ready to never miss another hot lead?
Get this AI Lead Scoring Agent and transform your contact forms into intelligent lead qualification systems. Your sales team will always know which prospects to call first, and you'll never waste time on cold leads again.
Stop treating all leads equally. Start prioritizing the ones ready to buy."
Schedule Appointments via Telegram with GPT-4o & Google Calendar,https://n8n.io/workflows/4446-schedule-appointments-via-telegram-with-gpt-4o-and-google-calendar/,"üîß How It Works
Telegram Trigger ‚Äì Listens for incoming messages from users via your Telegram bot.
AI Agent ‚Äì Processes the message to determine the user's intent (booking or canceling) and extracts necessary details like date, time, and participant names.
Google Calendar Node ‚Äì Depending on the intent:
Booking: Creates a new event in Google Calendar with the extracted details.
Canceling: Searches for the specified event and deletes it from the calendar.
Telegram Node ‚Äì Sends a confirmation message back to the user, informing them of the successful booking or cancellation.
üß† Why This is Useful
Managing appointments can be time-consuming. This workflow automates the process, allowing users to schedule or cancel meetings effortlessly through a simple chat interface. It's ideal for:
Solopreneurs managing their own schedules.
Small businesses coordinating meetings with clients.
Anyone looking to streamline their appointment management process.
ü™ú Setup Instructions
Set Up Telegram Bot:
Create a new bot using BotFather on Telegram.
Obtain the API token and set up the Telegram Trigger node in n8n with this token.
OpenAI Platform API required for OpenAI Chat Model
Connect to Google Calendar
For the full video tutorial, watch here:
https://youtu.be/GzWO7_1lyI8"
"Auto-Respond to Gmail Inquiries using OpenAI, Google Sheet & AI Agent",https://n8n.io/workflows/4413-auto-respond-to-gmail-inquiries-using-openai-google-sheet-and-ai-agent/,"Who is this for?
This workflow is ideal for:
Customer support teams looking to reduce manual response time
SaaS companies that frequently receive product inquiries
E-commerce stores with common customer questions about orders, shipping, and returns
What problem is this workflow solving?
Manually responding to repetitive customer emails is inefficient, prone to inconsistency, and time-consuming. This workflow solves the issue by:
Automatically replying to real customer inquiries 24/7
Ensuring every response is consistent, friendly, and based on approved knowledge
Preventing responses to non-inquiries like newsletters or confirmations
Logging every interaction for traceability, analysis, and compliance
What this workflow does
This AI-powered Gmail auto-responder intelligently handles inbound emails with the following steps:
Monitors your Gmail inbox for new incoming emails in real time
Classifies each email as either an ‚ÄúInquiry‚Äù or ‚ÄúNot Inquiry‚Äù using GPT-4
Gets context from a Google Sheets FAQ database
The context will be used to determine the most accurate and helpful response
Generates a professional reply only if it‚Äôs a valid inquiry (e.g., pricing, refund, product details)
Builds a context-aware, helpful response using verified knowledge only
Sends the reply to the original sender automatically
Logs everything to a Google Sheet ‚Äî original email, AI response, timestamp, and email address
Example Use Case:
An email comes in:
""Hi, I want to know your pricing and refund policy.""
The workflow:
Detects it‚Äôs an inquiry
Finds the pricing and refund FAQs in your Google Sheet
Sends back a professional response like:
""Hi! Thanks for reaching out. Our pricing starts at $99/month. Refunds can be requested within 30 days of purchase. Let us know if you have more questions!""
Logs the interaction to your ‚ÄúEnquiry_Log‚Äù tab
Setup
Copy the Google Sheet template here:
üëâ Gmail Auto-Responder ‚Äì Google Sheet Template
This contains:
A FAQ_Context tab (your knowledge base)
An Enquiry_Log tab (interaction logs)
Connect your Gmail account to the Gmail Trigger and Gmail Send nodes
Add your OpenAI API key in the classification and response generator nodes
Link the Google Sheet in both the FAQ lookup and logging nodes
Test with a sample email ‚Äî try asking a pricing and refund question to see the complete process in action
How to customize this workflow to your needs
Adjust tone or brand voice in the AI prompt for a more casual or formal reply
Modify classification rules if your use case includes more custom logic
Expand the FAQ database to include new questions and answers
Add multilingual support by customizing the AI prompt to detect and respond in different languages
Integrate CRM or ticketing systems (like HubSpot, Zendesk, or Notion) to log or escalate unanswered queries"
Extract and Structure Thai Documents to Google Sheets using Typhoon OCR and Llama 3.1,https://n8n.io/workflows/4300-extract-and-structure-thai-documents-to-google-sheets-using-typhoon-ocr-and-llama-31/,"‚ö†Ô∏è Note: This template requires a community node and works only on self-hosted n8n installations. It uses the Typhoon OCR Python package and custom command execution. Make sure to install required dependencies locally.
Who is this for?
This template is for developers, operations teams, and automation builders in Thailand (or any Thai-speaking environment) who regularly process PDFs or scanned documents in Thai and want to extract structured text into a Google Sheet.
It is ideal for:
Local government document processing
Thai-language enterprise paperwork
AI automation pipelines requiring Thai OCR
What problem does this solve?
Typhoon OCR is one of the most accurate OCR tools for Thai text. However, integrating it into an end-to-end workflow usually requires manual scripting and data wrangling.
This template solves that by:
Running Typhoon OCR on PDF files
Using AI to extract structured data fields
Automatically storing results in Google Sheets
What this workflow does
Trigger: Run manually or from any automation source
Read Files: Load local PDF files from a doc/ folder
Execute Command: Run Typhoon OCR on each file using a Python command
LLM Extraction: Send the OCR markdown to an AI model (e.g., GPT-4 or OpenRouter) to extract fields
Code Node: Parse the LLM output as JSON
Google Sheets: Append structured data into a spreadsheet
Setup
1. Install Requirements
Python 3.10+
typhoon-ocr: pip install typhoon-ocr
Install Poppler and add to system PATH (needed for pdftoppm, pdfinfo)
2. Create folders
Create a folder called doc in the same directory where n8n runs (or mount it via Docker)
3. Google Sheet
Create a Google Sheet with the following column headers:
book_id date subject detail signed_by signed_by2 contact download_url
You can use this example Google Sheet as a reference.
4. API Key
Export your TYPHOON_OCR_API_KEY and OPENAI_API_KEY in your environment (or set inside the command string in Execute Command node).
How to customize this workflow
Replace the LLM provider in the Basic LLM Chain node (currently supports OpenRouter)
Change output fields to match your data structure (adjust the prompt and Google Sheet headers)
Add trigger nodes (e.g., Dropbox Upload, Webhook) to automate input
About Typhoon OCR
Typhoon is a multilingual LLM and toolkit optimized for Thai NLP. It includes typhoon-ocr, a Python OCR library designed for Thai-centric documents. It is open-source, highly accurate, and works well in automation pipelines. Perfect for government paperwork, PDF reports, and multilingual documents in Southeast Asia."
Gmail to Telegram: Email Summaries with OpenAI GPT-4o,https://n8n.io/workflows/4245-gmail-to-telegram-email-summaries-with-openai-gpt-4o/,"üßë‚Äçüíº Who is this for?
This workflow is for anyone who receives too many emails and wants to stay informed without drowning in their inbox.
If you're constantly checking your Gmail and wish you had someone summarizing messages and sending just the important parts to your phone, this is for you. Especially useful for solopreneurs, customer support, busy professionals, or newsletter addicts.
üß† What problem is this workflow solving?
Email is powerful, but also overwhelming. Important info gets buried in threads, and staying on top of things can mean hours wasted scanning messages.
This workflow turns that chaos into clarity: as soon as a new email arrives, you get a concise AI-generated summary in Telegram ‚Äî straight to your pocket.
No more checking Gmail constantly. No more missing key updates. Just a clean, human-style summary, written in the language you choose.
‚öôÔ∏è What this workflow does
Watches your Gmail inbox for new messages
Prepares the content, including sender, subject, and message body
Sends it to OpenAI to generate a friendly, casual summary
Delivers that summary to your Telegram chat
All in seconds, completely automated.
üõ†Ô∏è Setup
Connect your accounts: Gmail, Telegram, and OpenAI credentials must be added to the respective nodes.
Set your Telegram chat ID: Use a bot like @userinfobot to get it.
Customize the language in the Set summary language node (default is English).
Activate the workflow ‚Äî and watch it go.
üß© How to customize this workflow to your needs
You can make this workflow your own in a few easy ways:
Summarize only some emails: Add a Filter node after the Gmail trigger (e.g., only messages from certain senders).
Change the tone or detail of summaries: Tweak the system prompt in the Summary generation agent.
Use a different model: Swap OpenAI‚Äôs GPT-4o for another provider like Claude or DeepSeek.
Translate to your preferred language: Just change ""english"" to ""espa√±ol"", ""fran√ßais"", etc."
AI-Powered Telegram Bot for Data Extraction with Bright Data MCP,https://n8n.io/workflows/4116-ai-powered-telegram-bot-for-data-extraction-with-bright-data-mcp/,"üìå AI Agent Template with Bright Data MCP Tool Integration
This template enables natural-language-driven automation using Bright Data MCP tools. It extracts all available tools from MCP, processes the user‚Äôs query through an AI agent, then dynamically selects and executes the appropriate tool.
‚ùì Problem It Solves
Traditional automation often requires users to understand APIs, interfaces, or scripts to perform backend tasks. The Bright Data MCP integration solves this by allowing natural language interaction, intelligently classifying user intent, and managing context-aware execution of complex operations‚Äîideal for data extraction, customer support, and workflow orchestration.
üß∞ Pre-requisites
Before deploying this template, make sure you have:
An active N8N instance (self-hosted or cloud).
A valid OpenRouter API key (or another compatible AI model).
Telegram bot and its API token
Access to the Bright Data MCP API with credentials.
Basic familiarity with N8N workflows and nodes.
‚öôÔ∏è Setup Instructions
Setup and obtain API token and other necessary information from Bright Data
In your Bright Data account, obtain the following information:
API token
Web Unlocker zone name (optional)
Browser Zone name (optional)
Host SSE server from STDIO command
The methods below will allow you to receive SSE (Server-Sent Events) from Bright Data MCP via a local Supergateway or Smithery
Method 1: Run Supergateway in a separate web service (Recommended)
This method will work for both cloud version and self-hosted N8N.
Signup to any cloud services of your choice (DigitalOcean, Heroku, Hetzner, Render, etc.).
For NPM based installation:
Create a new web service.
Choose Node.js as runtime environment and setup a custom server without repository.
In your server‚Äôs settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token WEB_UNLOCKER_ZONE=optional_zone_name BROWSER_ZONE=optional_browser_zone_name
Paste the following text as a start command: npx -y supergateway --stdio ""npx -y @brightdata/mcp"" --port 8000 --baseUrl http://localhost:8000 --ssePath /sse --messagePath /message
Deploy it and copy the web server URL, then append /sse into it.
Your SSE server should now be accessible at: https://your_server_url/sse
For Docker based installation:
Create a new web service.
Choose Docker as the runtime environment.
Set up your Docker environment by pulling the necessary images or creating a custom Dockerfile.
In your server‚Äôs settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token WEB_UNLOCKER_ZONE=optional_zone_name BROWSER_AUTH=optional_browser_auth
Use the following Docker command to run Supergateway: docker run -it --rm -p 8000:8000 supercorp/supergateway \ --stdio ""npx -y @brightdata/mcp /"" \ --port 8000
Deploy it and copy the web server URL, then append /sse into it.
Your SSE server should now be accessible at: https://your_server_url/sse
For more installation guides, please refer to https://github.com/supercorp-ai/supergateway.git.
Method 2: Run Supergateway in the same web service as the N8N instance
This method will only work for self-hosted N8N.
a. Set Required Environment Variables
In your server's settings to define environment variables or .env file, add:
API_TOKEN=your_brightdata_api_token
WEB_UNLOCKER_ZONE=optional_zone_name
BROWSER_ZONE=optional_browser_zone_name
b. Run Supergateway in Background
npx -y supergateway --stdio ""npx -y @brightdata/mcp"" --port 8000 --baseUrl http://localhost:8000 --ssePath /sse --messagePath /message
Use the command above to execute it through the cloud shell or set it as a pre-deploy command.
Your SSE server should now be accessible at:
http://localhost:8000/sse
For more installation guides, please refer to https://github.com/supercorp-ai/supergateway.git.
Method 3: Configure via Smithery.ai (Easiest)
If you don't want additional setup and want to test it right away, follow these instructions:
Visit https://smithery.ai/server/@luminati-io/brightdata-mcp/tools to:
Signup (if you are new to Smithery)
Create an API key
Define environment variables via a profile
Retrieve your SSE server HTTP URL
Import the Workflow
Open N8N.
Import the JSON workflow file included with this template.
Update any nodes referencing external services (e.g., OpenRouter, Telegram).
Setup Telegram Integration
If you haven't setup a bot in Telegram, below is the instruction how to create one using BotFather:
Search for @BotFather in Telegram and start a conversation with it.
Send the command /newbot to create a new bot. You'll be prompted to enter a name and a unique username for your bot.
BotFather will provide you with an access token, which you'll need to use to interact with the bot's API.
Edit the HTTP Request node in the workflow.
Configure the URL as follows:
https://api.telegram.org/bot+your_telegram_bot_token+/setWebhook?url=+your_webhook_url
Replace +your_telegram_bot_token+ with your actual Telegram bot token.
Replace +your_webhook_url+ with the URL from the Webhook Trigger node in the workflow.
This will set up Telegram to forward messages to your n8n agent.
üîÑ Workflow Functionality (Summary)
The user submits a message via chat.
Memory nodes retain context for multi-turn conversations.
The mapped tool is executed and results are returned contextually.
üß† Optional memory buffers and memory manager nodes keep the interaction context-aware.
üß© Use Cases
Data Scraping on Demand: Launch scraping tasks via chat.
Lead Generation Bots: Enrich or validate leads with MCP tools.
AI-Powered Customer Support: Classify and answer queries with real-time data tools.
Workflow Assistants: Let teams run backend processes like lookups or report generation using plain language.
üõ†Ô∏è Customization
Classifier Prompt & Logic: Tweak the AI‚Äôs prompt and tool-matching schema to better fit your use case.
Memory Configuration: Adjust retention policies and context depth.
Tool Execution Sub-Workflow: Extend for retries, logging, or chaining actions.
Omni-Channel Support: Connect via webhooks to chat interfaces like Slack, WhatsApp, Telegram, or custom UIs.
‚úÖ Summary
This template equips you with a powerful no-code/low-code AI agent that translates conversation into real-world action. Using Bright Data‚Äôs MCP tools through natural language, it enables teams to automate and scale data-driven tasks effortlessly."
Automated Lead Research ‚Äì From LinkedIn to Ready-to-Send Report,https://n8n.io/workflows/4191-automated-lead-research-from-linkedin-to-ready-to-send-report/,"AI Prospect Researcher ‚Äì Automated Lead Intelligence Workflow
This workflow is built for professionals and teams who want to scale their B2B outreach with context-rich, personalized communication. It automates the full prospect research process ‚Äî from pulling lead data and scraping LinkedIn profiles, to gathering real-time company insights and generating high-quality outreach reports with GPT-4.
Using a combination of Apify, Perplexity AI, and OpenAI, this system creates a structured Google Doc for each lead, along with a logged summary in Google Sheets. Whether you‚Äôre preparing for sales calls, writing cold emails, or enriching your CRM ‚Äî this tool delivers ready-to-use intelligence in minutes, without manual research.
The process is modular, production-ready, and suitable for agencies, SDR teams, or founders managing outbound on their own.
How it works
Once triggered, the workflow takes in a list of leads from Google Sheets. For each lead, it uses Apify to scrape both the LinkedIn profile and company page (no login or cookies required). Then, Perplexity AI fetches contextual insights and competitor data. GPT-4 validates the research and synthesizes a structured summary of the individual and their company. Finally, a complete outreach report is generated and saved in Google Docs, while key data is logged in Sheets for tracking or follow-up automation.
This is a powerful, production-grade automation for anyone serious about personalizing outreach without spending hours per lead."
"Automated Tweet Generator & Publisher with GPT-4, Discord, and Google Sheets",https://n8n.io/workflows/4075-automated-tweet-generator-and-publisher-with-gpt-4-discord-and-google-sheets/,"AI Twitter Content Machine ‚Äì Write, Refine & Publish Tweets on Autopilot
This workflow is perfect for creators, solopreneurs, and personal brands who want to consistently publish bold, high-performing content on X (Twitter) ‚Äî without writing a single line themselves. After a one-time setup, it automatically generates tweet ideas, writes in your voice, evaluates post quality, avoids duplicates, and publishes directly to Twitter. All approvals and rewrites are handled in a conversational loop powered by OpenAI, Discord, and Google Sheets.
Whether you‚Äôre building a personal brand or growing your startup audience, this tool will help you stay active, edgy, and relevant ‚Äî with zero friction.
How it works
Distill what your flow does in a few high-level steps.
Loads your brand brief from a sub-workflow.
Generates a tweet idea aligned with your tone.
Checks Google Sheets to ensure the idea hasn‚Äôt been used.
Writes the post.
Evaluates it using a feedback sub-workflow ‚Äî if the quality score is below 0.7, it rewrites the post.
Refines tone and voice using a Rewriter Agent that mimics your past content (from a Google Sheet).
Sends the final post to a Discord channel for manual approval.
On approval, posts directly to Twitter (X) and logs it to Google Sheets (History and Examples tabs).
Set up steps
Give users an idea of how long setup will take. Don‚Äôt describe every detail.
Keep detailed descriptions in sticky notes inside your workflow.
Key benefits
No burnout, no block ‚Äì Stop spending energy thinking what to tweet. AI handles everything.
Style-matching ‚Äì Posts sound like you, not a generic robot. Based on your real writing.
Fast & scalable ‚Äì Publish once or five times a day ‚Äî it‚Äôs up to you.
Avoid duplicates ‚Äì Each idea is checked against your post history.
Human-in-the-loop ‚Äì You approve final posts via Discord. No rogue tweets.
Integrations required
n8n
OpenAI API
Google Sheets
Twitter (OAuth2)
Discord (for approval)
Notion (optional for brand brief storage)"
"AI Newsletter Builder: Crawl Sites with Dumpling AI, Summarize with GPT-4o",https://n8n.io/workflows/4030-ai-newsletter-builder-crawl-sites-with-dumpling-ai-summarize-with-gpt-4o/,"Who is this for?
This workflow is built for newsletter writers, marketers, content creators, or anyone who curates and summarizes web articles. It‚Äôs especially helpful for virtual assistants and founders who need to quickly turn web content into digestible, branded newsletters using AI.
What problem is this workflow solving?
Manually reading, summarizing, and formatting multiple articles into a newsletter takes time and focus. This workflow automates the process using Dumpling AI for crawling, GPT-4o for summarization, and Gmail for delivery‚Äîso you can go from raw URLs to a polished email in minutes.
What this workflow does
Starts manually (can also be scheduled)
Reads a list of article URLs from Google Sheets
Sends URLs to Dumpling AI to crawl and extract content
Splits each article into a single item for processing
Uses a Code node to clean and structure article data
Uses an Edit Fields node to merge articles into one JSON block
GPT-4o summarizes and generates HTML content for the newsletter
Sends the formatted newsletter via Gmail
Setup
Google Sheets
Create a sheet with a column (A) for article URLs
Update the Read URLs from Google Sheet node to use your Sheet ID and tab name
Connect your Google account in the credentials
Dumpling AI
Sign up at https://app.dumplingai.com
Create an agent for web crawling under /crawl
Add your Dumpling API key in the HTTP headers of the Crawl Content with Dumpling AI node
Split Node
Breaks apart the array of articles from Dumpling AI so each article is processed individually
Code Node
Structures each article as JSON with title, url, and cleaned text content
Edit Fields Node
Gathers all structured articles back into a single JSON array to prepare for AI summarization
OpenAI (GPT-4o)
Processes the article list and returns a formatted subject line and HTML newsletter content
Gmail
Connect your Gmail account to send the AI-generated newsletter to your inbox or team
Update the recipient field in the Send HTML Email via Gmail node
How to customize this workflow to your needs
Replace the manual trigger with a Schedule node to send newsletters weekly
Modify the GPT-4o prompt to change tone (e.g., more professional, funny, casual)
Add filtering logic to skip low-value articles
Connect Slack, Airtable, or Notion for internal team usage
Change Gmail to SendGrid or Outlook if preferred
Final Notes
This workflow uses:
Dumpling AI /crawl endpoint to extract article content
Split, Code, and Edit Fields nodes to format multi-article input
GPT-4o for summarization and HTML formatting
Gmail for delivery
This setup eliminates manual steps and delivers fast, consistent newsletters powered by AI."
Automate Support Ticket Triage and Resolution with JIRA and AI,https://n8n.io/workflows/3868-automate-support-ticket-triage-and-resolution-with-jira-and-ai/,"This n8n template automates triaging of newly opened support tickets and issue resolution via JIRA.
If your organisation deals with a large number of support requests daily, automating triaging is a great use-case for introducing AI to your support teams. Extending the idea, we can also get AI to give a first attempt at resolving the issue intelligently.
How it works
A scheduled trigger picks up newly opened JIRA support tickets from the queue and discards any seen before.
An AI agent analyses the open ticket to add labels, priority on the seriousness of the issue and simplifies the description for better readability and understanding for human support.
Next, the agent attempts to address and resolve the issue by finding similar issues (by tags) which have been resolved.
Each similar issue has its comments analysed and summarised to identify the actual resolution and facts.
These summarises are then used as context for the AI agent to suggest a fix to the open ticket.
How to use
Simply connect your JIRA instance to the workflow and activate to start watching for open tickets. Depending on frequency, you may need to increase for decrease the intervals.
Define labels to use in the agent's system prompt.
Restrict to certain projects or issue types to suit your organisation.
Requirements
JIRA for issue management and support portal
OpenAI for LLM
Customising this workflow
Not using JIRA? Try swapping out the nodes for Linear or your issue management system of choice.
Try a different approach for issue resolution. You might want to try RAG approach where a knowledge base is used."
Build your own PostgreSQL MCP server,https://n8n.io/workflows/3631-build-your-own-postgresql-mcp-server/,"This n8n demonstrates how to build a simple PostgreSQL MCP server to manage your PostgreSQL database such as HR, Payroll, Sale, Inventory and More!
This MCP example is based off an official MCP reference implementation which can be found here -https://github.com/modelcontextprotocol/servers/tree/main/src/postgres
How it works
A MCP server trigger is used and connected to 5 tools: 2 postgreSQL and 3 custom workflow.
The 2 postgreSQL tools are simple read-only queries and as such, the postgreSQL tool can be simply used.
The 3 custom workflow tools are used for select, insert and update queries as these are operations which require a bit more discretion.
Whilst it may be easier to allow the agent to use raw SQL queries, we may find it a little safer to just allow for the parameters instead. The custom workflow tool allows us to define this restricted schema for tool input which we'll use to construct the SQL statement ourselves.
All 3 custom workflow tools trigger the same ""Execute workflow"" trigger in this very template which has a switch to route the operation to the correct handler.
Finally, we use our standard PostgreSQL node to handle select, insert and update operations. The responses are then sent back to the the MCP client.
How to use
This PostgreSQL MCP server allows any compatible MCP client to manage a PostgreSQL database by supporting select, create and update operations. You will need to have a database available before you can use this server.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""Please help me check if Alex has an entry in the users table. If not, please help me create a record for her.""
""What was the top selling product in the last week?""
""How many high priority support tickets are still open this morning?""
Requirements
PostgreSQL for database. This can be an external database such as Supabase or one you can host internally.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
If the scope of schemas or tables is too open, try restrict it so the MCP serves a specific purpose for business operations. eg. Confine the querying and editing to HR only tables before providing access to people in that department.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
"AI Agent: Scrape, Summarize & Save Articles to Notion (Gemini, Browserless)",https://n8n.io/workflows/3535-ai-agent-scrape-summarize-and-save-articles-to-notion-gemini-browserless/,"This n8n workflow automates the process of saving web articles or links shared in a chat conversation directly into a Notion database, using Google's Gemini AI and Browserless for web scraping.
Who is this AI automation template for?
It's useful for anyone wanting to reduce manual copy-pasting and organize web findings seamlessly within Notion. A smarter web clipping tool!
What this AI automation workflow does
Starts when a message is received
Uses a Google Gemini AI Agent node to understand the context and manage the subsequent steps. It identifies if a message contains a request to save an article/link.
If a URL is detected, it utilizes a tool configured with the Browserless API (via the HTTP Request node) to scrape the content of the web page.
Creates a new page in a specified Notion database, populating it with thea summary scraped content, in a specific format, never leaving out any important details. It also saves the original URL, smart tags, publication date, and other metadata extracted by the AI.
Posts a confirmation message (e.g., to a Discord channel) indicating whether the article was saved successfully or if an error occurred.
Setup
Import Workflow: Import this template into your n8n instance.
Configure Credentials & Notion Database:
Notion Database:
Create or designate a Notion database (like the example ""Knowledge Database"") where articles will be saved.
Ensure this database has the following properties (fields):
Name (Type: Text) - This will store the article title.
URL (Type: URL) - This will store the original article link.
Description (Type: Text) - This can store the AI-generated summary.
Tags (Type: Multi-select) - Optional, for categorization.
Publication Date (Type: Date) - *Optional,
store the date the article was published.
Ensure the n8n integration has access to this specific database.
If you require a different format to the Notion Database, not that you will have to update the Notion tool configuration in this n8n workflow accordingly.
Notion Credential: Obtain your Notion API key and add it as a Notion credential in n8n. Select this credential in the save_to_notion tool node.
Configure save_to_notion Tool: In the save_to_notion tool node within the workflow, set the 'Database ID' field to the ID of the Notion database you prepared above. Map the workflow data (URL, AI summary, etc.) to the corresponding database properties (URL, Description, etc.). In the blocks section of the notion tool, you can define a custom format for the research page, allowing the AI to fill in the exact details you want extracted from any web page!
Google Gemini AI: Obtain your API key from Google AI Studio or Google Cloud Console (if using Vertex AI) and add it as a credential. Select this credential in the ""Tools Agent"" node.
Discord (or other notification service): If using Discord notifications, create a Webhook URL (instructions) or set up a Bot Token. Add the credential in n8n and select it in the discord_notification tool node. Configure the target Channel ID.
Browserless/HTTP Request:
Cloud: Obtain your API key from Browserless and configure the website_scraper HTTP Request tool node with the correct API endpoint and authentication header.
Self-Hosted: Ensure your Browserless Docker container is running and accessible by n8n. Configure the website_scraper HTTP Request tool node with your self-hosted Browserless instance URL.
Activate Workflow: Save test and activate the workflow.
How to customize this workflow to your needs
Change AI Model: Experiment with different AI models supported by n8n (like OpenAI GPT models or Anthropic Claude) in the Agent node if Gemini 2.5 Pro doesn't fit your needs or budget, keeping in mind potential differences in context window size and processing capabilities for large content.
Modify Notion Saving: Adjust the save_to_notion tool node to map different data fields (e.g., change the summary style by modifying the AI prompt, add specific tags, or alter the page content structure) to your Notion database properties.
Adjust Scraping: Modify the prompt/instructions for the website_scraper tool or change the parameters sent to the Browserless API if you need different data extracted from the web pages. You could also swap Browserless for another scraping service/API accessible via the HTTP Request node."
Smart Email Auto-Responder Template using AI,https://n8n.io/workflows/3277-smart-email-auto-responder-template-using-ai/,"Smart Email Auto-Responder with AI Classification
Automatically Categorize and Reply to Emails using LangChain + Google Gemini + Gmail + SMTP + Brevo
This n8n workflow is designed to intelligently manage incoming emails and automatically send personalized responses based on the content. It classifies emails using LangChain's Text Classifier, sends HTML responses depending on the category, and updates Gmail and Brevo CRM accordingly.
Key Features
Triggers and Classifies Emails
Listens for new Gmail messages every hour
Uses AI-based classification to identify the type of inquiry For Example:
Guest Post
YouTube Review
Udemy Course Inquiry
Responds Automatically
Sends professional HTML replies customized for each type
Uses SMTP to deliver emails from your domain
Enhances Workflow with Automation
Marks processed emails as read
Applies Gmail labels
Adds sender to Brevo contact list
Optional AI Chat Integration
Uses Google Gemini (PaLM 2) to enhance classification or summarization
Tools & Integrations Required
Gmail account (OAuth2)
LangChain (Text Classifier node)
Google Gemini API account
SMTP credentials (e.g., Gmail SMTP, Brevo, etc.)
Brevo/Sendinblue account and API key
Step-by-Step Node Guide
1. Gmail Trigger
Polls Gmail every hour for new emails.
Filters out internal addresses (e.g., @syncbricks.com).
Avoids replying to already-responded emails (Re: subject filter).
2. LangChain Text Classifier
Uses AI to categorize the content of the email based on pre-defined categories:
Guest Post
Youtube
Udemy Courses
3. Google Gemini (PaLM) Chat Model (Optional)
Provides additional AI support to enhance classification accuracy.
Can be used to summarize or enrich the context if needed.
4. Email Send Nodes
Each response category has a separate SMTP node with a custom HTML email:
Guest Post Inquiry
YouTube Video Inquiry
Udemy Course Inquiry
5. Gmail: Mark as Read
Marks the email so it isn‚Äôt processed again.
6. Gmail: Apply Label
Adds a label (e.g., Handled by Bot) for organization.
7. Brevo: Create/Update Contact
Saves the sender to your CRM for future communication or marketing.
Email Templates Included
Guest Post Template
Includes pricing, website list, submission guidelines, and payment instructions.
YouTube Review Template
Includes package pricing, review samples, video thumbnails, and inquiry instructions.
Step by Step Tutorial
GET n8n Now
N8N COURSE
n8n Book
More courses:
http://lms.syncbricks.com
YouTube Channel:
https://youtube.com/@syncbricks
How to Use
Import the template into your n8n instance.
Configure your Gmail OAuth2 and SMTP credentials.
Set up your LangChain Text Classifier and Google Gemini API credentials.
Update label ID in the Gmail node and ensure all custom fields like from.value[0].name match your use case.
Run the workflow and watch it respond intelligently to new inquiries.
Best Practices
Always test with mock emails first.
Keep the Google Gemini node optional if you want to reduce cost/API calls.
Use Gmail filters to auto-label certain types of emails.
Monitor your Brevo contacts to track new leads.
Attribution & Support
Developed by Amjid Ali
This template took extensive time and effort to build. If you find it useful, please consider supporting my work.
Buy My Book:
Mastering n8n on Amazon
Full Courses & Tutorials:
http://lms.syncbricks.com
Follow Me Online:
LinkedIn: https://linkedin.com/in/amjidali
Website: https://amjidali.com
YouTube: https://youtube.com/@syncbricks"
Generate AI Videos from Text with HeyGen and Voice Cloning.,https://n8n.io/workflows/3054-generate-ai-videos-from-text-with-heygen-and-voice-cloning/,"üé• AI Video Generator with HeyGen
üöÄ Create AI-Powered Videos in n8n with HeyGen
This workflow enables you to generate realistic AI videos using HeyGen, an advanced AI platform for video automation. Simply input your text, choose an AI avatar and voice, and let HeyGen generate a high-quality video for you ‚Äì all within n8n!
‚úÖ Ideal for:
Content creators & marketers üèÜ
Automating personalized video messages üì©
AI-powered video tutorials & training materials üéì
üîß How It Works
1Ô∏è‚É£ Provide a text script ‚Äì This will be spoken in the AI-generated video.
2Ô∏è‚É£ Select an Avatar & Voice ‚Äì Choose from a variety of AI-generated avatars and voices.
3Ô∏è‚É£ Run the workflow ‚Äì HeyGen processes your request and generates a video.
4Ô∏è‚É£ Download your video ‚Äì Get the direct link to your AI-powered video!
‚ö° Setup Instructions
1Ô∏è‚É£ Get Your HeyGen API Key
Sign up for a HeyGen account.
Go to your account settings and retrieve your API Key.
2Ô∏è‚É£ Configure n8n Credentials
In n8n, create new credentials and select ""Custom Auth"" as the authentication type.
In the Name provide : X-Api-Key
And in the value paste your API key from Heygen
Update the 2 http node with the right credentials.
3Ô∏è‚É£ Select an AI Avatar & Voice
Browse available avatars & voices in your HeyGen account.
Copy the Avatar ID and Voice ID for your video.
4Ô∏è‚É£ Run the Workflow
Enter your text, avatar ID, and voice ID.
Execute the workflow ‚Äì your video will be generated automatically!
üéØ Why Use This Workflow?
‚úîÔ∏è Fully Automated ‚Äì No manual editing required!
‚úîÔ∏è Realistic AI Avatars ‚Äì Choose from a variety of digital avatars.
‚úîÔ∏è Seamless Integration ‚Äì Works directly within your n8n workflow.
‚úîÔ∏è Scalable & Fast ‚Äì Generate multiple videos in minutes.
üîó Start automating AI-powered video creation today with n8n & HeyGen!"
Open Deep Research - AI-Powered Autonomous Research Workflow,https://n8n.io/workflows/2883-open-deep-research-ai-powered-autonomous-research-workflow/,"Open Deep Research - AI-Powered Autonomous Research Workflow
Description
This workflow automates deep research by leveraging AI-driven search queries, web scraping, content analysis, and structured reporting. It enables autonomous research with iterative refinement, allowing users to collect, analyze, and summarize high-quality information efficiently.
How it works
üîπ User Input
The user submits a research topic via a chat message.
üß† AI Query Generation
A Basic LLM generates up to four refined search queries to retrieve relevant information.
üîé SERPAPI Google Search
The workflow loops through each generated query and retrieves top search results using the SerpAPI API.
üìÑ Jina AI Web Scraping
Extracts and summarizes webpage content from the URLs obtained via SerpAPI.
üìä AI-Powered Content Evaluation
An AI Agent evaluates the relevance and credibility of the extracted content.
üîÅ Iterative Search Refinement
If the AI finds insufficient or low-quality information, it generates new search queries to improve results.
üìú Final Report Generation
The AI compiles a structured markdown report, including sources with citations.
Set Up Instructions
üöÄ Estimated setup time: ~10-15 minutes
‚úÖ Required API Keys:
SerpAPI ‚Üí For Google Search results
Jina AI ‚Üí For text extraction
OpenRouter ‚Üí For AI-driven query generation and summarization
‚öôÔ∏è n8n Components Used:
AI Agents with memory buffering for iterative research
Loops to process multiple search queries efficiently
HTTP Requests for direct API interactions with SerpAPI and Jina AI
üìù Recommended Enhancements:
Add sticky notes in n8n to explain each step for new users
Implement Google Drive or Notion Integration to save reports automatically
üéØ Ideal for:
‚úîÔ∏è Researchers & Analysts - Automate background research
‚úîÔ∏è Journalists - Quickly gather reliable sources
‚úîÔ∏è Developers - Learn how to integrate multiple AI APIs into n8n
‚úîÔ∏è Students - Speed up literature reviews
üîó Completely free and open-source! üöÄ"
Respond to WhatsApp Messages with AI Like a Pro!,https://n8n.io/workflows/2466-respond-to-whatsapp-messages-with-ai-like-a-pro/,"This n8n template demonstrates the beginnings of building your own n8n-powered WhatsApp chatbot! Under the hood, utilise n8n's powerful AI features to handle different message types and use an AI agent to respond to the user. A powerful tool for any use-case!
How it works
Incoming WhatsApp Trigger provides a way to get messages into the workflow.
The message received is extracted and sent through 1 of 4 branches for processing.
Each processing branch uses AI to analyse, summarize or transcribe the message so that the AI agent can understand it. The supported types are text, image, audio (voice notes) and video.
The AI Agent is used to generate a response generally and uses a wikipedia tool for more complex queries.
Finally, the response message is sent back to the WhatsApp user using the WhatsApp node.
How to use
Once you have setup and configured your WhatsApp account, you'll need to activate your workflow to start processing messages.
Good to know: Large media files may negatively impact workflow performance.
Requirements
WhatsApp Buisness account
Google Gemini for LLM. Gemini is used specifically because it can accept audio and video files whereas at time of writing, many other providers like OpenAI's GPT, do not.
Customising this workflow
For performance reasons, consider detecting large audio and video before sending to the LLM. Pre-processing such files may allow your agent to perform better.
Go beyond and create rich and engagement customer experiences by responding using images, audio and video instead of just text!"
"Automate RSS Content to Blog Posts with GPT-4o, WordPress & LinkedIn Publishing",https://n8n.io/workflows/4226-automate-rss-content-to-blog-posts-with-gpt-4o-wordpress-and-linkedin-publishing/,"Automated Blog Post Review and Multi-Platform Publishing Workflow with RSS Feeds
Description
This workflow automates the process of generating, reviewing, and publishing blog posts across multiple platforms, now enhanced with support for RSS Feeds as a content source.
It streamlines the management of blog posts by fetching content from RSS Feeds, formatting, storing, reviewing, templating, and publishing to platforms like LinkedIn and WordPress.
The workflow is split into three key flows:
Initial Flow: Fetches content from RSS Feeds, prepares and stores blog post data, sends a review email with approval/rejection links.
Approval Flow: Handles review actions via a webhook to update the status in Google Sheets.
Status Update Flow: Monitors status changes and publishes approved posts.
Target Audience
Content creators, bloggers, and digital marketers.
Teams managing multi-platform content publishing.
Users familiar with n8n, Google Sheets, LinkedIn, and RSS Feeds.
Problem Solved
Manually managing blog posts, especially when sourcing content from RSS Feeds, can be time-consuming and error-prone
This workflow addresses:
Content Sourcing: Fetches blog posts from RSS Feeds for automated processing
Content Formatting: Automatically formats and stores blog posts.
Review Process: Simplifies approval with email notifications and webhook triggers.
Multi-Platform Publishing: Publishes to LinkedIn, WordPress and optionally Medium) with delays to avoid rate limits
Status Tracking: Tracks approval and publishing status in Google Sheets.
Setup Instructions
Prerequisites
n8n Instance: Ensure you have an active n8n instance
RSS Feed URL: Identify an RSS Feed URL (e.g., a blog‚Äôs feed like https://example.com/feed)
Google Sheets: Create a spreadsheet with columns: Title, Blogpost, Publication Date, Keywords, Status, Published, Featured Image, articleUrl, Rendered Blog.
Sheet Name: Posts Initial
Add a dropdown for Status: Pending, Approved, Rejected.
Gmail Account: For sending review and notification emails.
LinkedIn Account: For publishing posts (OAuth credentials needed).
Optional: WordPress.com or Medium account for additional publishing.
Customization Guidance
Below is a detailed breakdown of each flow and node, including setup instructions.
üîπ Initial Flow: Fetch from RSS Feeds, Prepare, and Send for Review.
Purpose: Fetches blog posts from an RSS Feed, formats them, extracts images, stores data, and sends a review email.
Fetch from RSS Feed
Type: RSS Feed
Purpose: Retrieves blog posts from an RSS Feed
Configuration:
URL: https://example.com/feed (replace with your RSS Feed URL)
Limit: 1 (or adjust based on your needs)
Setup: Ensure the RSS Feed URL is valid and accessible; test the node to verify it fetches posts
Set Fields
Type: Set
Purpose: Maps RSS Feed data to blog post fields
Setup: Adjust field mappings based on your RSS Feed‚Äôs structure
Format Blog Post for Storage
Type: Code
Purpose: Cleans up the blog post content.
Extract Featured Image
Type: Code
Purpose: Extracts or generates a featured image URL.
Setup: Ensure originalHtml contains image data; otherwise, it uses a placeholder.
Store Blog Posts Initial
Type: Google Sheets
Purpose: Stores initial blog post data
Setup: Ensure Google Sheets credentials are set up and the spreadsheet has the required columns.
Set Fields for Email
Type: Set
Purpose: Prepares fields for the review email.
Setup: Replace https://your-n8n-instance with your n8n instance URL.
Prepare Email HTML
Type: Code
Purpose: Generates HTML email content with conditional image display
Setup: No additional configuration needed
Notify for Review (Gmail)
Type: Gmail
Purpose: Sends a review email with approval/rejection links
üîπ Approval Flow: Handle Review Actions
Purpose: Updates the blog post status based on approval/rejection
Webhook Trigger
Type: Webhook
Purpose: Triggers on approval/rejection link clicks
Configuration:
HTTP Method: GET
Path: approve-post
Response Code: 200
Response Data: {""message"": ""Status updated""}
Setup: Ensure the webhook URL matches the one in Set Fields for Email
Find Row to Update
Type: Google Sheets
Purpose: Retrieves all rows to find the matching blog post
Filter Row by Title
Type: Code
Purpose: Filters the row matching the blog post title
Setup: No additional configuration needed
Update Status on Approval
Type: Google Sheets
Purpose: Updates the status to Approved or Rejected
üîπ Status Update Flow: Publish Approved Posts
Purpose: Monitors status changes and publishes approved posts
Google Sheets Trigger (Fetch Row)
Type: Google Sheets Trigger
Purpose: Triggers when a row‚Äôs status is updated
Configuration:
Event: Update
Sheet Name: Posts Initial
Output Fields: title, status, published, featuredImage, articleUrl
Setup: Ensure Google Sheets credentials are set up
Router (Check Status)
Type: Router
Purpose: Routes based on status and published state
Configuration:
Route 1: Approved and Not Published
Condition: status equals Approved AND published equals NO
Route 2: Rejected
Condition: status equals Rejected
Route 3: Pending
Condition: status equals Pending
Setup: No additional configuration needed
Apply Blog Template
Store Blog Posts Final
Type: Google Sheets
Purpose: Stores the final HTML content
Configuration:
Operation: Update Row
Setup: Ensure the Rendered Blog column exists
Loop Over Blog Posts
Type: Split in Batches
Purpose: Processes each blog post individually
Configuration: Default settings
Setup: No additional configuration needed
Delay Between Posts
Type: Wait
Purpose: Adds a delay to avoid rate limits
Configuration:
Wait Type: Delay
Amount: 1 second
Setup: Adjust delay as needed for LinkedIn rate limits
Publish to LinkedIn
Type: LinkedIn
Purpose: Publishes the blog post to LinkedIn
Configuration:
Operation: Share Post
Author: urn:li:person:YOUR_PERSONAL_URN
Setup: Set up LinkedIn OAuth credentials and replace YOUR_PERSONAL_URN with your LinkedIn URN
Update Published State
Type: Google Sheets
Purpose: Updates the published status
Configuration:
Operation: Update Row
Setup: Ensure the Published column exists
Notify Team
Type: Gmail
Purpose: Notifies the team of successful publishing
Configuration:
The blog post ""{{ $json.title }}"" has been successfully published
Setup: Set up Gmail credentials; replace [Link] with the LinkedIn URL if captured
Notify Rejection (Gmail) (Route 2)
Type: Gmail
Purpose: Notifies on rejection
The blog post ""{{ $json.title }}"" has been rejected
Suggestions: Rewrite with more engaging content, adjust keywords, or verify facts
Please update the status in Google Sheets if you wish to revise and resubmit
Setup: Set up Gmail credentials
Wait for Status Update (Route 3)
Type: Wait
Purpose: Delays for status recheck
Configuration:
Wait Type: Delay
Duration: 24h
Setup: Adjust delay as needed
Conclusion
This workflow streamlines blog post management with RSS Feeds, making it ideal
for busy content creators and teams.
Customize it by adding more platforms adjusting delays, or enhancing notifications.
Share your feedback in the n8n community to help others benefit from this automation."
Create an Automated Customer Support Assistant with GPT-4o and GoHighLevel SMS,https://n8n.io/workflows/4223-create-an-automated-customer-support-assistant-with-gpt-4o-and-gohighlevel-sms/,"üìå AI Agent via GoHighLevel SMS with Website-Based Knowledgebase
This n8n workflow enables an AI agent to interact with users through GoHighLevel SMS, leveraging a knowledgebase dynamically built by scraping the company's website.
‚ùì Problem It Solves
Traditional customer support systems often require manual data entry and lack real-time updates from the company's website. This workflow automates the process by:
Scraping the company's website at set intervals to update the knowledgebase.
Integrating with GoHighLevel SMS to provide users with timely and accurate information.
Utilizing AI to interpret user queries and fetch relevant information from the updated knowledgebase.
üß∞ Pre-requisites
Before deploying this workflow, ensure you have:
An active n8n instance (self-hosted or cloud).
A valid OpenAI API key (or any compatible AI model).
A Bright Data account with Web Unlocker setup.
A GoHighLevel SMS LeadConnector account.
A GoHighLevel Marketplace App configured with the necessary scopes.
Installed n8n-nodes-brightdata community node for Bright Data integration (if self-hosted).
‚öôÔ∏è Setup Instructions
1. Install the Bright Data Community Node in n8n
For self-hosted n8n instances:
Navigate to Settings ‚Üí Community Nodes.
Click on Install.
In the search bar, enter n8n-nodes-brightdata.
Select the node from the list and click Install.
Docs: https://docs.n8n.io/integrations/community-nodes/installation/gui-install
2. Configure Bright Data Credentials
Obtain your API key from Bright Data.
In n8n, go to Credentials ‚Üí New, select HTTP Request.
Set authentication to Header Auth.
In Name, enter Authorization.
In Value, enter Bearer &lt;your_api_key_from_Bright_Data&gt;.
Save the credentials.
3. Configure OpenAI Credentials
Add your OpenAI API key to the relevant nodes.
If you want to use a different model, replace all OpenAI nodes accordingly.
4. Set Up GoHighLevel Integration
a. Create a GoHighLevel Marketplace App
Go to https://marketplace.gohighlevel.com
Click My Apps ‚Üí Create App
Set Distribution Type to Sub-Account
Add the following scopes:
locations.readonly contacts.readonly contacts.write opportunities.readonly opportunities.write users.readonly conversations/message.readonly conversations/message.write
Add your n8n OAuth Redirect URL as a redirect URI in the app settings.
Save and copy the Client ID and Client Secret.
b. Configure GoHighLevel Credentials in n8n
Go to Credentials ‚Üí New
Choose OAuth2 API
Input:
Client ID
Client Secret
Authorization URL: https://auth.gohighlevel.com/oauth/authorize
Access Token URL: https://auth.gohighlevel.com/oauth/token
Scopes:
locations.readonly contacts.readonly contacts.write opportunities.readonly opportunities.write users.readonly conversations/message.readonly conversations/message.write
Save and authenticate to complete setup.
Docs: https://docs.n8n.io/integrations/builtin/credentials/highlevel
üîÑ Workflow Functionality (Summary)
Scheduled Scraping: Scrapes website at user-defined intervals.
Edit Fields node: User defines the homepage or site to scrape.
Bright Data Node (self-hosted) OR HTTP Node (cloud users) used to perform scraping.
Knowledgebase Update: The scraped content is stored or indexed.
GoHighLevel SMS: Incoming user queries are received through SMS.
AI Processing: AI matches queries to relevant content.
Response Delivery: AI-generated answers are sent back via SMS.
üß© Use Cases
Customer Support Automation: Provide instant, accurate responses.
Lead Qualification: Automatically answer potential customer inquiries.
Internal Knowledge Distribution: Keep staff updated via SMS based on website info.
üõ†Ô∏è Customization
Scraping URLs: Adjust targets in the Edit Fields node.
Model Swap: Replace OpenAI nodes to use a different LLM.
Format Response: Customize output to match your tone or brand.
Other Channels: Expand to include chat apps or email responses.
Vector Databases: It is advisable to store the data into a third-party vector database services like Pinecone, Supabase, etc.
Chat Memory Node: This workflow is using Redis as a chat memory but you can use N8N built-in chat memory.
‚úÖ Summary
This n8n workflow combines Bright Data‚Äôs scraping tools and GoHighLevel‚Äôs SMS interface with AI query handling to deliver a real-time, conversational support experience. Ideal for businesses that want to turn their website into a live knowledge source via SMS, this agent keeps itself updated, smart, and customer-ready."
Automate Outbound Sales Calls to Qualified Leads with VAPI.ai and Google Sheets,https://n8n.io/workflows/4152-automate-outbound-sales-calls-to-qualified-leads-with-vapiai-and-google-sheets/,"This workflow automates outbound calls to qualified leads using VAPI.ai and Google Sheets. Here's how it works and how to set it up.
How It Works
Read Leads: The workflow starts by reading leads from a Google Sheet where the ""AI call status"" is marked as ""NO""
Batch Processing: Leads are processed one at a time (batch size = 1) to ensure proper sequencing
Variable Setup: Extracts the phone number and row number from each lead record
Trigger VAPI Call: Makes an API call to VAPI.ai to initiate an AI-powered outbound call
Update Status: Marks the lead as ""YES"" in the Google Sheet after the call is triggered to prevent duplicate calls
Detailed Setup Guide
Prerequisites
n8n instance (self-hosted or cloud)
Google Sheets account with OAuth2 credentials
VAPI.ai account with API access
Step 1: Google Sheets Setup
Create a Google Sheet with your leads data
Ensure you have these columns (adjust if needed):
Phone number (column E in the current setup)
AI call status (column F in the current setup)
Mark all leads you want to call with ""NO"" in the status column
Step 2: Google Sheets Credentials
In n8n, go to Credentials > Add New
Select ""Google Sheets OAuth2 API""
Follow the prompts to authenticate with your Google account
Name it (e.g., ""Google Sheets account 3"" as in the example)
Step 3: VAPI.ai Setup
Get your VAPI.ai API credentials
In n8n, go to Credentials > Add New
Select ""HTTP Header Auth""
Add your VAPI authorization header (typically ""Bearer YOUR_API_KEY"")
Name it (e.g., ""Header Auth account 4"" as in the example)"
Build an On-Premises AI Kaggle Competition Assistant with Qdrant RAG and Ollama,https://n8n.io/workflows/3967-build-an-on-premises-ai-kaggle-competition-assistant-with-qdrant-rag-and-ollama/,"LLM/RAG Kaggle Development Assistant
An on-premises, domain-specific AI assistant for Kaggle (tested on binary disaster-tweet classification), combining LLM, an n8n workflow engine, and Qdrant-backed Retrieval-Augmented Generation (RAG).
Deploy via containerized starter kit.
Needs high end GPU support or patience.
Initial chat should contain guidelines on what to to produce and the challenge guidelines.
Features
Coding Assistance
‚Ä¢ ""Real""-time Python code recommendations, debugging help, and data-science best practices
‚Ä¢ Multi-turn conversational context
Workflow Automation
‚Ä¢ n8n orchestration for LLM calls, document ingestion, and external API integrations
Retrieval-Augmented Generation (RAG)
‚Ä¢ Qdrant vector-database for competition-specific document lookup
‚Ä¢ On-demand retrieval of Kaggle competition guidelines, tutorials, and notebooks after convertion to HTML and ingestion into RAG
entirly On-Premises for Privacy
‚Ä¢ Locally hosted LLM (via Ollama) ‚Äì no external code or data transfer
ALIENTELLIGENCE/contentsummarizer:latest for summarizing
qwen3:8b for chat and coding
mxbai-embed-large:latest for embedding
‚Ä¢ GPU acceleration required
Based on:
https://n8n.io/workflows/2339 breakdown documents into study notes using templating mistralai and qdrant/"
Transform T-Shirt Mockups to Print-Ready Designs with GPT-4 Vision & Image AI,https://n8n.io/workflows/3959-transform-t-shirt-mockups-to-print-ready-designs-with-gpt-4-vision-and-image-ai/,"üß† What This Workflow Does
This n8n workflow allows you to upload a T-shirt mockup design (even if it's rough or outdated), and automatically turns it into a refined, print-ready artwork using the power of AI.
It starts with an image of a T-shirt design, analyzes it using OpenAI's vision model, and then generates a cleaner, upgraded prompt to be used with OpenAI‚Äôs image generation API (gpt-image-1). The final output is a new T-shirt graphic optimized for printing on solid black background, with no visible shirt or mockup framing.
‚öôÔ∏è How It Works
User Sends a T-shirt Mockup Image Link
The workflow begins when the user drops an image link (T-shirt mockup) into a chat interface or input trigger.
AI Analyzes the Image (OpenAI Vision)
Using OpenAI‚Äôs GPT-4 vision capabilities, the workflow extracts the key design elements from the image:
Characters, text, layout
Graphic style, composition
Visual tone and focus
AI Agent Creates a Refined Prompt
The extracted details are passed to an AI agent that:
Preserves the original layout and message
Enhances the visual composition and typography
Removes mockup elements like shirt collar, sleeves, shadows.
Locks the artwork on a pure black background only
Outputs a clean, artistic, JSON-safe one-line prompt for generation
Text Escaping for API Compatibility
A JavaScript function node escapes the prompt (quotes, slashes, line breaks) to make it safe for use in downstream JSON requests.
Image Generation via GPT-Image-1 API
The final prompt is sent to OpenAI‚Äôs gpt-image-1 to generate a brand-new artwork ‚Äî ideal for direct printing on a black T-shirt.
‚ö†Ô∏è Cost Notice for gpt-image-1 Usage
This workflow uses OpenAI's gpt-image-1 model to generate high-quality T-shirt artwork from refined prompts. Please note that this model is a paid service, and each image generation request may cost approximately $0.25 per design, depending on resolution and usage.
We strongly recommend users to review their OpenAI API usage plan and be mindful of costs when running this workflow, especially if generating in bulk or integrating into larger automation flows.
You can monitor your usage at: https://platform.openai.com/docs/models/gpt-image-1
(Optional) You can send the result to Telegram, upload to Notion, or store it in your design system.
‚úÖ Key Features
Works from any uploaded mockup image
Converts design concepts into print-ready artwork prompts
Avoids outputting shirt models, collars, or product mockups
Optimized for solid black background with no distractions
Modular and easy to connect with file delivery or approval flows
üöÄ How to Use
Import the .json workflow into n8n
Configure your OpenAI credentials for both vision and image APIs
Trigger the flow by sending an image url of a T-shirt mockup
Let the workflow generate and return a brand-new design from that concept"
Build your own Google Drive MCP server,https://n8n.io/workflows/3634-build-your-own-google-drive-mcp-server/,"This n8n demonstrates how to build a simple Google Drive MCP server to search and get contents of files from Google Drive.
This MCP example is based off an official MCP reference implementation which can be found here -https://github.com/modelcontextprotocol/servers/tree/main/src/gdrive
How it works
A MCP server trigger is used and connected to 1x Google Drive tool and 1x Custom Workflow tool.
The Google Drive tool is set to perform a search on files within our Google Drive folder.
The Custom Workflow tool downloads target files found in our drive and converts the binaries to their text representation. Eg. PDFs have only their text contents extracted and returned to the MCP client.
How to use
This Google Drive MCP server allows any compatible MCP client to manage a person or shared Google Drive. Simple select a drive or for better control, specify a folder within the drive to scope the operations to.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""Please help me search for last month's expense reports.""
""What does the company policy document say about cancellations and refunds?""
Requirements
Google Drive for documents.
OpenAI for image and audio understanding.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
Add additional capabilities such as renaming, moving and/or deleting files.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
Gmail MCP Server ‚Äì Your All‚Äëin‚ÄëOne AI Email Toolkit,https://n8n.io/workflows/3605-gmail-mcp-server-your-allinone-ai-email-toolkit/,"Gmail MCP Server
Expose Gmail‚Äôs full API as a single SSE ‚Äútool server‚Äù endpoint for your AI agents.
What it does
Spins up an MCP Trigger that streams Server‚ÄëSent Events to LangChain/N8N AI Agent nodes.
Maps 20+ common Gmail operations (search, send, reply, draft, label & thread management, mark read/unread, delete, etc.) to ai_tool connections, so agents can invoke them with a simple JSON payload.
Why you‚Äôll love it
Agent‚Äëready: Plug the SSE URL into any N8N Agent or any other AI tool that uses MCP and start reasoning over email immediately.
Extensible: Add more GmailTool operations or swap credentials without touching your agent logic.
How to use
Import the workflow (n8n ‚â• v1.88).
Set up a gmailOAuth2 credential and select it on the GmailTool nodes.
Open the Gmail MCP Server node, copy the SSE URL, and paste it into your AI agent‚Äôs ‚ÄúTool Server‚Äù field."
Automate Web Interactions with Claude 3.5 Haiku and Airtop Browser Agent,https://n8n.io/workflows/3592-automate-web-interactions-with-claude-35-haiku-and-airtop-browser-agent/,"About this AI Agent
This workflow is designed to automate web interactions by simulating a human user, using a combination of the Agent node and AI tools powered by Airtop.
How does this workflow works?
Form Submission Trigger: The workflow starts with a form submission trigger node named ""On form submission"". This node collects user instructions for the web AI agent, including a prompt and an optional Airtop profile name for sites requiring authentication.
AI Agent: The core of the workflow is the ""AI Agent"" node, which uses a smart web agent to manage a remote web browser. It is designed to fulfill user requests by interacting with the browser through various tools.
Browser Session Management
Start Browser: The ""Start browser"" node initiates a new browser session and window. It is essential for obtaining the sessionId and windowId required for subsequent operations.
Session and Window Management: The workflow includes nodes for creating and managing browser sessions and windows, such as ""Session"" and ""Window"".
Web Interaction Tools:
Load URL: This node loads a specified URL into the browser window.
Query: The ""Query"" node allows the agent to ask questions and extract information from the current web page.
Click: This node simulates clicking on elements within the web page.
Type: The ""Type"" node types text into specified elements on the page.
Session Termination: The ""End session"" node is used to terminate the browser session once the tasks are completed.
Output Handling
Structured Output Parser: This node processes the agent's results into a structured format.
Output: The final results are set and prepared for output.
Slack Integration: Although currently disabled, there is a ""Slack"" node intended to send messages to a Slack channel, potentially for notifications or live view URLs.
Seting up your agent
Airtop API Credentials:
Users must have valid Airtop API credentials to interact with the web browser tools. This includes nodes like ""Click"", ""Query"", ""Load URL"", ""End session"", ""Type"", ""Session"", and ""Window"".
Slack API Credentials (Optional):
If users want to enable Slack notifications, they need to configure Slack OAuth2 credentials. The Slack node is currently disabled but can be used to send messages to a Slack channel.
Anthropic API Credentials:
The ""Claude 3.5 Haiku"" node requires Anthropic API credentials. Users need to have access to this API to utilize the language model features."
Automated Interview Scheduling with GPT-4o and Google Calendar Chat Bot,https://n8n.io/workflows/3363-automated-interview-scheduling-with-gpt-4o-and-google-calendar-chat-bot/,"‚ú® Overview
This workflow allows candidates to schedule interviews through a conversational AI assistant. It integrates with your Google Calendar to check for existing events and generates a list of available 30-minute weekday slots between 9 AM and 5 PM Eastern Time. Once the candidate selects a suitable time and provides their contact information, the AI bot automatically books the meeting on your calendar and confirms the appointment.
‚ö° Prerequisites
To use this workflow, you need an OpenAI account with access to the GPT-4o model, a Google account with a calendar that can be accessed through the Google Calendar API, and an active instance of n8n‚Äîeither self-hosted or via n8n cloud. Within n8n, you must have two credential configurations ready: one for Google Calendar using OAuth2 authentication, and another for your OpenAI API key.
üîê API Credentials Setup
For Google Calendar, go to the Google Cloud Console and create a new project. Enable the Google Calendar API, then create OAuth2 credentials by selecting ‚ÄúWeb Application‚Äù as the application type. Add http://localhost:5678/rest/oauth2-credential/callback as the redirect URI if using local n8n. After that, go to n8n, navigate to the Credentials section, and create a new Google Calendar OAuth2 credential using your account. For OpenAI, visit platform.openai.com to retrieve your API key. Then go to the n8n Credentials page, create a new credential for OpenAI, paste your key, and name it for reference.
üîß How to Make This Workflow Yours
To customize the workflow for your use, start by replacing all instances of the calendar email rbreen.ynteractive@gmail.com with your own Google Calendar email. This email is referenced in multiple places, including Google Calendar nodes and the ToolWorkflow JSON for the node named ""Run Get Availability."" Also update any instances where the Google Calendar credential is labeled as Google Calendar account to match your own credential name within n8n. Do the same for the OpenAI credential label, replacing OpenAi account with the name of your own credential.
Next, go to the node labeled Candidate Chat and copy the webhook URL. This is the public chat interface where candidates will engage with the bot‚Äîshare this URL with them through email, your website, or anywhere you want to allow access. Optionally, you can also tweak the system message in the Interview Scheduler node to modify the tone, language, or logic used during conversations. If you want to add branding, update the title, subtitle, and inputPlaceholder in the Candidate Chat node, and consider modifying the final confirmation message in Final Response to User to reflect your brand voice. You can also update the business rules such as time zone, working hours, or default duration by editing the logic in the Generate 30 Minute Timeslots code node.
üß© Workflow Explanation
This workflow begins with the Candidate Chat node, which triggers when a user visits the public chat URL. The Interview Scheduler node acts as an AI agent, guiding the user through providing their email, phone number, and preferred interview time. It checks availability using the Run Get Availability tool, which in turn reads your calendar and compares it with generated free time slots from the Generate 30 Minute Timeslots node. The check day names tool helps the AI interpret natural language date expressions like ‚Äúnext Tuesday.‚Äù
The schedule is only populated with 30-minute weekday slots from 9 AM to 5 PM Eastern Time, and no events are scheduled if they overlap with existing ones. When a suitable time is confirmed, the AI formats the result into structured JSON, creates an event on your Google Calendar, and sends a confirmation back to the user with all relevant meeting details.
üöÄ Deployment Steps
To deploy the interview scheduler, import the provided workflow JSON into your n8n instance. Update the Google Calendar email, OpenAI and Google credential labels, system prompts, and branding as needed. Test the connections to ensure the API credentials are working correctly. Once everything is configured, copy and share the public chat URL from the Candidate Chat node. When candidates engage with the chat, the workflow will walk them through the interview booking process, check your availability, and finalize the booking automatically.
üí° Additional Tips
By default, the workflow avoids scheduling interviews on weekends and outside of 9‚Äì5 EST. Each interview lasts exactly 30 minutes, and overlapping with existing events is prevented. The assistant does not reveal details about other meetings. You can customize every part of this workflow to fit your use case, including subworkflows like Get Availability and check day names, or even white-label it for client use. This workflow is ready to become your AI-powered interview scheduling assistant."
Automate Sales for Digital Products & SaaS with AI (GPT-4o),https://n8n.io/workflows/3342-automate-sales-for-digital-products-and-saas-with-ai-gpt-4o/,"Skyrocket Your Sales Outreach with AI-Powered Automation!
Tired of manually finding leads, collecting emails, and sending messages that get ignored? Let AI do the heavy lifting.
Introducing AI-Powered Cold Outreach Engine, an n8n workflow that automates prospecting, email discovery, and personalized outreach, designed for digital products, SaaS, and online services.
üöÄ How It Works
1Ô∏è‚É£ Smart Prospecting
Enter your product name, description, and link.
The AI searches Google Maps for businesses in your niche.
It extracts website URLs, filtering out irrelevant results.
2Ô∏è‚É£ Email Discovery
Scrapes professional emails from websites.
Generates a clean, targeted list.
3Ô∏è‚É£ AI-Powered, SEO-Optimized Emails
GPT-4o analyzes website content.
Crafts concise, personalized outreach emails (<200 words).
Uses SEO-friendly language with strategic keywords.
Embeds your product link naturally in a compelling CTA.
Sends via Gmail or SMTP with smart delays for better deliverability.
üî• Why It Stands Out
‚úÖ Saves Time ‚Äì Automates lead generation & outreach.
‚úÖ Scales Effortlessly ‚Äì Finds and targets ideal prospects.
‚úÖ SEO-Optimized ‚Äì GPT-4o enhances discoverability.
‚úÖ Boosts Replies ‚Äì Personalized emails = higher engagement.
‚úÖ Drives Conversions ‚Äì Directs traffic to your product page.
üîß What You Need
n8n (Cloud or self-hosted).
OpenAI GPT-4o (API costs apply).
Gmail + Google Cloud OR SMTP node for email sending.
Optional: Jina AI for advanced data extraction.
‚ö° Quick Setup
Import the workflow into n8n.
Connect GPT-4o & Gmail or SMTP.
Add your product details.
Test & launch üöÄ
üí° Sell Smarter, Not Harder
This isn‚Äôt just automation‚Äîit‚Äôs a growth engine. Let AI handle outreach while you focus on scaling.
üîó Get started today!
‚ö†Ô∏è Disclaimer
API Fees Apply ‚Äì OpenAI, Google services may have costs.
Email Compliance ‚Äì Follow Gmail/SMTP limits & anti-spam laws (e.g., CAN-SPAM, GDPR).
Scraping Updates ‚Äì Website structures may change over time."
"IT Support Chatbot with Google Drive, Pinecone & Gemini | AI Doc Processing",https://n8n.io/workflows/3192-it-support-chatbot-with-google-drive-pinecone-and-gemini-or-ai-doc-processing/,"This n8n template empowers IT support teams by automating document ingestion and instant query resolution through a conversational AI. It integrates Google Drive, Pinecone, and a Chat AI agent (using Google Gemini/OpenRouter) to transform static support documents into an interactive, searchable knowledge base. With two interlinked workflows‚Äîone for processing support documents and one for handling chat queries‚Äîemployees receive fast, context-aware answers directly from your support documentation.
Overview
Document Ingestion Workflow
Google Drive Trigger: Monitors a specified folder for new file uploads (e.g., updated support documents).
File Download & Extraction: Automatically downloads new files and extracts text content.
Data Cleaning & Text Splitting: Utilizes a Code node to remove line breaks, trim extra spaces, and strip special characters, while a text splitter segments the content into manageable chunks.
Embedding & Storage: Generates text embeddings using Google Gemini and stores them in a Pinecone vector store for rapid similarity search.
Chat Query Workflow
Chat Trigger: Initiates when an employee sends a support query.
Vector Search & Context Retrieval: Retrieves the top relevant document segments from Pinecone based on similarity scores.
Prompt Construction: A Code node combines the retrieved document snippets with the user‚Äôs query into a detailed prompt.
AI Agent Response: The constructed prompt is sent to an AI agent (using OpenRouter Chat Model) to generate a clear, step-by-step solution.
Key Benefits & Use Case
Imagine a large organization where every IT support document‚Äîfrom troubleshooting guides to system configurations‚Äîis stored in a single Google Drive folder. When an employee encounters an issue (e.g., ‚ÄúHow do I reset my VPN credentials?‚Äù), they simply type the query into a chat interface. Instantly, the workflow retrieves the most relevant context from the ingested documents and provides a detailed, actionable answer. This process reduces resolution times, enhances support consistency, and significantly lightens the load on IT staff.
Prerequisites
A valid Google Drive account with access to the designated folder.
A Pinecone account for storing and retrieving text embeddings.
Google Gemini (or OpenRouter) credentials to power the Chat AI agent.
An operational n8n instance configured with the necessary nodes and credentials.
Workflow Details
1 Document Ingestion Workflow
Google Drive Trigger Node:
Listens for file creation events in the specified folder.
Google Drive Download Node:
Downloads the newly added file.
Extract from File Node:
Extracts text content from the downloaded file.
Code Node (Data Cleaning):
Cleans the extracted text by removing line breaks, trimming spaces, and eliminating special characters.
Recursive Text Splitter Node:
Segments the cleaned text into manageable chunks.
Pinecone Vector Store Node:
Generates embeddings (via Google Gemini) and uploads the chunks to Pinecone.
2 Chat Query Workflow
Chat Trigger Node:
Receives incoming user queries.
Pinecone Vector Store Node (Query):
Searches for relevant document chunks based on the query.
Code Node (Context Builder):
Sorts the retrieved documents by relevance and constructs a prompt merging the context with the query.
AI Agent Node:
Sends the prompt to the Chat AI agent, which returns a detailed answer.
How to Use
Import the Template:
Import the template into your n8n instance.
Configure the Google Drive Trigger:
Set the folder ID (e.g., 1RQvAHIw8cQbtwI9ZvdVV0k0x6TM6H12P) and connect your Google Drive credentials.
Set Up Pinecone Nodes:
Enter your Pinecone index details and credentials.
Configure the Chat AI Agent:
Provide your Google Gemini (or OpenRouter) API credentials.
Test the Workflows:
Validate the document ingestion workflow by uploading a sample support document.
Validate the chat query workflow by sending a test query and verifying the returned support information.
Additional Notes
Ensure all credentials (Google Drive, Pinecone, and Chat AI) are correctly set up and tested before deploying the workflows in production.
The template is fully customizable. Adjust the text cleaning, splitting parameters, or the number of document chunks retrieved based on your support documentation's size and structure.
This template not only enhances IT support efficiency but also offers a scalable solution for managing and leveraging growing volumes of support content."
Get Real-time Crypto Token Insights via Telegram with DexScreener and GPT-4o,https://n8n.io/workflows/3178-get-real-time-crypto-token-insights-via-telegram-with-dexscreener-and-gpt-4o/,"Instantly access real-time decentralized exchange (DEX) insights directly in Telegram! This workflow integrates the DexScreener API with GPT-4o-powered AI and Telegram, allowing users to fetch the latest blockchain token analytics, liquidity pools, and trending tokens effortlessly. Ideal for crypto traders, DeFi analysts, and investors who need actionable market data at their fingertips.
How It Works
A Telegram bot listens for user queries about tokens or trading pairs.
The workflow interacts with the DexScreener API (no API key required) to fetch real-time data, including:
Token fundamentals (profiles, images, descriptions, and links)
Trending and boosted tokens (hyped projects, potential market movers)
Trading pair analytics (liquidity, price action, volumes, volatility)
Order and payment activity (transaction insights, investor movements)
Liquidity pool depth (market stability, capital flows)
Multi-chain pair comparisons (performance tracking across networks)
An AI-powered language model (GPT-4o-mini) enhances responses for better insights.
The workflow logs session data to improve user interaction tracking.
The requested DEX insights are sent back via Telegram in an easy-to-read format.
What You Can Do with This Agent
This AI-driven Telegram bot enables you to:
‚úÖ Track trending and boosted tokens before they gain mainstream traction.
‚úÖ Monitor real-time liquidity pools to assess token stability.
‚úÖ Analyze active trading pairs across different blockchains.
‚úÖ Identify transaction trends by checking paid orders for tokens.
‚úÖ Compare market activity with detailed trading pair analysis.
‚úÖ Receive instant insights with AI-enhanced responses for deeper understanding.
Set Up Steps
Create a Telegram Bot
Use @BotFather on Telegram to create a bot and obtain an API token.
Configure Telegram API Credentials in n8n
Add your Telegram bot token under Telegram API credentials.
Deploy and Test
Send a query (e.g., ""SOL/USDC"") to your Telegram bot and receive real-time insights instantly!
üöÄ Unlock powerful, real-time DEX insights directly in Telegram‚Äîno API key required!
üì∫ Setup Video Tutorial
Watch the full setup guide on YouTube:"
"Build an AI-Powered Tech Radar Advisor with SQL DB, RAG, and Routing Agents",https://n8n.io/workflows/3151-build-an-ai-powered-tech-radar-advisor-with-sql-db-rag-and-routing-agents/,"AI-Powered Tech Radar Advisor
This project is built on top of the famous open source ThoughtWorks Tech Radar.
You can use this template to build your own AI-Powered Tech Radar Advisor for your company or group of companies.
Target Audience
This template is perfect for:
Tech Audit & Governance Leaders: Those seeking to build a tech landscape AI platform portal.
Tech Leaders & Architects: Those aiming to provide modern AI platforms that help others understand the rationale behind strategic technology adoption.
Product Managers: Professionals looking to align product innovation with the company's current tech trends.
IT & Engineering Teams: Teams that need to aggregate, analyze, and visualize technology data from multiple sources efficiently.
Digital Transformation Experts: Innovators aiming to leverage AI for actionable insights and strategic recommendations.
Data Analysts & Scientists: Individuals who want to combine structured SQL analysis with advanced semantic search using vector databases.
Developers: Those interested in integrating RAG chatbot functionality with conversation storage.
1. Description
Tech Constellation is an AI-powered Tech Radar solution designed to help organizations visualize and steer their technology adoption strategy.
It seamlessly ingests data from a Tech Radar Google Sheet‚Äîconverting it into both a MySQL database and a vector index‚Äîto consolidate your tech landscape in one place.
The platform integrates an interactive AI chat interface powered by four specialized agents:
AI Agent Router: Analyzes and routes user queries to the most suitable processing agent.
SQL Agent: Executes precise SQL queries on structured data.
RAG Agent: Leverages semantic, vector-based search for in-depth insights.
Output Guardrail Agent: Validates responses to ensure they remain on-topic and accurate.
This powerful template is perfect for technology leaders, product managers, engineering teams, and digital transformation experts looking to make data-driven decisions aligned with strategic initiatives across groups of parent-child companies.
2. Features
Data Ingestion
A Google Sheet containing tech radar data is used as the primary source.
The data is ingested and converted into a MySQL database.
Simultaneously, the data is indexed into a vector database for semantic (vector-based) search.
Interactive AI Chat
Chat Integration: An AI-powered chat interface allows users to ask questions about the tech radar.
Customizable AI Agents:
AI Agent Router: Determines the query type and routes it to the appropriate agent.
SQL Agent: Processes queries using SQL on structured data.
RAG Agent: Performs vector-based searches on document-like data.
Output Guardrail Agent: Validates queries and ensures that the responses remain on-topic and accurate.
Usage Examples
Tell me, is TechnologyABC adopted or on hold, and why?
List all the tools that are considered part of the strategic direction for company3 but are not adopted.
Project Links & Additional Details
GitHub Repository (Frontend Interface Source Code): github.com/dragonjump/techconstellation
Try It: https://scaler.my"
"Technical stock analysis with Telegram, Airtable and a GPT-powered AI Agent",https://n8n.io/workflows/3053-technical-stock-analysis-with-telegram-airtable-and-a-gpt-powered-ai-agent/,"Video Guide
I prepared a detailed guide that demonstrates the complete process of building a trading agent automation using n8n and Telegram, seamlessly integrating various functions for stock analysis.
Youtube Link
Who is this for?
This workflow is perfect for traders, financial analysts, and developers looking to automate stock analysis interactions via Telegram. It‚Äôs especially valuable for those who want to leverage AI tools for technical analysis without needing to write complex code.
What problem does this workflow solve?
Many traders desire real-time analysis of stock data but lack the technical expertise or tools to perform in-depth analysis. This workflow allows users to easily interact with an AI trading agent through Telegram for seamless stock analysis, chart generation, and technical evaluation, all while eliminating the need for manual interventions.
What this workflow does
This workflow utilizes n8n to construct an end-to-end automation process for stock analysis through Telegram communication. The setup involves:
Receiving messages via a Telegram bot.
Processing audio or text messages for trading queries.
Transcribing audio using OpenAI API for interpretation.
Gathering and displaying charts based on user-specified parameters.
Performing technical analysis on generated charts.
Sending back the analyzed results through Telegram.
Setup
Prepare Airtable:
Create simple table to store tickers.
Prepare Telegram Bot:
Ensure your Telegram bot is set up correctly and listening for new messages.
Replace Credentials:
Update all nodes with the correct credentials and API keys for services involved.
Configure API Endpoints:
Ensure chart service URLs are correctly set to interact with the corresponding APIs properly.
Start Interaction:
Message your bot to initiate analysis; specify ticker symbols and desired chart styles as required."
Analyze Reddit Posts with AI to Identify Business Opportunities,https://n8n.io/workflows/2978-analyze-reddit-posts-with-ai-to-identify-business-opportunities/,"Use case
Manually monitoring Reddit for viable business ideas is time-consuming and inconsistent. This workflow automatically analyzes trending Reddit discussions using AI to surface high-potential opportunities, filter irrelevant content, and generate actionable insights - saving entrepreneurs 10+ hours weekly in market research.
What this workflow does
This AI-powered workflow automatically collects trending Reddit discussions, analyzes posts for viable business opportunities using GPT-4, applies smart filters to exclude low-value content, and generates scored opportunity reports with market insights. It identifies unmet customer needs through sentiment analysis, prioritizes high-potential ideas using custom criteria, and outputs structured data to Google Sheets for actionable decision-making.
Setup
Add Reddit,Google and OpenAI credentials
Configure target subreddits in Subreddit node
Test workflow by testing workflow
Review generated opportunity report in Google Sheets
How to adjust this template
Change data sources: Replace Reddit trigger with Twitter/X or Hacker News API
Modify criteria: Adjust scoring thresholds in Opportunity Calculator node
Add integrations:
Create automatic Slack alerts for urgent opportunities
Generate draft business plans using AI Document Writer"
‚ö°üìΩÔ∏è Ultimate AI-Powered Chatbot for YouTube Summarization & Analysis,https://n8n.io/workflows/2956-ultimate-ai-powered-chatbot-for-youtube-summarization-and-analysis/,"üé• YouTube Video AI Agent Workflow
This n8n workflow template allows you to interact with an AI agent that extracts details and the transcript of a YouTube video using a provided video ID. Once the details and transcript are retrieved, you can chat with the AI agent to explore or analyze the video's content in a conversational and insightful manner.
üåü How the Workflow Works
üîó Input Video ID: The user provides a YouTube video ID as input to the workflow.
üìÑ Data Retrieval: The workflow fetches essential details about the video (e.g., title, description, upload date) and retrieves its transcript using YouTube's Data API and additional tools for transcript extraction.
ü§ñ AI Agent Interaction: The extracted details and transcript are processed by an AI-powered agent. Users can then ask questions or engage in a conversation with the agent about the video's content, such as:
Summarizing the transcript.
Analyzing key points.
Clarifying specific sections.
üí¨ Dynamic Responses: The AI agent uses natural language processing (NLP) to generate contextual and accurate responses based on the video data, ensuring a smooth and intuitive interaction.
üöÄ Use Cases
üìä Content Analysis: Quickly analyze long YouTube videos by querying specific sections or extracting summaries.
üìö Research and Learning: Gain insights from educational videos or tutorials without watching them entirely.
‚úçÔ∏è Content Creation: Repurpose transcripts into blogs, social media posts, or other formats efficiently.
‚ôø Accessibility: Provide an alternative, text-based way to interact with video content for users who prefer reading over watching.
üõ†Ô∏è Resources for Getting Started
Google Cloud Console (for API setup): Visit Google Cloud's Get Started Guide to configure your API access.
YouTube Data API Key Setup: Follow this guide to create and manage your YouTube Data API key.
Install n8n Locally: Refer to this installation guide for setting up n8n on your local machine.
‚ú® Sample Prompts
""Tell me about this YouTube video with id: JWfNLF_g_V0""
""Can you provide a list of key takeaways from this video with id: [youtube-video-id]?"""
AI Youtube Trend Finder Based On Niche,https://n8n.io/workflows/2606-ai-youtube-trend-finder-based-on-niche/,"Youtube Video
This n8n workflow is designed to assist YouTube content creators in identifying trending topics within a specific niche. By leveraging YouTube's search and data APIs, it gathers and analyzes video performance metrics from the past two days to provide insights into what content is gaining traction. Here's how the workflow operates:
Trigger Setup: The workflow begins when a user sends a query through the chat_message_received node. If no niche is provided, the AI prompts the user to select or input one.
AI Agent (Language Model): The central node utilizes a GPT-based AI agent to:
Understand the user's niche or content preferences.
Generate tailored search terms related to the niche.
Process YouTube API responses and summarize trends using insights such as common themes, tags, and audience engagement metrics (views, likes, and comments).
YouTube Search: The youtube_search node runs a secondary workflow to query YouTube for relevant videos published within the last two days. It retrieves basic video data such as video IDs, relevance scores, and publication dates.
Video Details Retrieval: The workflow fetches additional details for each video:
Video Snippet: Metadata like title, description, and tags.
Video Statistics: Metrics such as views, likes, and comments.
Content Details: Video duration, ensuring only content longer than 3 minutes and 30 seconds is analyzed.
Data Processing:
Video metadata is cleaned, sanitized, and stored in memory.
Tags, titles, and descriptions are analyzed to identify patterns and trends across multiple videos.
Output: The workflow compiles insights and presents them to the user, highlighting:
The most common themes or patterns within the niche.
URLs to trending videos and their respective channels.
Engagement statistics, helping the user understand the popularity of the content.
Key Notes for Setup:
API Keys: Ensure valid YouTube API credentials are configured in the get_videos, find_video_snippet, find_video_statistics, and find_video_data nodes.
Memory Buffer: The window_buffer_memory node ensures the AI agent retains context during analysis, enhancing the quality of the generated insights.
Search Term Customization: The AI agent dynamically creates search terms based on the user‚Äôs niche to improve search precision.
Use Case:
This workflow is ideal for YouTubers or marketers seeking data-driven inspiration for creating content that aligns with current trends, maximizing the potential to engage their audience.
Example Output:
For the niche ""digital marketing"":
Trending Topic: Videos about ""mental triggers"" and ""psychological marketing.""
Tags: ""SEO,"" ""Conversion Rates,"" ""Social Proof.""
Engagement: Videos with over 200K views and high likes/comment ratios are leading trends.
Video links:
https://www.youtube.com/watch?v=video_id1
https://www.youtube.com/watch?v=video_id2"
Turn Monoprix Delivery Emails into Calendar Events using ChatGPT and Google Calendar,https://n8n.io/workflows/4771-turn-monoprix-delivery-emails-into-calendar-events-using-chatgpt-and-google-calendar/,"Intro:
The purpose of this workflow is to simply convert you planned Grocery delivery confirmation email to a Google Calendar event in your family calendar. While based on a Monoprix.fr email format, it is applicable/adaptable to almost anything else.
How it works:
It is triggered by reception of the confirmation email on your Gmail. The workflow then extracts relevant data using ChatGPT, formats it, and creates a Google Calendar event.
Steps to use it:
Import template in your n8n
Update credentials for Gmail, Google Calendar, and ChatGPT
Test workflow based on confirmation email received
Activate workflow"
AI Email Classifier & Auto-Delete for Gmail (SPAM/OFFER Cleaner),https://n8n.io/workflows/4507-ai-email-classifier-and-auto-delete-for-gmail-spamoffer-cleaner/,"This workflow is designed for freelancers, solopreneurs, and business owners who receive a high volume of irrelevant messages in their Gmail inbox ‚Äî from cold offers to spammy promotions ‚Äî and want to automatically filter and delete them using AI. Its main purpose is to scan new emails with the help of OpenAI, classify their content, and automatically delete those considered marketing (OFFER) or junk (SPAM). The result is a cleaner inbox without the need to manually sift through low-value messages.
The classification logic uses a detailed system prompt with practical examples, so even complex or borderline messages are categorized accurately. Important emails ‚Äî such as payment confirmations, shipping updates, or genuine business inquiries ‚Äî remain untouched. This helps maintain a professional inbox with only valuable and relevant communication.
The entire process runs automatically in the background and can be customized further ‚Äî for example, to archive instead of delete, or log deleted emails for review.
How it works
When triggered (every hour), the workflow fetches new Gmail messages using the Gmail Trigger node. Each message is passed to an AI classifier powered by OpenAI, which reads the message body (email snippet) and returns one of three labels:
SPAM: Obvious junk messages, scams, or low-effort bulk messages
OFFER: Cold outreach, discount promotions, cart reminders, or generic advertising
IMPORTANT: Valuable information for the user, even if commercial (e.g., invoices, order updates, personal inquiries)
The workflow then routes the result through an IF node. If the message is marked as SPAM or OFFER, it is immediately deleted from Gmail via the Gmail Delete node. Emails marked as IMPORTANT are ignored and remain in the inbox.
The classification is entirely AI-driven based on message content ‚Äî sender address, headers, or metadata are not used.
How to set up
To get started, simply connect two credentials:
A Gmail account using OAuth2 (via the Gmail Trigger and Gmail Delete nodes)
An OpenAI API key (used by the AI classifier node)
No advanced setup is needed beyond these two connections.
Optionally, you can review or modify the system prompt used for classification ‚Äî it‚Äôs available inside the workflow‚Äôs LangChain AI Agent node. The prompt is in English, so it‚Äôs recommended to use this workflow with English-language emails for best results.
By default, the workflow deletes matching emails immediately. If you prefer safer testing, you can modify the Gmail node to archive, label, or log emails instead of deleting them.
The full workflow takes around 5‚Äì10 minutes to configure and includes a sticky note with additional instructions and warnings."
"AI-Powered Lead Scoring with Salesforce, GPT-4o, and Slack with Data Masking",https://n8n.io/workflows/4592-ai-powered-lead-scoring-with-salesforce-gpt-4o-and-slack-with-data-masking/,"Boost your sales team‚Äôs efficiency with an end-to-end, privacy-first lead-scoring engine‚Äîready to drop straight into your n8n instance.
üîπ What it does
Salesforce Trigger watches for new or updated Leads every hour.
HTTP Request fetches the full record so you never miss a field.
Mask Data (JS Code) automatically tokenises PII (name, email, address, etc.) before any external call‚Äîideal for GDPR/SOC 2 compliance.
OpenAI (GPT-4o) scores each lead 0-100, assigns a grade A-F, lists key reasons, recommends one next action, and even drafts a personalised email template.
Unmask Data (JS Code) swaps the tokens back in only when you explicitly need them‚Äîso sensitive data never leaks to logs or AI prompts.
Slack Node delivers a concise, team-friendly summary (score, grade, reasons, next step, and draft email) right to the rep who needs it.
üîπ Why you‚Äôll love it
Security by design ‚Äì field-level masking with reversible tokens.
No-code friendly ‚Äì clear sticky notes explain every step; swap Salesforce for any CRM in minutes.
AI you can trust ‚Äì scoring rubric baked into the system prompt for consistent results.
Instant hand-off ‚Äì reps get an actionable Slack message instead of another spreadsheet.
Perfect for rev-ops teams that want smarter prioritisation without rebuilding their stack‚Äîor exposing customer data. Plug it in, set your own masking list, and start converting the leads that matter most."
AI-Powered Asana Task Prioritization with GPT-4 and Pinecone Memory,https://n8n.io/workflows/4578-ai-powered-asana-task-prioritization-with-gpt-4-and-pinecone-memory/,"Replace manual task prioritization with intelligent AI reasoning that thinks like a Chief Operating Officer. This workflow automatically fetches your Asana tasks every morning, analyzes them using advanced AI models, and delivers the single most critical task with detailed reasoning - ensuring your team always focuses on what matters most.
‚ú® What This Workflow Does:
üìã Automated Task Collection: Fetches all assigned Asana tasks daily at 9 AM
ü§ñ AI-Powered Analysis: Uses OpenAI GPT-4 to evaluate urgency, impact, and strategic importance
üéØ Smart Prioritization: Identifies the #1 most critical task with detailed reasoning
üß† Contextual Memory: Leverages vector database for historical context and pattern recognition
üíæ Structured Storage: Saves prioritized tasks to PostgreSQL with full audit trail
üîÑ Continuous Learning: Builds organizational knowledge over time for better decisions
üîß Key Features:
Daily automation with zero manual intervention
Context-aware AI that learns from past prioritization decisions
Strategic reasoning explaining why each task is prioritized
Vector-powered memory using Pinecone for intelligent context retrieval
Clean structured output with task names, priority levels, and detailed justifications
Database integration for reporting and historical analysis
üìã Prerequisites:
Asana account with API access
OpenAI API key (GPT-4 recommended)
PostgreSQL database
Pinecone account (for vector storage and context)
üéØ Perfect For:
Operations teams managing multiple competing priorities
Startups needing systematic task management
Project managers juggling complex workflows
Leadership teams requiring strategic focus
Any organization wanting AI-driven operational intelligence
üí° How It Works:
Morning Automation: Triggers every day at 9 AM
Data Collection: Pulls all relevant tasks from Asana
AI Analysis: Evaluates each task using COO-level strategic thinking
Context Retrieval: Searches vector database for similar past tasks
Smart Prioritization: Identifies the single most important task
Structured Output: Delivers priority level with detailed reasoning
Data Storage: Saves results for reporting and continuous improvement
üì¶ What You Get:
Complete n8n workflow with all AI components configured
PostgreSQL database schema for task storage
Vector database setup for contextual intelligence
Comprehensive documentation and setup guide
Sample task data and output examples
üí° Need Help or Want to Learn More?
Created by Yaron Been - Automation & AI Specialist
üìß Support: Yaron@nofluff.online
üé• YouTube Tutorials: https://www.youtube.com/@YaronBeen/videos
üíº LinkedIn: https://www.linkedin.com/in/yaronbeen/
Discover more advanced automation workflows and AI integration tutorials on my channels!
üè∑Ô∏è Tags:
AI, OpenAI, Asana, Task Management, COO, Prioritization, Automation, Vector Database, Operations, GPT-4"
Intelligent Web & Local Search with Brave Search API and Google Gemini MCP Server,https://n8n.io/workflows/4559-intelligent-web-and-local-search-with-brave-search-api-and-google-gemini-mcp-server/,"Summary
This n8n workflow implements an AI-powered agent that intelligently uses the Brave Search API (via an external MCP service like Smithery) to perform both web and local searches. It understands natural language queries, selects the appropriate search tool, and exposes this enhanced capability as a single, callable MCP tool.
Key Features
ü§ñ Intelligent Tool Selection: AI agent decides between Brave's web search and local search tools based on user query context.
üåê MCP Microservice: Exposes complex search logic as a single, easy-to-integrate MCP tool (call_brave_search_agent).
üß† Powered by Google Gemini: Utilizes the gemini-2.5-flash-preview-05-20 LLM for advanced reasoning.
üó£Ô∏è Conversational Memory: Remembers context within a single execution flow.
üìù Customizable System Prompt: Tailor the AI's behavior and responses.
üß© Modular Design: Connects to external Brave Search MCP tools (e.g., from Smithery).
Benefits
üîå Simplified Integration: Easily add advanced, AI-driven search capabilities to other applications or agent systems.
üí∏ Reduced Client-Side LLM Costs: Offloads complex prompting and tool orchestration to n8n, minimizing token usage for client-side LLMs.
üîß Centralized Logic: Manage and update search strategies and AI behavior in one place.
üöÄ Extensible: Can be adapted to use other search tools or incorporate more complex decision-making.
Nodes Used
@n8n/n8n-nodes-langchain.mcpTrigger (MCP Server Trigger)
@n8n/n8n-nodes-langchain.toolWorkflow
@n8n/n8n-nodes-langchain.agent (AI Agent)
@n8n/n8n-nodes-langchain.lmChatGoogleGemini (Google Gemini Chat Model)
n8n-nodes-mcp.mcpClientTool (MCP Client Tool - for Brave Search)
@n8n/n8n-nodes-langchain.memoryBufferWindow (Simple Memory)
n8n-nodes-base.executeWorkflowTrigger (Workflow Start - for direct execution/testing)
Prerequisites
An active n8n instance (v1.22.5+ recommended).
A Google AI API key for using the Gemini LLM.
Access to an external MCP service that provides Brave Search tools (e.g., a Smithery account configured with their Brave Search MCP). This includes the MCP endpoint URL and any necessary authentication (like an API key for Smithery).
Setup Instructions
Import Workflow: Download the Brave_Search_Smithery_AI_Agent_MCP_Server.json file and import it into your n8n instance.
Configure LLM Credential:
Locate the 'Google Gemini Chat Model' node.
Select or create an n8n credential for ""Google Palm API"" (used for Gemini), providing your Google AI API key.
Configure Brave Search MCP Credential:
Locate the 'brave_web_search' and 'brave_local_search' (MCP Client) nodes.
Create a new n8n credential of type ""MCP Client HTTP API"".
Name: e.g., Smithery Brave Search Access
Base URL: Enter the URL of your Brave Search MCP endpoint from your provider (e.g., https://server.smithery.ai/@YOUR_PROFILE/brave-search/mcp).
Authentication: If your MCP provider requires an API key, select ""Header Auth"". Add a header with the name (e.g., X-API-Key) and value provided by your MCP service.
Assign this newly created credential to both the 'brave_web_search' and 'brave_local_search' nodes.
Note MCP Trigger Path:
Open the 'Brave Search MCP Server Trigger' node.
Copy its unique 'Path' (e.g., /cc8cc827-3e72-4029-8a9d-76519d1c136d). You will combine this with your n8n instance's base URL to get the full endpoint URL for clients.
How to Use
This workflow exposes an MCP tool named call_brave_search_agent. External clients can call this tool via the URL derived from the 'Brave Search MCP Server Trigger'.
Example Client MCP Configuration (e.g., for Roo Code):
""n8n-brave-search-agent"": {
  ""url"": ""https://YOUR_N8N_INSTANCE/mcp/cc8cc827-3e72-4029-8a9d-76519d1c136d/sse"",
  ""alwaysAllow"": [
    ""call_brave_search_agent""
  ]
}
Replace YOUR_N8N_INSTANCE with your n8n's public URL and ensure the path matches your trigger node.
Example Request:
Send a POST request to the trigger URL with a JSON body:
{
  ""input"": { ""query"": ""best coffee shops in London"" }
}
The agent will stream its response, including the summarized search results.
Customization
AI Behavior: Modify the System Prompt within the 'Brave Search AI Agent' node to fine-tune its decision-making, response style, or how it uses the search tools.
LLM Choice: Replace the 'Google Gemini Chat Model' node with any other compatible LLM node supported by n8n.
Search Tools: Adapt the workflow to use different or additional search tools by modifying the MCP Client nodes and updating the AI Agent's system prompt and tool definitions.
Further Information
GitHub Repository: https://github.com/jezweb/n8n
The workflow includes extensive sticky notes for in-canvas documentation.
Author
Jeremy Dawes (Jezweb)"
Generate Azure VM Timeline Reports with Google Gemini AI Chat Assistant,https://n8n.io/workflows/4513-generate-azure-vm-timeline-reports-with-google-gemini-ai-chat-assistant/,"An AI-powered chat assistant that analyzes Azure virtual machine activity and generates detailed timeline reports showing VM state changes, performance metrics, and operational events over time.
How It Works
The workflow starts with a chat trigger that accepts user queries about Azure VM analysis. A Google Gemini AI agent processes these requests and uses six specialized tools to gather comprehensive VM data from Azure APIs. The agent queries resource groups, retrieves VM configurations and instance views, pulls performance metrics (CPU, network, disk I/O), and collects activity log events. It then analyzes this data to create timeline reports showing what happened to VMs during specified periods, defaulting to the last 90 days unless the user specifies otherwise.
Prerequisites
To use this template, you'll need:
n8n instance (cloud or self-hosted)
Azure subscription with virtual machines
Microsoft Azure Monitor OAuth2 API credentials
Google Gemini API credentials
Proper Azure permissions to read VM data and activity logs
Setup Instructions
Import the template into n8n.
Configure credentials:
Add Microsoft Azure Monitor OAuth2 API credentials with read permissions for VMs and activity logs
Add Google Gemini API credentials
Update workflow parameters:
Open the ""Set Common Variables"" node
Replace &lt;your azure subscription id here&gt; with your actual Azure subscription ID
Configure triggers:
The chat trigger will automatically generate a webhook URL for receiving chat messages
No additional trigger configuration needed
Test the setup to ensure it works.
Security Considerations
Use minimum required Azure permissions (Reader role on subscription or resource groups). Store API credentials securely in n8n credential store. The Azure Monitor API has rate limits, so avoid excessive concurrent requests. Chat sessions use session-based memory that persists during conversations but doesn't retain data between separate chat sessions.
Extending the Template
You can add more Azure monitoring tools like disk metrics, network security group logs, or Application Insights data. The AI agent can be enhanced with additional tools for Azure cost analysis, security recommendations, or automated remediation actions. You could also integrate with alerting systems or export reports to external storage or reporting platforms."
Conversational Kubernetes Management with GPT-4o and MCP Integration,https://n8n.io/workflows/4023-conversational-kubernetes-management-with-gpt-4o-and-mcp-integration/,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n.
Conversational Kubernetes Management with GPT-4o and MCP Integration
This workflow enables you to manage Kubernetes clusters conversationally using OpenAI‚Äôs GPT-4o and a secure MCP (Model Context Protocol) server. It transforms natural language queries into actionable Kubernetes commands via a lightweight MCP API gateway ‚Äî perfect for developers and platform engineers seeking to simplify cluster interaction.
üöÄ Setup Instructions
Import the Workflow
Upload this template to your n8n instance.
Configure Required Credentials
OpenAI API Key: Add your GPT-4o API key in the credentials.
MCP Client Node: Set the URL and auth for your MCP server.
Test Kubernetes Access
Ensure your MCP server is correctly configured and has access to the target Kubernetes cluster.
üß© Prerequisites
n8n version 0.240.0 or later
Access to GPT-4o via OpenAI
A running MCP server
Kubernetes cluster credentials configured in your MCP backend
‚ö†Ô∏è Community Nodes Disclaimer
This workflow uses custom community nodes (e.g., MCP Client).
Make sure to review and trust these nodes before running in production.
üõ†Ô∏è How It Works
A webhook or chat input triggers the conversation.
GPT-4o interprets the message and generates structured Kubernetes queries.
MCP Client securely sends requests to your cluster.
The result is returned and formatted for easy reading.
üîß Customization Tips
Tweak the GPT-4o prompt to match your tone or technical level.
Extend MCP endpoints to support new Kubernetes actions.
Add alerting or monitoring integrations (e.g., Slack, Prometheus).
üñºÔ∏è Template Screenshot
üß† Example Prompts
Show me all pods in the default namespace.
Get logs for nginx pod in kube-system.
List all deployments in staging.
üìé Additional Resources
MCP Server on GitHub
OpenAI Documentation
n8n Docs
Build smarter Kubernetes workflows with the power of AI !"
Chatbot Appointment Scheduler With Google Calendar for Dental assistant,https://n8n.io/workflows/3131-chatbot-appointment-scheduler-with-google-calendar-for-dental-assistant/,"This workflow template is designed for dental assistants and anyone looking to automate appointment scheduling. It integrates Google Calendar for booking appointments and Google Sheets as a database to store patient information.
How It Works
The user interacts with the chatbot to schedule an appointment.
The chatbot collects necessary details and checks availability via Google Calendar.
If the requested time is available, the AI books the appointment.
If unavailable, the AI suggests alternative time slots.
Once booked, the AI logs the appointment details into Google Sheets for record-keeping.
Setup Instructions
üìå Watch this üé• Setup Video for detailed instructions on running and customizing this workflow.
Step 1: Set Up Credentials
OpenAI API Key (for chatbot functionality).
Google Account (for Google Sheets & Google Calendar integration).
Step 2: Choose the Right Tools
Select the correct Google Calendar in the Google Calendar tool.
Choose the appropriate Google Sheets file in the Google Sheets tool.
Step 3: Test
Run a test to ensure everything works correctly.
Once tested.
Example Templates
Below are sample Google Sheets template to help you get started."
üîêü¶ôPrivate & Local Ollama Self-Hosted + Dynamic LLM Router,https://n8n.io/workflows/3139-private-and-local-ollama-self-hosted-dynamic-llm-router/,"Who is this for?
This workflow template is designed for AI enthusiasts, developers, and privacy-conscious users who want to leverage the power of local large language models (LLMs) without sending data to external services. It's particularly valuable for those running Ollama locally who want intelligent routing between different specialized models.
What problem is this workflow solving?
When working with multiple local LLMs, each with different strengths and capabilities, it can be challenging to manually select the right model for each specific task. This workflow automatically analyzes user prompts and routes them to the most appropriate specialized Ollama model, ensuring optimal performance without requiring technical knowledge from the end user.
What this workflow does
This intelligent router:
Analyzes incoming user prompts to determine the nature of the request
Automatically selects the optimal Ollama model from your local collection based on task requirements
Routes requests between specialized models for different tasks:
Text-only models (qwq, llama3.2, phi4) for various reasoning and conversation tasks
Code-specific models (qwen2.5-coder) for programming assistance
Vision-capable models (granite3.2-vision, llama3.2-vision) for image analysis
Maintains conversation memory for consistent interactions
Processes everything locally for complete privacy and data security
Setup
Ensure you have Ollama installed and running locally
Pull the required models mentioned in the workflow using Ollama CLI (e.g., ollama pull phi4)
Configure the Ollama API credentials in n8n (default: http://127.0.0.1:11434)
Activate the workflow and start interacting through the chat interface
How to customize this workflow to your needs
Add or remove models from the router's decision framework based on your specific Ollama collection
Adjust the system prompts in the LLM Router to prioritize different model selection criteria
Modify the decision tree logic to better suit your specific use cases
Add additional preprocessing steps for specialized inputs
This workflow demonstrates how n8n can be used to create sophisticated AI orchestration systems that respect user privacy by keeping everything local while still providing intelligent model selection capabilities."
Reply to Outlook Emails with OpenAI,https://n8n.io/workflows/3089-reply-to-outlook-emails-with-openai/,"Who is this template for?
This template is for any Microsoft Outlook user who wants a trained AI agent to reason and reply on their behalf. Teach your agent tone and writing style to replicate your own, or develop a persona for a shared inbox.
Requirements
Outlook with authentication credentials
OpenAI account with authentication credentials
A few sample email replies of various lengths and topics
How it works:
Connect your Outlook account.
Select (filter) which email sender(s) your trained AI agent will reply to.
[Tip: pick a sender that has some repeatability either with a topic (ie. sales) or an individual (coworker@yourcompany.com)]
Connect your OpenAI account. Choose your AI model (ie. gpt-4o-mini)
Add Prompt (User Message) and select ""system message"" from the option below
Update the instructions by filling in your name (or persona), response style, and add full email replies from the topic or individual you want the AI agent to emulate.
[Tip: Add actual replies from your email sent folder, including your greeting and sign off. Paste each email sample between a set of <example> .... </example> tags]
Configure the reply (or reply all) to remain within the original email string
Test it! Send an email from the address to which your agent wants to respond. Check your sent (or draft) folder for the result.
Enjoy all the free time you now have!!
If you have questions or need assistance, email us at: support@teambisonandbird.com
++This template does not include retrieving email addresses out of the message or body of the email.++"
‚ú®üî™ Advanced AI Powered Document Parsing & Text Extraction with Llama Parse,https://n8n.io/workflows/3005-advanced-ai-powered-document-parsing-and-text-extraction-with-llama-parse/,"Description
This workflow automates document processing using LlamaParse to extract and analyze text from various file formats. It intelligently processes documents, extracts structured data, and delivers actionable insights through multiple channels.
How It Works
Document Ingestion & Processing üìÑ
Monitors Gmail for incoming attachments or accepts documents via webhook
Validates file formats against supported LlamaParse extensions
Uploads documents to LlamaParse for advanced text extraction
Stores original documents in Google Drive for reference
Intelligent Document Analysis üß†
Automatically classifies document types (invoices, reports, etc.)
Extracts structured data using customized AI prompts
Generates comprehensive document summaries with key insights
Converts unstructured text into organized JSON data
Invoice Processing Automation üíº
Extracts critical invoice details (dates, amounts, line items)
Organizes financial data into structured formats
Calculates tax breakdowns, subtotals, and payment information
Maintains detailed records for accounting purposes
Multi-Channel Delivery üì±
Saves extracted data to Google Sheets for tracking and analysis
Sends concise summaries via Telegram for immediate review
Creates searchable document archives in Google Drive
Updates spreadsheets with structured financial information
Setup Steps
Configure API Credentials üîë
Set up LlamaParse API connection
Configure Gmail OAuth for email monitoring
Set up Google Drive and Sheets integrations
Add Telegram bot credentials for notifications
Customize AI Processing ‚öôÔ∏è
Adjust document classification parameters
Modify extraction templates for specific document types
Fine-tune summary generation prompts
Customize invoice data extraction schema
Test and Deploy üöÄ
Test with sample documents of various formats
Verify data extraction accuracy
Confirm notification delivery
Monitor processing pipeline performance"
üîç Perplexity Research to HTML: AI-Powered Content Creation,https://n8n.io/workflows/2682-perplexity-research-to-html-ai-powered-content-creation/,"Transform simple queries into comprehensive, well-structured content with this n8n workflow that leverages Perplexity AI for research and GPT-4 for content transformation. Create professional blog posts and HTML content automatically while maintaining accuracy and depth.
Intelligent Research & Analysis
üöÄ Automated Research Pipeline
Harnesses Perplexity AI's advanced research capabilities
Processes complex topics into structured insights
Delivers comprehensive analysis in minutes instead of hours
üß† Smart Content Organization
Automatically structures content with clear hierarchies
Identifies and highlights key concepts
Maintains technical accuracy while improving readability
Creates SEO-friendly content structure
Content Transformation Features
üìù Dynamic Content Generation
Converts research into professional blog articles
Generates clean, responsive HTML output
Implements proper semantic structure
Includes metadata and categorization
üé® Professional Formatting
Responsive Tailwind CSS styling
Clean, modern HTML structure
Proper heading hierarchy
Mobile-friendly layouts
Blockquote highlighting for key insights
Perfect For
üìö Content Researchers
Save hours of manual research by automating the information gathering and structuring process.
‚úçÔ∏è Content Writers
Focus on creativity while the workflow handles research and technical formatting.
üåê Web Publishers
Generate publication-ready HTML content with modern styling and proper structure.
Technical Implementation
‚ö° Workflow Components
Webhook endpoint for query submission
Perplexity AI integration for research
GPT-4 powered content structuring
HTML transformation engine
Telegram notification system (optional)
Transform your content creation process with an intelligent system that handles research, writing, and formatting while you focus on strategy and creativity."
Generate & Optimize Brand Stories with Ollama LLMs and Google Sheets,https://n8n.io/workflows/4765-generate-and-optimize-brand-stories-with-ollama-llms-and-google-sheets/,"Overview
This n8n automation template allows marketers, branding teams, and creative professionals to auto-generate, evaluate, and iteratively optimize brand stories using Ollama-hosted LLMs and LangChain agents. Each story is refined until it meets quality criteria and is then saved to Google Sheets for publishing or reuse.
‚ö†Ô∏è Note: This template uses community nodes and requires a self-hosted n8n instance with LangChain and Ollama integrations.
What the Workflow Does
Receives a chat-triggered brand story request
Guides the user to generate a structured brand story
Evaluates the story for tone, uniqueness, quote inclusion, and emoji removal
Loops through optimization until the story is finalized
Saves the final story to Google Sheets
Target Users
Startup founders
Brand consultants
Social media strategists
Marketers building bios, taglines, intros
Step-by-Step Setup Instructions
Setup Prerequisites
Install self-hosted n8n with LangChain and Ollama nodes
Load phi4-mini and qwen3:4b models into your local Ollama instance
Create a Google Sheet with a sheet named BrandStories and columns:
Name
Final Story
Timestamp
Trigger Configuration
Set up the Chat Trigger node (@n8n/n8n-nodes-langchain.chatTrigger) with webhook ID:
fab30ad7-8a5a-4477-be98-1aa43b92b052
Customize Prompts
Update the Brand Storytelling Agent prompt to reflect your brand tone or story format
Optionally refine the Evaluator Agent criteria (e.g., enforce industry tone)
Google Sheets Setup
Use your Google Sheets OAuth2 credentials
Map fields to Name, Final Story, and Timestamp columns
Run the Flow
On new chat input, the system auto-generates a brand story
It loops between Evaluator and Optimizer agents until the output is labeled ""Finished""
Final output is saved to your Google Sheet
Flowchart (mermaid)
mermaid
Copy
Edit
flowchart TD
A[Chat Trigger: New Message] --> B[Brand Storytelling Agent]
B --> C[Set Bio Variable]
C --> D[Evaluator Agent]
D --> E{Is Output ""Finished?""}
E -- Yes --> F[Save Brand Story to Sheets]
E -- No --> G[Optimizer Agent]
G --> H[Update Bio Variable]
H --> D
Credentials Used
Google Sheets OAuth2 for storage
Ollama API (local models phi4-mini and qwen3:4b)"
Extract & Search ProductHunt Data with Bright Data MCP and Google Gemini AI,https://n8n.io/workflows/4823-extract-and-search-producthunt-data-with-bright-data-mcp-and-google-gemini-ai/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for?
This workflow template enables intelligent data extraction from ProductHunt using Bright Data‚Äôs Model Context Protocol (MCP) and processes search results with Google Gemini.
This workflow is designed for individuals and teams who need automated, intelligent discovery and analysis of new tech products. It's especially valuable for:
Startup Analysts & VC Researchers
Growth Hackers & Marketers
Recruiters & Tech Scouts
Product Managers & Innovation Teams
AI & Automation Enthusiasts
What problem is this workflow solving?
Traditional product discovery on ProductHunt is constrained by limited descriptions and requires repeated manual validation through web searches. Manually extracting and enriching this data is slow, repetitive, and error-prone.
This workflow solves the problem by:
Extracting real-time ProductHunt data using Bright Data‚Äôs MCP infrastructure to mimic real-user behavior and avoid blocks.
Performing contextual searches on Google for a specific product on ProductHunt to gather use cases, reviews, and related information.
Structuring results using Google Gemini LLM to provide human-readable insights and reduce noise.
Delivering results seamlessly by saving output to disk, updating Google Sheets, and sending Webhook alerts.
What this workflow does
Input Field Node
Define the ProductHunt category with the search term(s) you want to target. This is used to drive extraction and search operations.
Agent Operation Node
The agent performs two major tasks:
Extract from ProductHunt
Retrieves trending products from ProductHunt using Bright Data MCP
Contextual Google Search for the product the agent searches Google for deeper context, including:
Reviews
Competitor mentions
Real-world usage examples
LLM Node (Google Gemini)
Analyzes and summarizes extracted web content
Removes noise (ads, menus, etc.)
Structures content into bullet points, insights, or JSON objects
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
This workflow is flexible and modular, allowing you to adapt it for various research, product discovery, or trend analysis use cases. Below are the key customization points and how to modify them.
Define Your Target Products or Topics:
Change the input parameter to a specific ProductHunt category, tag, or keyword (e.g., ""AI tools"", ""SaaS"", ""DevOps"")
Change Output Destinations :
Save to Disk: Change the file format (.json, .csv, .md) or directory path
Google Sheet: Modify sheet name, structure (columns like Product, Summary, Link)
Webhook Notification: Point to a Slack/Discord/CRM/Webhook URL with payload mapping"
DNB Company Search & Extract with Bright Data and OpenAI 4o mini,https://n8n.io/workflows/4821-dnb-company-search-and-extract-with-bright-data-and-openai-4o-mini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
The DNB Company Search & Extract workflow is designed for professionals who need to gather structured business intelligence from Dun & Bradstreet (DNB).
It is ideal for:
Market Researchers
B2B Sales & Lead Generation Experts
Business Analysts
Investment Analysts
AI Developers Building Financial Knowledge Graphs
What problem is this workflow solving?
Gathering business information from the DNB website usually involves manual browsing, copying company details, and organizing them in spreadsheets.
This workflow automates the entire data collection pipeline ‚Äî from searching DNB via Google, scraping relevant pages, to structuring the data and saving it in usable formats.
What this workflow does
This workflow performs automated search, scraping, and structured extraction of DNB company profiles using Bright Data‚Äôs MCP search agents and OpenAI‚Äôs 4o mini model.
Here's what it includes:
Set Input Fields:
Provide search_query and webhook_notification_url.
Bright Data MCP Client (Search):
Performs Google search for the DNB company URL.
Markdown Scrape from DNB:
Scrapes the company page using Bright Data and returns it as markdown.
OpenAI LLM Extraction:
Transforms markdown into clean structured data.
Extracts business information (company name, size, address, industry, etc.)
Webhook Notification:
Sends structured response to your provided webhook.
Save to Disk:
Persists the structured data locally for logging or auditing.
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
In n8n, configure the OpenAi account credentials.
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>.
7. Update the Set input fields for search_query and webhook_notification_url.
8. Update the file name and path to persist on disk.
How to customize this workflow to your needs
Search Engine:
Default is Google, but you can change the MCP client engine to Bing, or Yandex if needed.
Company Scope:
Modify search query logic for niche filtering, e.g., ""biotech startups site:dnb.com"".
Structured Fields:
Customize the LLM prompt to extract additional fields like CEO name, revenue, or ratings.
Integrations:
Push output to Notion, Airtable, or CRMs like HubSpot using additional n8n nodes.
Formatting:
Convert output to PDF or CSV using built-in File and Spreadsheet nodes."
Create an Image Enhancement API Endpoint with Nero AI Business API,https://n8n.io/workflows/4682-create-an-image-enhancement-api-endpoint-with-nero-ai-business-api/,"How it works
This template uses the n8n AI agent node as an orchestrating agent that decides which tool (knowledge graph) to use based on the user's prompt.
How to use
Create an account and apply for an API key on https://ai.nero.com/ai-api.
Fill your key into the Create task and Query task status nodes.
Select an AI service and modify Create task node parameters, the API doc: https://ai.nero.com/ai-api/docs.
Execute the workflow so that the webhook starts listening.
Make a test request by postman or other tools, the test URL from the Webhook node.
You will receive the output in the webhook response.
Our API doc
Please create an account to access our API docs.
https://ai.nero.com/ai-api/docs.
Use cases
Large Scale Printing
Upscale images into ultra-sharp, billboard-ready masterpieces with 300+ DPI and billions of pixels.
Game Assets Compression
Improve your game performance with AI-Image Compression: Faster, Better & Lossless.
E-commerce Image Editing
Remove & replace your product image backgrounds, create virtual showrooms.
Photo Retouching
Remove & reduce grains & noises from images.
Face Animation
Transform static images into dynamic facial expression videos or GIFs with our cutting-edge Face Animation API
Photo Restoration
Our Al-driven Photo Restoration API offers advanced scratch removal, face enhancement, and image upscaling.
Colorize Photo
Transform black & white images into vivid colors.
Avatar Generator
Turn your selfie into custom avatars with different styles and backgrounds
Website Compression
Speed up your website, compress your images in bulk."
Sync Google Drive files with OpenAI vector store for Assistants,https://n8n.io/workflows/4681-sync-google-drive-files-with-openai-vector-store-for-assistants/,"Keeping AI assistant knowledge up-to-date is manual and time-consuming. When documents change in Google Drive, your OpenAI vector store becomes outdated, leading to incorrect or missing information in AI responses. This workflow eliminates manual syncing by automatically detecting file changes and updating your vector store accordingly.
How it works?
This workflow maintains perfect sync between your Google Drive folder and OpenAI vector store. It compares existing files, detects new uploads, modifications, and deletions, then automatically updates, creates, or removes vector store entries. The workflow is designed to be efficient and reliable, handling up to 100 files per sync cycle.
Scan Google Drive folder for all files and their modification dates
Query OpenAI vector store to check existing files and timestamps
Compare and identify changes - new files, updated files, or deleted files
Sync changes automatically - upload new files, update modified files, delete removed files from vector store
Schedule regular checks to maintain continuous sync
Who is it for?
Developers and teams building AI assistants that need to search through company documents, knowledge bases, or file repositories stored in Google Drive.
Setup
Setup takes approximately 15-20 minutes and requires configuring Google Drive and OpenAI API credentials, setting two variables (folder ID and vector store ID), and defining your sync schedule. Detailed implementation guides and credential setup tutorials are included to streamline the process.
What's included?
Your purchase includes comprehensive implementation support: detailed step-by-step walkthrough with screen recording, required API scopes explained clearly, tutorials for setting up Google Drive and OpenAI credentials, and access to our tech support forum for troubleshooting any implementation challenges."
Automate Gmail Labeling with Gemini AI & Build InfraNodus Knowledge Graph with Telegram Alerts,https://n8n.io/workflows/4647-automate-gmail-labeling-with-gemini-ai-and-build-infranodus-knowledge-graph-with-telegram-alerts/,"Automated Gmail Labeling and Brainstorming
This template can be used to automatically label your incoming Gmail messages with AI and to build a knowledge graph from the emails tagged with a specific label to brainstorm new ideas based on them.
You can also get notified about the emails with the most important labels via Telegram as well as receive new ideas as you are building a knowledge graph of incoming messages.
The idea generation is based on the InfraNodus knowledge graph content gap detection algorithm, which builds a network from your content and then finds a blind spot and uses AI to generate an interesting research question or idea that can be used to bridge this gap.
Why it works so well?
Think of all the business emails you receive that bypass the spam filters. Probably, they are personalized to you already. Now imagine if you build a knowledge graph from them for over a month. You will then have a ideation device based on your interests and marketing profile. Now, if you identify the gaps inside and generate interesting research questions based on them, you will come up with new interesting ideas that will be relevant (because they touch on the topics that matter to you), but novel, because they bridge them in new ways.
What is it useful for?
Automate Gmail incoming message labeling with the new Classifier n8n node ‚Äî much more advanced than the default Gmail labeling rules.
Get notified via Telegram (or a messenger of your choice) about the most important messages and be sure not to miss anything important.
Keep the messages with a certain label saved into knowledge graph for brainstorming and ideation.
Every time a new message of this category comes in, it's added into the graph, changing its structure, a new idea is generated. So instead of looking at each specific offer, you now use them to generate insights for you.
How it works
Step 1: This template can is triggered automatically when a new Gmail message arrives.
Note: you need to connect your Gmail account here in this node
Step 2: We use the new n8n AI Classifier Node to classify your email based on its content. You might need to update to n8n 1.94 version to make it work.
Note: we like to use Gemini AI for that classifier as it's the same company as Gmail, so should be safe with data
Step 3: After classifying the message, we label the message with the appropriate label.
Note: you need to create the labels before in your Gmail account
Step 4: For a certain category (e.g. ""Business"" you format the message and save it into your InfraNodus graph.
Note: specify your InfraNodus API here and choose the name of the graph. It will use the InfraNodus HTTP graphAndEntries endpoint and save your data to an InfraNodus graph.
By default, we save the text knowledge graph using the contextSettings parameters (it will only build a text graph of the content), but you can take an alternative setting from this InfraNodus HTTP node's settings and create a social knowledge graph, that will also show email senders in the graph itself.
Step 5 (optional): Generate an interesting insight question with the graphAndAdvice endpoint of InfraNodus.
Step 6 (optional): Then send this insight via Telegram to a chat.
Step 7 (optional): Link some important labels to the second Telegram notification node, so you receive important messages for specified labels.
Step 8 (optional): Send a Telegram notification
We use Telegram, because it takes only 30 seconds to set up a bot with an API (send /newbot to @botfather, unlike Discord or Slack, which is long and cumbersome to set up. You can also attach a Gmail send node and generate an email instead.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account or log in.
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Add this Authorization code in Steps 4 and 5 of the workflow.
Come up with the name of the graph and change it in the HTTP InfraNodus nodes in the steps 4 and 5 and also in the Telegram node in Step 6 that sends a link to the graph.
For additional text processing / idea generation settings you can use in the HTTP InfraNodus nodes, see the InfraNodus access points page. For example, in Step 4 you can change the text processing settings to build a social knowledge graph (settings are available in the Node's Notes section) and in Step 5 you can change the requestMode from question to idea to receive business ideas instead.
Authorize your Gmail account for Steps 2, 3, 7 and 8 Gmail nodes. The easiest way to set it up is to open a free Google Console API account and to create an OAuth access point for n8n. You can then reuse it with other Google services like Google Sheets, Drive, etc. So it's a useful thing to have in general.
Set up the Gemini AI API key using the instructions in the Step 2 Gemini AI classification node.
Set up the Telegram node bot for the Step 8. It takes only 30 seconds: just go to @botfather and type in /newbot and you'll have an API key ready. To get the conversation ID, follow the n8n / Telegram instructions in the node itself.
Once everything is ready, try to run the default automated workflow to test if everything works well.
Requirements
An InfraNodus account and API key
An Google Cloud API OAuth client and key for Gmail access
A Gemini AI API key
A Telegram bot API key
n8n version 1.94 and higher (for Text Classification AI node to work)
Customizing this workflow
Check our other n8n workflows at https://n8n.io/creators/infranodus/ for useful content gap analysis, expert panel, and marketing, and research workflows that utilize GraphRAG for better AI generation.
Finally, check out https://infranodus.com to learn more about our network analysis technology used to build knowledge graphs from text.
For support, please, contact https://support.noduslabs.com"
"Smart Customer Support System with GPT-4o, Gmail, Slack & Drive Knowledge Base'",https://n8n.io/workflows/4543-smart-customer-support-system-with-gpt-4o-gmail-slack-and-drive-knowledge-base/,"The AI Support Agent combines Gmail, Slack, and Google Drive into a seamless support workflow powered by GPT-4o and Pinecone.
üß† Email Monitoring ‚Äì New support emails are pulled from Gmail every minute.
üì§ Classification ‚Äì AI categorizes emails (e.g., billing, support, spam, urgent).
üìö Knowledge-Based Replies ‚Äì GPT-4o drafts personalized replies using your support documents synced from Google Drive and stored in Pinecone.
üì© Automatic Response ‚Äì The agent replies to the customer in the same Gmail thread.
üö® Escalation Detection ‚Äì If human support is needed, Slack is notified instantly.
üìä Logging ‚Äì Each interaction is logged in Google Sheets for tracking and analysis.
üîÅ Live Sync ‚Äì Any document added to your Google Drive folder is auto-loaded into the knowledge base for future AI responses.
üõ†Ô∏è Quick Setup Steps
üõ†Ô∏è Quick Setup Checklist
‚è± Time to Deploy: ~10‚Äì15 minutes
üîå 1. Connect Integrations
‚úÖ Gmail (OAuth2)
‚úÖ Google Drive (OAuth2)
‚úÖ Google Sheets (OAuth2)
‚úÖ OpenAI API Key
‚úÖ Pinecone API Key
‚úÖ Slack Webhook (for alerts)
üóÇÔ∏è 2. Update Workflow IDs
Replace the sample IDs in your nodes:
üìÅ Google Drive Folder ID ‚Üí Where your KB lives
üìä Google Sheet ID ‚Üí Where interactions are logged
üö® Slack Webhook URL ‚Üí Where urgent alerts go
üîé Pinecone Index ‚Üí Your vector storage index
üé® 3. Customize Prompt & Tone
Go to üîß ‚ÄúResponse Agent‚Äù Node
Update the System Prompt to reflect your brand‚Äôs tone:
e.g. ‚ÄúWe‚Äôre always here to help, and we reply fast.‚Äù
üìÇ 4. Upload Your Docs
Add .pdf, .txt, or .docx files to your synced Google Drive folder.
The agent will auto-read and embed them into Pinecone for AI-powered replies.
‚ñ∂Ô∏è 5. Run & Test
Send a test email from another account
‚úÖ Watch the reply come through Gmail
‚úÖ Check Slack for urgent alert
‚úÖ Confirm logging in Google Sheets
‚úÖ Done!"
Automated Revenue Predictions from Stripe Data with GPT-4 and Google Sheets,https://n8n.io/workflows/4577-automated-revenue-predictions-from-stripe-data-with-gpt-4-and-google-sheets/,"CFO Forecasting Agent - Marketplace Listing
Headlines (Choose Your Favorite)
Option 1 - Direct & Professional
""AI-Powered CFO Forecasting Agent: Automated Revenue Predictions from Stripe Data""
Option 2 - Benefit-Focused
""Automate Your Financial Forecasting: Daily Revenue Predictions with AI Intelligence""
Option 3 - Action-Oriented
""Transform Stripe Sales Data into Intelligent 3-Month Revenue Forecasts Automatically""
Marketplace Description
üöÄ AI-Powered Financial Forecasting on Autopilot
Turn your Stripe sales data into intelligent revenue forecasts with this comprehensive CFO Forecasting Agent. This workflow automatically analyzes your transaction history, identifies trends, and generates professional 3-month revenue predictions using OpenAI's GPT-4.
‚ú® What This Workflow Does:
üìä Automated Data Collection: Fetches and processes all Stripe charges daily
ü§ñ AI-Powered Analysis: Uses OpenAI GPT-4 to analyze trends and predict future revenue
üìà Structured Forecasting: Generates monthly forecasts with confidence levels and insights
üíæ Multi-Platform Storage: Saves results to both Supabase database and Google Sheets
üïí Scheduled Execution: Runs automatically every day to keep forecasts current
üß† Smart Context: Optional Pinecone integration for historical context and improved accuracy
üîß Key Features:
Daily automated execution at 9 AM
Structured JSON output with forecasts, trends, and confidence levels
Dual storage system for data backup and easy reporting
RAG-enabled for enhanced forecasting with historical context
Professional CFO-grade insights and trend analysis
üìã Prerequisites:
Stripe account with API access
OpenAI API key (GPT-4 recommended)
Google Sheets API credentials
Supabase account (optional)
Pinecone account (optional, for enhanced context)
üéØ Perfect For:
SaaS companies tracking subscription revenue
E-commerce businesses needing sales forecasts
Startups requiring investor-ready financial projections
Finance teams automating reporting workflows
üì¶ What You Get:
Complete n8n workflow with all nodes configured
Detailed documentation and setup instructions
Sample data structure and output formats
Ready-to-use Google Sheets template
üí° Need Help or Want to Learn More?
Created by Yaron Been - Automation & AI Specialist
üìß Support: Yaron@nofluff.online
üé• YouTube Tutorials: https://www.youtube.com/@YaronBeen/videos
üíº LinkedIn: https://www.linkedin.com/in/yaronbeen/
Get more automation tips, tutorials, and advanced workflows on my channels!
üè∑Ô∏è Tags:
AI, OpenAI, Stripe, Forecasting, Finance, CFO, Automation, Revenue, Analytics, GPT-4"
Generate and Publish SEO-Optimized Blog Posts to WordPress,https://n8n.io/workflows/4362-generate-and-publish-seo-optimized-blog-posts-to-wordpress/,"BlogBlitz is a powerful n8n workflow that automates the creation and publishing of SEO-optimized blog posts to WordPress, saving you hours of manual content creation. Triggered on a schedule or via Telegram, it generates high-quality, 1,500‚Äì2,500-word articles complete with titles, slugs, meta descriptions, images, and more.
üéØ Who is this for?
Bloggers who want fresh, consistent content.
Content marketers aiming for SEO efficiency.
WordPress site owners looking to automate blog publishing without sacrificing quality.
üö® Problem Solved
Manually creating engaging, SEO-friendly content is time-consuming and requires writing expertise. BlogBlitz solves this by:
Automating ideation, writing, formatting, and publishing.
Generating images and SEO elements.
Keeping your blog active and visible to search engines.
‚öôÔ∏è What This Workflow Does
Feature Description
Triggers - Runs every 3 hours via Schedule Trigger<br>- Or on-demand via Telegram command: /generate
Generates Content Uses OpenRouter to: <br>- Select a category (Technology, AI, etc.)<br>- Create a title, slug, focus keyphrase, and meta description
Writes Articles OpenAI generates 1,500‚Äì2,500-word articles:<br>- SEO-optimized<br>- Structured with headings<br>- Includes CTA
Adds Visuals Generates realistic featured images with OpenAI and uploads them to WordPress
SEO Features Generates:<br>- Optimized slug<br>- Focus keyphrase<br>- Meta description
Publishes Posts directly to WordPress:<br>- With correct category<br>- Featured image<br>- Author ID
Notifies Sends publish alerts via Discord webhook and Telegram message
üöÄ Setup Instructions
‚úÖ Requirements
Self-hosted or cloud n8n instance
Stable internet connection
üîê Credentials Needed
WordPress API (wp-json/wp/v2)
OpenAI API (text + image generation)
OpenRouter API (category & title generation)
Telegram Bot API
Discord Webhook
üîß WordPress Configuration
Set up post categories:
Technology [ID:3]
AI [ID:4]
Tech Fact [ID:7]
Tech History [ID:8]
Tech Tips [ID:9]
Use admin user ID for publishing (default: 1)
üß© Node Setup
Telegram Trigger ‚Äî Initiates workflow with /generate
Schedule Trigger ‚Äî Runs workflow every 3 hours
Edit Fields Node ‚Äî Centralizes variable setup (e.g., category IDs)
OpenRouter Node ‚Äî Generates topic and meta info
OpenAI Node ‚Äî Generates full article and image
WordPress Node ‚Äî Publishes post
Discord Node ‚Äî Sends publish alert
HTTP Request Node ‚Äî Handles image upload or utility calls
üõ†Ô∏è Customization Tips
Feature How to Customize
Categories Update category IDs in the WordPress Post Draft node
Schedule Modify the interval in the Schedule Trigger
Tone & Style Adjust prompts in the LLM Chain node
Notifications Add Slack, Email, or other channels
Image Style Change OpenAI prompt for ‚Äúvivid‚Äù, ‚Äúnatural‚Äù, etc.
üì¶ Pre-Requirements
n8n Instance (Cloud/Self-hosted)
Install: @n8n/n8n-nodes-langchain
All required API credentials configured
WordPress categories set
Admin user ID available
üîß Nodes Used
Telegram Trigger
Schedule Trigger
OpenRouter
OpenAI
WordPress
Discord
HTTP Request
‚úÖ Test the Workflow
Deploy and connect your n8n instance.
Send /generate to your Telegram bot.
Check your WordPress site for the newly published post!"
"Legal Case Research Extractor, Data Miner with Bright Data MCP & Google Gemini",https://n8n.io/workflows/4354-legal-case-research-extractor-data-miner-with-bright-data-mcp-and-google-gemini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
The Legal Case Research Extractor is a powerful automated workflow designed for legal tech teams, researchers, law firms, and data scientists focused on transforming unstructured legal case data into actionable, structured insights.
This workflow is tailored for:
Legal Researchers automating case law data mining
Litigation Support Teams handling large volumes of case records
LawTech Startups building AI-powered legal research assistants
Compliance Analysts extracting case-specific insights
AI Developers working on legal NLP, summarization, and search engines
What problem is this workflow solving?
Legal case data is often locked in semi-structured or raw HTML formats, scattered across jurisdiction-specific websites. Manually extracting and processing this data is tedious and inefficient.
This workflow automates:
Extraction of legal case data via Bright Data's powerful MCP infrastructure
Parsing of HTML into clean, readable text using Google Gemini LLM
Structuring and delivering the output through webhook and file storage
What this workflow does
Input
Set the Legal Case Research URL node is responsible for setting the legal case URL for the data extraction.
Bright Data MCP Data Extractor
Bright Data MCP Client For Legal Case Research node is responsible for the legal case extraction via the Bright Data MCP tool - scrape_as_html
Case Extractor
Google Gemini based Case Extractor is responsible for producing a paginated list of cases
Loop through Legal Case URLs
Receives a collection of legal case links to process
Each URL represents a different case from a target legal website
Bright Data MCP Scraping
Utilizes Bright Data‚Äôs scrape_as_html MCP mode
Retrieves raw HTML content of each legal case
Google Gemini LLM Extraction
Transforms raw HTML into clean, structured text
Performs additional information extraction if required (e.g., case summary, court, jurisdiction etc.)
Webhook Notification
Sends extracted legal case content to a configurable webhook URL
Enables downstream processing or storage in legal databases
Binary Conversion & File Persistence
Converts the structured text to binary format
Saves the final response to disk for archival or further processing
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
Target New Legal Portals
Modify the legal case input URLs to scrape from different state or federal case databases
Customize LLM Extraction
Modify the prompt to extract specific fields: case number, plaintiff, case summary, outcome, legal precedents etc.
Add a summarization step if needed
Enhance Loop Handling
Integrate with a Google Sheet or API to dynamically fetch case URLs
Add error handling logic to skip failed cases and log them
Improve Security & Compliance
Redact sensitive information before sending via webhook
Store processed case data in encrypted cloud storage
Output Formats
Save as PDF, JSON, or Markdown
Enable output to cloud storage (S3, Google Drive) or legal document management systems"
Track Personal Finances with GPT-4 via Telegram & Google Sheets,https://n8n.io/workflows/3932-track-personal-finances-with-gpt-4-via-telegram-and-google-sheets/,"Track your expenses, gain insights, and manage money smarter ‚Äì right from Telegram.
Who is this For
This workflow is perfect for individuals and freelancers who want financial control without app overload. Launch it in your n8n environment and start tracking your money smarter today!
Features
Natural Language Expense Input: Log expenses by simply messaging ‚ÄúSpent $50 on groceries‚Äù.
Income Logging: Add income sources such as salary, freelance, or passive income.
Voice Support: Speak to the bot‚ÄîWhisper API converts voice to text.
Google Sheets Integration: All data is synced to a spreadsheet in real time.
How It Works
User Input (Text or Voice):
Send messages on your telegram bot like:
‚ÄúSpent $25 on transportation‚Äù
‚ÄúReceived $300 from freelancing‚Äù
Or just voice record a message
AI Parsing & Categorization:
GPT-4 processes and categorizes the entry using pre-trained financial logic.
Data Logging:
n8n stores data in: Google Sheets (via Google Sheets API)
Video Demo:
See this youtube Video to explore ""how it works"".
Set Up Steps
Import the Workflow
Create a new workflow
Import the JSON file by clicking ""three dot"" (upper right corner) > ""import from file..."" (I will provide the JSON file after buying).
Set Up Telegram Bot
Create a bot via @BotFather
Create a telegram credential to connect telegram with n8n.
Grab the token from @BotFather and paste into the Telegram credential.
Connect that credential with telegram nodes.
Enable Google Sheets Integration
Create a google sheet credential to connect google sheet with n8n.
Connect that credential with Google sheets tools.
Create a google sheet including these columns: ""chat_id"", ""date_and_time"", ""income (dollar)"", ""income source"", ""cost (dollar)"", ""cost category""
Provide access that sheet to n8n.
Configure OpenAI (GPT & Whisper)
Create an OpenAi credential to connect openai with n8n.
Add your OpenAI API Key in the credentials tab
Connect that credential with openai nodes.
Pre-requisites
Necessary credentials:
Telegram Account
OpenAi Account
Google Sheets Account
N8N version 1.92.2 or upper
Customization Guidance
You can customize the ai agent prompt based on your needs.
And also you can customize the sheet's columns based on your needs.
And you can add additional features like:
Monthly budgeting alert
Expense report generation
Smart Recommendations for cost cutting tips
Ready to Automate Your Finances?
Buy the workflow for only 30$, You will get full setup guide after buying."
AI Marketing Agent for Lead Generation: Reddit + OpenRouter + Gmail,https://n8n.io/workflows/3935-ai-marketing-agent-for-lead-generation-reddit-openrouter-gmail/,"What it is
This n8n workflow monitors Reddit for posts relevant to a specific business or industry, identifies potential leads, and delivers them directly to your inbox.
Full tutorial (https://findleads.agency/blog/n8n-ai-agent-for-lead-generation-using-reddit-openai-gmail) for those wanting to build it themselves.
How It Works
A user submits their website URL and email through a form
The workflow analyzes the website to determine the industry and extract relevant keywords
It searches Reddit for posts containing those keywords and filters them based on custom engagement metrics (i.e. upvotes > 15, non-empty text content, posted within the last 90 days)
An AI agent (using OpenRouter's GPT-4.1-mini) analyzes each post to determine relevance and summarizes the key points
Relevant posts are stored in Google Sheets and formatted into a professional HTML email
The email is sent to the user's provided email address with a summary of potential leads
Setup
To run this workflow, you need to set up credentials in n8n for:
Reddit: Uses OAuth 2.0. Requires creating an app on Reddit to get a Client ID & Secret. (YT Tutorial for Reddit App Creation: https://youtu.be/zlGXtW4LAK8)
OpenRouter: Uses an API Key. Generate this key directly from your OpenRouter account settings. (YT Tutorial: https://youtu.be/Cq5Y3zpEhlc)
Google Sheets: Recommend OAuth2 (just connect by authenticating) or setup in Google Cloud Console (enable Sheets API, create OAuth Client ID with n8n redirect URI) to get a Client ID & Secret.
Gmail: Uses OAuth 2.0. Requires setup in Google Cloud Console to enable Gmail API and create OAuth credentials."
Adaptive RAG Strategy with Query Classification & Retrieval (Gemini & Qdrant),https://n8n.io/workflows/3459-adaptive-rag-strategy-with-query-classification-and-retrieval-gemini-and-qdrant/,"This n8n workflow implements a version of the Adaptive Retrieval-Augmented Generation (RAG) framework. It recognizes that the best way to retrieve information often depends on the type of question asked. Instead of a one-size-fits-all approach, this workflow adapts its strategy based on the user's query intent.
üåü How it Works
Receive Query: Takes a user query as input (along with context like a chat session ID and Vector Store collection ID if used as sub-workflow).
Classify Query: First, the workflow classifies the query into a predefined category. This template uses four examples:
Factual: For specific facts.
Analytical: For deeper explanations or comparisons.
Opinion: For subjective viewpoints.
Contextual: For questions relying on specific background.
Select & Adapt Strategy: Based on the classification, it selects a corresponding strategy to prepare for information retrieval. The example strategies aim to:
Factual: Refine the query for precision.
Analytical: Break the query into sub-questions for broad coverage.
Opinion: Identify different viewpoints to look for.
Contextual: Incorporate implied or user-specific context.
Retrieve Info: Uses the output of the selected strategy to search the specified knowledge base (Qdrant vector store - change as needed) for relevant documents.
Generate Response: Constructs a response using the retrieved documents, guided by a prompt tailored to the original query type.
By adapting the retrieval strategy, this workflow aims to provide more relevant results tailored to the user's intent.
‚öôÔ∏è Usage & Flexibility
Sub-Workflow: Designed to be called from other n8n workflows, passing user_query, chat_memory_key, and vector_store_id as inputs.
Chat Testing: Can also be triggered directly via the n8n Chat interface for easy testing and interaction.
Customizable Framework: The query categories (Factual, Analytical, etc.) and the associated retrieval strategies are examples. You can modify or replace them entirely to fit your specific domain or requirements.
üõ†Ô∏è Requirements
Credentials: You will need API credentials configured in your n8n instance for:
Google Gemini (AI Models)
Qdrant (Vector Store)"
Auto-Generate & Publish SEO Articles to WordPress with GPT-4 + Postgres Tracking,https://n8n.io/workflows/3887-auto-generate-and-publish-seo-articles-to-wordpress-with-gpt-4-postgres-tracking/,"üöÄ What this flow does
‚Ä¢ üîé Selects the least-used WordPress category (tracked in PostgreSQL)
‚Ä¢ ü§ñ Uses GPT (4-mini or better) to generate a fully formatted SEO article with headings, TOC, lists, CTA, and Yoast blocks
‚Ä¢ üñºÔ∏è Creates a placeholder cover image and uploads it to WordPress Media
‚Ä¢ üì¨ Publishes the final post via /wp-json/wp/v2/posts with correct category + featured image
‚Ä¢ üß† Logs the used category for future rotation (zero duplicates!)
‚öôÔ∏è Setup in 3 mins
üè∑Ô∏è Add your WordPress domain with a simple Set node:
    domain=https://yourdomain.com
üîê Create these 3 credentials in n8n:
    YOUR_WORDPRESS_CREDENTIAL ‚Äî for /media, /posts
    YOUR_POSTGRES_CREDENTIAL ‚Äî for category tracking
    YOUR_OPENAI_CREDENTIAL ‚Äî GPT-4-mini or better
üß± Run the SQL from docs to create the used_categories table
‚úÖ Manually test first 3‚Äì5 nodes to check WP auth, OpenAI response, and DB connection
üïí Then just schedule it and let the bot write for you.
üéØ Why it's awesome
This is your personal AI content writer + publisher ‚Äî perfect for:
‚Ä¢ üì∞ SEO content farms
‚Ä¢ üìà Affiliate blogs
‚Ä¢ üß∞ Micro niche sites
‚Ä¢ ü§´ PBNs with rotation-safe automation
No more manual uploads, broken categories, or GPT spam. Every post is structured, beautiful, and intelligently categorized."
Automated Resume Screening & Ranking with Llama 4 AI and Google Workspace,https://n8n.io/workflows/3838-automated-resume-screening-and-ranking-with-llama-4-ai-and-google-workspace/,"Target Audience
You will find this workflow or template perfect if you are in the internal talent acquisition teams, recruitment agencies, HR professionals, and hiring managers seeking to bulk automate the initial screening of CVs and resumes.
 Eg. Automatically get result of candidate who has been  shortlisted/rejected with its rationale and score automatically.
By eliminating manual evaluation and screening, you get smart AI-Agent helping you to have standardized efficient, and scalable solution for handling large volumes of applications.
With bulk automation, you can focus strategic decision-making rather than tedious screening tasks, ensuring a faster, more accurate, and fair hiring process.
Key focus
This workflow focusses on having a more organized file-folder management, trackable candidate cv, maintainable job description, autonomous ai-agent.
Organized Folder-File Structure ‚Äì CVs are automatically categorized based on their status, ensuring a structured workflow and easy retrieval
Candidate Tracker ‚Äì A real-time tracking system records the state of each CV, allowing recruiters to monitor the shortlisted, rejected, or KIV (Keep in View) candidates.
AI Agent for Decision Automation ‚Äì The AI autonomously orchestrates screening decisions, replacing manual LLM configurations with dynamic AI-driven evaluations for scalability and accuracy.
Maintainable Job Description Management ‚Äì A structured job description file ensures continuous updates, keeping hiring criteria flexible and aligned with recruitment needs.
Email Notifications ‚Äì The system automatically sends receipt confirmations upon processing completion, providing timely updates to recruiters.
Features - Workflow
Automated Resume Screening Workflow
This workflow leverages Groq Llama4 for intelligent resume analysis, speeding the screening process by generating a matching score, result (shortlisted/rejected/kiv), and key insights/rationale into their suitability for provided job description.
Step-by-Step Process:
Monitors Google Drive: Listens and checks for new resume cv in google drive .
Retrieve Resume: Downloads the CV resumes from google drive .
Extract Resume Data: Extract text content from CV resume PDF files
Extract Job Description Data: Extract text content from job description
Analyze with Groq:
Generate a matching score based on job requirements. [SCORE: 1-10]
Provide decision into their job suitability. [SHORTLISTED/REJECTED/KIV]
Provide actionable insights into their job suitability. [REASON]
This ensures a fast, efficient, and accurate screening process, eliminating manual evaluation.
Setup Guide
Step-by-Step Instructions
Ensure all credentials are ready and setup (groq, gdrive ,gmail, gsheet, gdoc)
View official n8n documentation on node setup accordingly.
See also the notes of setup .
Folder & File Setup
1. Create a google-drive folder like this
View directory example
2. Create a job description like this
View file example
3. Configure a tracker like this ( Candidate Name, AI Score,AI Verdict, AI Reason)
View file example
email conversations report as you like.
You are ready to go!"
YouTube Video to WordPress Blog Automation with Gemini AI & Affiliate Integration,https://n8n.io/workflows/3714-youtube-video-to-wordpress-blog-automation-with-gemini-ai-and-affiliate-integration/,"üöÄ YouTube to WordPress AI-Powered Automation
Transform your YouTube content into professional blog posts automatically!
This n8n template seamlessly converts your YouTube videos into fully-formatted, SEO-optimized WordPress blog posts using AI-powered content generation‚Äîall without lifting a finger.
üéØ What This Automation Does
üì∫ Monitors your YouTube channel for new video uploads
üîç Extracts key video information (title, description, URL, thumbnail)
ü§ñ Leverages AI to generate comprehensive blog content based on video material
‚úèÔ∏è Creates SEO-optimized titles, excerpts, and tags
üñºÔ∏è Formats posts beautifully with embedded videos and proper HTML structure
üìù Publishes directly to WordPress via secure API connection
‚è±Ô∏è Runs on your schedule ‚Äî daily, weekly, or after each new upload
üßë‚Äçüè´ Step-by-Step Video Tutorial
üé• Watch the implementation tutorial:

üìå See the complete workflow setup and content transformation in action.
üåê Useful Links
üîó Get started with n8n Cloud:
üëâ https://n8n.io/cloud/
üìò YouTube Data API documentation:
üëâ https://developers.google.com/youtube/v3
üìö WordPress REST API handbook:
üëâ https://developer.wordpress.org/rest-api/
üß† OpenAI API documentation:
üëâ https://platform.openai.com/docs/api-reference
üõ† Prerequisites
‚úÖ n8n installation (self-hosted or cloud)
‚úÖ YouTube Data API key
‚úÖ AI provider API key (OpenAI, Google Gemini, etc.)
‚úÖ WordPress website with REST API enabled
‚úÖ WordPress application password for secure authentication
üìã Step-by-Step Implementation
1Ô∏è‚É£ YouTube Video Detection
Youtube RSS Feedlink to fetch new video automatically
Implement filtering to process only videos not yet published to WordPress
2Ô∏è‚É£ Content Preparation
Extract essential video metadata (ID, title, description, thumbnail URL)
Structure the information for AI processing and WordPress publication
Format video thumbnails for use as featured images in blog posts
3Ô∏è‚É£ AI Content Generation
Configure your preferred AI provider (OpenAI, Gemini, etc.)
Create detailed prompts that instruct the AI to:
Maintain your brand voice and writing style
Structure content with proper headings and subheadings
Creates Internal Links from existing blogposts
Creators Extenal links using affilaite links and other link database.
Engaging SEO frendly.
Automatically monetizes by embedding social PartnerStack Links
Include key points from the video
Generate SEO-friendly paragraphs and conclusions
4Ô∏è‚É£ Content Enhancement
Format the AI-generated content with proper HTML structure
Add responsive embedded YouTube player
Include attribution links back to original video
Create custom intro and conclusion sections
5Ô∏è‚É£ WordPress Publishing
Securely connect to WordPress via REST API
Set post parameters (title, content, excerpt, categories, tags)
Configure post status (publish immediately or save as draft)
Add featured image using video thumbnail
6Ô∏è‚É£ Notification & Logging
Receive email or Slack notifications when new posts are published
Log workflow executions for troubleshooting
Track successful publications for reporting
üí∞ Automatic Affiliate Marketing Integration
Transform your blog posts into passive income generators by automatically embedding relevant affiliate links based on your video content:
PartnerStack Integration
Dynamically fetch relevant affiliate links from your PartnerStack account
Intelligently match products to video content using AI-powered relevance scoring
Automatically insert formatted affiliate links with proper disclosure notices
Track performance metrics for each generated post's affiliate conversions
Example setup: Connect PartnerStack API ‚Üí Filter relevant products ‚Üí Insert links in strategic post positions
Custom Affiliate Database via Airtable
Create a centralized product-keyword Airtable database:
Product name and description
Affiliate link with your unique ID
Category and keyword triggers
Commission rates and expiration dates
Auto-scan blog content to identify keyword matches
Insert highest-converting links based on historical performance
Rotate affiliate links to test different products and placements
Advanced Monetization Features
Amazon Associates product matching based on video content
Time-sensitive promo codes inserted for limited-time offers
Geo-targeted affiliate links based on visitor location
A/B testing system to optimize link placement and conversion
Automated disclosure statements to maintain FTC compliance
Implementation Steps
Connect to your PartnerStack account via API
Create an Airtable base with your affiliate product database
Configure the AI to identify product-mention opportunities
Add a post-processing node to insert relevant affiliate links
Implement tracking parameters to measure performance
This integration not only saves time manually adding affiliate links, but also increases conversion rates through intelligent product matching and optimal placement within your automatically generated blog content.
üí° Pro Tip: Create separate tracking IDs for your automated YouTube-to-WordPress posts to measure this specific channel's performance in your affiliate dashboards.
üí° Advanced Customizations
Content Templates
Create specialized formatting based on video categories:
Tutorial videos: Step-by-step instructions with timestamps
Review videos: Pros/cons sections with ratings
Interview videos: Question-answer format with speaker attribution
Category & Tag Management
Automatically organize your WordPress content:
Assign categories based on video topics or playlists
Generate tags from video keywords and descriptions
Create custom taxonomies for video series
Multi-Platform Distribution
Extend your content reach by adding:
Social media announcement posts
Email newsletter inclusion
Content syndication to Medium or LinkedIn
‚ö†Ô∏è Troubleshooting
Issue Solution
AI generation timeout Increase timeout settings or use chunked processing
WordPress API errors Verify application password and user permissions
Missing video embedding Check oEmbed settings in WordPress
Duplicate posts Implement content fingerprinting for detection
Poor content quality Refine AI prompts with better instructions
üôå Why Use This Template
Content creators typically spend 2-4 hours converting each video into a quality blog post. This automation:
‚è±Ô∏è Saves 10+ hours weekly for active YouTube creators
üîç Boosts SEO performance with multi-platform content
üåê Expands audience reach to text-preferring visitors
üí∞ Increases monetization opportunities across platforms
üß† Frees creative energy for producing better videos
Don't choose between video and written content‚Äîhave both automatically!
üöÄ Get Started Now
Import the template ‚Üí Connect your accounts ‚Üí Watch your blog grow alongside your channel.
üëâ AMJID ALI'S PROFILE
üëâ website
üëâ Explore more on youtube
Knowledge Base: youtube-api, wordpress-automation, content-repurposing, ai-blog-generation, n8n-workflow, creator-tools, video-to-text, multi-platform-publishing"
Automated Generation of AI Advertising Photos for Product Marketing,https://n8n.io/workflows/3700-automated-generation-of-ai-advertising-photos-for-product-marketing/,"How it works
This workflow automates the transformation of standard product images into professional product photography featuring human models
It uses AI to analyze product images, create tailored photography prompts, and generate high-quality enhanced versions
Set up steps
You'll need an OpenAI API key and access to gpt-image-1 (verify your organization)
Set up a Google Sheets spreadsheet with columns: Image-URL, Prompt, Output
Create a Google Drive folder to store the generated images
Requirements:
OpenAI API access (for image generation and analysis)
Google Sheets and Google Drive accounts
Basic product images (URLs) as input
The spreadsheet must contain a column named ""Image-URL"" with links to the product images
This workflow automatically:
Reads product image URLs from your Google Sheet
Downloads the images for processing
Analyzes each image to understand what product it contains
Creates specialized photography prompts ensuring each product is shown with a human model
Generates professional product photography using OpenAI's image generation capabilities
Uploads results to Google Drive and updates your spreadsheet with links
Extra:
You can also use the included simple image generation workflow to directly create images via prompt without product image input. This option lets you quickly generate images through the OpenAI API using just text prompts"
üì• Transform Google Drive Documents into Vector Embeddings,https://n8n.io/workflows/3647-transform-google-drive-documents-into-vector-embeddings/,"Automatically convert documents from Google Drive into vector embeddings using OpenAI, LangChain, and PGVector ‚Äî fully automated through n8n.
‚öôÔ∏è What It Does
This workflow monitors a Google Drive folder for new files, supports multiple file types (PDF, TXT, JSON), and processes them into vector embeddings using OpenAI‚Äôs text-embedding-3-small model. These embeddings are stored in a Postgres database using the PGVector extension, making them query-ready for semantic search or RAG-based AI agents.
After successful processing, files are moved to a separate ‚Äúvectorized‚Äù folder to avoid duplication.
üí° Use Cases
Powering Retrieval-Augmented Generation (RAG) AI agents
Semantic search across private documents
AI assistant knowledge ingestion
Automated document pipelines for indexing or classification
üß† Workflow Highlights
Trigger Options: Manual or Scheduled (3 AM daily by default)
Supported File Types: PDF, TXT, JSON
Embedding Stack: LangChain Text Splitter, OpenAI Embeddings, PGVector
Deduplication: Files are moved after processing
License: CC BY-SA 4.0
Author: AlexK1919
üõ† What You‚Äôll Need
Google Drive OAuth2 credentials (connected to Search Folder, Download File, and Move File nodes)
OpenAI API Key (used in the Embeddings OpenAI node)
Postgres + PGVector database (connected in the Postgres PGVector Store node)
üîß Step-by-Step Setup Instructions
Create Google OAuth2 credentials in n8n and connect them to all Google Drive nodes.
Set your source folder ID in the Search Folder node ‚Äî this is where incoming files are placed.
Set your processed folder ID in the Move File node ‚Äî files will be moved here after vectorization.
Ensure you have a PGVector-enabled Postgres instance and input the table name and collection in the Postgres PGVector Store node.
Add your OpenAI credentials to the Embeddings OpenAI node and select text-embedding-3-small.
Optional: Activate the Schedule Trigger node to run daily or configure your own schedule.
Run manually by triggering When clicking ‚ÄòTest workflow‚Äô for on-demand ingestion.
üß© Customization Tips
Want to support more file types or enhance the pipeline?
Add new extractors: Use Extract from File with other formats like DOCX, Markdown, or HTML.
Refine logic by file type: The Switch node routes files to the correct extraction method based on MIME type (application/pdf, text/plain, application/json).
Pre-process with OCR: Add an OCR step before extraction to handle scanned PDFs or images.
Add filters: Enhance the Search Folder or Switch node logic to skip specific files or folders.
üìÑ License
Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)
Use, adapt, and share - even commercially - as long as you give proper credit and share alike.
Full License Details"
"Auto Invoice & Receipt OCR to Google Sheets ‚Äì Drive, Gmail, & Telegram Triggers",https://n8n.io/workflows/3618-auto-invoice-and-receipt-ocr-to-google-sheets-drive-gmail-and-telegram-triggers/,"The Best Invoice & Receipt OCR Automation - FREE (for a limited time)
Automatically process invoices and receipts using Gemini OCR, extracting data directly into Google Sheets from multiple sources including Google Drive, Gmail, and Telegram. This powerful workflow ensures your bookkeeping is always accurate, organized, and efficient, significantly reducing manual effort and potential errors.
Who is this for?
This workflow is ideal for busy entrepreneurs, startup founders, freelancers, small business owners, bookkeepers, and accountants who aim to eliminate manual, repetitive, and error-prone bookkeeping tasks. Whether you regularly manage expenses from physical paper receipts, digital invoices, or email attachments, this workflow will dramatically streamline your bookkeeping processes and save you significant time and effort each month.
What problem is this workflow solving?
Manual data entry of invoices and receipts is notoriously tedious, incredibly time-consuming, and highly susceptible to human error. Mistakes in bookkeeping can lead to financial inaccuracies, compliance issues, and wasted resources. By automating the extraction of invoice data, this workflow streamlines your financial management process, significantly improves accuracy, reduces operational overhead, and allows you to redirect valuable resources and attention toward strategic, revenue-generating business activities.
What this workflow does
This template provides a powerful, automated solution for invoice and receipt data extraction using Google's Gemini API for OCR (Optical Character Recognition) via direct HTTP requests.
Main Flow (Google Drive Trigger): The main workflow triggers (Google Drive Trigger New Files) whenever a new file (PDF or image) is added to a designated Google Drive folder. It uses a SplitInBatches node (Loop Over Items) to process incoming files one by one. For each file, it:
Downloads the file (Google Drive Get Receipt).
Converts it to base64 (Move file to base64 string).
Sends the file data and a detailed prompt to the Gemini API (gemini-2.0-flash model specified in the Prompt node, sent via the Gemini API HTTP Request node). The prompt requests structured JSON output with specific fields and formatting (like comma decimal separators, no currency symbols, and a dedicated currency field).
Parses the JSON response (JSON to string, Parse string nodes).
Appends the extracted data (Invoice Date, Category, Sender, Currency, etc.), along with the original filename and a link to the file, to a specified Google Sheet (Add to Google Sheets).
Includes Wait nodes to help manage potential rate limits.
Supplementary flow 1 (Gmail Trigger): An additional trigger (Gmail Trigger) monitors your Gmail account for emails with a specific label. When a matching email with attachments arrives:
It loops through emails and attachments.
It renames the attachment using the format YYYY-MM-DD_SenderUsername (e.g., 2025-04-19_some.sender) using the Create File Name node.
It saves the renamed attachment to the designated Google Drive folder (Google Drive Save Files), which then triggers the core OCR process above.
Supplementary Trigger 2 (Telegram Trigger): Another optional trigger (Telegram Trigger Image) allows you to forward photos of physical receipts to your configured Telegram bot:
It renames the image file using the format YYYY-MM-DD_Telegram (e.g., 2025-04-19_Telegram) via the Create File Name For Telegram node.
It saves the renamed image to the designated Google Drive folder (Google Save Files 2), also triggering the core OCR process.
Setup
Credentials: Add the following credentials in n8n, using the names specified in the template or your own:
Google OAuth2: For Google Drive, Google Sheets, Gmail.
Telegram: For the Telegram bot trigger
Gemini API Key: Obtain a free API key from Google AI Studio. You will need to paste this key directly into the Query Parameters of the Gemini API (HTTP Request) node in the workflow.
Google Drive: Create a specific folder in your Google Drive where all invoices/receipts will be stored and processed from. Update the target Folder in the Google Drive Trigger New Files, Google Drive Save Files, and Google Save Files 2 nodes to use your designated folder.
Google Sheets: Create a new Google Sheet, or clone this Sheet template. Ensure it has columns matching the desired output fields (see default list below, including Currency). Update the target Spreadsheet and Sheet Name in the Add to Google Sheets node to point to your sheet.
Gmail (Optional): Create a label in Gmail (e.g., ""receipts""). Update the Label filter in the Gmail Trigger node to use the label you created. Consider setting up filters in Gmail to automatically apply this label to relevant emails.
Telegram (Optional): Configure the Telegram Trigger Image node with your bot credentials.
Gemini API Node: Open the Gemini API (HTTP Request) node. In the ""Query Parameters"" section, replace the placeholder API key with your actual Gemini API Key. Verify the URL uses the correct model name by referencing the Prompt node's model value (gemini-2.0-flash by default).
Customize Prompt: Open the Prompt (Set) node.
Crucially, change the first line ""My company is Sisu Digital..."" to reflect your company or remove it if not needed.
Review the list of categories and the specific formatting instructions (e.g., comma for decimals, no currency symbols, date format, ""N/A"" for notes) to ensure they match your requirements. You can adjust these details here.
Activate Workflow: Test each trigger path (Drive upload, labeled Gmail, Telegram photo) with sample files/emails/messages, then activate the workflow.
How to customize this workflow to your needs
Gemini Prompt/Model: Modify the prompt text, the list of categories, or the detailed formatting instructions within the Prompt (Set) node. You can also change the model value in the Prompt node to use a different Gemini model (ensure the model supports the API endpoint used in the HTTP Request node).
Extracted Fields: Modify the prompt in the Prompt node to extract different fields or change formatting. Update the parsing logic in the Parse string node if needed, and ensure your Google Sheet columns match in the Add to Google Sheets node. The default extracted fields (based on the updated prompt) are:
Invoice Date: (DD/MM/YYYY)
Category: (From predefined list)
Sender: (Issuing company)
Description: (Brief details)
Amount (0% VAT): (Net amount, comma decimal, no currency symbol)
VAT %: (Rate number, comma decimal, no % sign, ""0,0"" if none)
Total: (Gross amount, comma decimal, no currency symbol)
Currency: (Uppercase abbreviation, e.g., ""EUR"", ""USD"")
Note: (Relevant notes, or ""N/A"")
(Added by workflow): File Name, File URL
Triggers & Targets: Enable/disable or modify the Gmail and Telegram triggers. Select your desired Google Drive folder, Google Sheet, and Gmail label directly within the respective nodes.
File Naming: Adjust the Javascript code in the Create File Name (for Gmail) or Create File Name For Telegram nodes to change the renaming pattern. Current formats: YYYY-MM-DD_SenderUsername (Gmail), YYYY-MM-DD_Telegram (Telegram).
Configuration Method: Update configuration like folders, sheets, and labels directly within the relevant nodes. For easier management of multiple settings, consider adding a central 'Set' node at the beginning of the workflow to define these values and use expressions to reference them in other nodes.
Important Considerations
AI Accuracy: AI models can make mistakes. Always review the data extracted into Google Sheets for accuracy, especially with specific formatting like decimal separators. This workflow automates heavily but requires final verification.
Gemini API Usage: This template uses the Gemini API via HTTP Request. Be mindful of Google's API rate limits, usage policies, and potential costs associated with the model used (gemini-2.0-flash by default), especially if you change it. Monitor Google's terms.
Loops and Waits: The workflow uses SplitInBatches and Wait nodes to process items individually and pause execution, which helps prevent hitting API rate limits but processes files sequentially rather than in parallel.
Support Disclaimer
This workflow is provided as-is for your convenience. Use it responsibly, and feel free to build upon it for your unique needs!
This workflow has been thoroughly tested and is confirmed to be working flawlessly at the time of release. However, setup complexity or unexpected errors may occur depending on your environment, customizations, or API changes.
Please note that I cannot offer personalized support or troubleshooting for this template (without a fee!). If you encounter an issue, feel free to send me a message describing the problem. If the issue is determined to be a genuine bug within the shared template, I will do my best to fix it and update the workflow. Otherwise, the issue is likely due to a setup misconfiguration or modification on your side. In those cases, I recommend using ChatGPT or referring to n8n documentation to debug the problem."
"Automated Research Report Generation with AI, Wiki, Search & Gmail/Telegram",https://n8n.io/workflows/3579-automated-research-report-generation-with-ai-wiki-search-and-gmailtelegram/,"Automated Research Report Generation with OpenAI, Wikipedia, Google Search, Gmail/Telegram and PDF Output
Description
What Problem Does This Solve? üõ†Ô∏è
This workflow automates the process of generating professional research reports for researchers, students, and professionals. It eliminates manual research and report formatting by aggregating data, generating content with AI, and delivering the report as a PDF via Gmail or Telegram. Target audience: Researchers, students, educators, and professionals needing quick, formatted research reports.
What Does It Do? üåü
Aggregates research data from Wikipedia, Google Search, and SerpApi.
Refines user queries and generates structured content using OpenAI.
Converts the content into a professional HTML report, then to PDF.
Sends the PDF report via Gmail or Telegram.
Key Features üìã
Real-time data aggregation from multiple sources.
AI-driven content generation with OpenAI.
Automated HTML-to-PDF conversion for professional reports.
Flexible delivery via Gmail or Telegram.
Error handling for robust execution.
Setup Instructions
Prerequisites ‚öôÔ∏è
n8n Instance: Self-hosted or cloud n8n instance.
API Credentials:
OpenAI API: API key with GPT model access, stored in n8n credentials.
SerpApi (Google Search): API key from SerpApi, stored in n8n credentials (do not hardcode in nodes).
Gmail API: Credentials from Google Cloud Console with Gmail scope.
Telegram API: Bot token from BotFather on Telegram.
Installation Steps üì¶
Import the Workflow:
Copy the workflow JSON from the ""Template Code"" section below.
Import it into n8n via ""Import from File"" or ""Import from URL"".
Configure Credentials:
Add API credentials in n8n‚Äôs Credentials section for OpenAI, SerpApi, Gmail, and Telegram.
Assign credentials to respective nodes. For example:
In the SerpApi Google Search node, use n8n credentials for SerpApi: api_key={{ $credentials.SerpApiKey }}.
In the Send Research PDF on Gmail node, use Gmail credentials.
In the Send PDF to Telegram node, use Telegram bot credentials.
Set Up Nodes:
OpenAI Nodes (Research AI Agent, OpenAI Chat Model, OpenAI Chat Middle Memory): Update the model (e.g., gpt-4o) and prompt as needed.
Input Validation (Input Validation node): Ensure your input query format matches the expected structure (e.g., topic: ""AI ethics"").
Delivery Options (Send Research PDF on Gmail, Send PDF to Telegram): Configure recipient email or Telegram chat ID.
Test the Workflow:
Run the workflow by clicking the ""Test Workflow"" node.
Verify that the research report PDF is generated and sent via Gmail or Telegram.
How It Works
High-Level Steps üîç
Query Refinement: Refines the input query for better research.
Aggregate Data: Fetches data from Wikipedia, Google Search, and SerpApi.
Generate Report: Uses OpenAI to create a structured report.
Convert to PDF: Converts the report to HTML, then PDF.
Deliver Report: Sends the PDF via Gmail or Telegram.
Detailed descriptions are available in the sticky notes within the workflow screenshot above.
Node Names and Actions
Research and Report Generation
Test Workflow: Triggers the workflow for testing.
Input Validation: Validates the input query.
Query Refiner: Refines the query for better results.
Research AI Agent: Coordinates research using OpenAI.
OpenAI Chat Model: Generates content for the report.
Structured Output Parser: Parses OpenAI output into structured data.
OpenAI Chat Middle Memory: Retains context during research.
Wikipedia Google Search: Fetches data from Wikipedia.
SerpApi Google Search: Fetches data via SerpApi.
Merge Split Items: Merges data from multiple sources.
Aggregate: Aggregates all research data.
Generate PDF HTML: Creates an HTML report.
Convert HTML to PDF: Converts HTML to PDF.
Download PDF: Downloads the PDF file.
Send PDF to Telegram: Sends the PDF via Telegram.
Send Research PDF on Gmail: Sends the PDF via Gmail.
Customization Tips
Expand Data Sources üì°: Add more sources (e.g., academic databases) by adding nodes to Merge Split Items.
Change Report Style ‚úçÔ∏è: Update the Generate PDF HTML node to modify the HTML template (e.g., adjust styling or sections).
Alternative Delivery üìß: Add nodes to send the PDF via other platforms (e.g., Slack).
Adjust AI Model üß†: Modify the OpenAI Chat Model node to use a different model (e.g., gpt-3.5-turbo)."
Automate PDF Image Extraction & Analysis with GPT-4o and Google Drive,https://n8n.io/workflows/3567-automate-pdf-image-extraction-and-analysis-with-gpt-4o-and-google-drive/,"Use Case
Manually extracting images from PDF files for analysis is often slow and inefficient. Many users resort to taking screenshots of each page, uploading them to an AI tool like OpenAI for image analysis, and then manually copying the insights into a document. This manual process is time-consuming and prone to errors.
This workflow streamlines the entire process by automatically extracting images from a PDF, analyzing them using the GPT-4o model, and saving the results in seconds‚Äîeliminating the need for manual effort.
What This Workflow Does
Extracts all images from the uploaded PDF file automatically
The workflow scans each page of the PDF and identifies embedded images without manual intervention.
Uses the GPT-4o model to analyze each extracted image
Each image is processed through GPT-4o to generate descriptive insights, summaries, or context-specific analysis depending on the use case.
Saves the analysis results to a .txt file, including image URLs
The final output is a plain text file containing both the image URLs (e.g., hosted on cloud storage) and the corresponding GPT-4o analysis, ready for further use or sharing.
Setup
1.Set up your credentials when you first open the workflow. You‚Äôll need accounts for OpenAI, Convert API, and Google Drive.
2.Convert API does not rate-limit your API, sometimes you may receive 503 service unavailable error.
Nevertheless, it doesn‚Äôt mean that you cannot convert your file. It simply means that you should retry the conversion in a few seconds.
3.Upload a PDF with images to Google Drive.
4.Remove unnecessary parts and retrieve image-related information.
5.Integrate image and image analysis information together.
6.Analyze each image using the OPENAI GPT-4o model.
7.Retrieve all image analysis content and image URL
8.Integrate multiple image URLs and analysis content
9.Output content to a .txt file.
Template was created in n8n v1.83.2
How to Customize
Replace the manual trigger with a Google Drive trigger or other automation triggers
Change the image analysis model (e.g., switch or fine-tune GPT-4o)
Send the results to other platforms (e.g., Slack, Telegram, LINE, etc.) instead of saving to a .txt file"
AI-Powered YouTube Shorts Automation: Create & Publish with OpenAI & ElevenLabs,https://n8n.io/workflows/3553-ai-powered-youtube-shorts-automation-create-and-publish-with-openai-and-elevenlabs/,"üë• Who is this for?
Digital marketers, content creators, social media managers, and businesses who want to use AI marketing automation for YouTube Shorts without spending hours on production. This AI workflow helps anyone looking to create more short-form video marketing content without needing fancy editing skills or tons of time. It's perfect for marketing teams who want to automate content creation while keeping their brand's voice and boosting audience engagement through artificial intelligence technology.
üîç What problem does this workflow solve?
Let's be honest - creating high-performing YouTube Shorts consistently takes way too much work. You need AI script generation, voiceover production, video editing, and a solid content marketing strategy to keep your channel growing. Our intelligent automation workflow turns this whole headache into a simple two-click process, while still giving you videos that look and sound professional. This AI solution tackles the real marketing challenge of scaling short-form video production without sacrificing quality or burning through your team's resources.
‚öôÔ∏è What this workflow does
This all-in-one AI marketing automation solution handles your entire YouTube Shorts creation process through five easy steps:
üß† Smart Idea Generation: Advanced AI creates engaging video concepts, SEO-optimized titles, and descriptions that work with YouTube's algorithm and improve content marketing performance
üé§ Natural Voice Creation: Makes professional-sounding voiceovers using ElevenLabs' artificial intelligence technology, no recording equipment needed
üñºÔ∏è Automatic Visual Creation: Uses cutting-edge AI models to make custom images and animated clips that match your video marketing style and brand identity
üé¨ Smart Video Editing: AI-powered editing automatically combines all elements with smooth transitions for maximum viewer retention and marketing impact
üì± Easy Content Distribution: Automates the YouTube publishing process with proper metadata to maximize your marketing reach and content discovery
The whole marketing automation system runs through Telegram with just two human checkpoints - approving the initial AI idea and giving the final video a thumbs-up. This makes YouTube marketing automation so much easier while letting you keep control over the AI-generated content that gets published.
üõ†Ô∏è Setup (About 10-15 minutes)
Before using this AI marketing automation tool, you'll need:
n8n installation (cloud or self-hosted)
Telegram account (to interact with your workflow)
OpenAI API Key (for AI content generation)
ElevenLabs API Key (for AI voice creation)
Replicate API Key (for AI video generation)
Cloudinary account (for media asset storage)
Creatomate API Key (for automated video assembly)
0CodeKit API Key (for script processing automation)
YouTube channel with API access (for content publishing)
Just add your API keys to the ""Set API Keys"" node, connect your Telegram bot, and you're ready to start your automated marketing content creation. We've included a step-by-step video walkthrough in the workflow to make setup super easy for your AI marketing system.
üîß How to customize this workflow to your needs
This AI marketing workflow offers tons of flexibility to fit your specific content strategy:
Voice Options: Pick from different AI voices to match your brand's tone and what your audience prefers in marketing content
Visual Styles: Choose from over 40 different AI image models to get the perfect visual marketing approach
Video Effects: Try various artificial intelligence video models for different animation styles that make your content marketing more engaging
Content Tweaking: Adjust the AI prompts to perfectly align with your brand voice and content marketing strategy
YouTube Settings: Easily optimize category selection and metadata for better audience targeting with your automated marketing content
Every part of the AI automation workflow comes with easy-to-follow instructions in color-coded notes, so even if you're new to marketing automation tools, you can still customize everything to fit your needs.
Transform your YouTube Shorts marketing strategy with this complete AI automation solution that brings together multiple artificial intelligence technologies to help you create consistent, high-quality short-form video content that drives engagement and scales your digital marketing efforts without the usual production headaches."
"Slack AI Chatbot for business team with RAG, Claude 3.7 Sonnet and Google Drive",https://n8n.io/workflows/3414-slack-ai-chatbot-for-business-team-with-rag-claude-37-sonnet-and-google-drive/,"Imagine having an AI chatbot on Slack that seamlessly integrates with your company‚Äôs workflow, automating repetitive requests. No more digging through emails or documents to find answers about IT requests, company policies, or vacation days‚Äîjust ask the bot, and it will instantly provide the right information.
With its 24/7 availability, the chatbot ensures that team members get immediate support without waiting for a colleague to be online, making assistance faster and more efficient.
Moreover, this AI-powered bot serves as a central hub for internal communication, allowing everyone to quickly access procedures, documents, and company knowledge without searching manually. A simple Slack message is all it takes to get the information you need, enhancing productivity and collaboration across teams.
How It Works
Slack Trigger: The workflow starts when a user mentions the AI bot in a Slack channel. The trigger captures the message and forwards it to the AI Agent.
AI Agent Processing:
The AI Agent, powered by Anthropic's Claude 3.7 Sonnet model, processes the query.
It uses Retrieval-Augmented Generation (RAG) to fetch relevant information from the company‚Äôs internal knowledge base stored in Qdrant (a vector database).
A Simple Memory buffer retains recent conversation context (last 10 messages) for continuity.
Knowledge Retrieval:
The RAG tool searches Qdrant‚Äôs vector store using OpenAI embeddings to find the most relevant document chunks (top 10 matches).
Response Generation:
The AI synthesizes the retrieved data into a concise, structured response (1-2 sentences for the answer, 2-3 supporting details, and a source citation).
The response is formatted in Slack-friendly markdown (bullet points, blockquotes) and sent back to the user.
Set Up Steps
Prepare Qdrant Vector Database:
Create a Qdrant collection via HTTP request (Create collection node).
Optionally, refresh/clear the collection (Refresh collection node) before adding new documents.
Load Company Documents:
Fetch files from a Google Drive folder (Get folder ‚Üí Download Files).
Process documents: Split text into chunks (Token Splitter) and generate embeddings (Embeddings OpenAI2).
Store embeddings in Qdrant (Qdrant Vector Store1).
Configure Slack Bot:
Create a Slack bot via Slack API with required permissions
Add the bot to the desired Slack channel and note the channelId for the workflow.
Deploy AI Components:
Connect the AI Agent to Anthropic‚Äôs model, RAG tool, and memory buffer.
Ensure OpenAI embeddings are configured for both RAG and document processing.
Test & Activate:
Use the manual trigger (When clicking ‚ÄòTest workflow‚Äô) to validate document ingestion.
Activate the workflow to enable real-time Slack interactions.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Automatic Reminders For Follow-ups with AI and Human in the loop Gmail,https://n8n.io/workflows/3123-automatic-reminders-for-follow-ups-with-ai-and-human-in-the-loop-gmail/,"This n8n template extends the idea of follow-up reminders by having an AI agent suggest and book the next call or message to re-engage prospects which have been ignored.
What makes this template particularly interesting and actually usable is that it uses the Human-in-the-loop approach to wait for a user's approval before actually making the booking or otherwise not if the user declined.
A twist on a traditional idea where we can reduce the number of actionable tasks a human has to make by delegating them to AI.
How it works
A scheduled trigger checks your google calendar for sales meetings which happened a few days ago.
For each event, gmail search is used to figure out if a follow-up message has been sent or received from the other party since the meeting. If none, it might mean the user needs a reminder to follow-up.
For leads applicable for follow-up, we first get an AI Agent to find available meeting slots in the calendar.
These slots and reminder are sent to the user via send-and-approval mode of the gmail node. The user replies in natural language either picking a slot, suggesting an entirely new slot or declines the request.
When accepted, another AI Agent books the meeting in the calendar with the proposed dates and lead.
When declined, no action is taken.
How to use
Update all calendar nodes (+subnodes) to point to the right calendar. If this is a shared-purpose calendar, you may need to either filter or create a new calendar.
Update the gmail nodes to point to the right accounts.
Requirements
Google OAuth for Email and Calendar
OpenAI for LLM
Customising the template
Not using Google? Swap out for Microsoft Outlook/Calendar or something else.
Try swapping out or adding in additional send-for-approval methods such as telegram or whatsapp."
"Angie, Personal AI Assistant with Telegram Voice and Text",https://n8n.io/workflows/2462-angie-personal-ai-assistant-with-telegram-voice-and-text/,"How it works:
This project creates a personal AI assistant named Angie that operates through Telegram. Angie can summarize daily emails, look up calendar entries, remind users of upcoming tasks, and retrieve contact information. The assistant can interact with users via both voice and text inputs.
Step-by-step:
Telegram Trigger: The workflow starts with a Telegram trigger that listens for incoming message events. The system determines if the incoming message is voice or text. If voice, the voice file is retrieved and transcribed to text using OpenAI's API Speech to Text
AI Assistant: The telegram request is passed to the AI assistant (Angie).
Tools Integration: The AI assistant is equipped with several tools:
Get Email: Uses Gmail API to fetch recent emails, filtering by date.
Get Calendar: Retrieves calendar entries for specified dates.
Get Tasks: Connects to a Baserow (open-source Airtable alternative) database to fetch to-do list items.
Get Contacts: Also uses Baserow to retrieve contact information.
Response Generation: The AI formulates a response based on the gathered information and sends back to the user on Telegram"
Discord Channel Creation from Google Sheets with Member Notifications,https://n8n.io/workflows/4719-discord-channel-creation-from-google-sheets-with-member-notifications/,"Discord Channel Creation Automation - n8n Workflow
A comprehensive n8n automation that monitors Google Sheets for new project entries, automatically creates dedicated Discord channels, and sends formatted member notifications with all essential project details.
üìã Overview
This workflow provides an automated Discord channel creation solution that eliminates manual channel setup and ensures consistent team communication. Perfect for agencies, development teams, and project-based organizations that need to streamline their Discord workspace management.
‚ú® Key Features
üîç Automated Monitoring: Continuously watches Google Sheets for new entries requiring Discord channels
üì¢ Discord Integration: Creates dedicated channels using Discord API for organized communication
üìä Smart Filtering: Only processes valid entries without existing Discord channels
üìß Member Notifications: Sends formatted announcements with key project details
üìà Status Tracking: Updates Google Sheets with Discord channel IDs and completion status
üîÑ Sequential Processing: Handles multiple channel requests with proper workflow sequencing
‚ö° Error Handling: Built-in validation and error management
üéØ Customizable Messages: Flexible Discord notification templates
üéØ What This Workflow Does
Input
Sheet Data: New entries in Google Sheets requiring Discord channels
Discord Configuration: Server and category settings for channel creation
Notification Settings: Member notification preferences and mentions
Processing
Monitor Trigger: Watches Google Sheets for new row additions
Data Validation: Filters entries that need Discord channel creation
Channel Creation: Creates new Discord channel with specified naming convention
Sheet Update: Records Discord channel ID and status in Google Sheets
Status Check: Verifies successful channel creation before messaging
Member Notification: Sends formatted announcement to Discord channel
Additional Details: Sends follow-up message with supplementary information
Completion Tracking: Marks channel creation process as complete
Output Data Points
Field Description Example
Entry ID Unique identifier for the entry ENTRY-2025-001
Title/Name Name or title from the sheet entry New Marketing Campaign
Category/Type Category or type of entry Marketing Project
Discord Channel ID ID of created Discord channel 1234567890123456789
Channel URL Direct link to Discord channel https://discord.com/channels/...
Creation Status Current status of channel creation process Discord Created, Message Sent
Timestamp When the channel creation was completed 2025-06-06T09:00:00Z
üöÄ Setup Instructions
Prerequisites
n8n instance (self-hosted or cloud)
Google account with Sheets access
Discord server with bot permissions
10-15 minutes for setup
Step 1: Import the Workflow
Copy the JSON workflow code from the provided file
In n8n: Workflows ‚Üí + Add workflow ‚Üí Import from JSON
Paste JSON and click Import
Step 2: Configure Discord Integration
Create Discord Bot:
Go to Discord Developer Portal
Create new application and bot
Copy bot token for credentials
Add bot to your Discord server with proper permissions
Set up Discord credentials:
In n8n: Credentials ‚Üí + Add credential ‚Üí Discord Bot API
Enter your Discord bot token
Test the connection
Configure Discord settings:
Note your Discord server (guild) ID
Create or identify the category for new project channels
Update guild ID and category ID in workflow nodes
Step 3: Configure Google Sheets Integration
Create Channel Request Sheet:
Go to Google Sheets
Create new spreadsheet named ""Discord Channel Requests"" or similar
Copy the Sheet ID from URL: https://docs.google.com/spreadsheets/d/SHEET_ID_HERE/edit
Set up Google Sheets credentials:
In n8n: Credentials ‚Üí + Add credential ‚Üí Google Sheets OAuth2 API
Complete OAuth setup and test connection
Prepare your data sheet with required columns:
Column A: Timestamp (auto-filled by form)
Column B: Entry Name/Title
Column C: Category/Type
Column D: Description
Column E: Contact/Owner Information
Column F: Entry ID
Column G: Discord ID (will be auto-filled)
Column H: Discord Channel Creation Status (will be auto-filled)
Step 4: Update Workflow Settings
Update Google Sheets nodes:
Open ""Monitor New Project Entries"" node
Replace document ID with your Sheet ID
Select your Google Sheets credential
Choose correct sheet/tab name
Update Discord nodes:
Open all Discord nodes
Replace guild ID with your Discord server ID
Replace category ID with your project category ID
Select your Discord Bot credential
Configure notification settings:
Open Discord message nodes
Replace demo@example.com with actual email if needed
Customize team mentions and message content
Adjust notification timing as needed
Step 5: Test & Activate
Add test entry:
Add sample data to your Google Sheet
Ensure all required fields are filled
Leave Discord ID column empty for testing
Test the workflow:
Activate workflow (toggle switch)
Add new row to trigger workflow
Verify Discord channel creation
Check for member notifications
Confirm sheet updates
üìñ Usage Guide
Adding New Channel Requests
Navigate to your Google Sheets document
Add new entry with all required information
Leave Discord ID and status columns empty
Workflow will automatically process within minutes
Check Discord for new channel and notifications
Understanding Status Updates
The workflow uses intelligent status tracking:
Empty Discord ID: Entry needs channel creation
""Discord Created"": Channel created, ready for notifications
""Discord Created, Message Sent"": Process complete
Error states: Check execution logs for troubleshooting
Customizing Member Notifications
To modify notification content, edit Discord message nodes:
Main announcement: Update ""Send Project Announcement Message"" node
Additional details: Update ""Send Additional Project Details"" node
Member mentions: Replace @everyone with specific roles
Message formatting: Customize using Discord markdown
üîß Customization Options
Adding More Data Fields
Edit Google Sheets trigger and message nodes to include:
Budget or resource information
Timeline and deadlines
Assigned team members or owners
Priority levels
Additional context or requirements
Modifying Discord Structure
Customize channel organization:
// Example: Add category to channel name
""name"": ""{{ $json['Category/Type'] }}-{{ $json['Entry ID'] }}""

// Example: Set channel topic
""topic"": ""Channel for {{ $json['Title/Name'] }} - {{ $json['Category/Type'] }}""
üö® Troubleshooting
Common Issues & Solutions
Issue Cause Solution
""Discord permission denied"" Bot lacks required permissions in Discord server Ensure bot has ""Manage Channels"" and ""Send Messages"" permissions
""Google Sheets trigger not firing"" Incorrect sheet permissions or credential issues Re-authenticate Google Sheets, check sheet sharing settings
""Channel creation failed"" Invalid guild ID, category ID, or duplicate channel names Verify Discord IDs, ensure unique entry IDs
""Messages not sending"" Discord rate limiting or invalid channel references Add delays between messages, verify channel creation success
""Workflow execution failed"" Missing data fields or validation errors Ensure all required fields are populated
üìä Use Cases & Examples
1. Project Management Automation
Goal: Streamline project channel creation process
Monitor new project requests or approvals
Create dedicated project workspaces
Notify relevant team members instantly
Track channel setup completion
2. Event Organization
Goal: Organize events with dedicated Discord channels
Create channels for conferences, meetups, or workshops
Include event details in notifications
Tag organizers and participants
Maintain event communication history
3. Team Department Organization
Goal: Manage department or team-specific channels
Separate channels for different departments
Share department announcements and updates
Coordinate team assignments
Track departmental activities
4. Community Management
Goal: Organize community groups and special interest channels
Create channels for community groups
Share group information and guidelines
Facilitate member-to-member communication
Track community engagement and growth
üìà Performance & Limits
Expected Performance
Metric Value
Processing time 30-60 seconds per entry
Concurrent entries 5-10 simultaneous (depends on Discord limits)
Success rate 95%+ for valid data
Daily capacity 100+ entries (depends on rate limits)
Resource Usage
Memory: ~50MB per execution
Storage: Minimal (data stored in Google Sheets)
API calls: 3-5 Discord calls + 2-3 Google Sheets calls per entry
Execution time: 1-2 minutes for complete process
üìã Quick Setup Checklist
Before You Start
n8n instance running (self-hosted or cloud)
Google account with Sheets access
Discord server with admin permissions
Discord bot created and added to server
15 minutes for complete setup
Setup Steps
Import Workflow - Copy JSON and import to n8n
Configure Discord - Set up bot credentials and test
Create Google Sheet - New sheet with required columns
Set up Google Sheets credentials - OAuth setup and test
Update workflow settings - Replace IDs and credentials
Test with sample entry - Add test entry and verify
Activate workflow - Turn on monitoring trigger
üéâ Ready to Use!
Your workflow URL: https://your-n8n-instance.com/workflow/discord-channel-creation
üéØ Happy Channel Management! This workflow provides a solid foundation for automated Discord channel creation. Customize it to fit your specific team needs and communication requirements."
Automate HubSpot to Salesforce Lead Creation with Explorium AI Enrichment,https://n8n.io/workflows/4836-automate-hubspot-to-salesforce-lead-creation-with-explorium-ai-enrichment/,"Automatically enrich prospect data from HubSpot using Explorium and create leads in Salesforce
This n8n workflow streamlines the process of enriching prospect information by automatically pulling data from HubSpot, processing it through Explorium's AI-powered tools, and creating new leads in Salesforce with enhanced prospect details.
Credentials Required
To use this workflow, set up the following credentials in your n8n environment:
HubSpot
Type: App Token (or OAuth2 for broader compatibility)
Used for: triggering on new contacts, fetching contact data
Explorium API
Type: Generic Header Auth
Header: Authorization
Value: Bearer YOUR_API_KEY
Get explorium api key
Salesforce
Type: OAuth2 or Username/Password
Used for: creating new lead records
Go to Settings ‚Üí Credentials, create these three credentials, and assign them in the respective nodes before running the workflow.
Workflow Overview
Node 1: HubSpot Trigger
This node listens for real-time events from the connected HubSpot account. Once triggered, the node passes metadata about the event to the next step in the flow.
Node 2: HubSpot
This node fetches contact details from HubSpot after the trigger event.
Credential: Connected using a HubSpot App Token
Resource: Contact
Operation: Get Contact
Return All: Disabled
This node retrieves the full contact details needed for further processing and enrichment.
Node 3: Match prospect
This node sends each contact's data to Explorium's AI-powered prospect matching API in real time.
Method: POST
Endpoint: https://api.explorium.ai/v1/prospects/match
Authentication: Generic Header Auth (using a configured credential)
Headers: Content-Type: application/json
The request body is dynamically built from contact data, typically including: full_name, company_name, email, phone_number, linkedin. These fields are matched against Explorium's intelligence graph to return enriched or validated profiles.
Response Output: total_matches, matched_prospects, and a prospect_id. Each response is used downstream to enrich, validate, or create lead information.
Node 4: Filter
This node filters the output from the Match prospect step to ensure that only valid, matched results continue in the flow. Only records that contain at least one matched prospect with a non-null prospect_id are passed forward.
Status: Currently deactivated (as shown by the ""Deactivate"" label)
Node 5: Extract Prospect IDs from Matched Results
This node extracts all valid prospect_id values from previously matched prospects and compiles them into a flat array. It loops over all matched items, extracts each prospect_id from the matched_prospects array and returns a single object with an array of all prospect_ids.
Node 6: Explorium Enrich Contacts Information
This node performs bulk enrichment of contacts by querying Explorium with a list of matched prospect_ids.
Node Configuration:
Method: POST
Endpoint: https://api.explorium.ai/v1/prospects/contacts_information/bulk_enrich
Authentication: Header Auth (using saved credentials)
Headers: ""Content-Type"": ""application/json"", ""Accept"": ""application/json""
Returns enriched contact information, such as:
emails: professional/personal email addresses
phone_numbers: mobile and work numbers
professions_email, professional_email_status, mobile_phone
Node 7: Explorium Enrich Profiles
This additional enrichment node provides supplementary contact data enhancement, running in parallel with the primary enrichment process.
Node 8: Merge
This node combines multiple data streams from the parallel enrichment processes into a single output, allowing you to consolidate data from different Explorium enrichment endpoints. The ""combine"" setting indicates it will merge the incoming data streams rather than overwriting them.
Node 9: Code - flatten
This custom code node processes and transforms the merged enrichment data before creating the Salesforce lead. It can be used to:
Flatten nested data structures
Format data according to Salesforce field requirements
Apply business logic or data validation
Map Explorium fields to Salesforce lead properties
Handle data type conversions
Node 10: Salesforce
This final node creates new leads in Salesforce using the enriched data returned by Explorium.
Credential: Salesforce OAuth2 or Username/Password
Resource: Lead
Operation: Create Lead
The node creates new lead records with enriched information including contact details, company information, and professional data obtained through the Explorium enrichment process.
Workflow Flow Summary
Trigger: HubSpot webhook triggers on new/updated contacts
Fetch: Retrieve contact details from HubSpot
Match: Find prospect matches using Explorium
Filter: Keep only successfully matched prospects (currently deactivated)
Extract: Compile prospect IDs for bulk enrichment
Enrich: Parallel enrichment of contact information through multiple Explorium endpoints
Merge: Combine enrichment results
Transform: Flatten and prepare data for Salesforce (Code node)
Create: Create new lead records in Salesforce
This workflow ensures comprehensive data enrichment while maintaining data quality and providing a seamless integration between HubSpot prospect data and Salesforce lead creation. The parallel enrichment structure maximizes data collection efficiency before creating high-quality leads in your CRM system."
Generate Personalized Strava Ride Titles & Descriptions with DeepSeek AI,https://n8n.io/workflows/4736-generate-personalized-strava-ride-titles-and-descriptions-with-deepseek-ai/,"Title: Automatic Strava Titles & Descriptions Generation with AI
Description:
This n8n workflow connects your Strava account to an AI to automatically generate personalized titles and descriptions for every new cycling activity. It leverages the native Strava trigger to detect new activities, extracts and formats ride data, then queries an AI agent (OpenRouter, ChatGPT, etc.) with an optimized prompt to get a catchy title and inspiring description. The workflow then updates the Strava activity in real time, with zero manual intervention.
Key Features:
Secure connection to the Strava API (OAuth2)
Automatic triggering for every new activity
Intelligent data preparation and formatting
AI-powered generation of personalized content (title + description)
Instant update of the activity on Strava
Use Cases:
Cyclists wanting to automatically enhance their Strava rides
Sports content creators
Community management automation for sports groups
Prerequisites:
Strava account
Strava OAuth2 credentials set up in n8n
Access to a compatible AI agent (OpenRouter, ChatGPT, etc.)
Benefits:
Saves time
Advanced personalization
Boosts the appeal of every ride to your community"
"YouTube Video Summaries with GPT-4o, Slack Approvals & Reddit Posting",https://n8n.io/workflows/4581-youtube-video-summaries-with-gpt-4o-slack-approvals-and-reddit-posting/,"üöÄ AI-Powered YouTube Video Summary Distributor: From Channel to Community!
Workflow Overview
This sophisticated n8n automation transforms YouTube content discovery into a seamless, multi-platform intelligence sharing process. By intelligently connecting YouTube RSS, AI summarization, and content distribution platforms, the workflow:
Discovers New Content:
Monitors YouTube channels via RSS feed
Captures latest video uploads
Tracks content in real-time
AI-Powered Summarization:
Extracts video metadata
Generates concise, meaningful summaries
Leverages GPT-4o for intelligent content analysis
Intelligent Distribution:
Logs summaries in Google Sheets
Sends summaries to Slack for review
Publishes approved content to Reddit
Detailed Setup Instructions
1. YouTube Data API Configuration
Prerequisites
Google Cloud Console account
Enabled YouTube Data API v3
Setup Steps:
Go to Google Cloud Console
Create a new project
Enable YouTube Data API v3
Create credentials (API Key)
Store API key securely in n8n credentials
Obtain channel RSS feed URL
2. OpenAI API Setup
Prerequisites
OpenAI account
Active API subscription
Configuration:
Visit OpenAI Platform
Generate API key
Select GPT-4o model
Configure API key in n8n credentials
Set up billing and usage limits
3. Slack Integration
Prerequisites
Slack workspace
Slack app permissions
Setup Process:
Create a Slack app in your workspace
Configure OAuth scopes for sending messages
Install app to workspace
Obtain webhook or OAuth token
Configure in n8n Slack node
4. Reddit API Configuration
Prerequisites
Reddit account
Reddit application created
Steps:
Go to Reddit's app preferences
Create a new application
Obtain client ID and secret
Configure OAuth2 credentials in n8n
Select target subreddit
Workflow Customization
Channel Modification
Replace YouTube RSS feed URL in trigger node
Adjust channel_id parameter
Modify extraction logic if needed
Subreddit Customization
Change subreddit parameter in Reddit node
Adjust title and text formatting
AI Summarization Tuning
Modify system message in Summarizer Agent
Adjust prompt for different content types
Implement custom filtering
Key Customization Points
Modify RSS feed URL
Change target subreddit
Adjust AI summarization prompt
Add custom filtering logic
Implement multi-channel support
Technical Requirements
n8n v0.220.0 or higher
YouTube Data API v3
OpenAI API access
Slack workspace
Reddit application
Stable internet connection
Potential Use Cases
Content creator content tracking
Research and trend analysis
Social media content distribution
Automated content curation
Community engagement
Security Considerations
Use environment variables for API keys
Implement proper OAuth2 authentication
Respect platform usage guidelines
Maintain user privacy
Future Enhancement Roadmap
Multi-language support
Advanced content filtering
Sentiment analysis integration
Expanded platform distribution
Customizable summarization parameters
Workflow Visualization
[YouTube RSS Trigger]
    ‚¨áÔ∏è
[Extract Channel ID]
    ‚¨áÔ∏è
[Fetch Video Details]
    ‚¨áÔ∏è
[AI Summarization]
    ‚¨áÔ∏è
[Google Sheets Logging]
    ‚¨áÔ∏è
[Slack Approval]
    ‚¨áÔ∏è
[Reddit Publishing]
Hashtag Performance Boost üöÄ
#YouTubeAutomation #AIContentDistribution #WorkflowInnovation #ContentCuration #AIMarketing #DigitalMediaTech #AutomatedSummaries #CrossPlatformContent
Connect With Me
Ready to revolutionize your content workflow?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your content strategy with intelligent, automated workflows!
Note: Always test and customize the workflow to fit your specific use case and comply with platform guidelines."
Rank Math Bulk Title & Description Optimizer for WordPress,https://n8n.io/workflows/4646-rank-math-bulk-title-and-description-optimizer-for-wordpress/,"Bulk WordPress SEO Meta Optimizer with Rank Math & AI
This n8n workflow, along with its companion WordPress plugin (rank_math_tools.zip), automates the bulk optimization of your WordPress posts' Rank Math SEO titles and descriptions using AI.
Stop manually crafting metas and let AI supercharge your SEO efforts efficiently!
How it works:
This workflow streamlines your SEO process through several automated steps:
Fetches Post IDs: Retrieves all published post IDs from your WordPress site using a secure, custom API endpoint provided by the companion plugin.
Gathers Post Content: For each identified post, it accesses the existing title, slug, and main rendered content to provide context for the AI.
AI-Powered Meta Generation: Leverages an AI model of your choice via OpenRouter (defaulting to Claude 3.7 Sonnet) to generate an optimized SEO title and meta description. This generation is guided by a detailed, customizable prompt within the ""Create Meta Infos"" node, allowing you to define the style, tone, and SEO strategy.
Updates WordPress Metas: Sends the newly generated SEO title and description back to your WordPress site via another custom API endpoint. The companion plugin then updates the specific Rank Math fields for each post.
Refreshes Modified Date: The companion plugin automatically updates the post's 'last modified' date in WordPress whenever meta information is changed, which can be a positive signal for search engines.
Selective Processing: An ""IF"" node (""Should I Rewrite"") intelligently determines whether a post's metas need updating based on predefined conditions (e.g., empty metas or placeholder content), optimizing resource usage.
Setup Steps:
Get up and running in approximately 15-25 minutes by following these high-level steps. Detailed instructions are available in the comprehensive documentation provided with your purchase and within the sticky notes inside the n8n workflow itself.
Install the Companion WordPress Plugin:
Download the rank_math_tools.zip file.
Upload and activate it on your WordPress site.
(Estimated time: 2-5 minutes)
Configure WordPress API Authentication:
In your WordPress admin area, go to your user profile and create a new Application Password.
Ensure the user has at least an ""Editor"" role.
Copy the generated password.
(Estimated time: 2-5 minutes)
Set Up n8n Credentials:
In n8n, create a new ""WordPress API"" credential using your WordPress username, the Application Password you just copied, and your site's Base URL.
Create a new ""OpenRouter Api"" credential using your OpenRouter API key.
(Estimated time: 5-10 minutes, assuming you have your OpenRouter key ready)
Import and Configure the n8n Workflow:
Download the Bulk_Wordpress_Meta_Data_Optimizer_Gumroad.json workflow file.
Import it into your n8n instance.
Link your newly created WordPress and OpenRouter credentials to the respective nodes in the workflow.
Verify and update your WordPress site URL in the ""settings"" node (ensure it has a trailing slash /).
Review the ""Limit"" node ‚Äì it's pre-set to process only 5 items for safe initial testing.
(Estimated time: 5-10 minutes)
Benefits:
Full SEO Meta Automation: Drastically reduce manual effort by automatically generating and updating SEO titles and descriptions for all your posts.
Bulk Processing Power: Efficiently handle meta optimization for hundreds or thousands of WordPress posts.
Highly Customizable AI Output: Fine-tune the AI's generation style, keyword focus, and direct response techniques by editing the detailed prompt within the ""Create Meta Infos"" node.
Improved SEO Consistency & Quality: Ensure all your articles have relevant, compelling, and optimized meta tags, adhering to best practices.
Signal Freshness to Search Engines: The automatic update of the post 'last modified' date upon meta change can help improve crawl frequency and SEO.
Resource-Efficient Updates: The ""Should I Rewrite"" node prevents unnecessary processing and API calls for posts that already have satisfactory meta information.
Save Time & Focus on Content: Free up valuable time to concentrate on creating great content, rather than on repetitive SEO tasks.
To understand exactly how to use and customize this workflow in detail, please refer to the comprehensive step-by-step documentation provided with your purchase.
For professional n8n automation services for your business, contact:
Phil | Inforeole"
"Automate RSS Content with AI: Summarize, Notify & Archive",https://n8n.io/workflows/4503-automate-rss-content-with-ai-summarize-notify-and-archive/,"üöÄ Product Overview
This n8n workflow template automates the entire lifecycle of RSS feed monitoring‚Äîperfect for tech news aggregation, content curation, and AI-powered updates. It reads RSS links from a Google Sheets file, fetches content, summarizes it using GPT, sends digest messages to Discord, and stores clean content into a Google Sheets database for future use.
üîÑ How It Works
‚úÖ 1. Scheduled RSS Fetching
Triggers every 24 hours via Schedule Trigger.
Reads a list of RSS feed URLs from a centralized Google Sheets file.
Splits them for individual processing.
üîç 2. Feed Scanning & Filtering
Loops through each RSS link and retrieves the latest entries.
Filters out content older than 24 hours to avoid duplicates or outdated info.
ü§ñ 3. AI Summarization & Messaging
Aggregates filtered entries.
Summarizes content using an AI Agent (OpenAI Chat Model).
Sends beautifully formatted summaries directly to Discord as a message, keeping your community or team informed.
üóÉÔ∏è 4. Persistent Storage
Converts each item to Markdown.
Writes entries to a second Google Sheets file, including a rate limiter (wait step) to avoid API limits.
üí° Why Use This Template?
No-code setup: Drag, drop, and deploy in n8n.
AI-powered: Smart summarization reduces content noise.
Discord integration: Great for community managers, news bots, or internal updates.
Database-ready: Stores everything cleanly for later analysis or republishing.
üß© Use Cases
Daily tech news digest for Discord communities.
Content curation for newsletters.
Competitive intelligence via targeted RSS feeds.
Auto-archiving of blog or podcast updates."
GitHub Automation Hub: Complete API Controls for AI Agents,https://n8n.io/workflows/4629-github-automation-hub-complete-api-controls-for-ai-agents/,"Quick Setup Guide:
1Ô∏è‚É£ Import this template into your n8n instance.
2Ô∏è‚É£ Configure GitHub credentials for the nodes.
3Ô∏è‚É£ Connect your MCP client
THAT'S IT!
tl;dr
Supercharge your GitHub project management
and development by integrating powerful AI capabilities directly into your n8n workflows!
This template deploys a robust SSE endpoint specifically designed for n8n's MCP client (or any MCP tool - like cursor). It unlocks a comprehensive suite of pre-configured GitHub tools, enabling your AI to act as an intelligent, automated team member.
Empower your AI to Autonomously Handle GitHub Tasks:
üß† Triage Issues: Automatically categorize, label, and assign new issues.
üßπ Combat Spam: Instantly remove unwanted issues or comments.
üìÑ Summarize Content: Condense complex Pull Requests or lengthy issue discussions for quick insights, especially for non-technical team members.
üìß Automate Reporting: Draft and send natural language email updates to management about critical, triaged issues.
And much more!
Core Capabilities at Your AI's Fingertips:
This workflow equips your AI agent with extensive GitHub control:
‚úÖ File Operations: Create, Edit, Delete, Get, and List files.
‚úÖ Issue Management: Create, Edit, Get, Comment on, and Lock/Unlock issues.
‚úÖ PR Review Oversight: Create, Get, Update, and List all reviews for a Pull Request.
‚úÖ Release Orchestration: Create, Get, Update, Delete, and List repository releases.
‚úÖ Repository Insights: Access details, issues, licenses, profiles, PRs, popular paths, and referrers.
‚úÖ User & Org Management: Get user/organization repositories; invite users to organizations.
‚úÖ GitHub Actions Control: Get, List, analyze usage, Enable/Disable, and Dispatch workflows.
Setup Guide:
1Ô∏è‚É£ Import this template into your n8n instance.
2Ô∏è‚É£ Configure GitHub credentials for the nodes. Best Practice: Use a dedicated GitHub account or a Personal Access Token (PAT) with minimal necessary permissions.
3Ô∏è‚É£ Connect your MCP client or AI system to the exposed SSE webhook URL (e.g., http://localhost:5678/mcp/v3805-fhrs-4afc-o5c2-389hfasfwj/sse).
‚ö†Ô∏è (Disabled by Default) Advanced API Access: Raw HTTP Request tools (GET, POST, PATCH, PUT, DELETE). Enable with extreme caution and tightly scoped credentials.
Ideal For:
Developers: Automating routine coding tasks, PR checks.
Project Managers: Streamlining issue tracking, progress reporting.
Contributors: Simplifying project interactions.
Maintainers: Reducing manual effort in triage, spam filtering, releases.
Innovators: Seeking deep AI integration with GitHub.
Drastic Time & Cost Savings ‚è≥üí∞:
Combine this template with other AI-powered n8n workflows like the Discord Message Proxy or the AI-Powered GitHub Bot to achieve significant automation of complex GitHub tasks‚Äîoften for less than the cost of a single hour of manual work!
Important Security Note on Custom API Calls: The Custom <METHOD> Github Request nodes are disabled by default. Their power to make any API call requires careful consideration. If you enable them, ensure the associated GitHub credentials have the absolute minimum necessary scope to prevent unintended consequences. You are responsible for their safe use.
This template is your key to unlocking intelligent automation within your GitHub ecosystem. Streamline your development processes and save valuable time today!"
"Automated Book Summarization with DeepSeek AI, Qdrant Vector DB & Google Drive",https://n8n.io/workflows/4566-automated-book-summarization-with-deepseek-ai-qdrant-vector-db-and-google-drive/,"üìö AI Book Summarizer with Vector Search ‚Äì n8n Automation
Overview
This n8n workflow automates the process of summarizing uploaded books from Google Drive using vector databases and LLMs. It uses Cohere for embeddings, Qdrant for storage and retrieval, and DeepSeek or your preferred LLM for summarization and Q&A. Designed for researchers, students, and productivity enthusiasts!
Result Example
Problem üõ†Ô∏è
‚è≥ Reading full books or papers to extract core ideas can take hours.
üß† Manually summarizing or searching inside long documents is inefficient and overwhelming.
Solution ‚úÖ
Use this workflow to:
Upload a book to Google Drive üì•
Auto-split and embed the content into Qdrant üîç
Summarize it using DeepSeek or another LLM ü§ñ
Store the final summary back to Google Drive üì§
Clean up the vector store afterward üßπ
üî• Result
‚ö° Instant AI-generated book summary
üí° Ability to perform semantic search and question-answering
üìÅ Summary saved back to your cloud
üß† Enhanced productivity for learning and research
Setup ‚öôÔ∏è (4‚Äì8 minutes)
1. Google Drive Setup
üîó Connect Google Drive credentials
üìÅ Create an input folder (e.g., book_uploads)
üìÅ Create an output folder (e.g., book_summaries)
‚ö° Trigger: Use File Created node to monitor book_uploads
üì• Summary will be saved in book_summaries
2. LLM & Embeddings Setup
üîë Create and test API keys for:
DeepSeek/OpenAI for summarization
Cohere for embeddings
Qdrant for vector storage
üß™ Ensure all credentials are added in n8n
How It Works üåü
üìÇ A file is uploaded to Google Drive
‚¨áÔ∏è File is downloaded
üß± It's processed, split into chunks, and sent to Qdrant using Cohere embeddings
‚ùì A Q&A chain with vector retriever performs information extraction
üß† A DeepSeek AI Agent analyzes and summarizes the book
üì§ The summary is saved to your Drive
üßΩ The Qdrant vector collection is deleted (clean-up)
What‚Äôs Included üì¶
‚úÖ Google Drive integration (input/output)
‚úÖ File chunking and embedding using Cohere
‚úÖ Vector storage with Qdrant
‚úÖ Q&A with vector retrieval
‚úÖ Summarization via DeepSeek or other LLM
‚úÖ Clean-up for minimal storage overhead
Customization üé®
You can tailor it to your use case:
üßë‚Äçüè´ Adjust summarization prompt for study notes or executive summaries
üåç Add translation node for multilingual support
üîç Enable long-term memory by skipping vector deletion
üì® Send summaries to Notion, Slack, or Email
üß© Use other LLM providers (OpenAI, Claude, Gemini, etc.)
üåê Explore more workflows
‚ù§Ô∏è Buy more workflows at: adamcrafts
ü¶æ Custom workflows at: adamcrafts@cloudysoftwares.com
adamaicrafts@gmail.com
Build once, customize endlessly, and scale your video content like never before. üöÄ"
"AI-Powered Knowledge Assistant using Google Sheets, OpenAI, and Supabase Vector Search",https://n8n.io/workflows/4477-ai-powered-knowledge-assistant-using-google-sheets-openai-and-supabase-vector-search/,"AI-Powered GitHub Commit Reviewer
Overview
Workflow Name: AI-Powered GitHub Commit Reviewer
Author: Akhil
Purpose: This n8n workflow triggers on a GitHub push event, fetches commit diffs, formats them into HTML, runs an AI-powered code review using Groq LLM, and sends a detailed review via email.
How It Works (Step-by-Step)
1. GitHub Trigger
Node Type: n8n-nodes-base.githubTrigger
Purpose: Initiates the workflow on GitHub push events.
Repo: akhilv77/relevance
Output: JSON with commit and repo details.
2. Parser
Node Type: n8n-nodes-base.set
Purpose: Extracts key info (repo ID, name, commit SHA, file changes).
3. HTTP Request
Node Type: n8n-nodes-base.httpRequest
Purpose: Fetches commit diff details using GitHub API.
Auth: GitHub OAuth2 API.
4. Code (HTML Formatter)
Node Type: n8n-nodes-base.code
Purpose: Formats commit info and diffs into styled HTML.
Output: HTML report of commit details.
5. Groq Chat Model
Node Type: @n8n/n8n-nodes-langchain.lmChatGroq
Purpose: Provides the AI model (llama-3.1-8b-instant).
6. Simple Memory
Node Type: @n8n/n8n-nodes-langchain.memoryBufferWindow
Purpose: Maintains memory context for AI agent.
7. AI Agent
Node Type: @n8n/n8n-nodes-langchain.agent
Purpose: Executes AI-based code review.
Prompt: Reviews for bugs, style, grammar, and security. Outputs styled HTML.
8. Output Parser
Node Type: n8n-nodes-base.code
Purpose: Combines commit HTML with AI review into one HTML block.
9. Gmail
Node Type: n8n-nodes-base.gmail
Purpose: Sends review report via email.
Recipient: akhilgadiraju@gmail.com
10. End Workflow
Node Type: n8n-nodes-base.noOp
Purpose: Marks the end.
Customization Tips
GitHub Trigger: Change repo/owner or trigger events.
HTTP Request: Modify endpoint to get specific data.
AI Agent: Update the prompt to focus on different review aspects.
Groq Model: Swap for other supported LLMs if needed.
Memory: Use dynamic session key for per-commit reviews.
Email: Change recipient or email styling.
Error Handling
Use Error Trigger nodes to handle failures in:
GitHub API requests
LLM generation
Email delivery
Use Cases
Instant AI-powered feedback on code pushes.
Pre-human review suggestions.
Security and standards enforcement.
Developer onboarding assistance.
Required Credentials
Credential Used By Notes
GitHub API (ID PSygiwMjdjFDImYb) GitHub Trigger PAT with repo and admin:repo_hook
GitHub OAuth2 API HTTP Request OAuth2 token with repo scope
Groq - Akhil (ID HJl5cdJzjhf727zW) Groq Chat Model API Key from GroqCloud
Gmail OAuth2 - Akhil (ID wqFUFuFpF5eRAp4E) Gmail Gmail OAuth2 for sending email
Final Note
Made with ‚ù§Ô∏è using n8n by Akhil."
"Automated Brand Mentions Tracker With GPT-4o, Google Sheets, and Email",https://n8n.io/workflows/4443-automated-brand-mentions-tracker-with-gpt-4o-google-sheets-and-email/,"This workflow enables you to automate the daily monitoring of how an AI model (like ChatGPT) responds to specific queries relevant to your market. It identifies mentions of your brand and predefined competitors, logs detailed interactions in Google Sheets, and delivers a comprehensive email report.
Main Use Cases
Monitor how your brand is mentioned by AI in response to relevant user queries.
Track mentions of key competitors to understand AI's comparative positioning.
Gain insights into AI's current knowledge and portrayal of your brand and market landscape.
Automate daily intelligence gathering on AI-driven brand perception.
How it works
The workflow operates as a scheduled process, organized into these stages:
Configuration & Scheduling
Triggers daily (or can be run manually).
Key variables are defined within the workflow: your brand name (e.g., ""YourBrandName""), a list of queries to ask the AI, and a list of competitor names to track in responses.
AI Querying
For each predefined query, the workflow sends a request to the OpenAI ChatGPT API (via an HTTP Request node).
Response Analysis
Each AI response is processed by a Code node to:
Check if your brand name is mentioned (case-insensitive).
Identify if any of the listed competitors are mentioned (case-insensitive).
Extract the core AI response content (limited to 500 characters for brevity in logs/reports).
Data Logging to Google Sheets
Detailed results for each query‚Äîincluding timestamp, date, the query itself, query index, your brand name, the AI's response, whether your brand was mentioned, and any errors‚Äîare appended to a specified Google Sheet.
Email Report Generation
A comprehensive HTML email report is compiled. This report summarizes:
Total queries processed, number of times your brand was mentioned, total competitor mentions, and any errors encountered.
A summary of competitor mentions, listing each competitor and how many times they were mentioned.
A detailed table listing each query, whether your brand was mentioned, and which competitors (if any) were mentioned in the AI's response.
Automated Reporting
The generated HTML email report is sent to specified recipients, providing a daily snapshot of AI interactions.
Summary Flow:
Schedule/Workflow Trigger ‚Üí Initialize Brand, Queries, Competitors (in Code node) ‚Üí For each Query: Query ChatGPT API ‚Üí Process AI Response (Check for Brand & Competitor Mentions) ‚Üí Log Results to Google Sheets ‚Üí Generate Consolidated HTML Email Report ‚Üí Send Email Notification
Benefits:
Fully automated daily monitoring of AI responses concerning your brand and competitors.
Provides objective insights into how AI models are representing your brand in user interactions.
Delivers actionable competitive intelligence by tracking competitor mentions.
Centralized logging in Google Sheets for historical analysis and trend spotting.
Easily customizable with your specific brand, queries, competitor list, and reporting recipients."
üßæ Automated Invoice Processing with Mistral OCR + GPT-4o-mini,https://n8n.io/workflows/4331-automated-invoice-processing-with-mistral-ocr-gpt-4o-mini/,"Automate the management of your invoices with this intelligent, end-to-end AI-powered workflow.
This advanced n8n workflow combines Mistral AI's powerful OCR capabilities with OpenAI's GPT-4o-mini to automatically extract, structure, and process invoice data with exceptional accuracy and minimal setup.
üîç Key Features
‚úÖ Advanced Document Processing - Leverages Mistral's state-of-the-art OCR technology specifically optimized for complex document formats
‚úÖ AI-Powered Data Structuring - Uses OpenAI's GPT-4o-mini to intelligently identify and organize invoice elements
‚úÖ Multi-Page Support - Seamlessly processes invoices of any length with precise page handling
‚úÖ Fully Automated Workflow - From detection to structured data in seconds, with zero manual intervention
‚úÖ Custom JSON Schema - Extracts comprehensive invoice data including vendor details, line items, and financial summaries
üìã Extracted Data Includes
Vendor & customer information
Invoice numbers, dates, and payment terms
Line items with descriptions, quantities, and prices
Financial totals and transaction breakdowns
User accounts and query details
üîß Implementation Details
This workflow demonstrates how to:
Monitor Google Drive for new invoice uploads
Convert documents to Base64 for API processing
Make authenticated calls to Mistral's specialized OCR API
Process and combine multi-page document text
Use AI agents with customized system prompts for data extraction
Parse unstructured text into standardized JSON schemas
üöÄ Getting Started
Sign up for a Mistral AI account: https://console.mistral.ai/
Configure your OpenAI API key for GPT-4o-mini access
Set up your Google Drive credentials
Import this workflow and run!
üí° Real-World Use Cases
Finance Teams: Automate invoice processing and approval workflows
Accounting Departments: Extract data for bookkeeping systems
Expense Management: Process vendor invoices with minimal manual effort
Document Management: Convert physical or PDF invoices into structured data
Business Intelligence: Analyze spending patterns across vendors and categories
Take your document processing to the next level by combining specialized OCR technology with advanced AI language models!
#invoice-processing #ocr #mistral-ai #openai #gpt4o #document-extraction #ai-automation #finance-automation"
"Extract & Summarize B2B Leads from Crunchbase with Bright Data, GPT-4o & Google Sheets",https://n8n.io/workflows/4250-extract-and-summarize-b2b-leads-from-crunchbase-with-bright-data-gpt-4o-and-google-sheets/,"Who this is for
The Crunchbase B2B Lead Discovery Pipeline is designed for sales teams, B2B marketers, business analysts, and data operations teams who need a reliable way to extract, structure, and summarize company information from Crunchbase to fuel lead generation and market intelligence.
This workflow is ideal for:
Sales Development Reps (SDRs) - Needing structured leads from Crunchbase
Marketing Analysts - Generating segmented outreach lists
Growth Teams - Identifying trending B2B startups
RevOps Teams - Automating company research pipelines
Data Teams - Consolidating insights into Google Sheets for dashboards
What problem is this workflow solving?
Manual extraction of company data from Crunchbase is time-consuming, inconsistent, and often lacks the contextual summary required for sales enablement or growth targeting.
This workflow automates the extraction, transformation, summarization, and delivery of Crunchbase company data into structured formats, making it instantly usable for B2B targeting and analysis.
It solves:
The difficulty of scaling lead discovery from Crunchbase
The need to summarize raw textual content for quick insights
The lack of integration between web scraping, LLM processing, and storage
What this workflow does
Markdown to Textual Data Extractor: Takes raw scraped markdown from Crunchbase and converts it into readable plain text using a basic LLM chain
Structured Data Extraction: Applies a parsing model (OpenAI) to extract structured fields such as company name, funding rounds, industry tags, location, and founding year
Summarization Chain: Generates an executive summary from the raw Crunchbase text using a summarization prompt template
Send to Google Sheets: Adds the structured data and summary into a Google Sheet for team access and further processing
Persist to Disk: Saves both raw and structured data files locally for archiving or further use
Webhook Notification: Sends a structured payload to a webhook endpoint (e.g., Slack, CRM, internal tools) with lead insights
Pre-conditions
You need to have a Bright Data account and do the necessary setup as mentioned in the ""Setup"" section below.
You need to have an OpenAI Account.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, Configure the Google Sheet Credentials with your own account. Follow this documentation - Set Google Sheet Credential
In n8n, configure the OpenAi account credentials.
Ensure the URL and Bright Data zone name are correctly set in the Set URL, Filename and Bright Data Zone node.
Set the desired local path in the Write a file to disk node to save the responses.
How to customize this workflow to your needs
LLM Prompt Customization :
Modify the extraction prompt to include additional fields like revenue, social links, leadership team
Adjust summarization tone (e.g., executive summary, sales-focused snapshot or marketing digest)
File Persistence
Store raw markdown, extracted JSON, and summary text separately for audit/debug
Webhook Notification
Connect to CRM (e.g., HubSpot, Salesforce) via webhook to automatically create leads
Send Slack notifications to alert sales reps when a new high-potential company is discovered"
"Multi-Platform Social Media Publisher with Blotato, GPT-4 Mini & Airtable",https://n8n.io/workflows/3816-multi-platform-social-media-publisher-with-blotato-gpt-4-mini-and-airtable/,"How it works
‚Ä¢ Automates multi-platform social media posting (Instagram, YouTube, TikTok, etc.) using AI-generated content
‚Ä¢ Integrates Airtable, n8n, and Blotato for full content scheduling and publishing
‚Ä¢ Supports both image and video uploads with dynamic text and account routing
Set up steps
‚Ä¢ Takes ~15‚Äì30 minutes to set up depending on how many platforms you connect
‚Ä¢ Requires Airtable personal access token and Blotato API key
‚Ä¢ Uses sticky notes throughout the workflow to explain config, tokens, and troubleshooting clearly"
Generate SEO Content Audit Reports with DataForSEO and Google Search Console,https://n8n.io/workflows/3809-generate-seo-content-audit-reports-with-dataforseo-and-google-search-console/,"Introduction
The Content SEO Audit Workflow is a powerful automated solution that generates comprehensive SEO audit reports for websites.
By combining the crawling capabilities of DataForSEO with the search performance metrics from Google Search Console, this workflow delivers actionable insights into content quality, technical SEO issues, and performance optimization opportunities.
The workflow crawls up to 1,000 pages of a website, analyzes various SEO factors including metadata, content quality, internal linking, and search performance, and then generates a professional, branded HTML report that can be shared directly with clients.
The entire process is automated, transforming what would typically be hours of manual analysis into a streamlined workflow that produces consistent, thorough results.
This workflow bridges the gap between technical SEO auditing and practical, client-ready deliverables, making it an invaluable tool for SEO professionals and digital marketing agencies.
Who is this for?
This workflow is designed for SEO consultants, digital marketing agencies, and content strategists who need to perform comprehensive content audits for clients or their own websites. It's particularly valuable for professionals who:
Regularly conduct SEO audits as part of their service offerings
Need to provide branded, professional reports to clients
Want to automate the time-consuming process of content analysis
Require data-driven insights to inform content strategy decisions
Users should have basic familiarity with SEO concepts and metrics, as well as a basic understanding of how to set up API credentials in n8n.
While no coding knowledge is required to run the workflow, users should be comfortable with configuring workflow parameters and following setup instructions.
What problem is this workflow solving?
Content audits are essential for SEO strategy but are traditionally labor-intensive and time-consuming. This workflow addresses several key challenges:
Manual Data Collection: Gathering data from multiple sources (crawlers, Google Search Console, etc.) typically requires hours of work. This workflow automates the entire data collection process.
Inconsistent Analysis: Manual audits can suffer from inconsistency in methodology. This workflow applies the same comprehensive analysis criteria to every page, ensuring thorough and consistent results.
Report Generation: Creating professional, client-ready reports often requires additional design work after the analysis is complete. This workflow generates a fully branded HTML report automatically.
Data Integration: Correlating technical SEO issues with actual search performance metrics is difficult when working with separate tools. This workflow seamlessly integrates crawl data with Google Search Console metrics.
Scale Limitations: Manual audits become increasingly difficult with larger websites. This workflow can efficiently process up to 1,000 pages without additional effort.
What this workflow does
Overview
The Content SEO Audit Workflow crawls a specified website, analyzes its content for various SEO issues, retrieves performance data from Google Search Console, and generates a comprehensive HTML report.
The workflow identifies issues in five key categories: status issues (404 errors, redirects), content quality (thin content, readability), metadata SEO (title/description issues), internal linking (orphan pages, excessive click depth), and performance (underperforming content).
The final report includes executive summaries, detailed issue breakdowns, and actionable recommendations, all branded with your company's colors and logo.
Process
Initial Configuration: The workflow begins by setting parameters including the target domain, crawl limits, company information, and branding colors.
Website Crawling: The workflow creates a crawl task in DataForSEO and periodically checks its status until completion.
Data Collection: Once crawling is complete, the workflow:
Retrieves the raw audit data from DataForSEO
Extracts all URLs with status code 200 (successful pages)
Queries Google Search Console API for each URL to get clicks and impressions data
Identifies 404 and 301 pages and retrieves their source links
Data Analysis: The workflow analyzes the collected data to identify issues including:
Technical issues: 404 errors, redirects, canonicalization problems
Content issues: thin content, outdated content, readability problems
SEO metadata issues: missing/duplicate titles and descriptions, H1 problems
Internal linking issues: orphan pages, excessive click depth, low internal links
Performance issues: underperforming pages based on GSC data
Report Generation: Finally, the workflow:
Calculates a health score based on the severity and quantity of issues
Generates prioritized recommendations
Creates a comprehensive HTML report with interactive tables and visualizations
Customizes the report with your company's branding
Provides the report as a downloadable HTML file
Setup
To set up this workflow, follow these steps:
Import the workflow: Download the JSON file and import it into your n8n instance.
Configure DataForSEO credentials:
Create a DataForSEO account at https://app.dataforseo.com/api-access (they offer a free $1 credit for testing)
Add a new ""Basic Auth"" credential in n8n following the HTTP Request Authentication guide
Assign this credential to the ""Create Task"", ""Check Task Status"", ""Get Raw Audit Data"", and ""Get Source URLs Data"" nodes
Configure Google Search Console credentials:
Add a new ""Google OAuth2 API"" credential following the Google OAuth guide
Ensure your Google account has access to the Google Search Console property you want to analyze
Assign this credential to the ""Query GSC API"" node
Update the ""Set Fields"" node with:
dfs_domain: The website domain you want to audit
dfs_max_crawl_pages: Maximum number of pages to crawl (default: 1000)
dfs_enable_javascript: Whether to enable JavaScript rendering (default: false)
company_name: Your company name for the report branding
company_website: Your company website URL
company_logo_url: URL to your company logo
brand_primary_color: Your primary brand color (hex code)
brand_secondary_color: Your secondary brand color (hex code)
gsc_property_type: Set to ""domain"" or ""url"" depending on your Google Search Console property type
Run the workflow: Click ""Start"" and wait for it to complete (approximately 20 minutes for 500 pages).
Download the report: Once complete, download the HTML file from the ""Download Report"" node.
How to customize this workflow to your needs
This workflow can be adapted in several ways to better suit your specific requirements:
Adjust crawl parameters: Modify the ""Set Fields"" node to change:
The maximum number of pages to crawl (dfs_max_crawl_pages). This workflow supports up to 1000 pages.
Whether to enable JavaScript rendering for JavaScript-heavy sites (dfs_enable_javascript)
Customize issue detection thresholds: In the ""Build Report Structure"" code node, you can modify:
Word count thresholds for thin content detection (currently 1500 words)
Click depth thresholds (currently flags pages deeper than 4 clicks)
Title and description length parameters (currently 40-60 chars for titles, 70-155 for descriptions)
Readability score thresholds (currently flags Flesch-Kincaid scores below 55)
Modify the report design: In the ""Generate HTML Report"" code node, you can:
Adjust the HTML/CSS to change the report layout and styling
Add or remove sections from the report
Change the recommendations logic
Modify the health score calculation algorithm
Add additional data sources: You could extend the workflow by:
Adding Pagespeed Insights data for performance metrics
Incorporating backlink data from other APIs
Adding keyword ranking data from rank tracking APIs
Implement automated delivery: Add nodes after the ""Download Report"" to:
Send the report directly to clients via email
Upload it to cloud storage
Create a PDF version of the report"
Monitor competitors' websites for changes with OpenAI and Firecrawl,https://n8n.io/workflows/3101-monitor-competitors-websites-for-changes-with-openai-and-firecrawl/,"Who is this template for?
This workflow template is designed for people seeking alerts when certain specific changes are made to any web page. Leveraging agentic AI, it analyzes the page every day and autonomously decides whether to send you an e-mail notification.
Example use cases
Track price changes on [competitor's website]. Notify me when the price drops below ‚Ç¨50.
Monitor new blog posts on [industry leader's website] and summarize key insights.
Check [competitor's job page] for new job postings related to software development.
Watch for new product launches on [e-commerce site] and send me a summary.
Detect any changes in the terms and conditions of [specific website].
Track customer reviews for [specific product] on [review site] and extract key themes.
How it works
When clicking 'test workflow' in the editor, a new browser tab will open where you can fill in the details of your espionage assignment
Make sure you be as concise as possible when instructing AI. Instruct specific and to the point (see examples at the bottom).
After submission, the flow will start off by extracting both the relevant website url and an optimized prompt. OpenAI's structured outputs is utilized, followed by a code node to parse the results for further use.
From here on, the endless loop of daily checks will begin:
Initial scrape
1 day delay
Second scrape
AI agent decides whether or not to notify you
Back to step 1
You can cancel an espionage assignment at any time in the executions tab
Set up steps
Insert your OpenAI API key in the structured outputs node (second one)
Create a Firecrawl account and connect your Firecrawl API key in both 'Scrape page'-nodes
Connect your OpenAI account in the AI agents' model node
Connect your Gmail account in the AI agents' Gmail tool node"
‚ú®üìäMulti-AI Agent Chatbot for Postgres/Supabase DB and QuickCharts + Tool Router,https://n8n.io/workflows/3090-multi-ai-agent-chatbot-for-postgressupabase-db-and-quickcharts-tool-router/,"Multi-AI Agent Chatbot for Postgres/Supabase Databases and QuickChart Generation
Who is this for?
This workflow is ideal for data analysts, developers, and business intelligence teams who need an AI-powered chatbot to query Postgres/Supabase databases and generate dynamic charts for data visualization.
What problem does this solve?
It simplifies data exploration by combining conversational AI with database querying and chart generation. Users can interact with their database using natural language, retrieve insights, and visualize data without manual SQL queries or chart configuration.
What this workflow does
AI-Powered Chat Interface:
Accepts natural language prompts to query databases or generate charts.
Routes user requests through a tool agent system to determine the appropriate action (query or chart).
Database Querying:
Executes SQL queries on Postgres/Supabase databases based on user input.
Retrieves schema information, table definitions, and specific data records.
Dynamic Chart Generation:
Uses QuickChart to create bar charts, line charts, or other visualizations from database records.
Outputs a shareable chart URL or JSON configuration for further customization.
Memory Integration:
Maintains chat history using Postgres memory nodes, enabling context-aware interactions.
Workflow diagram showcasing AI agents, database querying, and chart generation paths.
Setup
Prerequisites:
A Postgres-compatible database (e.g., Supabase).
API credentials for OpenAI.
Configuration Steps:
Add your database connection credentials in the Postgres nodes.
Set up OpenAI credentials for GPT-4o-mini in the language model nodes.
Adjust the QuickChart schema in the ""QuickChart Object Schema"" node to fit your use case.
Testing:
Trigger the chat workflow via the ""When chat message received"" node.
Test with prompts like ""Generate a bar chart of sales data"" or ""Show me all users in the database.""
How to customize this workflow
Modify AI Prompts
Add Chart Types
Integrate Other Tools"
Create Social Media Content from Telegram with AI,https://n8n.io/workflows/3057-create-social-media-content-from-telegram-with-ai/,"Description:
Create Social Media Content from Telegram with AI
This n8n workflow empowers you to effortlessly generate social media content and captivating image prompts, all powered by AI. Simply send a topic request through Telegram (as a voice or text message), and watch as the workflow conducts research, crafts engaging social media posts, and creates detailed image prompts ready for use with your preferred AI art generation tool.
What does this workflow do?
This workflow streamlines the content creation process by automating research, social media content generation, and image prompt creation, triggered by a simple Telegram message.
Who is this for?
Social Media Managers: Quickly generate engaging content and image ideas for various platforms.
Content Creators: Overcome writer's block and discover fresh content ideas with AI assistance.
Marketing Teams: Boost productivity by automating social media content research and drafting.
Anyone looking to leverage AI for efficient and creative social media content creation.
Benefits
Effortless Content and Image Prompt Generation: Automate the creation of social media posts and detailed image prompts.
AI-Powered Creativity: Leverage the power of LLMs to generate original content ideas and captivating image prompts.
Increased Efficiency: Save time and resources by automating the research and content creation process.
Voice-to-Content: Use voice messages to request content, making content creation even more accessible.
Enhanced Engagement: Create high-quality, attention-grabbing content that resonates with your audience.
How it Works
Receive Request: The workflow listens for incoming voice or text messages on Telegram containing your content request.
Process Voice (if necessary): If the message is a voice message, it's transcribed into text using OpenAI's Whisper API.
AI Takes Over: The AI agent, powered by an OpenAI Chat Model and SerpAPI, conducts online research based on your request.
Content and Image Prompt Generation: The AI agent generates engaging social media content and a detailed image prompt based on the research.
Image Generation (Optional): You can use the generated image prompt with your preferred AI art generation tool (e.g., DALL-E, Stable Diffusion) to create a visual.
Output: The workflow provides you with the social media content and the detailed image prompt, ready for you to use or refine.
n8n Nodes Used
Telegram Trigger
Switch
Telegram (for fetching voice messages)
OpenAI (Whisper API for voice-to-text)
Set (for preparing variables)
AI Agent (with OpenAI Chat Model and SerpAPI tool)
HTTP Request (for optional image generation)
Extract from File (for optional image processing)
Set (for final output)
Prerequisites
Active n8n instance
Telegram account with a bot
OpenAI API key
SerpAPI account
Hugging Face API key (if you want to generate images within the workflow)
Setup
Import the workflow JSON into your n8n instance.
Configure the Telegram Trigger node with your Telegram bot token.
Set up the OpenAI and SerpAPI credentials in the respective nodes.
If you want to generate images directly within the workflow, configure the HTTP Request node with your Hugging Face API key.
Test the workflow by sending a voice or text message to your Telegram bot with a topic request.
This workflow combines the convenience of Telegram with the power of AI to provide a seamless content creation experience. Start generating engaging social media content today!"
üöÄ TikTok Video Automation Tool ‚ú® ‚Äì Highly Optimized with OpenAI & Replicate,https://n8n.io/workflows/3004-tiktok-video-automation-tool-highly-optimized-with-openai-and-replicate/,"üöÄ TikTok Video Automation Tool ‚ú® (Frequent Updates)
Create viral TikTok videos effortlessly ‚Äî no editing skills required!
Who is this for? üéØ
‚úÖ Content Creators: Pump out engaging short videos in minutes.
‚úÖ Marketing Agencies: Deliver high-quality client content ‚Äî fast.
‚úÖ Business Owners: Promote your brand with attention-grabbing TikToks.
What problem does this solve? üõ†Ô∏è
Short-form video content is king, but creating it takes time, skill, and effort. This tool automates the entire process, from scriptwriting to video production, ensuring SEO-optimized, high-quality content without any manual editing.
How it works üåü
1Ô∏è‚É£ Enter your video idea and choose where to receive the final video (TikTok upload or link via WhatsApp, Gmail, etc.).
2Ô∏è‚É£ AI crafts a high-engagement script with SEO optimization.
3Ô∏è‚É£ Voiceover is generated with ultra-realistic AI narration.
4Ô∏è‚É£ Relevant visuals are selected to perfectly match the script.
5Ô∏è‚É£ Your TikTok video is assembled and either directly uploaded to TikTok or delivered via a shareable link.
Quick & Easy Setup ‚öôÔ∏è (5-10 min)
üîπ Connect required APIs (most have free plans).
üîπ Follow the step-by-step setup (video tutorial included).
üîπ Start generating professional TikTok videos instantly!
Required APIs üîó
Content & Voiceover: OpenAI (Paid), ElevenLabs (Free)
Media Processing: Cloudinary (Free), Replicate (Paid)
Video Assembly: 0codekit (Free), Creatomate (Free)
Optional Integrations üîó
Messaging & Sharing: WhatsApp, Telegram, Gmail, Outlook
Direct Upload: TikTok
Customization üé®
Adjust script styles & voiceover preferences.
Modify visuals to align with your brand.
Optimize video length and format.
üöÄ Start automating your TikTok content today and grow your audience on autopilot!"
üí°üåê Essential Multipage Website Scraper with Jina.ai,https://n8n.io/workflows/2957-essential-multipage-website-scraper-with-jinaai/,"üí°üåê Essential Multipage Website Scraper with Jina.ai
Use responsibly and follow local rules and regulations
This N8N workflow enables automated multi-page website scraping using Jina.ai's powerful web scraping capabilities, with seamless integration to Google Drive for content storage. Here's how it works:
Main Features
The workflow automatically scrapes multiple pages from a website's sitemap and saves each page's content as a separate Google Drive document.
Key Components
Input Configuration
Starts with a sitemap URL (default: https://ai.pydantic.dev/sitemap.xml)
Processes the sitemap to extract individual page URLs
Includes filtering options to target specific topics or pages
Scraping Process
Uses Jina.ai's web scraper to extract content from each URL
Converts webpage content into clean markdown format
Extracts page titles automatically for document naming
Storage Integration
Creates individual Google Drive documents for each scraped page
Names documents using the format ""URL - Page Title""
Saves content in markdown format for better readability
Usage Instructions
Set your target website's sitemap URL in the ""Set Website URL"" node
Configure the ""Filter By Topics or Pages"" node to select specific content
Adjust the ""Limit"" node (default: 20 pages) to control batch size
Connect your Google Drive account
Run the workflow to begin automated scraping
Additional Features
Built-in rate limiting through the Wait node to prevent overloading servers
Batch processing capability for handling large sitemaps
The workflow requires no API key for Jina.ai, making it accessible for immediate use while maintaining responsible scraping practices."
Query Perplexity AI from your n8n workflows,https://n8n.io/workflows/2824-query-perplexity-ai-from-your-n8n-workflows/,"This workflow illustrates how to use Perplexity AI in your n8n workflow.
Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question.
Credentials Setup
1/ Go to the perplexity dashboard, purchase some credits and create an API Key
https://www.perplexity.ai/settings/api
2/ In the perplexity Request node, use Generic Credentials, Header Auth.
For the name, use the value ""Authorization""
And for the value ""Bearer pplx-e4...59ea"" (Your Perplexity Api Key)
AI Model
Sonar Pro is the current top model used by perplexity.
If you want to use a different one, check this page:
https://docs.perplexity.ai/guides/model-cards"
AI agent for Instagram DM/inbox. Manychat + Open AI integration,https://n8n.io/workflows/2718-ai-agent-for-instagram-dminbox-manychat-open-ai-integration/,"Automate Instagram DMs with OpenAI GPT and ManyChat
How It Works:
Once connected, GPT will automatically initiate conversations with messages from new recipients in Intagram.
Who Is This For?
This workflow is ideal for
marketers,
business owners
content creators
who want to automatically respond to Instagram direct messages using OpenAI GPT.
By integrating ManyChat, you can manage conversations, nurture leads, and provide instant replies at scale.
What This Workflow Does
Captures incoming Instagram DMs through ManyChat‚Äôs integration.
Processes messages with GPT to generate a relevant response.
Delivers instant replies back to Instagram users, creating efficient, AI-driven communication.
Setup
Import the Template: Copy the n8n workflow into your workspace.
OpenAI Credentials: Add your OpenAI API key in n8n so GPT can generate responses.
ManyChat Account: Create (or log in to) your ManyChat account.
Connect Instagram: Link your Instagram profile as a channel in ManyChat.
ManyChat Custom Field: Create a custom field for storing user input or conversation context.
Configure Default Reply: In ManyChat, set up the default Instagram reply flow to point to your n8n webhook.
Add External Request: Create an external request step in ManyChat to send messages to n8n.
Test the Flow: Send yourself a DM on Instagram to confirm the workflow triggers and GPT responds correctly.
Instructions and links:
Notion instruction
Register in ManyChat"
Hacker News to Video Content,https://n8n.io/workflows/2557-hacker-news-to-video-content/,"Hacker News to Video Content
Overview
This workflow converts trending articles from Hacker News into engaging video content. It integrates AI-based tools to analyze, summarize, and generate multimedia content, making it ideal for content creators, educators, and marketers.
Features
Article Retrieval:
Pulls trending articles from Hacker News.
Limits the number of articles to process (configurable).
Content Analysis:
Uses OpenAI's GPT model to:
Summarize articles.
Assess their relevance to specific topics like automation or AI.
Extract key image URLs.
Image and Video Generation:
Leonardo.ai: Creates stunning AI-generated images based on extracted prompts.
RunwayML: Converts images into high-quality videos.
Structured Content Creation:
Parses content into structured formats for easy reuse.
Generates newsletter-friendly blurbs and social media-ready captions.
Cloud Integration:
Uploads generated assets to:
Dropbox
Google Drive
Microsoft OneDrive
MinIO
Social Media Posting (Optional):
Supports posting to YouTube, X (Twitter), LinkedIn, and Instagram.
Workflow Steps
1. Trigger
Initiated manually via the ""Test Workflow"" button.
2. Fetch Articles
Retrieves articles from Hacker News.
Limits the results to avoid processing overload.
3. Content Filtering
Evaluates if articles are related to AI/Automation using OpenAI's language model.
4. Image and Video Generation
Generates:
AI-driven image prompts via Leonardo.ai.
Videos using RunwayML.
5. Asset Management
Saves the output to cloud storage services or uploads directly to social media platforms.
Prerequisites
API Keys:
Hacker News
OpenAI
Leonardo.ai
RunwayML
Creatomate
n8n Installation:
Ensure n8n is installed and configured locally or on a server.
Credentials:
Set up credentials in n8n for all external services used in the workflow.
Customization
Replace Hacker News with any other data source node if needed.
Configure the ""Article Analysis"" node for different topics.
Adjust the cloud storage services or add custom storage options.
Usage
Import this workflow into your n8n instance.
Configure your API credentials.
Trigger the workflow manually or schedule it as needed.
Check the outputs in your preferred cloud storage or social media platform.
Notes
Extend this workflow further by automating social media posting or newsletter integration.
For any questions, refer to the official documentation or reach out to the creator.
About the Creator
This workflow was built by AlexK1919, an AI-native workflow automation architect. Check out the overview video for a quick demo.
Tools Used
Leonardo.ai
RunwayML
Creatomate
Hacker News API
OpenAI GPT
Feel free to adapt and extend this workflow to meet your specific needs! üéâ"
Flux AI Image Generator,https://n8n.io/workflows/2417-flux-ai-image-generator/,"Easily generate images with Black Forest's Flux Text-to-Image AI models using Hugging Face‚Äôs Inference API. This template serves a webform where you can enter prompts and select predefined visual styles that are customizable with no-code. The workflow integrates seamlessly with Hugging Face's free tier, and it‚Äôs easy to modify for any Text-to-Image model that supports API access.
Try it
Curious what this template does? Try a public version here: https://devrel.app.n8n.cloud/form/flux
Set Up
Watch this quick set up video üëá
Accounts required
Huggingface.co account (free)
Cloudflare.com account (free - used for storage; but can be swapped easily e.g. GDrive)
Key Features:
Text-to-Image Creation: Generates unique visuals based on your prompt and style.
Hugging Face Integration: Utilizes Hugging Face‚Äôs Inference API for reliable image generation.
Customizable Visual Styles: Select from preset styles or easily add your own.
Adaptable: Swap in any Hugging Face Text-to-Image model that supports API calls.
Ideal for:
Creators: Rapidly create visuals for projects.
Marketers: Prototype campaign visuals.
Developers: Test different AI image models effortlessly.
How It Works:
You submit an image prompt via the webform and select a visual style, which appends style instructions to your prompt. The Hugging Face Inference API then generates and returns the image, which gets hosted on Cloudflare S3. The workflow can be easily adjusted to use other models and styles for complete flexibility."
Generate Contextual YouTube Comments Automatically with GPT-4o,https://n8n.io/workflows/4580-generate-contextual-youtube-comments-automatically-with-gpt-4o/,"Workflow Overview
This cutting-edge n8n workflow is a powerful automation tool designed to revolutionize how content creators and marketers engage with YouTube channels. By leveraging AI and the YouTube Data API, this workflow automatically:
Discovers New Content:
Monitors a specific YouTube channel
Retrieves the latest video in real-time
Checks for new uploads at regular intervals
Generates Intelligent Comments:
Uses advanced AI (OpenAI's GPT models) to analyze video metadata
Crafts contextually relevant, human-like comments
Ensures each comment feels organic and engaging
Seamless Deployment:
Automatically posts the AI-generated comment directly on the video
Eliminates manual interaction
Increases potential channel visibility and engagement
Key Benefits
ü§ñ Full Automation: No manual comment writing required
üí° Smart Contextual Comments: AI understands video content
‚è±Ô∏è Time-Saving: Instant engagement without human intervention
üìà Potential Increased Visibility: Regular, intelligent interactions
Setup Requirements
YouTube Data API Credentials
Obtain a Google Cloud API key
Configure channel ID you want to target
Set up OAuth2 authentication for comment posting
OpenAI API Access
Create an OpenAI account
Generate an API key for comment generation
Select preferred GPT model (GPT-4o, GPT-3.5, etc.)
n8n Installation
Install n8n (cloud or self-hosted)
Import the workflow configuration
Configure API credentials
Set up scheduling preferences
Potential Use Cases
Content Creators monitoring competitor channels
Marketing teams maintaining online presence
Social media managers automating engagement
Researchers tracking specific YouTube channels
Future Enhancements
Logging comment history
Dynamic OAuth2 token management
Multi-channel support
Sentiment analysis for comment generation
Connect With Me
Got questions? Want to dive deeper?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
**Unlock the power of AI-driven YouTube engagement ‚Äì automate, optimize, and amplify your online# Automate YouTube Engagement with GPT-4o Generated Comments
Workflow Overview
This n8n automation leverages AI to streamline YouTube channel engagement, providing a sophisticated solution for content interaction. By combining the YouTube Data API and OpenAI's GPT-4o, the workflow:
Intelligent Content Discovery:
Dynamically monitors specified YouTube channels
Real-time detection of new video uploads
Configurable monitoring intervals
AI-Powered Comment Generation:
Utilizes GPT-4o for contextual analysis
Generates nuanced, platform-appropriate comments
Ensures authentic, relevant interactions
Automated Engagement:
Seamlessly posts AI-crafted comments
Enhances channel visibility
Reduces manual social media management
Key Benefits
ü§ñ Advanced Automation: AI-driven engagement
üí° Contextual Intelligence: GPT-4o powered insights
‚è±Ô∏è Efficiency Optimization: Instant, scalable interactions
üìà Strategic Visibility: Consistent, meaningful channel presence
Detailed Setup Instructions
Prerequisites
n8n instance (cloud or self-hosted)
YouTube Data API access
OpenAI API key
Target YouTube channel(s)
Configuration Steps
YouTube Data API Setup
Create a Google Cloud project
Enable YouTube Data API v3
Generate OAuth2 credentials
Store credentials securely in n8n
OpenAI API Configuration
Create OpenAI account
Generate API key
Select GPT-4o model
Configure API key in n8n credentials
Workflow Customization
Replace placeholder channel IDs
Adjust monitoring frequency
Customize AI prompt for comment generation
Configure OAuth2 authentication
Workflow Customization Options
Modify AI prompt to match specific content styles
Add keyword filters for video selection
Implement multi-channel support
Create custom engagement rules
Potential Use Cases
Content creator audience engagement
Brand social media management
Community interaction automation
Research and monitoring
Ethical Considerations
Maintain transparency about AI-generated comments
Respect platform guidelines
Avoid spam or misleading interactions
Ensure comments add genuine value
Future Enhancement Roadmap
Advanced sentiment analysis
Multi-language support
Engagement performance tracking
Adaptive comment generation
Security Best Practices
Never hardcode API keys
Use n8n's credential management
Implement secure OAuth2 authentication
Regularly rotate API credentials
Technical Requirements
n8n v0.220.0 or higher
YouTube Data API v3
OpenAI API access
Stable internet connection
Workflow Architecture
[YouTube Channel Trigger]
    ‚¨áÔ∏è
[Fetch Latest Video]
    ‚¨áÔ∏è
[AI Comment Generation]
    ‚¨áÔ∏è
[Post Comment]
#YouTubeAutomation #AIEngagement #ContentMarketing #SocialMediaTech #GPT4Automation #WorkflowInnovation #AIComments #DigitalMarketing
Connect With Me
Exploring AI-Powered Social Media Automation?
üìß Email: Yaron@nofluff.online
üé• YouTube: @YaronBeen
üíº LinkedIn: Yaron Been
Transform your YouTube engagement with intelligent, responsible automation!
Note: This workflow template is a starting point. Always customize and test thoroughly in your specific environment."
"Generate LinkedIn Posts with GPT-4, Preview on WhatsApp, and Auto-Publish",https://n8n.io/workflows/4419-generate-linkedin-posts-with-gpt-4-preview-on-whatsapp-and-auto-publish/,"# Workflow Overview
Transform your LinkedIn presence with this powerful AI-driven automation workflow that generates and posts high-quality thought leadership content automatically.
Let this AI Agent, generate content for you, sends it to your whatsapp for you to review and if its ok Approve and it will post! if you don't like it, Decline and it will regenerate another and send again for preview and approval.
Stage 1: Automated Content Scheduling
Schedule Trigger runs every 6 hours (customizable)
Topic Selection randomly chooses 3 AI-related topics from a predefined list
Ensures fresh, varied content for each generation cycle
Stage 2: AI Content Generation
GPT-4 Integration creates unique LinkedIn posts using structured prompts
Content Format: Hook ‚Üí Problem/Opportunity ‚Üí Insight ‚Üí Strategic Takeaway ‚Üí CTA
Output: 300-word maximum posts with 3-5 relevant hashtags
Uniqueness: Each post is completely original and industry-specific
Stage 3: Content Processing & Formatting
JSON Parsing extracts title, body, and hashtags from AI response
Error Handling includes fallback mechanisms for robust operation
LinkedIn Formatting prepares content for optimal platform display
Stage 4: Preview & Approval System
WhatsApp Integration sends generated content to your phone for review
Approval Gateway waits for your decision (approve/decline)
Quality Control ensures only approved content reaches your LinkedIn profile
Stage 5: Automated Publishing
LinkedIn API posts approved content automatically
Success Notifications confirm successful publishing via WhatsApp
Professional Formatting includes proper spacing and emoji structure
Stage 6: Smart Restart System
Decline Handling automatically generates new content if declined
Continuous Operation ensures consistent content flow
Tracking Notifications keep you informed of all workflow activities
Prerequisites & Required Credentials
Essential API Credentials
OpenAI API Key (for GPT-4 content generation)
WhatsApp Business API credentials and phone number ID
LinkedIn Developer Account with API access
n8n Cloud or Self-hosted instance
Technical Requirements
Active LinkedIn Business profile
WhatsApp Business account
Basic understanding of n8n workflow management
Step-by-Step Setup Instructions
Configure OpenAI Integration
Add your OpenAI API key to the ""OpenAI GPT-4 Model"" node
Ensure GPT-4 access is enabled on your OpenAI account
Test connection before proceeding
2. Setup WhatsApp Business API
Obtain WhatsApp Business API credentials from Meta
Configure phone number ID in all WhatsApp nodes
Replace ""YOUR_PHONE_NUMBER"" with your actual number
Test message delivery
3. Connect LinkedIn Account
Set up LinkedIn Developer App
Obtain your LinkedIn Person ID
Configure OAuth authentication
Test posting permissions
4. Customize Content Topics
Edit the ""Prepare Search Topics"" function node
Modify the searchTerms array for your industry
Add relevant AI topics for your niche
Save and test topic randomization
5. Adjust Posting Schedule
Modify the ""Schedule Trigger"" node
Change interval from 6 hours to your preferred timing
Consider your audience's active hours
Enable the trigger to start automation
Customization Options
Content Personalization
Industry Focus: Modify AI topics in the search terms array
Writing Style: Adjust the system prompt in ""AI Content Generator""
Post Length: Customize maximum word count (default: 300 words)
Hashtag Strategy: Modify hashtag generation rules
Workflow Timing
Posting Frequency: Adjust schedule trigger interval
Approval Timeout: Configure WhatsApp wait duration
Retry Logic: Customize restart behavior for declined content
Notification Preferences
WhatsApp Templates: Customize approval and notification messages
Success Tracking: Modify confirmation message content
Error Handling: Adjust failure notification settings
**Business Benefits
Time Savings
**
10+ Hours Weekly: Eliminate manual content creation and posting
Consistent Presence: Maintain regular LinkedIn activity automatically
Quality Control: Review and approve all content before publishing
Lead Generation Impact
Thought Leadership: Position yourself as an AI industry expert
Engagement Growth: High-quality content drives more interactions
Network Expansion: Consistent posting attracts relevant connections
Authority Building: Regular insights establish professional credibility
ROI Expectations
Monthly Investment: $10-20 in API costs
Setup Time: 15 minutes initial configuration
Long-term Value: Increased visibility, leads, and business opportunities
Troubleshooting & Support
Common Issues
API Rate Limits: Monitor usage and upgrade plans if needed
Content Quality: Adjust prompts for better AI output
Approval Delays: Check WhatsApp connectivity and permissions
Optimization Tips
Topic Rotation: Regularly update search terms for fresh content
Performance Monitoring: Track engagement metrics on posted content
Prompt Refinement: Continuously improve AI generation prompts"
Extract Google Ads Creatives by Domain with SerpAPI and Export to CSV,https://n8n.io/workflows/4616-extract-google-ads-creatives-by-domain-with-serpapi-and-export-to-csv/,"üßæ Template: Extract Ad Creatives from Google‚Äôs Ads Transparency Center
This n8n workflow pulls ad creatives from Google's Ads Transparency Center using SerpApi, filtered by a specific domain and region. It extracts, filters, categorizes, and exports ads into neatly formatted CSV files for easy analysis.
üë§ Who‚Äôs it for?
Marketing Analysts researching competitive PPC strategies
Ad Intelligence Teams monitoring creatives from specific brands
Digital Marketers gathering visual and copy trends
Journalists & Watchdogs reviewing ad activity transparency
‚úÖ Features
Fetch creatives using SerpApi's google_ads_transparency_center engine
Filter results to include only ads with an exact match to your target domain
Categorize by ad format: text, image, or video
Export CSVs: Generates a downloadable file for each format under the /files/ directory
üõ† How to Use
Edit the ‚ÄúSet Domain & Region‚Äù node
domain: e.g. example.com
region: SerpApi numeric region code ‚Üí See codes
Add your SerpApi API key
In the ‚ÄúGet Ads Page 1‚Äù node‚Äôs credentials section.
Run the workflow
Click ""Test workflow"" to initiate the process.
Download your results
Navigate to /files/ to find:
text_{domain}_ads.csv
image_{domain}_ads.csv
video_{domain}_ads.csv
üìå Notes
Only the first page (up to 50 creatives) is fetched; pagination is not included.
Sticky Notes inside the workflow nodes offer helpful internal annotations.
CSV files include creative-level details: ad copy, images, video links, etc."
"Hostinger Form Lead Capture & Qualification with OpenAI, Beehiiv & Google Sheets",https://n8n.io/workflows/4575-hostinger-form-lead-capture-and-qualification-with-openai-beehiiv-and-google-sheets/,"This n8n workflow provides a robust solution for Hostinger website owners looking to streamline their lead capture and qualification process. By integrating AI and popular marketing tools, it ensures efficient management of new leads.
How it works
This workflow automates the process of capturing leads from a Hostinger website form, qualifying them using an AI model, and syncing them with Beehiiv and Google Sheets.
A new form submission on your Hostinger website triggers the workflow via Gmail.
An AI model (OpenAI) extracts and qualifies relevant data from the form response, transforming it into a structured JSON format.
The qualified lead is then added as a new subscriber to your Beehiiv newsletter.
Finally, the lead information is appended to a Google Sheet for comprehensive tracking and record-keeping.
Set up steps
Create an account on Hostinger and set up a form on your website.
Configure a Gmail trigger to capture form submissions from your Hostinger email sender.
Ensure your email is mapped correctly for the Hostinger sender.
Set up your OpenAI API key for the ""Extract & Qualify Message Model"" node.
Create an account on Beehiiv. Obtain your Beehiiv API key and create your credentials.
Set up a Google Sheet and configure the ""insert in Sheets"" node with the correct mapping.
Approximate setup time: 15-30 minutes
Why you need this?
You need this workflow if you're a Hostinger website owner who wants to:
Automate lead capture: Say goodbye to manual lead processing and ensure no lead slips through the cracks.
Qualify leads intelligently: Leverage AI to instantly extract and qualify crucial lead information, saving you valuable time and improving lead quality.
Grow your audience efficiently: Seamlessly add qualified leads to your Beehiiv newsletter, nurturing them from the moment they express interest.
Maintain organized records: Automatically centralize all your lead data in a Google Sheet for easy access, analysis, and follow-up.
Boost productivity: Free up your time from repetitive administrative tasks and focus on growing your business.
üëâ Need Help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
üöö Estimate Driving Time and Distance for Logistics with Open Route API,https://n8n.io/workflows/4564-estimate-driving-time-and-distance-for-logistics-with-open-route-api/,"Tags: Supply Chain, Logistics, Route Planning, Transportation, GPS API
Context
Hi! I‚Äôm Samir ‚Äî a Supply Chain Engineer and Data Scientist based in Paris, and founder of LogiGreen Consulting.
I help companies improve their logistics operations using data, AI, and automation to reduce costs and minimize environmental footprint.
Let‚Äôs use n8n to build smarter and greener transport operations!
üì¨ For business inquiries, you can add find me on LinkedIn
Who is this template for?
This workflow is designed for logistics and transport teams who want to automate distance and travel time calculations for truck shipments.
Ideal for:
Control tower dashboards
Transport cost simulations
Route optimization studies
How does it work?
This n8n workflow connects to a Google Sheet where you store city-to-city shipment lanes, and uses the OpenRouteService API to calculate:
üìè Distance (in meters)
‚è±Ô∏è Travel time (in seconds)
ü™™ Number of route steps
Steps:
‚úÖ Load departure/destination city coordinates from a Google Sheet
üîÅ Loop through each record
üöö Query OpenRouteService using the truck (driving-hgv) profile
üßæ Extract and store results: distance, duration, number of steps
üì§ Update the Google Sheet with new values
What do I need to get started?
This workflow is beginner-friendly and requires:
A Google Sheet with route pairs (departure and destination coordinates)
A free OpenRouteService API key
üëâ Get one here
Next Steps
üóíÔ∏è Follow the sticky notes inside the workflow to:
Select your sheet
Plug in your API key
Launch the flow!

üé• Check the Tutorial
üöÄ You can customize the workflow to:
Add CO2 emission estimates for Sustainability Reporting
Connect to your TMS via API or EDI
This template was built using n8n v1.93.0
Submitted: June 1, 2025"
Upload Google Drive Files to an InfraNodus Graph,https://n8n.io/workflows/4486-upload-google-drive-files-to-an-infranodus-graph/,"This template can be used to upload the files in your Google drive to an InfraNodus knowledge graph.
The InfraNodus graph will then reveal the main topics and ideas in your collection of documents and show the content gaps in them. You can also use the built-in AI to converse with the documents.
You can also access the InfraNodus Graphs via its GraphRAG API to re-use them in your other n8n workflows for high-quality content retrieval and knowledge base optimization.
The template showcases the use of multiple n8n nodes and processes:
Extracting documents from a Google Drive folder
text extraction
optional: high-quality PDF conversion using ConvertAPI
InfraNodus knowledge graph generation
Note: If you want to Sync your Google drive to an InfraNodus graph, check out our other workflow
How it works
Here's a description of this workflow step by step:
Find all the files in a specific Google drive folder
For each file found: reiterate the workflow and
Identify the type of the file (TXT, PDF, Markdown)
For TXT and Markdown files extract the text data
For PDF files use a special PDF to Text convertor to extract the text data. (Optional: using ConvertAPI for better quality PDF conversion)
Forward everything to the InfraNodus graphAndStatements API endpoint with the name of the new graph, the text field with the text data, the text settings, and doNotSave=false to create a new graph
Reiterate through another file.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Use that API key to set up authorization for the InfraNodus tool in the workflow.
If you want to upload the files to an existing graph, you should copy its name from InfraNodus. Otherwise you can specify any name you want.
Requirements
An InfraNodus account and API key
A Google Drive account and authorization (you will need to set it up via Google Cloud using the n8n instructions provided in the Google Drive node).
Customizing this workflow
You can use Dropbox instead of Google Drive.
You can also modify this workflow slightly to make it Sync with a Google Drive when the new files appear in it.
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20267019838108-Upload-Sync-Your-Google-Drive-Folder-with-InfraNodus-using-n8n"
AI-Powered Loom Video Q&A with Gemini-2.5 and Slack Notifications,https://n8n.io/workflows/4472-ai-powered-loom-video-qanda-with-gemini-25-and-slack-notifications/,"How to use the workflow
This workflow takes a Loom link, extracts the video ID, uses the Loom API to download the video, then sends it to Gemini along with your question. Finally, it sends the output to Slack.
To use it, you just need to add your own API key for Gemini and Slack connection.
Click the link above to get your Gemini API key, then add a generic ""Query auth"" type credential in n8n. The name will be ""key"" and the value will be your API key.
One way to customize this workflow would be to make the trigger any received email, extract the Loom link, and run an auto-prompt like ""Describe this video in detail""."
"Extract & Categorize Receipt Data with Google OCR, OpenRouter AI & Telegram",https://n8n.io/workflows/4020-extract-and-categorize-receipt-data-with-google-ocr-openrouter-ai-and-telegram/,"Effortlessly track your expenses with MoneyMate, an n8n workflow that transforms receipts into organized financial insights.
Upload a photo or text via Telegram, and let MoneyMate extract key details‚Äîstore info, transaction dates, items, and totals‚Äîusing Google Vision OCR and AI-powered parsing via OpenRouter.
It categorizes expenses (e.g., Food & Beverages, Transport, Household) and delivers a clean, emoji-rich summary back to your Telegram chat. Handles zero-total errors with a friendly nudge to double-check inputs.
Perfect for freelancers, small business owners, or anyone seeking hassle-free expense management. No database required, ensuring privacy and simplicity. Deploy MoneyMate and take control of your finances today!
Key Features
üì± Telegram Integration: Input via photo or text, receive summaries instantly.
üì∏ Receipt Scanning: Converts receipt images to text using Google Vision API.
ü§ñ AI Parsing: Categorizes transactions with OpenRouter‚Äôs AI analysis.
üõ°Ô∏è Privacy-First: Processes data on-the-fly without storage.
‚ö†Ô∏è Smart Error Handling: Catches zero totals with user-friendly prompts.
üìä Flexible Categories: Supports Income/Expense and custom expense types.
Ideal For
Budget-conscious individuals managing personal finances.
Entrepreneurs tracking business expenses.
Teams needing quick, automated expense reporting.
Pre-Requirements
n8n Instance: A running n8n instance (cloud or self-hosted).
Credentials:
Telegram: A bot token and webhook setup (obtained via BotFather). For more information, please refer to Telegram bots creation
Google Cloud: A service account with Google Vision API enabled and API key. For more informations, please refer to Google cloud Vision
OpenRouter: An account with API access for AI language model usage.
Telegram Bot: A configured Telegram bot to receive inputs and send summaries.
Setup Instructions
Import Workflow: Copy the MoneyMate workflow JSON and import it into your n8n instance using the ""Import Workflow"" option.
Set Up Telegram Bot: Create a bot via BotFather on Telegram to get a token and set up a webhook. For detailed steps, refer to n8n‚Äôs Telegram setup guide.
Configure Credentials:
In the Telegram Trigger, Send Error Message, and Send Expense Summary nodes, add Telegram API credentials with your bot token.
In the Get Telegram File and Download Image nodes, ensure Telegram API credentials are linked.
In the Google Vision OCR node, add Google Cloud credentials with Google Vision API access.
In the OpenRouter AI Model node, set up OpenRouter API credentials.
Test the Workflow: Send a test receipt photo or text (e.g., ""Lunch 50,000 IDR"") via Telegram and verify the summary in your chat.
Activate: Enable the workflow in n8n to run automatically for each input.
Customization Options
Add Categories: Modify the AI Categorizer node to include new expense types (e.g., Entertainment).
Change Output Format: Adjust the Format Summary Message node to include more details like taxes or payment methods.
Switch AI Model: In the OpenRouter AI Model node, select a different OpenRouter model for better parsing.
Store Data: Add a Google Sheets node after Parse Receipt Data to save expense records.
Enhance Errors: Include an email notification node after Check Invalid Input for failed inputs.
Why Choose MoneyMate?
Save time, reduce manual entry, and gain clarity on your spending with MoneyMate‚Äôs AI-driven workflow. Ready to streamline your finances? Get MoneyMate now!"
Generate a Personal Newsfeed Using Bright Data Web Scraping and GPT-4.1,https://n8n.io/workflows/4272-generate-a-personal-newsfeed-using-bright-data-web-scraping-and-gpt-41/,"How it Works
Disclaimer: This template is for self-hosted n8n instances only.
This workflow is designed for developers, data analysts, and automation enthusiasts seeking to automate personalized news collection and delivery. It seamlessly combines n8n, OpenAI (e.g., GPT-4.1), and Bright Data‚Äôs Model Context Protocol (MCP) to collect, extract, and email the latest global news headlines.
On a schedule or via a manual trigger, the workflow prompts an AI agent to gather fresh news. The agent leverages context-aware memory and integrated MCP tools to conduct both search engine queries and direct web page scraping in real time, delivering more than just meta search results‚Äîit extracts actual on-page headlines and trusted links. Results are formatted and delivered automatically by email via your SMTP provider, requiring zero manual effort once configured.
Who is this for?
Developers, data engineers, or automation pros wanting an AI-powered, fully automated newsfeed
Teams needing up-to-date news digests from trusted global sources
Anyone self-hosting n8n who wishes to combine advanced LLMs with real-time web data
Setup Steps
Setup time: Approx. 15‚Äì30 minutes (n8n install, API configuration, node setup)
Requirements:
Self-hosted n8n instance
OpenAI API key
Bright Data MCP account credentials
SMTP/email provider details
Install the community MCP node (n8n-nodes-mcp) for n8n and set up Bright Data MCP access.
Configure these nodes:
Schedule Trigger: For automated delivery at your chosen interval.
Edit Fields: To inject your AI news collection prompt.
AI Agent: Connects to OpenAI and MCP, enabled with memory for context.
OpenAI Chat Model: Connects via your OpenAI credentials.
MCP Clients: Configure at least two‚Äîone for search (e.g. search_engine) and one for scraping (e.g. scrape_as_markdown).
Send Email: Set up with recipient and SMTP information.
Credentials must be entered into their respective nodes for successful execution.
Customization Guidance
Prompt Tweaks: Refine your AI news prompt to target specific genres, regions, or sources, or broaden/narrow the coverage as needed.
Tool Configuration: Carefully define tool descriptions and parameters in MCP client nodes so the agent can pick the best tool for each step (e.g., only scrape real news sites).
Delivery Settings: Adjust email recipient(s) and SMTP details as needed.
Workflow Enhancements: Use sticky notes in n8n for extended documentation, alternate prompts, or troubleshooting tips.
Run Frequency: Set schedule as needed‚Äîfrom hourly to daily updates.
Once configured, this workflow will automatically gather, extract, and email curated news headlines and links‚Äîno manual curation required!"
Automate SAP Business Partner Analysis with OpenAI GPT-4o & Gmail Reporting,https://n8n.io/workflows/4287-automate-sap-business-partner-analysis-with-openai-gpt-4o-and-gmail-reporting/,"Who is this for?
This workflow is intended for micro businesses, SAP users, companies, users or owners who need system simplicity in SAP. In micro businesses, SAP users or individuals, there must be the task of manually inputting a lot of data or here in partner businesses in SAP, the reality is that it is very time consuming and tiring, especially since we have to arrange it first or click one by one and send it. This is also a form of community service in n8n and n8n companies, as well as dedication to small businesses using SAP, SAP users, companies or owners who use SAP so that reality is no longer tiring and is able to answer existing reality problems automatically.
How it works?
Easy explanation:
The First Manual Trigger Node is used when clicking to start the program running. It's like starting by clicking the button first.
After that the HTTP Request will be sent to the next node, here containing SAP.
After that, each function will program and process it into clean data.
Then after that it will be sent to Gmail via the Gmail node. The AI Agent will carry out the task of reporting, analyzing and sending it via Gmail, here using model Open AI.
You can immediately see the data thoroughly, because the data has been processed. If you want to process it again, just direct the AI system to process it again.
And all duties and responsibilities have been completed
Very practical and easy, plus its automatic nature. I'm sure you can understand it wisely.
Set up instructions
Complete what is in the nodes as stated in the notes column.
Because the other nodes have been set up neatly, what you have to set up is, first, you have to connect the Gmail and Open AI ""Credential Accounts"" only.
After that, enter the email address you want to send or receive in the Gmail node parameters.
Well, it's very simple.
Everything is organized wisely, so you just follow the notes or setup instructions provided, or you can add other things according to your specific purpose and congratulations you are ready to use it.
Requirements
As a reminder:
It must be arranged in nodes, what your business description looks like, also according to the conditions of your small business, so that the AI Agent matches your business knowledge base. Must have (if not, make sure you are registered) in each ""Credential Account"" by following the guide on how to do it n8n the guide is very complete. Don't forget to save, and make sure the workflow is active and don't forget about ""Account Credentials"".
How to customize this workflow to your needs
You can directly organize your business knowledge base in these nodes, so that according to your business in its field and what services it sells or has or here is your business partner, you can then carry out tasks and answer them."
TrustPilot SaaS Product Review Tracker with Bright Data & OpenAI,https://n8n.io/workflows/4280-trustpilot-saas-product-review-tracker-with-bright-data-and-openai/,"Who this is for
The TrustPilot SaaS Product Review Tracker is designed for product managers, SaaS growth teams, customer experience analysts, and marketing teams who need to extract, summarize, and analyze customer feedback at scale from TrustPilot.
This workflow is tailored for:
Product Managers - Monitoring feedback to drive feature improvements
Customer Support & CX Teams - Identifying sentiment trends or recurring issues
Marketing & Growth Teams - Leveraging testimonials and market perception
Data Analysts - Tracking competitor reviews and benchmarking
Founders & Executives - Wanting aggregated insights into customer satisfaction
What problem is this workflow solving?
Manually monitoring, extracting, and summarizing TrustPilot reviews is time-consuming, fragmented, and hard to scale across multiple SaaS products.
This workflow automates that process from unlocking the data behind anti-bot layers to summarizing and storing customer insights enabling teams to respond faster, spot trends, and make data-backed product decisions.
This workflow solves:
The challenge of scraping protected review data (using Bright Data Web Unlocker)
The need for structured insights from unstructured review content
The lack of automated delivery to storage and alerting systems like Google Sheets or webhooks
What this workflow does
Extract TrustPilot Reviews: Uses Bright Data Web Unlocker to bypass anti-bot protections and pull markdown-based content from product review pages
Convert Markdown to Text: Leverages a basic LLM chain to clean and convert scraped markdown into plain text
Structured Information Extraction: Uses OpenAI GPT-4o via the Information Extractor node to extract fields like product name, review date, rating, and reviewer sentiment
Summarization Chain: Generates concise summaries of overall review sentiment and themes using OpenAI
Merge & Aggregate Output: Consolidates individual extracted records into a structured batch output
Outbound Data Delivery:
Google Sheets ‚Äì Appends summary and structured review data
Write to Disk ‚Äì Persists raw and processed content locally
Webhook Notification ‚Äì Sends a real-time alert with summarized insights
Pre-conditions
You need to have a Bright Data account and do the necessary setup as mentioned in the ""Setup"" section below.
You need to have an OpenAI Account.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, Configure the Google Sheet Credentials with your own account. Follow this documentation - Set Google Sheet Credential
In n8n, configure the OpenAi account credentials.
Ensure the URL and Bright Data zone name are correctly set in the Set URL, Filename and Bright Data Zone node.
Set the desired local path in the Write a file to disk node to save the responses.
How to customize this workflow to your needs
Target Multiple Products :
Configure the Bright Data input URL dynamically for different SaaS product TrustPilot URLs
Loop through a product list and run parallel jobs for each
Customize Extraction Fields :
Update the prompt in the Information Extractor to include:
Review title
Response from company
Specific feature mentions
Competitor references
Tune Summarization Style
Change tone: executive summary, customer pain-point focus, or marketing quote extract
Enable sentiment aggregation (e.g., 30% negative, 50% neutral, 20% positive)
Expand Output Destinations
Push to Notion, Airtable, or CRM tools using additional webhook nodes
Generate and send PDF reports (via PDFKit or HTML-to-PDF nodes)
Schedule summary digests via Gmail or Slack"
Automated Email Assistant for Suppliers using OpenAI and Google Sheets,https://n8n.io/workflows/4067-automated-email-assistant-for-suppliers-using-openai-and-google-sheets/,"Automated Email Assistant for Busy Professionals
This assistant is designed for people who don't have time to write and send emails to suppliers. With just one request, it drafts and sends clear, professional messages automatically.
How It Works
The user makes a request (e.g., ‚ÄúSend an email to my fruit supplier asking for a quote on 1 crate of mangoes.‚Äù).
Workflow:
The AI agent searches for the supplier in a Google Sheets database.
It automatically drafts the email using OpenAI (with the tone and style you define).
It sends the email using your Gmail account connected through n8n.
This assistant uses:
Google Sheets to manage your suppliers (name and email).
OpenAI to generate clear, natural messages.
MCP (client-server logic) to handle request processing.
Gmail as the sending channel for automated emails.
Setup Instructions
Create a Google Sheets document with the supplier name and email, like this:
Supplier name Email
Proveedor de frutas Alvarez fruteriaalvarez@alvarez.com
Connect your Google Sheets and Gmail accounts within n8n.
Add your OpenAI API key.
Test the automation by chatting with the integrated assistant.
It will generate and send the email automatically to the indicated supplier.
Requirements
OpenAI API key to generate email content.
Gmail account connected via OAuth2.
Google Sheets document with your supplier database.
n8n instance (cloud or self-hosted).
Customization
Adjust the OpenAI prompt to make the email tone more formal, casual, or technical.
Add custom fields to your supplier sheet (location, notes, special conditions).
Replace Google Sheets with a real database like Supabase or PostgreSQL for greater scalability."
Tesla News and Sentiment Analyst Tool (Powered by DeepSeek Chat),https://n8n.io/workflows/4093-tesla-news-and-sentiment-analyst-tool-powered-by-deepseek-chat/,"üì∞ This AI-powered agent performs real-time sentiment analysis on Tesla (TSLA) news to support trading decisions.
It aggregates headlines from 5 trusted sources and uses DeepSeek Chat to classify sentiment and generate structured summaries. This tool is a critical sub-agent in the broader Tesla Quant Trading AI Agent system.
‚ö†Ô∏è Not standalone ‚Äî this agent is designed to be executed by the Tesla Quant Trading AI Agent.
‚öôÔ∏è Requires: DeepSeek Chat API Key
üîå Workflow Role
This tool processes Tesla-related news and produces output like:
{
  ""sentiment"": ""bullish"",
  ""summary"": ""Tesla stock rallied today after strong delivery numbers and Cybertruck updates. Analysts remain optimistic."",
  ""topHeadlines"": [
    ""Tesla beats Q2 delivery forecast ‚Äì Yahoo Finance"",
    ""Cybertruck ramps up in Texas ‚Äì Electrek"",
    ""Berlin Gigafactory expands battery production ‚Äì CleanTechnica""
  ]
}
Its output feeds directly into the master trading agent‚Äôs final trade report.
üì∞ News Sources Used
This agent collects real-time headlines from:
Google News (filtered by ‚ÄúTesla‚Äù or ‚ÄúTSLA‚Äù)
Yahoo Finance (TSLA-specific feed)
Electrek (Tesla archive)
CleanTechnica (Tesla sustainability news)
TeslaNorth (app/product release updates)
These five tools are always queried together to ensure market-wide signal coverage.
ü§ñ What the Agent Does
Pulls headlines from all 5 Tesla-specific RSS feeds
Uses DeepSeek Chat to:
Analyze narrative tone (bullish / bearish / neutral)
Identify macro/financial drivers
Generate a 2‚Äì3 sentence summary
Return top 3‚Äì5 headlines
Outputs structured JSON for downstream use
üõ†Ô∏è Setup Instructions
1. Install & Name
Import this file and name it: Tesla_News_and_Sentiment_Analyst_Tool
2. Add DeepSeek API Credentials
Go to: Credentials ‚Üí Add New ‚Üí DeepSeek API
Save as: DeepSeek account
3. Internet Access Required
Ensure RSS feeds can fetch live headlines
Works best with a cloud-hosted n8n instance or tunnel-enabled local install
4. Must Be Triggered by Parent
Triggered via Execute Workflow by the Tesla Quant Trading AI Agent
Requires these inputs:
message: optional query context
sessionId: passed to maintain short-term memory across executions
üß† Agent Architecture
Node Name Function
DeepSeek Chat Model Performs AI-based sentiment analysis
Tesla News and Sentiment Analyst Combines results, formats output in strict JSON
Simple Memory Stores session-level context (short-term memory)
5x RSS nodes Aggregate Tesla news from trusted media outlets
üìå Sticky Notes Included
üü¢ Trigger from Parent Workflow ‚Äì Executed only by main TSLA agent
üü† News Feeds Overview ‚Äì Lists and explains each of the 5 feeds
üß† DeepSeek Chat Notes ‚Äì Describes LLM behavior and parsing role
üîµ Short-Term Memory ‚Äì Buffers sentiment context during user session
üìò Sentiment Analyst Agent ‚Äì Summarizes key responsibilities
üìé Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
This architecture, workflow structure, and prompt design are licensed for educational and operational use only. Commercial resale or rebranding prohibited without authorization.
üîó Creator: Don Jayamaha
üîó Templates: https://n8n.io/creators/don-the-gem-dealer/
üöÄ Power your TSLA trading with AI-driven sentiment‚Äîbuilt with DeepSeek Chat and 5 trusted news sources.
This tool is required by the Tesla Quant Trading AI Agent."
"AI-Powered RAG Q&A Chatbot with OpenAI, Google Sheets, Glide & Supabase",https://n8n.io/workflows/4071-ai-powered-rag-qanda-chatbot-with-openai-google-sheets-glide-and-supabase/,"Automate AI-Powered RAG System with Contextual Q&A, Google Sheets Integration, and Glide Frontend‚ÄîPowered by n8n, OpenAI, Supabase, and Google Apps Script.
Tools & Services Used
Glide (Frontend for user interactions)
Google Sheets (Stores questions and answers)
Google Apps Script (Forms + media upload handling)
OpenAI (Embeddings + GPT-4 to rank and generate answers)
Supabase (Optional for image hosting)
n8n (Automation logic and backend glue)
Workflow Overview
This automation performs the following steps:
Trigger: Webhook receives a user question from a Glide frontend.
Fetch Data: Retrieves Q&A entries (and optionally, image URLs) from a connected Google Sheet.
Rank Relevance:
OpenAI Embeddings rank the relevance of stored questions to the new input.
Top matches are passed to a GPT-4 prompt for answer generation.
Generate Answer:
GPT-4 creates a contextual answer using the best match.
Optional: Includes media URL if attached to the matched answer.
Response: Sends the formatted answer back to Glide frontend (text + optional image).
Prerequisites
Active accounts and API keys for:
OpenAI (API key with GPT-4 and embedding access)
Google Sheets (linked via Service Account or OAuth2 credentials)
Glide App (with a form to submit questions)
Supabase (optional, if hosting user-uploaded media)
n8n Self-hosted (for community nodes and webhook access)
How to Use This Template
Step 1: Import the Template
Upload the provided JSON into your self-hosted n8n instance.
Requires installation of the Community Node: @n8n/n8n-nodes-openai-embeddings
Step 2: Configure Credentials
Webhook Trigger: Replace the Glide webhook URL with your actual endpoint from your app.
Google Sheets Node: Set the spreadsheet ID and worksheet name to fetch your Q&A dataset.
OpenAI Nodes:
Insert your OpenAI API key.
Confirm that both text-embedding-ada-002 (or similar) and gpt-4 are available.
Supabase or Apps Script (Optional): Ensure image links are correctly formed in the Sheet.
Step 3: Customize Prompts and Thresholds
Update the OpenAI ranking prompt or similarity threshold in the Function or Code node.
Tailor GPT-4 prompts to return concise or detailed answers based on your use case.
Initial Test Run
Simulate a Question:
Trigger the Glide form with a sample question.
Check n8n logs for:
Matching score/ranking success
Correct retrieval from Sheets
GPT-4 answer generation
Image URL handling (if enabled)
Production Prep
Add error-handling nodes for cases when Sheets are empty or GPT-4 fails.
Set up logging via Google Sheets, Supabase, or external database.
Rate-limit or debounce requests from Glide if needed.
Use Cases
Customer Support Portals: Offer instant Q&A with your business data.
Internal Knowledgebase: Employees can query training manuals and SOPs.
AI Concierge: Combine structured answers with friendly tone for guest interactions.
Disclaimer
Validate Glide form inputs before full deployment.
Ensure your OpenAI and Google Sheets usage stays within quota limits."
"AI Talent Screener ‚Äì CV Parser, Job Fit Evaluator & Email Notifier",https://n8n.io/workflows/4055-ai-talent-screener-cv-parser-job-fit-evaluator-and-email-notifier/,"AI Talent Screener ‚Äì CV Parser, Job Fit Evaluator & Email Notifier
Who is this for?
This template is ideal for HR teams, recruiters, staffing agencies, and tech-enabled hiring managers looking to automate the intake, analysis, and shortlisting of job applicants directly from CV submissions.
What problem is this workflow solving?
Manually evaluating resumes for job openings is time-consuming and prone to bias or oversight. This workflow uses AI to extract candidate data, analyze fit based on job descriptions, and make data-backed shortlisting decisions ‚Äî automatically updating Google Sheets and notifying both Talent Acquisition and candidates.
What this workflow does
This workflow automates your recruitment pipeline by:
1. Accepting CV submissions via an embedded form.
2. Saving resumes to Google Drive.
3. Extracting and parsing resume content using AI.
4. Fetching the relevant job description from Google Sheets.
5. Summarizing both job and applicant profiles.
6. Conducting an AI-powered fit evaluation (semantic match, red flags, soft skills, etc.).
7. Updating detailed analysis in a centralized Google Sheet.
8. Notifying the TA team for approval.
9. Automatically sending shortlisting or rejection emails to candidates based on approval.
Setup
1. Connect credentials for:
Google Sheets
Google Drive
OpenAI API
SMTP email (for notification)
2. Update the Google Sheets document IDs and folder IDs in the relevant nodes.
3. Adjust the form fields in the ""Form Trigger"" node if needed.
4. Customize email sender/receiver addresses in the email nodes.
5. Enable the workflow and embed the form on your career page.
For convenience, the sticky notes guide you through what each block does.
How to customize this workflow to your needs
Change job roles by editing the dropdown options in the form.
Modify evaluation criteria in the ‚ÄúSemantic Fit & Evaluation by HR Expert‚Äù prompt.
Add more fields (e.g., GitHub, portfolio links) to the form and extend parsing.
Connect to ATS or Slack if you'd like to integrate further."
"Conversational PostgreSQL Agent with Visuals, Multi-KPI, and Data Editing (MCP)",https://n8n.io/workflows/3903-conversational-postgresql-agent-with-visuals-multi-kpi-and-data-editing-mcp/,"Ask your PostgreSQL database complex questions and receive clear summaries, charts, and even update or insert data ‚Äî all through one smart agent powered by n8n‚Äôs Model Context Protocol (MCP).
Supports:
Multi-KPI insights in one prompt
Auto-generated QuickChart bar/pie charts
Natural-language inserts and updates
Markdown-friendly output for dashboards
üöÄ Why This Version Stands Out
This version goes beyond reporting:
üìà Auto-generates charts (QuickChart)
üßÆ Answers multiple KPIs in one message
‚úçÔ∏è Add and update records securely
üß† Uses up to 30 planned steps for smart reasoning
üí∞ Estimated cost per run: ~$0.02
üí¨ Example Output
üß∞ Key Components
MCP Server Trigger ‚Üí Receives natural queries
Claude 3.5 Haiku ‚Üí Plans, reasons, splits tasks
DeepSeek ‚Üí SQL and QuickChart generation
checkdatabase subflow ‚Üí Validates SQL
Plot Tool ‚Üí Converts data to QuickChart URLs
Insert/Update nodes ‚Üí Edits PostgreSQL records
Markdown Formatter ‚Üí Combines output into readable message
ü§ñ Model Configuration Notes
This workflow uses two models:
Claude 3.5 Haiku (Anthropic)
Used as the MCP agent for reasoning, planning, and tool calling. Claude is the native model for MCP and delivers reliable results in fewer steps.
DeepSeek
Used in:
checkdatabase for SQL generation
Plot Tool for QuickChart JSON generation
üß† All models are modular ‚Äî you can plug in OpenAI, Gemini, or Mistral if desired.
üîê Security by Design
No raw SQL from user input
Fully parameterized queries
Structured tool calling with validation
Safe output format (text + chart links)
üß™ Try This Prompt
‚ÄúShow me top 5 products by revenue, revenue per month chart, and best customers.‚Äù
Expected output:
3 KPIs
Multiple SQL queries
2‚Äì3 QuickChart links
Markdown summary for dashboard/Slack
üõ† How to Use
Import:
Build_your_own_PostgreSQL_MCP_server__visuals_capable_.json
checkdatabase.json
Plot_tool.json
Create your PostgreSQL credential under ‚ÄúCredentials‚Äù in n8n:
Must match the name used in the workflow (e.g., Postgres account 3)
Assign AI models:
Claude 3.5 Haiku ‚Üí MCP agent (Claude 3.5 MCP Agent)
DeepSeek ‚Üí LLM nodes inside checkdatabase and Plot Tool
Trigger the workflow using the URL from the MCP Server Trigger node
(e.g., in a chatbot, HTTP request, or Webhook UI)
üì¶ End-User Setup Guide
If you're using this template for the first time, follow these exact steps:
Go to your n8n dashboard and import all three workflows (main + subflows)
Create a PostgreSQL credential using your host, database, user, and password
Go to the Claude and DeepSeek nodes, and connect them to your account(s)
Use the Webhook URL in the MCP Server Trigger to connect your chatbot or frontend
Send a prompt like:
‚ÄúShow me revenue per month, top 5 products, and a chart of best customers.‚Äù
Optional:
You can increase the MCP Agent‚Äôs MaxIterations to go deeper (default is 30)
You can use Switch nodes to limit access to certain tables or actions
Insert/Update nodes are already included and can be safely enabled
‚úÖ Once this is done, your AI assistant will:
Read from your database
Visualize data via QuickChart
Insert or update rows
Respond in clear, markdown-formatted summaries
üîó More Templates by the Same Creator
PostgreSQL Conversational Agent with Claude & DeepSeek (Multi-KPI, Secure)
Conversing with Data: Transforming Text into SQL Queries and Visual Curves
Customer Feedback Analysis with AI, QuickChart & HTML Report Generator"
"Build a Document QA System with RAG using Milvus, Cohere, and OpenAI for Google Drive",https://n8n.io/workflows/3848-build-a-document-qa-system-with-rag-using-milvus-cohere-and-openai-for-google-drive/,"Template Description
This template creates a powerful Retrieval Augmented Generation (RAG) AI agent workflow in n8n. It monitors a specified Google Drive folder for new PDF files, extracts their content, generates vector embeddings using Cohere, and stores these embeddings in a Milvus vector database. Subsequently, it enables a RAG agent that can retrieve relevant information from the Milvus database based on user queries and generate responses using OpenAI, enhanced by the retrieved context.
Functionality
The workflow automates the process of ingesting documents into a vector database for use with a RAG system.
Watch New Files: Triggers when a new file (specifically targeting PDFs) is added to a designated Google Drive folder.
Download New: Downloads the newly added file from Google Drive.
Extract from File: Extracts text content from the downloaded PDF file.
Default Data Loader / Set Chunks: Processes the extracted text, splitting it into manageable chunks for embedding.
Embeddings Cohere: Generates vector embeddings for each text chunk using the Cohere API.
Insert into Milvus: Inserts the generated vector embeddings and associated metadata into a Milvus vector database.
When chat message received: Adapt the trigger tool to fit your needs.
RAG Agent: Orchestrates the RAG process.
Retrieve from Milvus: Queries the Milvus database with the user's chat query to find the most relevant chunks.
Memory: Manages conversation history for the RAG agent to optimize cost and response speed.
OpenAI / Cohere embeddings: Uses ChatGPT 4o for text generation.
Requirements
To use this template, you will need:
An n8n instance (cloud or self-hosted).
Access to a Google Drive account to monitor a folder.
A Milvus instance or access to a Milvus cloud service like Zilliz.
A Cohere API key for generating embeddings.
An OpenAI API key for the RAG agent's text generation.
Usage
Set up the required credentials in n8n for Google Drive, Milvus, Cohere, and OpenAI.
Configure the ""Watch New Files"" node to point to the Google Drive folder you want to monitor for PDFs.
Ensure your Milvus instance is running and the target cluster is set up correctly.
Activate the workflow.
Add PDF files to the monitored Google Drive folder. The workflow will automatically process them and insert their embeddings into Milvus.
Interact with the RAG agent. The agent will use the data in Milvus to provide context-aware answers.
Benefits
Automates document ingestion for RAG applications.
Leverages Milvus for high-performance vector storage and search.
Uses Cohere for generating high-quality text embeddings.
Enables building a context-aware AI agent using your own documents.
Suggested improvements
Support for More File Types: Extend the ""Watch New Files"" node and subsequent extraction steps to handle various document types (e.g., .docx, .txt, .csv, web pages) in addition to PDFs.
Error Handling and Notifications: Implement robust error handling for each step of the workflow (e.g., failed downloads, extraction errors, Milvus insertion failures) and add notification mechanisms (e.g., email, Slack) to alert the user.
Get in touch with us
Contact us at https://1node.ai"
Auto-Post Breaking News Content Using Perplexity AI to X (Twitter),https://n8n.io/workflows/3822-auto-post-breaking-news-content-using-perplexity-ai-to-x-twitter/,"Stay ahead of the curve and keep your followers informed‚Äîautomatically.
This n8n workflow uses Perplexity AI to generate insightful answers to scheduled queries, then auto-posts the responses directly to X (Twitter).
‚öôÔ∏è What this workflow does
Scheduled Trigger ‚Äì Runs at set times (daily, hourly, etc.).
searchQuery ‚Äì Define what kind of trending or relevant insight you want (e.g. ‚Äúlatest AI trends‚Äù).
set API Key ‚Äì Securely insert your Perplexity API key.
Perplexity API Call ‚Äì Fetches a short, insightful response to your query.
Post to X ‚Äì Automatically publishes the result as a tweet.
üß© Requirements
An n8n account (self-hosted or cloud)
A Perplexity API key
A connected X (Twitter) account via n8n‚Äôs credentials
‚úÖ Setup Steps
Add this workflow into your n8n account.
Edit the searchQuery node with a topic (e.g. ‚ÄúWhat‚Äôs new in ecommerce automation?‚Äù).
Paste your Perplexity API key into the set API key node.
Connect your X (Twitter) account in the final node.
Adjust the schedule timing to suit your content frequency.
üí° Ideas to Improve
üí¨ Add a formatting step to shorten or hashtag the response.
üìä Pull multiple trending questions and auto-schedule posts.
üîÅ Loop responses to queue a full week of content.
üåê Translate content before posting to reach a global audience.
üÜò Need help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
"Scrape Web Data with Bright Data, Google Gemini and MCP Automated AI Agent",https://n8n.io/workflows/3778-scrape-web-data-with-bright-data-google-gemini-and-mcp-automated-ai-agent/,"Disclaimer
This template is only available on n8n self-hosted as it's making use of the community node for MCP Client.
Who this is for?
The Scrape Web Data with Bright Data and MCP Automated AI Agent workflow is built for professionals who need to automate large-scale, intelligent data extraction by utilizing the Bright Data MCP Server and Google Gemini.
This solution is ideal for:
Data Analysts - Who require structured, enriched datasets for analysis and reporting.
Marketing Researchers - Seeking fresh market intelligence from dynamic web sources.
Product Managers - Who want competitive product and feature insights from various websites.
AI Developers - Aiming to feed web data into downstream machine learning models.
Growth Hackers - Looking for high-quality data to fuel campaigns, research, or strategic targeting.
What problem is this workflow solving?
Manually scraping websites, cleaning raw HTML data, and generating useful insights from it can be slow, error-prone, and non-scalable.
This workflow solves these problems by:
Automating complex web data extraction through Bright Data‚Äôs MCP Server.
Reducing the human effort needed for cleaning, parsing, and analyzing unstructured web content.
Allowing seamless integration into further automation processes.
What this workflow does?
This n8n workflow performs the following steps:
Trigger: Start manually.
Input URL(s): Specify the URL to perform the web scrapping.
Web Scraping (Bright Data): Use Bright Data‚Äôs MCP Server tools to accomplish the web data scrapping with markdown and html format.
Store / Output: Save results into disk and also performs a Webhook notification.
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>.
8. Update the LinkedIn URL person and company workflow.
9. Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
10. Update the file name and path to persist on disk.
How to customize this workflow to your needs
Different Inputs: Instead of static URLs, accept URLs dynamically via webhook or form submissions.
Outputs: Update the Webhook endpoints to send the response to Slack channels, Airtable, Notion, CRM systems, etc."
"Chat with Your Email History using Telegram, Mistral and Pgvector for RAG",https://n8n.io/workflows/3763-chat-with-your-email-history-using-telegram-mistral-and-pgvector-for-rag/,"Who is this for?
Everyone! Did you dream of asking an AI ""what hotel did I stay in for holidays last summer?"" or ""what were my marks last semester like?"".
Dream no more, as vector similarity searches and this workflow are the foundations to make it possible (as long as the information appears in your e-mails üòÖ).
100% Local and Open Source!
This workflow is designed to use locally-hosted open source. Ollama as LLM provider, nomic-embed-text as the embeddings model, and pgvector as the vector database engine, on top of Postgres.
Structured AND Vectorized
This workflow combines structured and semantic search on your e-mail.
No need for enterprise setups!
Leverage the convenience of n8n and open source to get a bleeding edge solution.
Setup
You will need a PGVector database with embeddings for all your email. Use my other template Gmail to Vector Embeddings with PGVector and Ollama to set it up in a breeze!
Make a copy of my Email Assistant: Convert Natural Language to SQL Queries with Phi4-mini and PostgreSQL, you will need it for structured searches.
Install this template and modify the Call the SQL composer Workflow step, to point at your copy of the SQL workflow.
Adjust the rest of necessary steps: Telegram Trigger, AI Chat model, AI Embeddings...
Activate the workflow and chat around!"
Build an MCP Server with Google Calendar,https://n8n.io/workflows/3569-build-an-mcp-server-with-google-calendar/,"Who is this for?
This template is designed for anyone who wants to integrate MCP with their AI Agents. Whether you're a developer, a data analyst, or an automation enthusiast, if you're looking to leverage the power of MCP and Google Calendar in your n8n workflows, this template is for you.
What problem is this workflow solving?
This template caters to MCP beginners seeking a hands - on example and developers looking to integrate Google Calendar MCP service. When integrating MCP with Google Calendar, manually updating AI Agents after changes to Google Calendar tools on the MCP Server is time - consuming and error - prone.
This template automates the process, enabling the AI Agent to instantly recognize changes made to Google Calendar on the MCP Server. In project management, for example, it ensures that task schedule updates in Google Calendar are automatically detected by the AI Agent. With detailed steps, it simplifies the integration process for all users.
What this workflow does
This workflow focuses on integrating MCP with Google Calendar within n8n. Specifically, it allows you to build an MCP Server and Client using Google Calendar nodes in n8n. Any changes made to the Google Calendar tools on the MCP Server are automatically recognized by the MCP Client in the workflow. This means that you can make changes to your Google Calendar (such as adding, deleting, or modifying events) on the MCP Server, and the MCP Client in the n8n workflow will immediately detect these changes without any manual intervention.
Setup
Requirements
An active n8n account.
Access to Google Calendar API. You need to enable the Google Calendar API, and create the necessary credentials (OAuth 2.0 client ID).
Basic knowledge of n8n workflows and MCP concepts.
Step - by - step guide
Create a new workflow in n8n: Log in to your n8n account and create a new workflow.
Add Google Calendar nodes: Search for and add the Google Calendar nodes to your workflow. Configure the nodes with your Google Calendar API credentials.
Set up the MCP Server and Client: Use the appropriate nodes in n8n to set up the MCP Server and Client. Connect the Google Calendar nodes to the MCP nodes as required.
Test the workflow: Make some changes to your Google Calendar on the MCP Server and check if the MCP Client in the n8n workflow can detect these changes.
How to customize this workflow to your needs
If you want to customize this workflow, you can:
Modify the triggers: You can change the conditions under which the MCP Client detects changes. For example, you can set it to detect only specific types of events in Google Calendar.
Integrate with other services: You can add more nodes to the workflow to integrate with other services, such as sending notifications to Slack or saving data to a database when a change is detected."
Automated Resume Review System Using OpenAI + Google Sheets,https://n8n.io/workflows/3318-automated-resume-review-system-using-openai-google-sheets/,"Who is this for?
This workflow is ideal for:
HR teams and recruiters seeking to streamline resume screening.
Hiring managers who want quick, summarized candidate insights.
Recruitment agencies handling large volumes of applicant data.
Startups and small businesses looking to automate hiring without complex systems.
AI and automation professionals who want to build smart HR workflows using n8n and OpenAI.
What problem is this workflow solving? / Use Case
Manually reviewing resumes is time-consuming, inconsistent, and prone to human bias.
This workflow automates the resume intake and evaluation process‚Äîensuring that each applicant is screened, summarized, and scored using a consistent, data-driven method.
It enhances efficiency and supports better hiring decisions.
What this workflow does
Accepts resume submissions via form and saves files to Google Drive.
Extracts key information from resumes using AI (e.g., name, contact, education, experience).
Summarizes candidate qualifications into a short, readable profile.
Allows HR to rate applicants and leave comments.
Logs all extracted data and evaluations into a centralized Google Sheet for tracking.
Setup
Resume is submitted through an n8n form.
The uploaded file is automatically stored in Google Drive.
n8n uses OpenAI and document parsing tools to extract candidate data.
Extracted information is structured and summarized using GPT.
A review form is triggered for internal HR rating and notes.
All data is appended to a Google Sheet for records and filtering.
How to customize this workflow to your needs
Change the form tool (e.g., Typeform, Tally, or custom HTML) based on your stack.
Adapt the summary prompt to align with your specific role requirements.
Add filters to auto-flag top-tier candidates based on score or skills.
Integrate Slack or email to notify hiring managers when top resumes are processed.
Connect to your ATS if you want to push processed resumes into your recruitment system."
Get Live Crypto Market Data with AI-Powered CoinMarketCap Agent,https://n8n.io/workflows/3422-get-live-crypto-market-data-with-ai-powered-coinmarketcap-agent/,"Access real-time cryptocurrency prices, market rankings, metadata, and global stats‚Äîpowered by GPT-4o and CoinMarketCap!
This modular AI-powered agent is part of a broader CoinMarketCap multi-agent system designed for crypto analysts, traders, and developers. It uses the CoinMarketCap API and intelligently routes queries to the correct tool using AI.
This agent can be used standalone or triggered by a supervisor AI agent for multi-agent orchestration.
Supported API Tools (6 Total)
This agent intelligently selects from the following tools to answer your crypto-related questions:
üîç Tool Summary
Crypto Map ‚Äì Lookup CoinMarketCap IDs and active coins
Crypto Info ‚Äì Get metadata, whitepapers, and social links
Crypto Listings ‚Äì Ranked coins by market cap
CoinMarketCap Price ‚Äì Live prices, volume, and supply
Global Metrics ‚Äì Total market cap, BTC dominance
Price Conversion ‚Äì Convert between crypto and fiat
What You Can Do with This Agent
üîπ Get live prices and volume for tokens (e.g., BTC, ETH, SOL)
üîπ Convert crypto ‚Üí fiat or fiat ‚Üí crypto instantly
üîπ Retrieve whitepapers, logos, and website links for any token
üîπ Analyze total market cap, BTC dominance, and circulating supply
üîπ Discover new tokens and track their CoinMarketCap IDs
üîπ View the top 100 coins ranked by market cap or volume
Example Queries
‚úÖ ""What is the CoinMarketCap ID for PEPE?""
‚úÖ ""Show me the top 10 cryptocurrencies by market cap.""
‚úÖ ""Convert 5 ETH to USD.""
‚úÖ ""What‚Äôs the 24h volume for ADA?""
‚úÖ ""Get the global market cap and BTC dominance.""
AI Architecture
AI Brain: GPT-4o-mini
Memory: Session buffer with sessionId
Agent Type: Subworkflow AI tool
Connected APIs: 6 CoinMarketCap endpoints
Trigger Mode: Executes when called by a supervisor (via message and sessionId inputs)
Setup Instructions
Get a CoinMarketCap API Key
Register here: https://coinmarketcap.com/api/
Configure Credentials in n8n
Use HTTP Header Auth with your API key for each connected endpoint
Connect This Agent to a Supervisor Workflow (Optional)
Trigger this agent using Execute Workflow with inputs message and sessionId
Test Prompts
Try asking: ‚ÄúConvert 1000 DOGE to BTC‚Äù or ‚ÄúTop 5 coins in EUR‚Äù
Included Sticky Notes
Crypto Agent Guide ‚Äì Agent overview, node map, and endpoint details
Usage Instructions ‚Äì Step-by-step usage and sample prompts
Error Handling & Licensing ‚Äì Troubleshooting and IP rights
‚úÖ Final Notes
This agent is part of the CoinMarketCap AI Analyst System, which includes multiple specialized agents for cryptocurrencies, exchanges, community data, and DEX insights. Visit my Creator profile to find the full suite of tools.
Get smarter about crypto‚Äîanalyze the market in real time with AI and CoinMarketCap."
LinkedIn Profile Finder via Form using Bright Data & GPT-4o-mini,https://n8n.io/workflows/3335-linkedin-profile-finder-via-form-using-bright-data-and-gpt-4o-mini/,"This n8n workflow template automates the process of finding LinkedIn profiles for a person based on their name, and company. It scrapes Google search results via Bright Data, parses the results with GPT-4o-mini, and delivers a personalized follow-up email with insights and suggested outreach steps.
üöÄ What It Does
Accepts a user-submitted form with a person‚Äôs full name, and company.
Performs a Google search using Bright Data to find LinkedIn profiles and company data.
Uses GPT-4o-mini to parse HTML results and identify matching profiles.
Filters and selects the most relevant LinkedIn entry.
Analyzes the data to generate a buyer persona and follow-up strategy.
Sends a styled email with insights and outreach steps.
üõ†Ô∏è Step-by-Step Setup
Deploy the form trigger to accept person data (name, position, company).
Build a Google search query from user input.
Scrape search results using Bright Data.
Extract HTML content using the HTML node.
Use GPT-4o-mini to parse LinkedIn entries and company insights.
Filter for matches based on user input.
Merge relevant data and generate personalized outreach content.
Send email to a predefined address.
Show a final confirmation message to the user.
üß† How It Works: Workflow Overview
Trigger: When User Completes Form
Search: Edit Url LinkedIn, Get LinkedIn Entry on Google, Extract Body and Title, Parse Google Results
Matching: Extract Parsed Results, Filter, Limit, IF LinkedIn Profile is Found?
Fallback: Form Not Found if no match
Company Lookup: Edit Company Search, Get Company on Google, Parse Results, Split Out
Content Generation: Merge, Create a Followup for Company and Person
Email Delivery: Send Email, Form Email Sent
üì® Final Output
An HTML-styled email (using Tailwind CSS) with:
Matched LinkedIn profile
Company insights
Persona-based outreach strategy
üîê Credentials Used
BrightData account for scraping Google search results
OpenAI account for GPT-4o-mini-powered parsing and content generation
SMTP account for sending follow-up emails
‚ùìQuestions?
Template and node created by Miquel Colomer and n8nhackers.
Need help customizing or deploying? Contact us for consulting and support."
Effortless Job Hunting: Let this Automation Find Your Next Role,https://n8n.io/workflows/3051-effortless-job-hunting-let-this-automation-find-your-next-role/,"Find Job Postings from LinkedIn, Indeed, and Glassdoor and Save Them to Google Sheets Using AI
Overview
Effortlessly discover and apply to jobs tailored to your profile‚ÄîAI handles the search, you handle the interviews.
Say goodbye to endless job board scrolling. This automation leverages AI to analyze your resume, identify your skills, experience, and more, to match you with the most relevant job opportunities. It sources job postings from LinkedIn, Indeed, Glassdoor, ZipRecruiter, Monster, and other public job sites on the web. With seamless integration and automatic organization of results, you can focus on applying rather than searching.
Key Features
Intelligent Resume Parsing
Extracts key information from your PDF resume using AI.
Identifies skills, experience, education, and job preferences.
Targeted Job Matching
Uses the parsed resume data to search for jobs that align with your profile.
Ensures relevance by analyzing job descriptions for matching criteria.
Automated Data Organization
Compiles job listings into a structured Google Spreadsheet.
Eliminates the need for manual data entry, saving valuable time.
Easy Access and Review
Stores results in a familiar Google Sheets format for easy tracking.
Allows for filtering and sorting to prioritize applications.
Setup Instructions
Prerequisites
A free API key for the job search service.
Google Drive and Google Sheets accounts.
An updated resume in PDF format.
Step 1: Connect Your Resume Parsing AI
Upload your PDF resume to Google Drive.
Configure the AI parser node in n8n to extract relevant information.
Map the extracted fields (e.g., skills, job title, experience) for job searching.
Step 2: Automate the Job Search
Use the extracted data to perform a job search on LinkedIn, Indeed, Glassdoor, and other supported job sites.
Retrieve job postings based on relevant keywords and location preferences.
Step 3: Save Job Listings to Google Sheets
Create a new Google Sheet to store job listings.
Set up the automation to write job details (e.g., title, company, location, link) into the sheet.
Format the sheet for better readability and tracking.
Step 4: Review and Apply to Jobs
Open your Google Sheet to view job matches.
Click on the links to apply directly on the respective job sites.
Update the status of each job application as you progress.
Why Use This Automation?
Saves Time: Automates job searching and listing compilation.
Enhances Efficiency: Eliminates manual scrolling and data entry.
Improves Organization: Keeps all job opportunities in a structured format.
Optimizes Your Job Hunt: Increases chances of landing the perfect role.
Designed specifically for job seekers aiming to optimize their search process, this automation integrates with Google Drive and Sheets, streamlining your job hunt and enhancing your chances of finding the right opportunity. Get started today and accelerate your career growth!"
AI-Powered Research with Jina AI Deep Search,https://n8n.io/workflows/3068-ai-powered-research-with-jina-ai-deep-search/,"Unlock AI-Driven Research with Jina AI (No API Key Needed!)
Following the success of Open Deep Research 1.0, we are excited to introduce an improved and fully free version: AI-Powered Research with Jina AI Deep Search.
This workflow leverages Jina AI‚Äôs Deep Search API, a free and powerful AI research tool that requires no API key. It automates querying, analyzing, and formatting research reports, making AI-driven research accessible to everyone.
Key Features
No API Keys Required - Start researching instantly without setup hassle.
Automated Deep Search - Uses Jina AI to fetch relevant and high-quality information.
Structured AI Reports - Generates clear, well-formatted research documents in markdown.
Flexible and Customizable - Modify the workflow to fit your specific research needs.
Ideal for Researchers, Writers & Students - Speed up your research workflow.
Use Cases
This workflow is particularly useful for:
Researchers - Quickly gather and summarize academic papers, online sources, and deep web content.
Writers & Journalists - Automate background research for articles, essays, and investigative reports.
Students & Educators - Generate structured reports for assignments, literature reviews, or presentations.
Content Creators - Find reliable sources for blog posts, videos, or social media content.
Data Analysts - Retrieve contextual insights from various online sources for reports and analysis.
How It Works
The user submits a research query via chat.
The workflow sends the query to Jina AI‚Äôs Deep Search API.
The AI processes and generates a well-structured research report.
A code node formats the response into clean markdown.
The final output is a structured, easy-to-read AI-generated report.
Pre-Conditions & Requirements
An n8n instance (self-hosted or cloud).
No API keys needed ‚Äì Jina AI Deep Search is completely free.
Basic knowledge of n8n workflow automation is recommended for customization.
Customization Options
This workflow is fully modular, allowing users to:
Modify the query prompt to refine the research focus.
Adjust the report formatting to match personal or professional needs.
Expand the workflow by adding additional AI tools or data sources.
Integrate it with other workflows in n8n to enhance automation.
Users are free to connect it with other workflows, add custom nodes, or tweak existing configurations.
Getting Started
Setup Time: Less than 5 minutes
Import the workflow into n8n.
Run the workflow and input a research topic.
Receive a fully formatted AI-generated research report.
Try It Now!
Start your AI-powered research with Jina AI Deep Search today!
Get the workflow on n8n.io"
Copy Viral Reels with Gemini AI,https://n8n.io/workflows/2993-copy-viral-reels-with-gemini-ai/,"Video Guide
I prepared a detailed guide that shows the whole process of building an AI tool to analyze Instagram Reels using n8n.
Youtube Link
Who is this for?
This workflow is ideal for social media analysts, digital marketers, and content creators who want to leverage data-driven insights from their Instagram Reels. It's particularly useful for those looking to automate the analysis of video performance to inform strategy and content creation.
What problem does this workflow solve?
Analyzing video performance on Instagram can be tedious and time-consuming, requiring multiple steps and data extraction. This workflow automates the process of fetching, analyzing, and recording insights from Instagram Reels, making it simpler for users to track engagement metrics without manual intervention.
What this workflow does
This workflow integrates several services to analyze Instagram Reels, allowing users to:
Automatically fetch recent Reels from specified creators.
Analyze the most-watched videos for insights.
Store and manage data in Airtable for easy access and reporting.
Initial Trigger: The process begins with a manual trigger that can later be modified for scheduled automation.
Data Retrieval: It connects to Airtable to fetch a list of creators and their respective Instagram Reels.
Video Analysis: It handles the fetching, downloading, and uploading of videos for analysis using an external service, simplifying performance tracking through a structured query process.
Record Management: It saves relevant metrics and insights into Airtable, ensuring that users can access and organize their video analytics effectively.
Setup
Create accounts:
Set up Airtable, Edify, n8n, and Gemini accounts.
Prepare triggers and modules:
Replace credentials in each node accordingly.
Configure data flow:
Ensure modules are set to fetch and analyze the correct data fields as outlined in the guide.
Test the workflow:
Run the scenario manually to confirm that data is fetched and analyzed correctly."
‚úçÔ∏èüåÑ Your First Wordpress + AI Content Creator - Quick Start,https://n8n.io/workflows/2981-your-first-wordpress-ai-content-creator-quick-start/,"‚úçÔ∏èüåÑ WordPress + AI Content Creator
This workflow automates the creation and publishing of multi-reading-level content for WordPress blogs. It leverages AI to generate optimized articles, automatically creates featured images, and provides versions of the content at different reading levels (Grade 2, 5, and 9).
How It Works
Content Generation & Processing üéØ
Starts with a manual trigger and a user-defined blog topic
Uses AI to create a structured blog post with proper HTML formatting
Separates and validates the title and content components
Saves a draft version to Google Drive for backup
Multi-Reading Level Versions üìö
Automatically rewrites the content for different reading levels:
Grade 9: Sophisticated language with appropriate metaphors
Grade 5: Simplified with light humor and age-appropriate examples
Grade 2: Basic language with simple metaphors and child-friendly explanations
WordPress Integration üåê
Creates a draft post in WordPress with the Grade 9 version
Generates a relevant featured image using Pollinations.ai
Automatically uploads and sets the featured image
Sends success/error notifications via Telegram
Setup Steps
Configure API Credentials üîë
Set up WordPress API connection
Configure OpenAI API access
Set up Google Drive integration
Add Telegram bot credentials for notifications
Customize Content Parameters ‚öôÔ∏è
Adjust reading level prompts as needed
Modify image generation settings
Set WordPress post parameters
Test and Deploy üöÄ
Run a test with a sample topic
Verify all reading level versions
Check WordPress draft creation
Confirm notification system
This workflow is perfect for content creators who need to maintain a consistent blog presence while catering to different audience reading levels. It's especially useful for educational content, news sites, or any platform that needs to communicate complex topics to diverse audiences."
Allow Users to Send a Sequence of Messages to an AI Agent in Telegram,https://n8n.io/workflows/2917-allow-users-to-send-a-sequence-of-messages-to-an-ai-agent-in-telegram/,"Use Case
When creating chatbots that interface through applications such as Telegram and WhatsApp, users can often sends multiple shorter messages in quick succession, in place of a single, longer message. This workflow accounts for this behaviour.
What it Does
This workflow allows users to send several messages in quick succession, treating them as one coherent conversation instead of separate messages requiring individual responses.
How it Works
When messages arrive, they are stored in a Supabase PostgreSQL table
The system waits briefly to see if additional messages arrive
If no new messages arrive within the waiting period, all queued messages are:
Combined and processed as a single conversation
Responded to with one unified reply
Deleted from the queue
Setup
Create a table in Supabase called message_queue. It needs to have the following columns: user_id (uint8), message (text), and message_id (uint8)
Add your Telegram, Supabase, OpenAI, and PostgreSQL credentials
Activate the workflow and test by sending multiple messages the Telegram bot in one go
Wait ten seconds after which you will receive a single reply to all of your messages
How to Modify it to Your Needs
Change the value of Wait Amount in the Wait 10 Seconds node in order to to modify the buffering window
Add a System Message to the AI Agent to tailor it to your specific use case
Replace the OpenAI sub-node to use a different language model"
AI Agent with Ollama for current weather and wiki,https://n8n.io/workflows/2931-ai-agent-with-ollama-for-current-weather-and-wiki/,"This workflow template demonstrates how to create an AI-powered agent that provides users with current weather information and Wikipedia summaries. By integrating n8n with Ollama's local Large Language Models (LLMs), this template offers a seamless and privacy-conscious solution for real-time data retrieval and summarization.
Who is this for?
Developers and Enthusiasts: Individuals interested in building AI-driven workflows without relying on external APIs.
Privacy-Conscious Users: Those who prefer processing data locally to maintain control over their information.
Educators and Students: Learners seeking hands-on experience with AI integrations and workflow automation.
What problem does this workflow solve?
Accessing up-to-date weather information and concise Wikipedia summaries typically requires multiple API calls to external services, which can raise privacy concerns and incur costs. This workflow addresses these issues by utilizing Ollama's self-hosted LLMs within n8n, enabling users to retrieve and process information locally.
What this workflow does:
User Input Capture: Begins with a chat interface where users can input queries.
AI Processing: The input is sent to an AI Agent node configured with Ollama's LLMs, which interprets the query and determines the required actions.
Weather Retrieval: For weather-related queries, the workflow fetches current weather data from a specified source.
Wikipedia Summarization: For queries seeking information, it retrieves relevant Wikipedia content and generates concise summaries.
Setup:
Install Required Tools:
Ollama: Install and run Ollama to manage local LLMs.
Configure n8n Workflow:
Import the provided workflow template into your n8n instance.
Set up the AI Agent node to connect with Ollama's API.
Ensure nodes responsible for fetching weather data and Wikipedia content are correctly configured.
Run the Workflow:
Start the workflow and interact with the chat interface to test various queries.
How to customize this workflow to your needs:
Automate Triggers: Set up scheduled triggers to provide users with regular updates, such as daily weather forecasts or featured Wikipedia articles."
"A Very Simple ""Human in the Loop"" Email Response System Using AI and IMAP",https://n8n.io/workflows/2907-a-very-simple-human-in-the-loop-email-response-system-using-ai-and-imap/,"Functionality
This workflow automates the handling of incoming emails by summarizing their content, generating appropriate responses, and validating the responses through a ""Human-in-the-Loop"" system. It integrates with IMAP email services (e.g., Gmail, Outlook) and uses AI models to streamline the email response process.
The workflow ensures that all AI-generated responses are reviewed by a human before being sent, maintaining a high level of professionalism and accuracy. This approach is particularly useful for businesses that receive a high volume of emails and need to respond quickly while ensuring quality control.
How It Works
Email Trigger:
The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new messages. When a new email arrives, it triggers the workflow.
Email Preprocessing:
The Markdown node converts the email's HTML content into plain text for easier processing by the AI models.
Email Summarization:
The Email Summarization Chain node uses an AI model (OpenAI) to generate a concise summary of the email. The summary is limited to 100 words and is written in a professional tone.
Email Response Generation:
The Write email node uses an AI model (OpenAI) to draft a professional response to the email. The response is based on the email content and is limited to 100 words.
Human-in-the-Loop Approval:
The Set Email text node prepares the drafted response for approval.
The Approve Email node sends the drafted response to a human approver (e.g., an internal email address) for review. The email includes:
The original message.
The AI-generated response.
The Approved? node checks if the response has been approved by the human reviewer. If approved, the workflow proceeds to send the response; otherwise, it stops.
Sending the Response:
The Send Email node sends the approved response back to the original sender.
Key Features
Automated Email Summarization: Summarizes incoming emails to provide a quick overview of the content.
AI-Powered Response Generation: Drafts professional responses to emails using AI.
Human-in-the-Loop Approval: Ensures all AI-generated responses are reviewed and approved by a human before being sent.
IMAP Integration: Works with IMAP email services like Gmail and Outlook.
Efficient Email Management: Reduces the time and effort required to handle incoming emails while maintaining high-quality responses.
This workflow is ideal for businesses looking to automate their email response process while maintaining control over the quality of outgoing communications. It leverages AI to handle repetitive tasks and ensures that all responses are reviewed by a human, providing a balance between automation and human oversight."
"AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack",https://n8n.io/workflows/2799-ai-powered-information-monitoring-with-openai-google-sheets-jina-ai-and-slack/,"Check Legal Regulations:
This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry!
üìå Purpose
This workflow enables automated and AI-driven topic monitoring, delivering concise article summaries directly to a Slack channel in a structured and easy-to-read format.
It allows users to stay informed on specific topics of interest effortlessly, without manually checking multiple sources, ensuring a time-efficient and focused monitoring experience.
To get started, copy the Google Sheets template required for this workflow from here.
üéØ Target Audience
This workflow is designed for:
Industry professionals looking to track key developments in their field.
Research teams who need up-to-date insights on specific topics.
Companies aiming to keep their teams informed with relevant content.
‚öôÔ∏è How It Works
Trigger: A Scheduler initiates the workflow at regular intervals (default: every hour).
Data Retrieval:
RSS feeds are fetched using the RSS Read node.
Previously monitored articles are checked in Google Sheets to avoid duplicates.
Content Processing:
The article relevance is assessed using OpenAI (GPT-4o-mini).
Relevant articles are scraped using Jina AI to extract content.
Summaries are generated and formatted for Slack.
Output:
Summaries are posted to the specified Slack channel.
Article metadata is stored in Google Sheets for tracking.
üõ†Ô∏è Key APIs and Nodes Used
Scheduler Node: Triggers the workflow periodically.
RSS Read: Fetches the latest articles from defined RSS feeds.
Google Sheets: Stores monitored articles and manages feed URLs.
OpenAI API (GPT-4o-mini): Classifies article relevance and generates summaries.
Jina AI API: Extracts the full content of relevant articles.
Slack API: Posts formatted messages to Slack channels.
This workflow provides an efficient and intelligent way to stay informed about your topics of interest, directly within Slack."
Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI,https://n8n.io/workflows/2705-chat-with-github-api-documentation-rag-powered-chatbot-with-pinecone-and-openai/,"This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API.
You could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use.
How it works:
Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository.
Chunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning.
Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database.
Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model.
Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding.
Response Generation: The retrieved chunks and your original question are fed to OpenAI's gpt-4o-mini LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable.
Set up steps:
Create accounts: You'll need accounts with OpenAI and Pinecone.
API keys: Obtain API keys for both services.
Configure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys.
Import the workflow: Import this workflow into your n8n instance.
Pinecone Index: Ensure you have a Pinecone index named ""n8n-demo"" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box.
Setup Time: Approximately 15-20 minutes.
Why use this workflow?
Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot.
Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases.
n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs."
"Analyze tradingview.com charts with Chrome extension, N8N and OpenAI",https://n8n.io/workflows/2642-analyze-tradingviewcom-charts-with-chrome-extension-n8n-and-openai/,"This flow is supported by a Chrome plugin created with Cursor AI.
The idea was to create a Chrome plugin and a backend service in N8N to do chart analytics with OpenAI. It's a good sample on how to submit a screenshot from the browser to N8N.
Who is it for?
N8N developers who want to learn about using a Chrome plugin, an N8N webhook and OpenAI.
What opportunity does it present?
This sample opens up a whole range of N8N connected Chrome extensions that can analyze screenshots by using OpenAI.
What this workflow does?
The workflow contains:
a webhook trigger
an OpenAI node with GPT-4O-MINI and Analyze Image selected
a response node to send back the Text that was created after analysing the screenshot.

All this is needed to talk to the Chrome extension which is created with Cursor AI.
The idea is to visit the tradingview.com crypto charts, click the Chrome plugin and get back analytics about the shown chart in understandable language. This is driven by the N8N flow.
With the new image analytics capabilities of OpenAI this opens up a world of opportunities.
Requirements/setup
OpenAI API key
Cursor AI installed
The Chrome extension. Download
The N8N JSON code. Download
How to customize it to your needs?
Both the Chrome extension and N8N flow can be adapted to use on other websites. You can consider:
analyzing a financial screen and ask questions about the data shown
analyzing other charts
extending the N8N workflow with other AI nodes
With AI and image analytics the sky is the limit and in some cases it saves you from creating complex API integrations.
Download Chrome extension"
"AI Powered Web Scraping with Jina, Google Sheets and OpenAI : the EASY way",https://n8n.io/workflows/2552-ai-powered-web-scraping-with-jina-google-sheets-and-openai-the-easy-way/,"Purpose of workflow:
The purpose of this workflow is to automate scraping of a website, transforming it into a structured format, and loading it directly into a Google Sheets spreadsheet.
How it works:
Web Scraping: Uses the Jina AI service to scrape website data and convert it into LLM-friendly text.
Information Extraction: Employs an AI node to extract specific book details (title, price, availability, image URL, product URL) from the scraped data.
Data Splitting: Splits the extracted information into individual book entries.
Google Sheets Integration: Automatically populates a Google Sheets spreadsheet with the structured book data.
Step by step setup:
Set up Jina AI service:
Sign up for a Jina AI account and obtain an API key.
Configure the HTTP Request node:
Enter the Jina AI URL with the target website.
Add the API key to the request headers for authentication.
Set up the Information Extractor node:
Use Claude AI to generate a JSON schema for data extraction.
Upload a screenshot of the target website to Claude AI.
Ask Claude AI to suggest a JSON schema for extracting required information.
Copy the generated schema into the Information Extractor node.
Configure the Split node:
Set it up to separate the extracted data into individual book entries.
Set up the Google Sheets node:
Create a Google Sheets spreadsheet with columns for title, price, availability, image URL, and product URL.
Configure the node to map the extracted data to the appropriate columns."
Real-Time Startup Intelligence: Crunchbase Monitoring with AI Summaries & Email Alerts,https://n8n.io/workflows/4792-real-time-startup-intelligence-crunchbase-monitoring-with-ai-summaries-and-email-alerts/,"Automated monitoring system that tracks startup activities, funding events, and company updates in real-time, providing valuable market intelligence.
üöÄ What It Does
Real-time monitoring of startup activities
Funding alerts and updates
Competitor tracking
Industry trend analysis
Customizable watchlists
üéØ Perfect For
Venture capitalists
Startup founders
Business development teams
Market researchers
Investment analysts
‚öôÔ∏è Key Benefits
‚úÖ Stay ahead of market movements
‚úÖ Never miss important funding rounds
‚úÖ Track competitor activities
‚úÖ Identify emerging trends
‚úÖ Save hours of manual research
üîß What You Need
Crunchbase API access
n8n instance
Notification preferences (email/Slack/Teams)
üìä Data Points Tracked
New funding rounds
Company updates
Leadership changes
Product launches
Market expansions
üõ†Ô∏è Setup & Support
Quick Setup
Deploy in 20 minutes with our step-by-step configuration guide
üì∫ Watch Tutorial
üíº Get Expert Support
üìß Direct Help
Stay informed about the startup ecosystem with automated monitoring and alerts. Make data-driven decisions with timely, relevant information."
üöö CO2 Emissions of Freight Shipments with Carbon Interface API and GPT-4o,https://n8n.io/workflows/4757-co2-emissions-of-freight-shipments-with-carbon-interface-api-and-gpt-4o/,"Tags: Sustainability, Supply Chain, AI Agent, CO2 Emissions, Carbon Interface API, Logistics, Automation
Context
Hi! I‚Äôm Samir ‚Äî a Supply Chain Engineer and Data Scientist based in Paris, and founder of LogiGreen Consulting.
I help logistics teams reduce their environmental footprint by combining AI automation and carbon estimation APIs.
This workflow is part of our green logistics initiative, allowing businesses to track the CO‚ÇÇ emissions of last-mile or regional shipments.
Automate carbon tracking for shipping operations with n8n!
üì¨ For business inquiries, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is designed for logistics coordinators, transportation planners, or sustainability officers who want to estimate and record emissions for B2B shipments.
Let‚Äôs imagine your carrier sends a shipment confirmation email after a pickup is scheduled:
An AI Agent reads the email and extracts structured data: addresses, distance, cargo weight, and delivery time.
The Carbon Interface API is then called to calculate CO‚ÇÇ emissions based on weight and distance, and the results are stored in a Google Sheet.
How does it work?
This workflow automates the process of tracking CO‚ÇÇ emissions for scheduled shipments:
üì® Gmail Trigger captures shipment confirmation emails
üß† AI Agent parses the shipment info (pickup, delivery, weight, distance)
üöö Carbon Interface API estimates CO‚ÇÇ emissions
üìä Google Sheets is used to store shipment metadata and carbon results
Steps:
üíå Trigger on new shipment confirmation email
üß† Extract structured shipment info with AI Agent
üìã Store metadata in Google Sheets
‚öôÔ∏è Call Carbon Interface API with weight and distance
üì• Append estimated CO‚ÇÇ emissions to the shipment row
What do I need to get started?
You‚Äôll need:
A Gmail account to receive shipment confirmation emails
A Google Sheet to track shipment data and CO‚ÇÇ
A free Carbon Interface API key
OpenAI access for using the AI Agent parser
A few sample emails from your logistics provider to test
Next Steps
üóíÔ∏è Use the sticky notes in the n8n canvas to:
Add your Gmail and Carbon Interface credentials
Try with a sample shipment confirmation email
Check your Google Sheet to verify emissions and timestamps
This template was built using n8n v1.93.0
Submitted: June 7, 2025"
Generate Captions from Autocomplete Ideas using Dumpling AI + GPT-4o,https://n8n.io/workflows/4633-generate-captions-from-autocomplete-ideas-using-dumpling-ai-gpt-4o/,"üìå Who is this for?
This workflow is perfect for social media managers, content creators, digital marketers, and copywriters who want to save time and stay relevant by automatically generating fresh caption ideas based on trending search behavior.
üí° What problem is this workflow solving?
Manually coming up with engaging social media content is time-consuming and often hit-or-miss. This workflow leverages Google autocomplete to identify what users are searching for, then uses AI to convert those suggestions into short, engaging captions for use on platforms like Instagram, LinkedIn, or Twitter.
‚öôÔ∏è What this workflow does
This automation runs daily at noon and transforms trending search topics into ready-to-use captions:
Run Every Day at 12 PM
A schedule trigger that activates the workflow once per day.
Get Search Keywords from Google Sheet
Pulls a list of base search phrases from a Google Sheet which serve as the starting point for getting trending autocomplete terms.
Fetch Autocomplete Suggestions (Dumpling AI)
Calls Dumpling AI‚Äôs get-autocomplete endpoint using the base phrase to return Google autocomplete suggestions.
API Reference ‚Üí
Format Suggestions into Array
Formats the list of returned suggestions into an array format that can be looped through.
Loop Through Each Autocomplete Suggestion
Splits the array into individual suggestions to process each one separately.
Generate Caption from Suggestion (GPT-4o)
Sends each suggestion to GPT-4o with a detailed system prompt to create a short, human-sounding, engaging caption under 280 characters.
Save Keyword & Generated Caption to Google Sheet
Saves both the original search suggestion and the generated caption to another tab in Google Sheets for content scheduling or review.
This workflow combines real-time search trend data with the power of AI to keep your social media feed fresh, relevant, and consistent ‚Äî all without lifting a finger."
"Extract, Summarize & Analyze Amazon Price Drops with Bright Data & Google Gemini",https://n8n.io/workflows/4611-extract-summarize-and-analyze-amazon-price-drops-with-bright-data-and-google-gemini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
This n8n-powered automation uses Bright Data's MCP Client to extract real-time data from a price drop site listing the amazon products, including price changes and related product details.
The extracted data is enriched with structured data transformation, content summarization, and sentiment analysis using Google Gemini LLM.
The Amazon Price Drop Intelligence Engine is designed for:
Ecommerce Analysts who need timely updates on competitor pricing trends
Brand Managers seeking to understand consumer sentiment around pricing
Data Scientists building pricing models or enrichment pipelines
Affiliate Marketers looking to optimize campaigns based on dynamic pricing
AI Developers automating product intelligence pipelines
What problem is this workflow solving?
This workflow solves several key pain points:
Reliable Scraping: Uses Bright Data MCP, a managed crawling platform that handles proxies, captchas, and site structure changes automatically.
Insight Generation: Transforms unstructured HTML into structured data and then into human-readable summaries using Google Gemini LLM.
Sentiment Context: Goes beyond raw pricing data to reveal how customers feel about the price change, helping businesses and researchers measure consumer reaction.
Automated Reporting: Aggregates and stores data for easy access and downstream automation (e.g., dashboards, notifications, pricing models).
What this workflow does
Scrape price drop site with Bright Data MCP
The workflow begins by scraping targeted price drop site for Amazon listings using Bright Data's Model Context Protocol (MCP).
You can configure this to target:
Structured Data Extraction
Once the HTML content is retrieved, Google Gemini is employed to:
Parse and structure the product information (title, price, discount, brand, ratings)
Summarization & Sentiment Analysis
The extracted data is passed through an LLM chain to:
Generate a concise summary of the product and its recent price movement
Perform sentiment analysis on user reviews and public perception
Store the Results
Save to disk for archiving or bulk processing
Updated in a Google Sheet, making it instantly shareable with your team or integrated into a BI dashboard
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
Target different platforms: Switch Amazon for Walmart, eBay, or any ecommerce source using Bright Data‚Äôs flexible scraping infrastructure.
Enrich with more LLM tasks: Add brand tone analysis, category classification, or competitive benchmarking using Gemini prompts.
Visualize output: Pipe the Google Sheet to Looker Studio, Tableau, or Power BI.
Notification integrations: Add Slack, Discord, or email notifications for price drop alerts."
Extract Details from Receipts via Telegram with Tesseract and Llama,https://n8n.io/workflows/4361-extract-details-from-receipts-via-telegram-with-tesseract-and-llama/,"Tesseract - Money Mate Workflow Description

Disclaimer: This template requires the n8n-nodes-tesseractjs community node, which is only available on self-hosted n8n instances. You‚Äôll need a self-hosted n8n setup to use this workflow.
Who is this for?
This workflow is designed for individuals, freelancers, or small business owners who want an easy way to track expenses using Telegram. It‚Äôs ideal for anyone looking to digitize receipts‚Äîwhether from photos or text messages‚Äîusing free tools, without needing advanced technical skills.
What problem does this workflow solve?
Manually entering receipt details into a spreadsheet or app is time-consuming and prone to mistakes. This workflow automates the process by extracting information from receipt images or text messages sent via Telegram, categorizing expenses, and sending back a clear, formatted summary. It saves time, reduces errors, and makes expense tracking effortless.
What this workflow does
The workflow listens for messages sent to a Telegram bot, which can be either text descriptions of expenses or photos of receipts. If a photo is sent, Tesseract (an open-source text recognition tool) extracts the text. If text is sent, it‚Äôs processed directly. An AI model (LLaMA via OpenRouter) analyzes the input, categorizes it into expense types (e.g., Food & Beverages, Household, Transport), and creates a structured summary including store name, date, items, total, and category. The summary is then sent back to the user‚Äôs Telegram chat.
Setup Instructions
Follow these step-by-step instructions to set up the workflow. No advanced technical knowledge is required, but you‚Äôll need a self-hosted n8n instance.
Set Up a Self-Hosted n8n Instance:
If you don‚Äôt have n8n installed, follow the n8n self-hosting guide to set it up. You can use platforms like Docker or a cloud provider (e.g., DigitalOcean, AWS).
Ensure your n8n instance is running and accessible via a web browser.
Install the Tesseract Community Node:
In your n8n instance, go to Settings > Community Nodes in the sidebar.
Click Install a Community Node, then enter n8n-nodes-tesseractjs in the search bar.
Click Install and wait for confirmation. This node enables receipt image processing.
If you encounter issues, check the n8n community nodes documentation for troubleshooting.
Create a Telegram Bot:
Open Telegram and search for @BotFather to start a new bot.
Send /start to BotFather, then /newbot to create your bot.
Follow the prompts to name your bot (e.g., ‚ÄúMoneyMateBot‚Äù).
BotFather will provide a Bot Token (e.g., 23872837287:ExampleExampleExample). Copy this token.
In n8n, go to Credentials > Add Credential, select Telegram API, and paste the token. Name the credential (e.g., ‚ÄúMoneyMateBot‚Äù) and save.
Set Up OpenRouter for AI Processing:
Sign up for a free account at OpenRouter.
In your OpenRouter dashboard, generate an API Key under the API section.
In n8n, go to Credentials > Add Credential, select OpenRouter API, and paste the API key. Name it (e.g., ‚ÄúOpenRouter Account‚Äù) and save.
The free tier of OpenRouter‚Äôs LLaMA model is sufficient for this workflow.
Import and Configure the Workflow:
Download the workflow JSON file (provided separately or copy from the source).
In n8n, go to Workflows > Import Workflow and upload the JSON file.
Open the imported workflow (‚ÄúTesseract - Money Mate‚Äù).
Ensure the Telegram Trigger and Send Expense Summary nodes use the Telegram credential you created.
Ensure the AI Analyzer node uses the OpenRouter credential.
Save the workflow.
Test the Workflow:
Activate the workflow by toggling the Active switch in n8n.
In Telegram, find your bot (e.g., @MoneyMateBot) and send /start.
Test with a sample input (see ‚ÄúExample Inputs‚Äù below).
Check the n8n workflow execution panel to ensure data flows correctly.
If errors occur, double-check credentials and node connections.
Activate for Continuous Use:
Once tested, keep the workflow active in n8n.
Your bot will now process any text or image sent to it via Telegram.
Example Inputs/Formats
To help the workflow process your data accurately, use clear and structured inputs. Below are examples of valid inputs:
Text Input Example:
Send a message to your Telegram bot like this:
Bought coffee at Starbucks, Jalan Sudirman, yesterday. Total Rp 50,000. 2 lattes, each Rp 25,000.
Expected Output:
hello [Your Name]

Ini Rekap Belanjamu
üìã Store: Starbucks
üìç Location: Jalan Sudirman
üìÖ Date: 2025-05-26
üõí Items:
- Latte: Rp 25,000
- Latte: Rp 25,000
üí∏ Total: Rp 50,000
üìå Category: Food & Beverages
Image Input Example:
Upload a photo of a receipt to your Telegram bot. The receipt should contain:
Store name (e.g., ‚ÄúAlfamart‚Äù)
Address (e.g., ‚ÄúJl. Gatot Subroto, Jakarta‚Äù)
Date and time (e.g., ‚Äú27/05/2025 14:00‚Äù)
Items with prices (e.g., ‚ÄúBread Rp 15,000‚Äù, ‚ÄúMilk Rp 20,000‚Äù)
Total amount (e.g., ‚ÄúTotal: Rp 35,000‚Äù)
Expected Output:
hello [Your Name]

Ini Rekap Belanjamu
üìã Store: Alfamart
üìç Location: Jl. Gatot Subroto, Jakarta
üìÖ Date: 2025-05-27 14:00
üõí Items:
- Bread: Rp 15,000
- Milk: Rp 20,000
üí∏ Total: Rp 35,000
üìå Category: Household
Tips for Images:
Ensure the receipt is well-lit and text is readable.
Avoid blurry or angled photos for better Tesseract accuracy.
How to Customize This Workflow
Change Expense Categories: In the AI Categorizer node, edit the prompt to include custom categories (e.g., add ‚ÄúEntertainment‚Äù or ‚ÄúUtilities‚Äù to the list: Food & Beverages, Household, Transport).
Modify Response Format: In the Format Summary Message node, adjust the JavaScript code to change how the summary looks (e.g., add emojis, reorder fields).
Save to a Database: Add a node (e.g., Google Sheets or PostgreSQL) after the Format Summary Message node to store summaries.
Support Other Languages: In the AI Categorizer node, update the prompt to handle additional languages (e.g., Spanish, Mandarin) by specifying them in the instructions.
Add Error Handling: Enhance the Check Invalid Input node to catch more edge cases, like invalid dates.
All Free, End-to-End
This workflow is 100% free! It leverages:
Telegram Bot API: Free via BotFather.
Tesseract: Open-source text recognition.
LLaMA via OpenRouter: Free tier available for AI processing.
Enjoy automating your expense tracking without any cost!
Feedback or Questions?
Visit khmuhtadin.com to share ideas or get help.
Want to support this project? Buy me a coffee at buymeacoffee.com/khmuhtadin! ‚òï"
Evaluate RAG Response Accuracy with OpenAI: Document Groundedness Metric,https://n8n.io/workflows/4426-evaluate-rag-response-accuracy-with-openai-document-groundedness-metric/,"This n8n template demonstrates how to calculate the evaluation metric ""RAG document groundedness"" which in this scenario, measures the ability to provide or reference information included only in retrieved vector store documents.
The scoring approach is adapted from https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_groundedness
How it works
This evaluation works best for an agent that requires document retrieval from a vector store or similar source.
For our scoring, we need to collect the agent's response and the documents retrieved and use an LLM to assess if the former is based off the latter.
A key factor is to look out information in the response which is not mentioned in the documents.
A high score indicates LLM adherence and alignment whereas a low score could signal inadequate prompt or model hallucination.
Requirements
n8n version 1.94+
Check out this Google Sheet for a sample data https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing"
Evaluate AI Agent Response Correctness with OpenAI and RAGAS Methodology,https://n8n.io/workflows/4424-evaluate-ai-agent-response-correctness-with-openai-and-ragas-methodology/,"This n8n template demonstrates how to calculate the evaluation metric ""Correctness"" which in this scenario, measures the compares and classifies the agent's response against a set of ground truths.
The scoring approach is adapted from the open-source evaluations project RAGAS and you can see the source here https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_correctness.py
How it works
This evaluation works best where the agent's response is allowed to be more verbose and conversational.
For our scoring, we classify the agent's response into 3 buckets: True Positive (in answer and ground truth), False Positive (in answer but not ground truth) and False Negative (not in answer but in ground truth).
We also calculate an average similarity score on the agent's response against all ground truths.
The classification and the similarity score is then averaged to give the final score.
A high score indicates the agent is accurate whereas a low score could indicate the agent has incorrect training data or is not providing a comprehensive enough answer.
Requirements
n8n version 1.94+
Check out this Google Sheet for a sample data https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing"
Website Downtime Alert via LINE + Supabase Log,https://n8n.io/workflows/4379-website-downtime-alert-via-line-supabase-log/,"This workflow automatically checks the status of your websites using UptimeRobot API. If any site is down or unstable, it will:
Generate a natural-language alert message using GPT-4o
Push the message to a LINE group (with funny IT-style encouragement)
Log all DOWN status entries into your Supabase database
Wait 30 minutes before repeating
üîß How It Works
Schedule Trigger ‚Äì Runs on a fixed interval (every few minutes).
UptimeRobot Node ‚Äì Fetches website monitor data.
Code Node (Filter) ‚Äì Filters only websites with status 8 (may be down) or 9 (down).
IF Node ‚Äì If any site is down, proceed.
LangChain LLM Node ‚Äì Formats alert with a humorous message using GPT-4o.
Line Notify (HTTP Request) ‚Äì Sends the alert to your LINE group.
Loop Over Items ‚Äì Loops through all monitors.
Filter Down (Status = 9) ‚Äì Selects only ‚Äúfully down‚Äù sites.
Supabase Node ‚Äì Logs these into synlora_uptime_down table.
Wait Node ‚Äì Delays next alert by 30 minutes to avoid spamming.
‚öôÔ∏è Setup Steps
Required:
üîó UptimeRobot API Key
üì≤ LINE Channel Access Token and Group ID
üß† OpenAI Key (GPT-4o Mini)
üóÉÔ∏è Supabase Project & Table
Step-by-step:
Go to UptimeRobot ‚Üí Get API key and ensure monitors are set up.
Create a Supabase table with fields: website, status, uptime_id.
Create a LINE Messaging API bot, join it to your group, and get:
Access Token
Group ID (userId or groupId)
Add your OpenAI API Key for GPT-4o Mini (or switch to your preferred LLM).
Import the workflow JSON into n8n.
Set credentials in all necessary nodes.
Activate the workflow."
Auto-Generate & Distribute LinkedIn Posts with GPT-4 to Profile & Groups,https://n8n.io/workflows/4374-auto-generate-and-distribute-linkedin-posts-with-gpt-4-to-profile-and-groups/,"LinkedIn AI Agent: Auto-Post Creator & Multi-Group Distributor
Transform simple topic ideas into engaging LinkedIn posts and automatically distribute them across your profile and multiple LinkedIn groups. This powerful n8n workflow combines AI content generation with intelligent distribution, helping you maintain a consistent professional presence while maximizing your reach across relevant communities.
üöÄ How It Works
This sophisticated 6-step automation turns content ideas into LinkedIn success:
Step 1: Smart Content Monitoring
The workflow continuously monitors your Google Sheets for new post topics marked as ""Pending"", checking every minute for fresh content to process.
Step 2: AI-Powered Content Generation
GPT-4 transforms your basic topic into a professionally crafted LinkedIn post featuring:
Compelling opening hooks that grab attention
3-4 informative paragraphs with valuable insights
Strategic questions to encourage engagement
4-6 relevant hashtags for discoverability
Professional emoji placement for visual appeal
Optimized formatting for LinkedIn's platform
Step 3: Professional Formatting
The workflow ensures your content meets LinkedIn's technical requirements with proper JSON formatting, character limits, and special character handling.
Step 4: Personal Profile Publishing
Your generated post is automatically published to your personal LinkedIn profile, maintaining your professional brand presence.
Step 5: Multi-Group Distribution
The same content is intelligently distributed across all your specified LinkedIn groups, amplifying your reach to targeted professional communities.
Step 6: Status Management
The workflow automatically updates your Google Sheets to mark posts as ""Posted"", providing clear tracking of your content pipeline.
‚öôÔ∏è Setup Steps
Prerequisites
Active LinkedIn account with API access
Google Sheets access for content management
OpenAI API key with GPT-4 access
LinkedIn group memberships with posting permissions
n8n instance (cloud or self-hosted)
Required Google Sheets Structure
Sheet 1 - Main Content:
ID LinkedIn Post Title Status
1 AI Trends in 2024 Pending
2 Remote Work Tips Posted
Sheet 2 - Groups:
GroupIds
-------------
123456789
987654321
456789123
Note: Collect LinkedIn group IDs from groups where you have posting permissions. These can be found in the group URL or through LinkedIn's API.
Configuration Steps
Credential Setup
Google Sheets OAuth2: Access your content spreadsheet
OpenAI API Key: Required for AI content generation
LinkedIn OAuth2: Enable profile and group posting
HTTP Authentication: Configure LinkedIn API headers
Google Sheets Preparation
Create spreadsheet with the required two-sheet structure
Populate group IDs from your joined LinkedIn groups
Add initial post topics with ""Pending"" status
Ensure proper column naming and data types
LinkedIn Group Setup
Join relevant professional LinkedIn groups
Verify posting permissions in each group
Collect group IDs using LinkedIn's interface or API
Test posting permissions before full automation
AI Content Customization
The default prompt generates professional LinkedIn content, but can be customized for:
Industry-specific terminology and trends
Company voice and brand guidelines
Target audience preferences
Content style (educational, promotional, thought leadership)
Workflow Activation
Import the workflow JSON into your n8n instance
Configure all credential connections
Test with sample content before going live
Activate the Google Sheets trigger
üéØ Use Cases
Content Creators & Influencers
Consistent Posting: Maintain regular LinkedIn presence without daily manual work
Audience Growth: Reach multiple professional communities simultaneously
Content Scaling: Transform brief ideas into full-length engaging posts
Brand Building: Establish thought leadership across industry groups
Marketing Teams
Lead Generation: Share valuable content across targeted professional groups
Brand Awareness: Increase visibility in relevant industry communities
Thought Leadership: Position company experts as industry authorities
Content Distribution: Maximize reach of marketing messages and insights
Sales Professionals
Pipeline Building: Share insights that attract potential clients
Network Expansion: Engage with prospects across multiple professional groups
Authority Building: Establish credibility through valuable content sharing
Relationship Nurturing: Maintain visibility with existing connections
Consultants & Freelancers
Client Acquisition: Demonstrate expertise to potential clients
Professional Branding: Build reputation across industry-specific groups
Service Promotion: Share case studies and success stories broadly
Network Building: Connect with peers and potential collaborators
Business Leaders & Executives
Industry Influence: Share strategic insights across professional networks
Talent Attraction: Showcase company culture and opportunities
Partnership Development: Connect with potential business partners
Market Education: Share expertise to influence industry conversations
üîß Advanced Customization Options
Content Strategy Enhancement
Multi-Tone Generation: Create different content styles for various audiences
Industry Templates: Pre-built prompts for specific professional sectors
Engagement Optimization: A/B testing different post formats and styles
Content Calendar Integration: Schedule posts for optimal timing
Distribution Intelligence
Group Performance Tracking: Monitor engagement across different groups
Selective Distribution: Choose specific groups based on content type
Audience Segmentation: Tailor content for different professional communities
Engagement Analysis: Track which groups provide best ROI
Content Quality Control
Approval Workflows: Add human review before automatic posting
Content Scoring: Rate post quality before distribution
Brand Compliance: Ensure posts meet company guidelines
Duplicate Detection: Avoid posting similar content too frequently
Extended Integration Options
CRM Integration: Track leads generated from LinkedIn posts
Analytics Platforms: Monitor engagement and conversion metrics
Content Management: Connect to existing content planning tools
Social Media Expansion: Extend to other professional platforms
üìä Content Generation Features
AI Writing Capabilities
The workflow generates LinkedIn posts that include:
Professional Structure:
Attention-grabbing opening statements
Well-organized multi-paragraph content
Clear value propositions and insights
Strategic calls-to-action for engagement
LinkedIn Optimization:
Platform-specific formatting and spacing
Proper hashtag research and placement
Emoji usage that enhances readability
Character count optimization for maximum impact
Engagement Drivers:
Thought-provoking questions to encourage comments
Industry insights that spark discussions
Personal anecdotes that build connections
Actionable tips that provide immediate value
Sample Output
Input Topic: ""Remote Work Productivity Tips""
Generated Post:
üè† Working from home has taught me that productivity isn't about being busy‚Äîit's about being intentional.
After managing remote teams for 3 years, I've discovered that the most successful remote workers share three key habits that transform their daily performance.
First, they create physical boundaries even in small spaces. A dedicated workspace signals to your brain that it's time to focus, even if it's just a corner of your kitchen table.
Second, they batch similar tasks together. Instead of jumping between emails, calls, and deep work, they group activities to maintain mental flow and reduce cognitive switching costs.
Third, they communicate proactively rather than reactively. They share progress updates before being asked and clarify expectations upfront to avoid confusion later.
What's your most effective remote work strategy? I'd love to hear what's working for your team! üí™
#RemoteWork #Productivity #WorkFromHome #Leadership #TeamManagement #ProfessionalDevelopment
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
LinkedIn API Limitations
Respect posting frequency limits to avoid account restrictions
Monitor API usage and implement appropriate delays between posts
Ensure compliance with LinkedIn's terms of service
Maintain authentic engagement rather than purely automated interactions
Group Posting Permissions
Verify membership status and posting rights before adding group IDs
Some groups require administrator approval for posts
Monitor group rules and community guidelines
Remove inactive or restricted groups from your list
Content Quality Control
Review AI-generated content periodically for brand consistency
Adjust prompts based on engagement performance
Maintain a balance between automation and personal touch
Monitor comments and engage authentically with your audience
Optimization Strategies
Performance Enhancement
Track engagement metrics across different groups
A/B test posting times and content formats
Refine prompts based on successful post patterns
Gradually expand to new groups based on performance
Content Strategy
Develop content themes that resonate with your target audience
Create series of related posts for deeper engagement
Balance promotional content with value-driven insights
Maintain consistency in voice and messaging
Network Growth
Engage with comments on your automated posts
Connect with active commenters to expand your network
Participate in group discussions beyond your own posts
Build genuine relationships through authentic interactions
üìà Success Metrics
Engagement Indicators
Post Reach: Total views across profile and groups
Interaction Rate: Comments, likes, and shares per post
Network Growth: New connections from content engagement
Group Performance: Which communities provide best engagement
Business Impact Measurements
Lead Generation: Connections and inquiries from LinkedIn posts
Brand Awareness: Mentions and sharing of your content
Thought Leadership: Recognition as industry expert
Professional Opportunities: Speaking, collaboration, or job opportunities
üìû Questions & Support
Need help setting up or optimizing your LinkedIn AI Agent workflow?
üìß Direct Technical Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Expertise: LinkedIn API integration, AI prompt optimization, workflow scaling
üé• Comprehensive Learning Resources
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup walkthrough and configuration
Advanced customization techniques and strategies
LinkedIn API best practices and limitations
Content strategy optimization for maximum engagement
Troubleshooting common integration issues
ü§ù Professional Networking & Updates
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing automation support and advice
Share your LinkedIn growth success stories
Get early access to new workflow templates and features
Join discussions about LinkedIn marketing automation
üí¨ Support Request Guidelines
Include in your support message:
Your current LinkedIn strategy and goals
Target audience and industry focus
Specific LinkedIn groups you want to target
Any technical errors or integration issues
Current content creation process and pain points"
Generate Product Ad Copy & CTAs with GPT-4 for Slack and Airtable,https://n8n.io/workflows/4372-generate-product-ad-copy-and-ctas-with-gpt-4-for-slack-and-airtable/,"‚ö° AI Copywriter Pro: Instant Ad Copy & CTA Generator
Transform product details into compelling marketing copy in seconds. This intelligent n8n workflow takes basic product information and generates professional ad copy with powerful calls-to-action using GPT-4, then distributes the results to your team via Slack or stores them in Airtable for future use.
üöÄ How It Works
This streamlined 4-step automation delivers professional marketing copy instantly:
Step 1: Simple Form Input
Users submit basic product information through a clean web form - just product name and key features required.
Step 2: AI-Powered Generation
The workflow sends product details to GPT-4 with a specialized copywriting prompt that creates:
Compelling 2-sentence ad copy optimized for conversions
3 powerful call-to-action phrases with different urgency levels
Professional tone tailored for marketing campaigns
Step 3: Structured Processing
The AI response is automatically parsed into clean, labeled fields ready for immediate use across marketing channels.
Step 4: Multi-Channel Distribution
Generated content is simultaneously:
Posted to your designated Slack channel for team review
Logged in Airtable for campaign tracking and reuse
‚öôÔ∏è Setup Steps
Prerequisites
Active OpenAI API account with GPT-4 access
Slack workspace with admin permissions
Airtable account for content storage
n8n instance (cloud or self-hosted)
Configuration Steps
1. Credential Setup
OpenAI API Key: Required for GPT-4 content generation
Slack OAuth2: Needed for posting messages to channels
Airtable Personal Access Token: Essential for database operations
2. Form Configuration
The workflow automatically generates a web form at: your-n8n-instance.com/form/[webhook-id]
Form fields are pre-configured for:
Product Name (text input)
Product Features (textarea for detailed descriptions)
Form can be embedded on websites or shared as standalone link
3. Slack Integration Setup
Connect your Slack workspace to n8n
Create or select target channel for ad copy notifications
Update the channelId parameter with your channel's ID
Test connection with sample message
4. Airtable Database Preparation
Create new base or use existing one
Set up table with columns:
Product Name (Single line text)
Product Features (Long text)
Ad Copy (Long text)
CTAs (Long text)
Copy base and table IDs from Airtable URLs
Update workflow parameters accordingly
5. AI Prompt Customization
The default prompt generates versatile marketing copy, but you can customize for:
Industry-specific terminology
Brand voice and tone
Target audience demographics
Campaign objectives (awareness, conversion, retention)
üéØ Use Cases
E-commerce & Retail
Product Launch Campaigns: Generate copy for new product announcements
Seasonal Promotions: Create urgency-driven messaging for sales events
Category Pages: Develop compelling descriptions for product collections
Social Media Ads: Produce scroll-stopping copy for Facebook and Instagram
SaaS & Technology
Feature Announcements: Craft clear, benefit-focused messaging
Free Trial Campaigns: Generate conversion-optimized trial signup copy
Case Study Promotions: Create compelling success story headlines
Webinar Marketing: Develop registration-driving event descriptions
Agency & Freelance
Client Presentations: Rapidly prototype copy concepts for pitches
A/B Testing: Generate multiple copy variations for campaign testing
Campaign Brainstorming: Kickstart creative sessions with AI-generated ideas
Content Calendar Filling: Populate social media schedules efficiently
Startup & Small Business
MVP Marketing: Create professional copy without hiring agencies
Investor Pitches: Generate compelling product descriptions
Website Content: Populate landing pages with conversion-focused copy
Email Campaigns: Develop subject lines and promotional content
Content Marketing
Blog Post Promotions: Generate social media copy for article shares
Newsletter CTAs: Create compelling subscription and engagement prompts
Video Descriptions: Develop YouTube and social video copy
Podcast Promotions: Generate episode descriptions and listener CTAs
üîß Advanced Customization Options
Multi-Tone Generation
Modify the prompt to generate different copy styles:
- Professional & Corporate: ""Generate formal, trust-building copy...""
- Casual & Friendly: ""Create conversational, approachable messaging...""
- Urgent & Scarcity: ""Develop time-sensitive, action-driving copy...""
- Luxury & Premium: ""Craft sophisticated, high-end positioning...""
Batch Processing Enhancement
Add nodes for processing multiple products:
Split in Batches: Handle bulk product lists
Merge: Combine results for team review
Item Lists: Process CSV uploads of product catalogs
Quality Control Integration
Implement approval workflows:
Human Review Step: Add manual approval before publishing
Sentiment Analysis: Score copy for emotional impact
Brand Guidelines Check: Validate against company voice standards
A/B Test Setup: Generate multiple variations automatically
Extended Distribution
Connect additional platforms:
Email Marketing: Send to Mailchimp, ConvertKit, or HubSpot
Social Media: Post directly to Facebook, Twitter, LinkedIn
CMS Integration: Push to WordPress, Webflow, or custom sites
Project Management: Create tasks in Asana, Trello, or Monday.com
üìä Output Examples
Sample Input:
Product Name: EcoSmart Water Bottle
Product Features: Self-cleaning UV technology, 24-hour temperature retention, BPA-free stainless steel, leak-proof design, 500ml capacity
Generated Ad Copy:
Stay hydrated and eco-conscious with the EcoSmart Water Bottle‚Äîfeaturing revolutionary self-cleaning UV technology that eliminates 99.9% of bacteria while keeping your drinks perfectly chilled for 24 hours! Experience the future of hydration with premium stainless steel construction that's as durable as it is sustainable.

CTAs:
‚Ä¢ Order Your EcoSmart Bottle Today
‚Ä¢ Join the Clean Hydration Revolution  
‚Ä¢ Get 24-Hour Freshness Now
Slack Notification Format:
üéØ NEW AD COPY GENERATED

Product: EcoSmart Water Bottle

üìù Ad Copy:
Stay hydrated and eco-conscious with the EcoSmart Water Bottle‚Äîfeaturing revolutionary self-cleaning UV technology that eliminates 99.9% of bacteria while keeping your drinks perfectly chilled for 24 hours! Experience the future of hydration with premium stainless steel construction that's as durable as it is sustainable.

üî• Call-to-Actions:
‚Ä¢ Order Your EcoSmart Bottle Today
‚Ä¢ Join the Clean Hydration Revolution  
‚Ä¢ Get 24-Hour Freshness Now

Ready to use in campaigns! üöÄ
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
API Rate Limits
Monitor OpenAI usage dashboard
Implement delay nodes for high-volume processing
Consider upgrading to higher tier plans
Inconsistent Output Quality
Refine prompts with specific examples
Add output validation rules
Include brand guidelines in system prompt
Integration Failures
Verify all API credentials are current
Test connections individually before full workflow
Check service status pages for outages
Optimization Strategies
Cost Management
Use GPT-4o-mini for routine copy generation
Reserve GPT-4 for premium campaigns
Cache common responses to reduce API calls
Quality Enhancement
A/B test different prompt variations
Collect team feedback on generated copy
Maintain library of high-performing examples
Workflow Efficiency
Set up monitoring alerts for failures
Create backup workflows for critical campaigns
Document customizations for team reference
üìà Performance Tracking
Key Metrics to Monitor
Generation Speed: Average time from input to output
Approval Rate: Percentage of AI copy used without edits
Campaign Performance: Click-through rates of AI-generated content
Cost Per Copy: API costs per generated piece
Success Indicators
Reduced copywriting turnaround time (target: under 30 seconds)
Increased campaign launch frequency
Higher team satisfaction with copy quality
Measurable improvement in conversion rates
üìû Questions & Support
Need help setting up or customizing your AI Copywriter Pro workflow?
üìß Direct Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Specialties: Custom prompt engineering, integration troubleshooting, workflow optimization
üé• Video Tutorials
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup walkthrough
Advanced customization techniques
Integration with popular marketing tools
Prompt engineering masterclass
ü§ù Professional Network
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing automation support
Share your copywriting success stories
Get early access to new workflow templates
üí¨ What to Include in Support Requests
Your specific industry or use case
Current marketing tools and platforms
Sample products you want to test with
Any custom requirements or brand guidelines
Screenshots of any errors or unexpected outputs
Ready to revolutionize your copywriting process? Deploy this workflow and watch your marketing team's productivity soar while maintaining professional, conversion-focused messaging!"
Classify Lead Sentiment with Google Gemini and Send WhatsApp Responses via Typeform & Supabase,https://n8n.io/workflows/4322-classify-lead-sentiment-with-google-gemini-and-send-whatsapp-responses-via-typeform-and-supabase/,"Automatically classify incoming leads based on the sentiment of their message using Google Gemini, store them in Supabase by category, and send tailored WhatsApp messages via the official WhatsApp Cloud API.
‚úÖ Use Case:
This workflow is ideal for sales, onboarding, and customer support teams who want to:
Understand the tone and urgency of each lead
Prioritize hot leads instantly
Send smart, automatic WhatsApp replies based on user sentiment
üß† How it works:
Capture lead via a Typeform webhook
Clean and structure the data (name, email, message, etc.)
Run sentiment analysis using Google Gemini to classify the message as:
Positive ‚Üí Hot Lead
Neutral ‚Üí Warm Lead
Negative ‚Üí Cold Lead
Store lead data in Supabase under the corresponding category
Merge data to unify flow paths
Send WhatsApp message using the official WhatsApp Cloud API, with a custom reply for each sentiment result
üîß Tools used:
Typeform (incoming data)
Google Gemini (AI-based sentiment classification)
Supabase (database)
WhatsApp Cloud API (response automation)
üè∑ Tags:
AI, Sentiment Analysis, Lead Qualification, Supabase, WhatsApp, Gemini, Typeform, CRM, Automation, Customer Engagement"
Monitor SEO Keyword Rankings with LLaMA AI & Apify Google SERP Scraping,https://n8n.io/workflows/4301-monitor-seo-keyword-rankings-with-llama-ai-and-apify-google-serp-scraping/,"Who is this template for?
This SEO Keyword Monitoring workflow template is perfect for SEO professionals, digital marketing agencies, website owners, and content strategists who need to track their search rankings and get actionable insights when they're not performing well. Whether you're managing multiple client sites, monitoring your own brand's visibility, or conducting competitive analysis, this automation provides comprehensive rank tracking with AI-powered recommendations.
What problem does this workflow solve?
Manual keyword rank tracking is time-consuming and often provides limited actionable insights. SEO professionals typically struggle with:
Manually checking search rankings across different countries and languages
Identifying why a website isn't ranking for target keywords
Getting specific, actionable recommendations for SEO improvements
Tracking competitor performance and market positioning
Scaling rank monitoring across multiple keywords and domains
Generating professional reports for clients or stakeholders
What this workflow does
This n8n workflow automates comprehensive SEO keyword monitoring with intelligent analysis and reporting. It tracks your rankings in Google search results and provides AI-powered insights when your site isn't performing as expected.
Here's what it includes:
Multi-language web form that accepts keyword, domain, country (24 options), and language (12 options)
Intelligent localization that converts country/language selections into proper API codes
Real-time Google SERP scraping using Apify's Google Search API (up to 100 results per query)
Automated rank detection that checks if your domain appears in the search results
Dual email reporting system:
Success reports: Beautiful HTML tables showing your rankings, competitor positions, titles, URLs, and descriptions
AI-powered improvement reports: When your site doesn't rank, an AI agent (LLaMA 70B) analyzes the search results and provides specific, actionable SEO recommendations
Professional email formatting with HTML markup for easy sharing with clients or teams
Setup
Getting started is straightforward:
Connect your Apify account
Sign up for a free Apify account ü°•
Get your Personal API Token from Settings ‚Üí API & Integrations
Replace YOUR_API_TOKEN in the HTTP Request node with your actual token
Configure the AI model
The workflow uses Groq AI with LLaMA 70B by default
Connect your Groq account or replace with OpenAI, Claude, or another LLM
The AI agent analyzes search results and provides tailored SEO recommendations
Set up email delivery / data export
Configure the Mailjet nodes with your email credentials
Or replace with Gmail, Outlook, SMTP, or other email providers
Or replace with/add Google Sheets, Airtable, Notion or similar service, for data storage
Set your sender and recipient email addresses
Test the workflow
Click ""Test workflow"" to access the web form
Enter a keyword, domain, country, and language
Check your email for either ranking results or AI-powered recommendations
Activate the workflow
Turn on the trigger so you can access the form anytime
Share the form URL with team members or clients
How to customize this workflow
This template is highly flexible and can be adapted for various SEO monitoring needs:
Scale up monitoring: Add loops to track multiple keywords simultaneously
Alternative outputs: Replace email nodes with Google Sheets, Airtable, or Notion for data storage
Team notifications: Connect to Slack, Discord, or Microsoft Teams for instant alerts
Scheduled monitoring: Add cron triggers for daily, weekly, or monthly automated checks
Enhanced analysis: Integrate additional AI models for deeper competitive analysis
Custom reporting: Modify the HTML templates to match your brand or client requirements
Data persistence: Add database connections to store historical ranking data
Competitor tracking: Expand the logic to monitor multiple domains for the same keywords
Key features
24 country support: Track rankings in major markets worldwide
12 language options: Monitor multilingual SEO performance
AI-powered insights: Get specific recommendations when rankings are low
Professional reporting: HTML-formatted emails ready for client delivery
Competitor analysis: See who's ranking above you with full SERP data
Scalable architecture: Easy to extend for enterprise-level monitoring
This workflow transforms manual rank checking into an intelligent, automated system that not only tracks your performance but actively helps you improve it with AI-driven recommendations. Developed by Gegenfeld ü°• and codecope ü°• in Berlin, Germany."
Improve AI Agent System Prompts with GPT-4o Feedback Analysis and Email Delivery,https://n8n.io/workflows/4197-improve-ai-agent-system-prompts-with-gpt-4o-feedback-analysis-and-email-delivery/,"AI Agent System Prompt 'Auto-Tuner'
This workflow configures an AI agent which provides an edited system prompt for an autonomous AI agent Based on the following pieces of information provided by the user in an input form:
Agent name
Agent purpose
What's working
What's not working
Current system prompt
There are two additional form elements that I've marked as non-required but if you want to force more detail from the user you can mark these as required:
Example prompt
Example output
This information gets sent to the AI agent which is configured with a system prompt of its own and the form elements are concatenated into a user prompt prompting the agent to evaluate the system prompt, deliver an improved version, and provide some notes for logging. The output structure is constrained with JSON. OpenAI 4o is recommended for its overall strong adherence to structured outputs.
Once the agent delivers its improved system prompt, this gets passed to the user via email notification. The final delivery stage can be alternated according to user preference
When This Is Useful
Anyone working on AI agent configurations will likely be familiar with the pivotal importance of the system prompt in directing the desired behavior of the agent.
Frequently this requires long hours of iteration before a consistent desired behaviour is achieved.
Sometimes we can figure out what's working and not based on our own intuition and experience, but at other times soliciting the outside perspective of another AI tool can be a helpful way to consider alternative explanations or improve our own prompt engineering.
This configuration is intended to speed up this iterative process and reduce the amount of time we spend working on system prompts to configure effective agent workflows"
"AI Content Creation and Publishing Engine with Mistral, Creatomate, and YouTube",https://n8n.io/workflows/4087-ai-content-creation-and-publishing-engine-with-mistral-creatomate-and-youtube/,"Description
This n8n workflow automates the entire process of creating and publishing AI-generated videos, triggered by a simple message from a Telegram bot (YTAdmin). It transforms a text prompt into a structured video with scenes, visuals, and voiceover, stores assets in MongoDB, renders the final output using Creatomate, and uploads the video to YouTube. Throughout the process, YTAdmin receives real-time updates on the workflow‚Äôs progress. This is ideal for content creators, marketers, or businesses looking to scale video production using automation and AI.
You can see a video demonstrating this template in action here: https://www.youtube.com/watch?v=EjI-ChpJ4xA&t=200s
How it Works
Trigger: Message from YTAdmin (Telegram Bot)
The flow starts when YTAdmin sends a content prompt.
Generate Structured Content
A Mistral language model processes the input and outputs structured content, typically broken into scenes.
Split & Process Content into Scenes
The content is split into categorized parts for scene generation.
Generate Media Assets
For each scene:
Images: Generated using OpenAI‚Äôs image model.
Voiceovers: Created using OpenAI‚Äôs text-to-speech.
Audio files are encoded and stored in MongoDB.
Scene Composition
Assets are grouped into coherent scenes.
Render with Creatomate
A complete payload is generated and sent to the Creatomate rendering API to produce the video.
Progress messages are sent to YTAdmin.
The flow pauses briefly to avoid rate limits.
Render Callback
Once Creatomate completes rendering, it sends a callback to the flow.
If the render fails, an error message is sent to YTAdmin.
If the render succeeds, the flow proceeds to post-processing.
Generate Title & Description
A second Mistral prompt generates a compelling title and description for YouTube.
Upload to YouTube
The rendered video is retrieved from Creatomate.
It‚Äôs uploaded to YouTube with the AI-generated metadata.
Final Update
A success message is sent to YTAdmin, confirming upload completion.
Set Up Steps (Approx. 10‚Äì15 Minutes)Step 1: Set Up YTAdmin Bot
Create a Telegram bot via BotFather and get your API token.
Add this token in n8n's Telegram credentials and link to the ""Receive Message from YTAdmin"" trigger.
Step 2: Connect Your AI Providers
Mistral: Add your API key under HTTP Request or AI Model nodes.
OpenAI: Create an account at platform.openai.com and obtain an API key. Use it for both image generation and voiceover synthesis.
Step 3: Configure Audio File Storage with MongoDB via Custom API
Receives the Base64 encoded audio data sent in the request body.
Connects to the configured MongoDB instance (connection details are managed securely within the API- code below).
Uses the MongoDB driver and GridFS to store the audio data.
Returns the unique _id (ObjectId) of the stored file in GridFS as a response.
This _id is crucial as it will be used in subsequent steps to generate the download URL for the audio file.
My API code can be found here for reference: https://github.com/nanabrownsnr/YTAutomation.git
Step 4: Set Up Creatomate
Create a Creatomate account, define your video templates, and retrieve your API key.
Configure the HTTP request node to match your Creatomate payload requirements.
Step 5: Connect YouTube
In n8n, add OAuth2 credentials for your YouTube account.
Make sure your Google Cloud project has YouTube Data API enabled.
Step 6: Deploy and Test
Send a message to YTAdmin and monitor the flow in n8n.
Verify that content is generated, media is created, and the final video is rendered and uploaded.
Customization Options
Change the AI Prompts
Modify the generation prompts to adjust tone, voice, or content type (e.g., news recaps, product videos, educational summaries).
Switch Messaging Platform
Replace Telegram (YTAdmin) with Slack, Discord, or WhatsApp by swapping out the trigger and response nodes.
Add Subtitles or Effects
Integrate Whisper or another speech-to-text tool to generate subtitles.
Add overlay or transition effects in the Creatomate video payload.
Use Local File Storage Instead of MongoDB
Swap out MongoDB upload http nodes with filesystem or S3-compatible storage.
Repurpose for Other Platforms
Swap YouTube upload with TikTok, Instagram, or Vimeo endpoints for broader publishing.
Need Help or Want to Customize This Workflow?
If you'd like assistance setting this up or adapting it for a different use case, feel free to reach out to me at nanabrownsnr@gmail.com. I'm happy to help!"
Tesla Financial Market Data Analyst Tool (Multi-Timeframe Technical AI Agent),https://n8n.io/workflows/4094-tesla-financial-market-data-analyst-tool-multi-timeframe-technical-ai-agent/,"üìä This AI sub-agent aggregates Tesla (TSLA) trading signals across multiple timeframes using real-time technical indicators and candlestick behavior.
It is a core component of the Tesla Quant Trading AI system. Powered by GPT-4.1, it consolidates 15-minute, 1-hour, and 1-day indicators, adds candlestick pattern data, and produces a unified JSON signal for downstream use by the master agent.
‚ö†Ô∏è This agent is not standalone. It is triggered by the Tesla Quant Trading AI Agent via Execute Workflow.
üß† Requires: 4 connected sub-agents and Alpha Vantage Premium API Key
üîå Required Sub-Workflows
To use this workflow, you must install:
Tesla 15min Indicators Tool
Tesla 1hour Indicators Tool
Tesla 1day Indicators Tool
Tesla 1hour and 1day Klines Tool
Tesla Quant Technical Indicators Webhooks Tool (provides Alpha Vantage data)
üß† What This Agent Does
Fetches pre-cleaned 20-point JSON outputs from the 4 sub-agents listed above
Analyzes each timeframe individually:
15m: momentum and short-term setups
1h: confirmation of emerging trends
1d: macro positioning and trend alignment
Klines: candlestick reversal patterns and volume divergence
Generates a structured final signal in JSON with:
Trading stance: Buy, Sell, Hold, or Cautious
Confidence score (0.0‚Äì1.0)
Multi-timeframe indicator breakdown
Candlestick and volume divergence annotations
üìã Sample Output
{
  ""summary"": ""TSLA momentum is weakening short-term. 1h MACD shows bearish crossover, RSI declining. 1d candles confirm potential reversal setup."",
  ""signal"": ""Cautious Sell"",
  ""confidence"": 0.81,
  ""multiTimeframeInsights"": {
    ""15m"": { ""RSI"": 68.3, ""MACD"": { ""macd"": 0.53, ""signal"": 0.61 }, ... },
    ""1h"": { ""RSI"": 65.0, ""MACD"": { ""macd"": -0.32, ""signal"": 0.11 }, ... },
    ""1d"": { ""BBANDS"": { ... }, ... },
    ""candlestickPatterns"": { ""1h"": ""Doji"", ""1d"": ""Bearish Engulfing"" },
    ""volumeDivergence"": { ""1h"": ""Bearish"", ""1d"": ""Neutral"" }
  }
}
üõ†Ô∏è Setup Instructions
Import this workflow into n8n
Name it: Tesla_Financial_Market_Data_Analyst_Tool
Add Required API Credentials
Alpha Vantage Premium (via HTTP Query Auth)
OpenAI GPT-4.1 for reasoning and synthesis
Link Required Sub-Agents
Connect the 4 tool workflows listed above to their respective Tool Workflow nodes
Connect the webhook provider for data fetches
Set Up as Sub-Agent
This workflow must be triggered using Execute Workflow from the parent agent
Pass in:
message (optional context)
sessionId (used for memory continuity)
üßæ Sticky Notes Provided
üìò Tesla Financial Market Data Analyst ‚Äî Core logic overview
üìà 15m / 1h / 1d Tool Notes ‚Äî Indicator lists + use cases
üïØÔ∏è Klines Tool Note ‚Äî Candlestick and volume divergence patterns
üß† GPT Reasoning Note ‚Äî GPT-4.1 handles final synthesis
üß© Sub-Workflow Trigger ‚Äî Proper integration with parent agent
üß† Memory Buffer ‚Äî Maintains session context across evaluations
üîí Licensing & Support
¬© 2025 Treasurium Capital Limited Company
The logic, prompt design, and multi-agent architecture are proprietary and IP-protected.
For support or collaboration inquiries:
üîó Don Jayamaha ‚Äì LinkedIn
üîó n8n Creator Profile
üöÄ Unify your Tesla trading logic across timeframes‚Äîautomated, AI-powered, and built for scalers and swing traders."
AI-Powered Auto-Generate Exam Questions and Answers from Google Docs with Gemini,https://n8n.io/workflows/4008-ai-powered-auto-generate-exam-questions-and-answers-from-google-docs-with-gemini/,"This workflow automates the creation of exam questions (both open-ended and multiple-choice) from educational content stored in Google Docs, using AI-powered analysis and vector database retrieval
This workflow saves educators hours of manual work while ensuring high-quality, curriculum-aligned assessments. Let me know if you'd like help adapting it for specific subjects!
Use Cases
Educators: Rapidly generate quizzes, midterms, or flashcards.
E-learning platforms: Automate question banks for courses.
Corporate training: Create assessments for employee onboarding.
Technical Requirements:
APIs: Google Gemini, OpenAI, Qdrant, Google Workspace.
n8n Nodes: LangChain, Google Sheets/Docs, HTTP requests, code blocks.
This workflow combines AI efficiency with human-curated quality, making it a powerful tool for modern education and training.
Advantages of This Workflow
‚úÖ Fully Automated Exam Generation: From document to fully formatted quiz content with no manual intervention.
‚úÖ Supports Comprehension and Critical Thinking: Questions are designed to go beyond factual recall, including inference and application.
‚úÖ Uses AI and RAG for Accuracy: Ensures that answers are grounded in the document content, reducing hallucination.
‚úÖ Seamless Google Integration: Pulls content from Google Docs and writes outputs to Google Sheets.
‚úÖ Scalable for Any Subject: Works with any article or content domain as input.
‚úÖ Modular and Customizable: Can be easily adapted to generate different question types or to use other LLMs or storage systems.
How It Works
Document Ingestion:
The workflow starts by fetching an educational document (e.g., textbook chapter, lecture notes) from Google Docs.
Converts the document to Markdown for structured processing.
AI Processing:
Splits text into chunks and generates vector embeddings (via OpenAI) for semantic analysis.
Stores embeddings in Qdrant (vector database) for retrieval.
Question Generation:
Open-ended questions: Google Gemini AI creates 10 critical-thinking questions.
Multiple-choice questions: Generates 10 MCQs (1 correct + 3 plausible distractors) using RAG to validate answers against the vector DB.
Answer Validation:
For open questions: Retrieves context-aware answers from the vector store.
For MCQs: Ensures distractors are incorrect but believable via AI cross-checking.
Output:
Saves questions/answers to Google Sheets in two tabs:
Open questions: Question + AI-generated answer.
Closed questions: MCQ + options + correct answer.
Set Up Steps
Prerequisites:
APIs/Accounts:
Google Workspace (Docs + Sheets).
OpenAI (for embeddings).
Google Gemini (for question generation).
Qdrant (vector DB ‚Äì self-hosted or cloud).
n8n Nodes: Ensure LangChain, Google Sheets/Docs, and HTTP request nodes are installed.
Configure Connections:
Link credentials for:
Google Docs/Sheets (OAuth2).
OpenAI (API key).
Google Gemini (API key).
Qdrant (URL + API key).
Customize Input:
Replace the default Google Doc ID in the ""Get Doc"" node with your source document.
Adjust chunk size/overlap (Token Splitter node) for optimal text processing.
Tweak Question Generation:
Modify prompts in:
""Open questions"" node: Adjust criteria (e.g., difficulty, question types).
""Closed questions"" node: Edit MCQ formatting rules.
Output Settings:
Update the Google Sheet ID in ""Write open"" and ""Write closed"" nodes.
Map columns in Google Sheets to match question/answer formats.
Run & Automate:
Trigger manually (""Test workflow"") or schedule periodic runs (e.g., for updated content).
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Generate Images with GPT-image-1 and Store in Google Drive with Cost Tracking,https://n8n.io/workflows/3710-generate-images-with-gpt-image-1-and-store-in-google-drive-with-cost-tracking/,"How it works
Receive a chat input as an image prompt.
Call OpenAI's gpt-image-1 API to generate an image.
Split the returned images and process them one by one.
Upload each generated image to Google Drive.
Save image links and thumbnails to a Google Sheets document.
Record token usage and estimated cost into a separate sheet.
Set up steps
Connect your OpenAI API credentials for image generation.
Connect your Google Drive and Google Sheets accounts.
Set the destination folder in Google Drive.
Set the target Google Sheet and specify the correct sheet tabs.
The setup usually takes around 5-10 minutes.
Detailed field mappings are already pre-configured inside the workflow.
Additional tips and instructions are included as sticky notes inside the workflow.
Google Sheet copy url
Copy Sheet Link"
Generate and Edit Images with OpenAI's GPT-Image-1 Model,https://n8n.io/workflows/3696-generate-and-edit-images-with-openais-gpt-image-1-model/,"This template integrates OpenAI's image generation and editing endpoints via the GPT-Image-1 model to visually create and manipulate images based on prompts. It features base64 conversion, binary handling, and prompt chaining.
Perfect for marketing, design, product visuals and creative workflows.
üõ†Ô∏è Requirements
OpenAI account with access to gpt-image-1(probably you need organizations verifications for access to that model)
OpenAI API credentials configured in n8n
A self-hosted or cloud n8n instance
Basic familiarity with the n8n UI (no programming required)
üîß Step-by-step Instructions
Step 1: Manual Trigger
Starts the workflow on click. Ideal for testing the generation and edit logic.
Step 2: Generate Image
The Create image call node sends a prompt to OpenAI and returns a base64 image.
Example prompt:
A cyberpunk city at night with flying cars and neon lights
Step 3: Convert to Binary
The base64 image is converted into a usable binary PNG file with the Convert json binary to File node.
Step 4: Edit the Image
The binary file is passed to OpenAI‚Äôs /images/edits endpoint. A new prompt applies changes to the image.
Example:
Add a glowing robot in the foreground with a neon sword
‚úÖ Supports model: gpt-image-1
‚ö†Ô∏è Requires binary file (not base64)
Step 5: Final Conversion
Converts the final edited image from base64 to file so it can be downloaded or used in other nodes.
üéØ Real-World Use Cases
üé® Artists & Creators: concept art and illustration variations
üõçÔ∏è E-commerce: auto-generate product mockups
üì∞ Marketing: create eye-catching blog or social visuals
üí° Bonus Ideas
Add a Telegram or Slack node to generate or edit images via chat
Use a Webhook to feed prompts from a form or frontend
Add a mask to restrict edits to specific areas (e.g., background only)"
"üóûÔ∏è AI-Powered Sustainability Newsletter for Marketing with Gmail, GPT-4o",https://n8n.io/workflows/3684-ai-powered-sustainability-newsletter-for-marketing-with-gmail-gpt-4o/,"Tags: Sustainability, Web Scraping, OpenAI, Google Sheets, Newsletter, Marketing
Context
Hey! I‚Äôm Samir, a Supply Chain Engineer and Data Scientist from Paris, and the founder of LogiGreen Consulting.
We use AI, automation, and data to support sustainable business practices for small, medium and large companies.
I use this workflow to bring awareness about sustainability and promote my business by delivering automated daily news digests.
Promote your business with a fully automated newsletter powered by AI!
This n8n workflow scrapes articles from the official EU news website and sends a daily curated digest, highlighting only the most relevant sustainability news.
üì¨ For business inquiries, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is useful for:
Business owners who want to promote their service or products with a fully automated newsletter
Sustainability professionals staying informed on EU climate news
Consultants and analysts working on CSRD, Green Deal, or ESG initiatives
Corporate communications teams tracking relevant EU activity
Media curators building newsletters
What does it do?
This n8n workflow:
‚è∞ Triggers automatically every morning
üåç Scrapes articles from the EU Commission News Portal
üß† Uses OpenAI GPT-4o to classify each article for sustainability relevance
üìÑ Stores the results in a Google Sheet for tracking
üßæ Generates a beautiful HTML digest email, including titles, summaries, and images
üì¨ Sends the digest via Gmail to your mailing list
How it works
Trigger at 08:30 every morning
Scrape and extract article blocks from the EU news site
Use OpenAI to decide if articles are sustainability-related
Store relevant entries in Google Sheets
Generate HTML email with a professional layout and logo
Send the digest via Gmail to a configured recipient list
What do I need to get started?
You‚Äôll need:
A Google Sheet connected to your n8n instance
An OpenAI account with GPT-4 or GPT-4o access
A Gmail OAuth credential setup
Follow the Guide!
Follow the sticky notes inside the workflow or check out my step-by-step tutorial on how to configure and deploy it.

üé• Watch My Tutorial
Notes
You can customize the system prompt to adjust how AI classifies ‚Äúsustainability‚Äù
Works well for tracking updates relevant to climate action, green transition, and circular economy
This workflow was built using n8n version 1.85.4
Submitted: April 24, 2025"
"üê∂ AI Petshop Assistant with GPT-4o, Google Calendar & WhatsApp/Instagram/FB",https://n8n.io/workflows/3683-ai-petshop-assistant-with-gpt-4o-google-calendar-and-whatsappinstagramfb/,"Hi! I‚Äôm Amanda üê∂üíï
This is honestly one of the cutest, smartest workflows I‚Äôve ever built. It was designed with a lot of love to help petshops give their customers fast, modern, and sweet service ‚Äî without losing the personal touch.
Imagine a client sending a message on WhatsApp, Instagram or Messenger, and in seconds, they‚Äôre greeted with a kind message, a helpful menu, and everything they need. No delays. No stress. Just smooth, intelligent automation.
This AI agent chats, books, checks, confirms, charges, and saves ‚Äî all powered by GPT-4o, LangChain, Google Calendar, Asaas, Supabase, and more. It‚Äôs like having a super capable receptionist who works 24/7 and never forgets a detail üêæ
üåü What this workflow does (and it does it with love)
üí¨ Multichannel AI Support
Serve customers across WhatsApp (official or Evolution API), Instagram Direct, Facebook Messenger, and more ‚Äî all in one single flow.
üß† Smart, friendly conversations
Powered by OpenAI GPT-4o, this assistant understands natural language, voice messages, even PDFs and images. It keeps context, remembers the client, and replies with clarity and warmth.
üóìÔ∏è Real-time booking with Google Calendar
Book baths, check-ups or grooming sessions instantly. It checks availability, suggests times, and even handles rescheduling when needed.
üßæ Automatic billing with Asaas
It can generate payment links or invoices through the Asaas API and send them straight to the customer ‚Äî no manual work involved.
üì¶ Check product availability
Clients can ask things like ‚ÄúDo you have Royal Canin puppy food?‚Äù and get a real-time answer based on your inventory stored in Supabase.
üìÇ Understands voice, images & files
Whether it‚Äôs a voice note describing symptoms or a picture of a vaccine card, the assistant processes and responds intelligently.
üíå Fully customizable tone and flow
You can adjust the AI‚Äôs prompt, tweak the tone of voice, include emojis, and make the whole flow match your brand‚Äôs style perfectly.
üíæ Save customer history
All conversations, bookings, names, preferences and contact info are stored in Supabase ‚Äî safe and organized.
üïí Handles time rules beautifully
The flow respects your opening hours and avoids scheduling on holidays or weekends (unless you say otherwise). It can even respond politely if a message arrives after hours.
üõ†Ô∏è How to set it up
Connect your APIs: OpenAI, Supabase, Google Calendar, Asaas, Evolution or WhatsApp API
Customize your business hours, greeting text and booking options
Add your services menu or calendar rules
Test and go live ‚Äî your smart, kind AI receptionist is ready to help üíõ
‚úÖ Works on n8n Cloud and Self-hosted
üîí Secure credential management built into n8n
üêæ Recommended for users with basic n8n experience
Want to adapt this flow to your business?
‚ù§Ô∏è Buy Workflows: https://iloveflows.gumroad.com
Oi! Eu sou a Amanda üê∂üíï
E olha‚Ä¶ esse aqui √© um dos meus fluxos favoritos da vida! Ele foi feito com muito amor pra ajudar petshops a oferecerem um atendimento moderno, gentil e autom√°tico ‚Äî sem perder o toque humano, sabe?
Imagina s√≥: seu cliente envia uma mensagem por WhatsApp, Instagram ou Facebook, e em segundos, ele j√° recebe um ‚Äúoi‚Äù carinhoso, com menu de op√ß√µes, hor√°rios dispon√≠veis e tudo o que precisa. ‚ú®
Esse fluxo conversa, entende, agenda, confirma, cobra, atualiza e registra ‚Äî tudo com a intelig√™ncia do GPT-4o e v√°rias integra√ß√µes superpoderosas. √â tipo ter uma atendente virtual carism√°tica, atenta e que nunca tira f√©rias üêæ
üåü O que esse fluxo faz (e faz com amor)
üí¨ Atende por multicanais
Isso mesmo! Seu cliente pode falar com voc√™ pelo WhatsApp API oficial, Evolution API, Instagram Direct, Messenger do Facebook e muito mais. Um s√≥ fluxo pra todos os lugares onde seu cliente est√°.
üß† Conversa com intelig√™ncia (e simpatia)
O assistente usa OpenAI GPT-4o com LangChain pra entender texto, √°udio e at√© imagens. Ele mant√©m o contexto da conversa, lembra do cliente, responde com naturalidade e se adapta √† personalidade do seu petshop.
üóìÔ∏è Faz agendamentos em tempo real
Seja banho, tosa, consulta ou vacina√ß√£o, o fluxo se conecta com o Google Calendar e encontra o hor√°rio perfeito. Se o cliente quiser reagendar, ele oferece novas datas com todo o cuidado.
üßæ Gera cobran√ßas autom√°ticas
Integrado com a API do Asaas, o assistente pode gerar boletos ou links de pagamento e enviar para o cliente direto no chat. Tudo isso com confirma√ß√£o autom√°tica.
üì¶ Consulta produtos no estoque
O cliente pode perguntar: ‚Äútem ra√ß√£o Golden para gatos de 10kg?‚Äù ‚Äî e pronto, o assistente verifica no Supabase e responde rapidinho se tem ou n√£o, com base no que voc√™ cadastrou.
üìÇ Entende arquivos, √°udios e imagens
Sim! Se o cliente mandar um √°udio explicando o problema do pet, ou at√© uma foto da carteirinha de vacina√ß√£o, o assistente analisa com a IA e responde com base nessas informa√ß√µes.
üíå Personaliza o atendimento
Voc√™ pode editar o prompt da IA, mudar o tom de voz, escolher emojis, ajustar frases‚Ä¶ assim, o atendimento fica com a carinha e a linguagem da sua marca.
üíæ Salva tudo com seguran√ßa
O fluxo guarda todo o hist√≥rico no Supabase: nome do cliente, hor√°rio do atendimento, prefer√™ncia, pedidos anteriores, endere√ßo e muito mais.
üïì Respeita hor√°rios e feriados
N√£o tem risco de marcar agendamentos fora do hor√°rio. Voc√™ define os dias e per√≠odos de atendimento, e ele segue direitinho ‚Äî com lembretes e mensagens fofas se o cliente escrever fora do hor√°rio.
üõ†Ô∏è Como configurar?
Conecte suas APIs: OpenAI, Supabase, Google Calendar, Asaas, Evolution (ou WhatsApp API oficial)
Preencha seus hor√°rios de atendimento e personalize seu menu
Adicione links do seu card√°pio de servi√ßos, regras ou pol√≠tica de agendamento
Teste tudo com carinho e pronto! Voc√™ tem um atendente virtual incr√≠vel üíï
‚úÖ Compat√≠vel com n8n Cloud e n8n Auto-hospedado
üîí Seguran√ßa total usando credenciais protegidas no n8n
üêæ Recomendado para quem j√° conhece um pouquinho de n8n
Quer adaptar esse fluxo pro seu sistema?
‚ù§Ô∏è Buy Workflows: https://iloveflows.gumroad.com"
Build your own Youtube MCP server,https://n8n.io/workflows/3637-build-your-own-youtube-mcp-server/,"This n8n demonstrates how to build a simple Youtube MCP server to look up videos on Youtube and download their transcripts for research purposes.
Youtube videos are a great source of new and updated information on a variety of cutting edge developments but they''re are not always simple to understand and lengthy videos may take too much time. Using this MCP server, you extract and feed their transcripts for your AI agents which then allows it to breakdown the content into manageble learnings and insights.
How it works
A MCP server trigger is used and connected to 3 custom workflow tools: Youtube Search, Youtube Transcripts and Usage Reports.
Both Youtube tools use an external scraping service called APIFY.com. This is my preference as it's a much simpler interface and there are no rate limits.
The Youtube Search fetches 10 results based on the user's query.
The Youtube Transcripts downloads the subtitles from one or more given URL.
The usage reports pulls in your monthly APIFY.com monthly spending and limits as a way to check your account.
How to use
This Apify Youtube MCP server allows any compatible MCP client to research YouTube videos for any desired topic. An Apify account is required however to connect and use the service.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Alternatively, connect any n8n AI agent with the MCP client tool.
Try the following queries in your MCP client:
""what is MCP?""
""How can I use MCP in n8n?""
""How can I use Apify's official MCP server?""
Requirements
APIFY.com for Youtube Scraping. This is a paid service but there is a $5 free tier which is ample for this template.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
Add as many APIFY.com actors as required for your use-case or users. Consider using Apify's official MCP server for 4000+ available tools.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
"üßë‚Äçüéì AI Powered Language Teacher with Telegram, Google Sheet and GPT-4o",https://n8n.io/workflows/3295-ai-powered-language-teacher-with-telegram-google-sheet-and-gpt-4o/,"Tags: Productivity, Education, Learning, Language
Context
I‚Äôm a Supply Chain Data Scientist from Paris who lived six years in China ‚Äî and yes, learning Mandarin while working full-time was tough.
Learning Mandarin as an adult can be very difficult, especially if you have a full-time job. With AI, you can now have a Chinese tutor available 24/7 on your phone ‚Äî no excuses left!
It is with this spirit that I designed this workflow to support fellow Mandarin learners with a Chinese Teacher powered by GPT-4o.
Boost your language skills with AI using N8N!
üì¨ For business inquiries, you can add me on LinkedIn
Who is this template for?
This workflow template is designed for language learners and educators who need support to learn a vocabulary list in Mandarin (or any other language) using Open AI GPT-4o, an AI agent and a Telegram Bot to interact with users.
For the vocabulary list, you can use another template shared in my profile üâë Generate Anki Flash Cards for Language Learning with Google Translate and GPT-4o to generate the Google Sheet needed in this workflow.
How does it work?
The workflow loads a vocabulary list stored in your Google Sheet.
The bot will:
üì• Load your vocabulary list from Google Sheets
üß† Generate multiple-choice questions with GPT-4o
‚úÖ Evaluate your answer and give instant feedback
üîÅ Loop to the next word until you're fluent
These fields will be automatically added to a Google Sheet, ready to be loaded in Anki to create flash cards.
What do I need to start?
This workflow does not require any advanced programming skills.
Prerequisite
A Google Drive Account with a folder including a Google Sheet filled with the vocabulary list you want to learn.
API Credentials: Open AI API for the chat model, Google Drive API and Google Sheets API activated with OAuth2 credentials
A Telegram Bot with its token recorded in the Telegram Node Credentials
A Google Sheet with two columns (initialText: words in your own language, translatedText: words in the target language)
Next Steps
Follow the sticky notes to set up the parameters inside each node and get ready to pump your learning skills.

üé• Watch My Tutorial
üöÄ Curious how N8N can supercharge learning or supply chain?
üì¨ Let‚Äôs connect on LinkedIn
Notes
This workflow can be used for any language. In the AI Agent prompt, you just need to replace Chinese with your language.
This workflow has been created with N8N 1.82.1
Submitted: March 23th, 2025"
"üé• Analyze YouTube Video for Summaries, Transcripts & Content + Google Gemini AI",https://n8n.io/workflows/3289-analyze-youtube-video-for-summaries-transcripts-and-content-google-gemini-ai/,"üé• Analyze YouTube Video for Summaries, Transcripts & Content + Google Gemini
Who is this for?
This workflow is ideal for content creators, video marketers, and research professionals who need to extract actionable insights, detailed transcripts, or metadata from YouTube videos efficiently. It is particularly useful for those leveraging AI tools to analyze video content and optimize audience engagement.
What problem does this workflow solve? / Use case
Analyzing video content manually can be time-consuming and prone to errors. This workflow automates the process by extracting key metadata, generating summaries, and providing structured transcripts tailored to specific use cases. It helps users save time and ensures accurate data extraction for content optimization.
What this workflow does
Extracts audience-specific metadata (e.g., video type, tone, key topics, engagement drivers).
Generates customized outputs based on six prompt types:
Default: Actionable insights and strategies.
Transcribe: Verbatim transcription.
Timestamps: Timestamped dialogue.
Summary: Concise bullet-point summary.
Scene: Visual descriptions of settings and techniques.
Clips: High-engagement video segments with timestamps.
Saves extracted data as a text file in Google Drive.
Sends analyzed outputs via Gmail or provides them in a completion form.
Setup
Configure API keys:
Add your Google API key as an environment variable.
Input requirements:
Provide the YouTube video ID (e.g., wBuULAoJxok).
Select a prompt type from the dropdown menu.
Connect credentials:
Set up Google Drive and Gmail integrations in n8n.
How to customize this workflow to your needs
Modify the metadata prompt to extract additional fields relevant to your use case.
Adjust the output format for summaries or transcripts based on your preferences (e.g., structured bullets or plain text).
Add nodes to integrate with other platforms like Slack or Notion for further collaboration.
Example Usage
Input: YouTube video ID (wBuULAoJxok) and prompt type (summary).
Output: A concise summary highlighting actionable insights, tools, and resources mentioned in the video."
Stock Market Technical Analysis with GPT-4o and TradingView for Telegram,https://n8n.io/workflows/3202-stock-market-technical-analysis-with-gpt-4o-and-tradingview-for-telegram/,"Overview: Transform Your Trading with AI-Driven Technical Analysis
The Stock Market Technical Analysis Bot is an advanced n8n workflow that brings professional-grade stock analysis to Telegram users. This powerful AI agent analyzes stock charts in real-time, providing detailed technical insights that would typically require expensive software or expert knowledge.
By combining artificial intelligence with technical analysis indicators, this bot delivers actionable trading insights directly to your Telegram chat, making sophisticated market analysis accessible to traders of all experience levels.
Key Features and Capabilities
Advanced Technical Analysis Tools
Japanese Candlestick Pattern Recognition: Automatically identifies bullish engulfing, doji, hammer patterns and more
MACD Indicator Analysis: Detects bullish/bearish crossovers and divergence signals
Volume Trend Analysis: Validates price movements with volume confirmation
Support & Resistance Level Identification: Pinpoints key price levels for potential reversals
Bollinger Bands & RSI Analysis: Measures volatility and overbought/oversold conditions
Real-Time TradingView Charts: Professional-quality charts with multiple indicators
AI-Powered Trading Assistant
Natural Language Interface: Interact with the bot using simple conversational commands
Context-Aware Responses: The bot remembers your previous interactions for more relevant analysis
GPT-4o Integration: Leverages advanced language models for detailed explanations
Actionable Trading Insights: Receive clear, jargon-free analysis with practical implications
Technical Implementation
Telegram Bot Integration: Seamless messaging experience with instant responses
n8n Workflow Automation: No-code solution for complex trading analysis
Memory System: Maintains conversation context for personalized interactions
API Connections: Integrates with TradingView, chart-img.com, and OpenAI
Setup Instructions
Prerequisites
n8n Instance: A running n8n installation with the following nodes:
Telegram nodes
LangChain nodes
HTTP Request nodes
Code node
Required API Credentials:
Telegram Bot API token (obtain from @BotFather)
OpenAI API key
Chart-img.com API key
Installation Process
Import the workflow template into your n8n instance
Configure your Telegram bot credentials
Set up your OpenAI API credentials
Enter your chart-img.com API key in the HTTP Request node
Activate the workflow to start receiving analysis requests
How It Works: The Technical Analysis Process
1. User Interaction Flow
When a user sends a message to the Telegram bot:
The Telegram Trigger node captures the incoming message
The AI Agent processes the request using natural language understanding
If a stock symbol is detected, the GetChart tool is invoked
2. Chart Generation System
The workflow creates professional TradingView charts with:
Candlestick price data
Bollinger Bands for volatility measurement
Volume indicators for trade validation
RSI (Relative Strength Index) for momentum analysis
3. AI Analysis Engine
The GPT-4o model analyzes the chart and provides:
Detailed breakdown of candlestick patterns
MACD trend confirmation signals
Volume analysis to validate price movements
Support and resistance level identification
Overall market sentiment assessment
4. Response Delivery
The analysis is formatted and sent back to the user via Telegram, including:
The generated chart image
Comprehensive technical analysis text
Actionable insights based on the indicators
Use Cases and Applications
For Day Traders
Receive quick technical analysis before making trading decisions
Validate your own analysis with AI-generated insights
Track multiple stocks efficiently throughout the trading day
For Swing Traders
Analyze medium-term trends and potential entry/exit points
Identify key support and resistance levels for stop-loss placement
Get objective analysis to complement your trading strategy
For Trading Communities
Share consistent, AI-generated analysis among group members
Create a common analytical framework for discussion
Reduce the learning curve for technical analysis concepts
For Financial Educators
Demonstrate technical analysis concepts with real-time examples
Provide students with accessible tools for practice
Illustrate the application of various technical indicators
Advanced Configuration Options
The workflow can be customized to:
Add additional technical indicators (Fibonacci retracements, Moving Averages, etc.)
Modify the analysis parameters for different trading styles
Integrate with other financial data sources for fundamental analysis
Connect to different messaging platforms beyond Telegram
Create scheduled analysis for watchlist stocks
Troubleshooting Common Issues
If you encounter problems:
Authentication Errors: Verify all API credentials are correctly configured
Telegram Connection Issues: Ensure your Telegram bot is properly set up
Chart Generation Failures: Check your chart-img.com API key and quota
Slow Response Times: Consider upgrading your OpenAI plan for faster processing
Missing Indicators: Verify the chart configuration in the HTTP Request node
About This Template
This n8n workflow template demonstrates the power of combining conversational AI with technical analysis tools to create a sophisticated financial assistant. It showcases advanced workflow automation features including AI integration, external API connections, and complex data processing.
By leveraging the latest advancements in AI and technical analysis, this template provides traders with professional-grade insights without requiring expensive software or deep technical expertise.
Keywords: stock market analysis bot, AI trading assistant, technical analysis indicators, n8n workflow, Telegram trading bot, candlestick pattern recognition, MACD analysis, trading signals, automated stock analysis, GPT-4o financial analysis, TradingView charts, RSI indicator, volume analysis, support resistance levels, day trading bot, swing trading analysis, AI financial advisor"
Generate Content From Google Sheets to X with GPT-4,https://n8n.io/workflows/3161-generate-content-from-google-sheets-to-x-with-gpt-4/,"AI-Powered Social Media Content Automation
üßë‚Äçüíª Who is this for?
This workflow is perfect for social media managers, content creators, and digital marketers who want to save time by automating social media post generation and publishing across platforms.
üìå What problem does this solve?
Manually generating and scheduling social media content is time-consuming and repetitive. This workflow automates content creation and publishing, allowing you to:
Streamline content generation using AI
Ensure consistent posting across social media platforms
Track published posts in Google Sheets
üîç What this workflow does:
Fetches content ideas from a Google Sheet.
Generates social media posts using OpenAI's GPT-4.
Checks the target platform (e.g., Twitter/X, LinkedIn).
Posts the content to the chosen social media platform.
Updates the Google Sheet with the generated post and timestamp.
üõ†Ô∏è Setup Guide:
Connect Google Sheets: Ensure you have a Google Sheet with content ideas (columns: Idea, Status, Generated Post).
Set up OpenAI API Key: Provide your OpenAI API key for GPT-4.
Configure Social Media Accounts: Link your Twitter/X or other social media accounts using n8n's built-in nodes.
Test the Workflow: Run the workflow to verify automation.
Schedule Automation: Set a recurring trigger (e.g., daily) to automate posting.
üîß Customization Tips:
Adjust prompt inputs in the OpenAI node to tailor the tone and style.
Add more platforms (e.g., Instagram, Facebook) by duplicating the social media node.
Include analytics tracking for engagement insights.
üìä Example Use Cases:
Automatically generate and share daily motivational quotes.
Post product updates and announcements.
Share curated industry news and insights.
This workflow saves time and keeps your social media presence active and engaging effortlessly. üöÄ"
Automated Faceless YouTube Video Generator Using Leonardo AI and Creatomate,https://n8n.io/workflows/2971-automated-faceless-youtube-video-generator-using-leonardo-ai-and-creatomate/,"Automate Your Video Content Creation: AI-Powered Video Generation
This n8n template streamlines the creation of high-quality, faceless videos, automating the entire process from scriptwriting to final video production. Leveraging AI tools like Leonardo AI and Creatomate, this workflow empowers content creators to efficiently produce engaging videos without on-camera presence.
Who is this for?
This template is ideal for:
Content creators looking to scale their presence on YouTube, Instagram, and TikTok.
Marketers seeking to automate video marketing campaigns.
Anyone wanting to produce professional-looking videos quickly and easily.
Individuals wishing to create faceless video content.
What problem is this workflow solving?
Creating engaging videos can be time-consuming and resource-intensive. This workflow eliminates the manual effort involved in scripting, image sourcing, and video editing, allowing creators to focus on strategy and content ideation. It addresses the challenge of consistent video production by automating repetitive tasks.
What this workflow does:
This workflow automates the following steps:
AI-Driven Scriptwriting: Uses an LLM (default: DeepSeek V3) to generate a compelling video script based on your system prompt and desired number of scenarios.
AI Image Generation: Integrates with Leonardo AI to create visually appealing images for each scenario in the script. You define the image style and resolution.
Automated Video Editing: Connects to Creatomate to assemble the script and images into a polished video, ready for upload. You‚Äôll use a pre-created ‚ÄúAI generated story template‚Äù within Creatomate.
Setup:
Video Script Generation: Provide a detailed system prompt describing your video‚Äôs topic, target audience, and key message. Specify the number of scenarios. Select your preferred LLM (DeepSeek V3 is the default).
Image Generation: Connect your Leonardo AI account. Choose your preferred image generation model and set the video resolution. Craft a detailed prompt for Leonardo AI, defining the image style (e.g., ‚Äúcinematic,‚Äù ‚Äúcartoon,‚Äù ‚Äúrealistic‚Äù).
Video Editing: Link your Creatomate account. Create an empty ‚ÄúAI generated story template‚Äù in Creatomate. Copy the cURL from the Creatomate template to the video generation node. Link your ElevenLabs account to Creatomate and choose the desired voice.
How to customize this workflow:
Script Customization: Refine the system prompt to guide the AI. Experiment with different LLMs.
Image Style: Adjust the Leonardo AI prompt. Explore different image generation models.
Video Editing: Modify the Creatomate template to customize transitions, text overlays, and other elements. Add voiceover using the ElevenLabs integration in Creatomate.
Category:
Marketing, Social Media, Content Creation"
üåê Confluence Page AI Chatbot Workflow,https://n8n.io/workflows/3012-confluence-page-ai-chatbot-workflow/,"üåê Confluence Page AI Chatbot Workflow
This n8n workflow template enables users to interact with an AI-powered chatbot designed to retrieve, process, and analyze content from Confluence pages. By leveraging Confluence's REST API and an AI agent, the workflow facilitates seamless communication and contextual insights based on Confluence page data.
üåü How the Workflow Works
üîó Input Chat Message
The workflow begins when a user sends a chat message containing a query or request for information about a specific Confluence page.
üìÑ Data Retrieval
The workflow uses the Confluence REST API to fetch page details by ID, including its body in the desired format (e.g., storage, view).
The retrieved HTML content is converted into Markdown for easier processing.
ü§ñ AI Agent Interaction
An AI-powered agent processes the Markdown content and provides dynamic responses to user queries. The agent is context-aware, ensuring accurate and relevant answers based on the Confluence page's content.
üí¨ Dynamic Responses
Users can interact with the chatbot to:
Summarize the page's content.
Extract specific details or sections.
Clarify complex information.
Analyze key points or insights.
üöÄ Use Cases
üìö Knowledge Management: Quickly access and analyze information stored in Confluence without manually searching through pages.
üìä Team Collaboration: Facilitate discussions by summarizing or explaining page content during team chats.
üîç Research and Documentation: Extract critical insights from large documentation repositories for efficient decision-making.
‚ôø Accessibility: Provide an alternative way to interact with Confluence content for users who prefer conversational interfaces.
üõ†Ô∏è Resources for Getting Started
Confluence API Setup:
Generate an API token for authentication via Atlassian's account management portal.
Refer to Confluence's REST API documentation for endpoint details and usage instructions.
n8n Installation:
Install n8n locally or on a server using the official installation guide.
AI Agent Configuration:
Set up OpenAI or other supported language models for natural language processing."
üé¨ YouTube Shorts Automation Tool üöÄ,https://n8n.io/workflows/2941-youtube-shorts-automation-tool/,"üé¨ YouTube Shorts Automation Tool üöÄ
Automate the creation of high-performing YouTube Shorts in minutes!
Who is this for? üéØ
Content Creators: Generate engaging short videos effortlessly.
Marketing Agencies: Produce client-ready content quickly.
Business Owners: Promote products/services through viral short-form content.
What problem does this solve? üõ†Ô∏è
Creating short-form video content is time-consuming, complex, and skill-intensive. This workflow automates video creation, eliminating the need for video editing expertise while ensuring SEO optimization, high-quality visuals, and professional voiceovers.
How it works üåü
Enter your video idea into the chat interface.
AI generates a script optimized for engagement and SEO.
Voiceover is created with realistic AI narration.
Relevant visuals are selected to match the script.
The video is assembled and delivered via a shareable link.
Setup ‚öôÔ∏è (5-10 min)
Connect required APIs (most have free tiers).
Follow the guided setup (video tutorial included).
Start generating professional YouTube Shorts instantly!
Required APIs üîó
Content Generation: OpenAI, ElevenLabs (script & voiceover)
Media Processing: Cloudinary, Replicate (images & storage)
Integration Tools: 0codekit, Creatomate (video assembly)
Customization üé®
Adjust script styles & voiceover preferences.
Modify visuals to match your brand.
Optimize video length and format.
üöÄ Start automating your YouTube Shorts today and grow your audience effortlessly!"
"‚ú® Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini",https://n8n.io/workflows/2563-vision-based-ai-agent-scraper-with-google-sheets-scrapingbee-and-gemini/,"Important Notes:
Check Legal Regulations:
This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry!
Workflow Description:
üòÆ‚Äçüí® Tired of struggling with XPath, CSS selectors, or DOM specificity when scraping ?
This AI-powered solution is here to simplify your workflow! With a vision-based AI Agent, you can extract data effortlessly without worrying about how the DOM is structured.
This workflow leverages a vision-based AI Agent, integrated with Google Sheets, ScrapingBee, and the Gemini-1.5-Pro model, to extract structured data from webpages. The AI Agent primarily uses screenshots for data extraction but switches to HTML scraping when necessary, ensuring high accuracy.
Key Features:
Google Sheets Integration: Manage URLs to scrape and store structured results.
ScrapingBee: Capture full-page screenshots and retrieve HTML data for fallback extraction.
AI-Powered Data Parsing: Use Gemini-1.5-Pro for vision-based scraping and a Structured Output Parser to format extracted data into JSON.
Token Efficiency: HTML is converted to Markdown to optimize processing costs.
This template is designed for e-commerce scraping but can be customized for various use cases."
Chat with local LLMs using n8n and Ollama,https://n8n.io/workflows/2384-chat-with-local-llms-using-n8n-and-ollama/,"Chat with local LLMs using n8n and Ollama
This n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.
Use cases
Private AI Interactions
Ideal for scenarios where data privacy and confidentiality are important.
Cost-Effective LLM Usage
Avoid ongoing cloud API costs by running models on your own hardware.
Experimentation & Learning
A great way to explore and experiment with different LLMs in a local, controlled environment.
Prototyping & Development
Build and test AI-powered applications without relying on external services.
How it works
When chat message received: Captures the user's input from the chat interface.
Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.
Delivers the LLM's response back to the chat interface.
Set up steps
Make sure Ollama is installed and running on your machine before executing this workflow.
Edit the Ollama address if different from the default."
‚úàÔ∏è CO2 Emissions of Business Travels with Carbon Interface API and GPT-4o,https://n8n.io/workflows/4756-co2-emissions-of-business-travels-with-carbon-interface-api-and-gpt-4o/,"Tags: Sustainability, Business Travel, Carbon Emissions, Flight Tracking, Carbon Interface API
Context
Hi! I‚Äôm Samir ‚Äî a Supply Chain Engineer and Data Scientist based in Paris, and founder of LogiGreen Consulting.
I help companies monitor and reduce their environmental footprint by combining AI automation, carbon estimation APIs, and workflow automation.
This workflow is part of our sustainability reporting initiative, allowing businesses to track the CO‚ÇÇ emissions of employee flights.
Automate carbon tracking for your business travel with AI-powered workflows in n8n!
üì¨ For business inquiries, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is designed for travel managers, sustainability teams, or finance teams who need to measure and report on emissions from business travel.
Let‚Äôs imagine your company receives a flight confirmation email:
The AI Agent reads the email and extracts structured data, such as flight dates, airport codes, and number of passengers.
Then, the Carbon Interface API is called to estimate CO‚ÇÇ emissions, which are stored in a Google Sheet for sustainability reporting.
How does it work?
This workflow automates the end-to-end process of tracking flight emissions from email to CO‚ÇÇ estimation:
üì® Gmail Trigger captures booking confirmations
üß† AI Agent extracts structured data (airports, dates, flight numbers)
‚úàÔ∏è Each flight leg is processed individually
üåç Carbon Interface API returns distance and carbon emissions
üìÑ A second Google Sheet node appends the emission data for reporting
Steps:
üíå Trigger on new flight confirmation email
üß† Extract structured trip data using AI Agent (flights, airports, dates)
üìë Store flight metadata in Google Sheets
üß≠ For each leg, call the Carbon Interface API
üì• Append distance, CO‚ÇÇ in kg, and timestamp to the flight row
What do I need to get started?
You‚Äôll need:
A Gmail account receiving SAP Concur or travel confirmation emails
A Google Sheet to record trip metadata and CO‚ÇÇ emissions
A free Carbon Interface API key
Access to OpenAI for parsing the email via AI Agent
A few sample flight confirmation emails to test
Next Steps
üóíÔ∏è Use the sticky notes in the n8n canvas to:
Add your Gmail and Carbon Interface credentials
Send a sample booking email to your inbox
Verify that emissions and distances are correctly added to your sheet
This template was built using n8n v1.93.0
Submitted: June 7, 2025"
Segment PDFs by Table of Contents with Gemini AI and Chunkr.ai,https://n8n.io/workflows/4697-segment-pdfs-by-table-of-contents-with-gemini-ai-and-chunkrai/,"Intelligently Segment PDFs by Table of Contents
This workflow empowers you to automatically process PDF documents, intelligently identify or generate a hierarchical Table of Contents (ToC), and then segment the entire document's content based on these ToC headings. It effectively breaks down a large PDF into its constituent sections, each paired with its corresponding heading and hierarchical level.
Why It's Useful
Unlock the true structure of your PDFs for granular access and advanced processing:
AI Agent Tool: A key use case is to provide this workflow as a tool to an AI agent. The agent can then use the segmented output to ""read"" and navigate to specific sections of a document to answer questions, extract information, or perform tasks with much greater accuracy and efficiency.
Targeted Content Extraction: Programmatically pull out specific chapters or subsections for focused analysis, summarization, reporting, or repurposing content.
Enhanced RAG Systems: Improve your Retrieval Augmented Generation (RAG) pipelines by feeding them well-defined, contextually relevant document sections instead of entire, monolithic PDFs. This leads to more precise AI-generated responses.
Modular Document Processing: Process different parts of a document using distinct logic in subsequent n8n workflows by acting on individual sections.
Data Preparation: Seamlessly convert lengthy PDFs into a structured format where each section (including its heading, level, and content in multiple formats) becomes a distinct, manageable item.
How It Works
Ingestion & Advanced Parsing: The workflow ingests a PDF (via a provided URL or a pre-set one for manual runs). It then utilizes Chunkr.ai to perform Optical Character Recognition (OCR) and parse the document into detailed structural elements, extracting text, HTML, and Markdown for each segment.
AI-Powered Table of Contents Generation: A Google Gemini AI model analyzes the initial pages of the document (where a ToC often resides) along with section headers extracted by Chunkr as a fallback. This allows it to construct an accurate, hierarchical Table of Contents in a structured JSON format, even if the PDF lacks an explicit ToC or if it's poorly formatted.
Precise Content Segmentation: Sophisticated custom code then meticulously maps the AI-generated ToC headings to their corresponding content within the parsed document from Chunkr. It intelligently determines the precise start and end of each section.
Structured & Flexible Output:
The primary output provides each identified section as an individual n8n item. Each item includes the heading text, its hierarchical level (e.g., 1, 1.1, 2), and the full content of that section in Text, HTML, and Markdown formats.
Optionally, the workflow can also reconstruct the entire document into a single, navigable HTML file or a clean Markdown file.
What You Need
To run this workflow, you'll need:
Input PDF:
When triggered by another workflow: A URL pointing to the PDF document.
When triggered manually: The workflow uses a pre-configured sample PDF from Google Drive for demonstration (this can be customized).
Chunkr.ai API Key: Required for the initial parsing and OCR of the PDF document. You'll need to insert this into the relevant HTTP Request nodes.
Google Gemini API Credentials: Necessary for the AI model to intelligently generate the Table of Contents. This should be configured in the Google Gemini Chat Model nodes.
Outputs
The workflow primarily generates:
Individual Document Sections: A series of n8n items. Each item represents a distinct section of the PDF and contains:
heading: The text of the section heading.
headingLevel: The hierarchical level of the heading (e.g., 1 for H1, 2 for H2).
sectionText: The plain text content of the section.
sectionHTML: The HTML content of the section.
sectionMarkdown: The Markdown content of the section.
Alternatively, you can configure the workflow to output:
Full Reconstructed Document:
A single HTML file representing the entire processed document.
A single Markdown file representing the entire processed document.
This workflow is ideal for anyone looking to deconstruct PDFs into meaningful, manageable parts for advanced automation, AI integration, or detailed content analysis."
"Automate Meeting Prep & Lead Enrichment with Bright Data, Cal.com & Airtable",https://n8n.io/workflows/4505-automate-meeting-prep-and-lead-enrichment-with-bright-data-calcom-and-airtable/,"This workflow makes it easier to prepare for meetings and calls by researching your lead right before the call and creates a high-level meeting prep that is sent to your email. This removes the extra steps needed by teams to learn their leads, research, and prepare for the upcoming calls.
How does it work
This workflow starts when We Capture the webhook from cal.com for new bookings.
Ensure you have a field on the form to collect LinkedIn posts. This can be optional or mandatory depending on your preferences.
When a new event is booked, we will add the leads to an Airtable CRM for appointments and new bookings. This table will contain all the items and items needed to enrich and maintain your CRM.
If the lead has linkedin then we do research on LinkedIn for their content and posts and perform a lead enrichment to get as much info as we can about the leads and create a new meeting prep.
What you need
Bright data API
Cal.com account/calendar. Other calendars can be used too for this eg calendly, Google Calendar, etc with a few tweaks
CRM - This can be anything not just airtable
Setting it up
Create/update your calendar to allow collecting users LinkedIn profiles/bios
Add a new webhook to and subscribe to the desired events like below
Map the fields from the webhook to match your CRM. If you have no CRM make a copy of this Airtable CRM and map the fields to your account. We will be using the Base and table ID to make the mapping easier
Setup your Bright Data API and select the data source as linkedin for the scraping
You can edit more data on the bio as needed
Update this info to the CRM under the table lead enrichment and map accordingly
You can update the prompt on the AI models or work with them as is.
Update the Gmail node to send the meeting preps to you and finally update the CRM with the generated Meeting prep
This automated process can save your team a couple of minutes each day otherwise spent on other client fulfillment items.
If you would like to learn more about n8n templates like this, feel free to reach out via Linkedin
Happy productivity!!"
Email News Briefing by Keyword from Bright Data with AI Summary,https://n8n.io/workflows/4570-email-news-briefing-by-keyword-from-bright-data-with-ai-summary/,"This n8n workflow automatically retrieves recent Reuters news articles related to a user-specified keyword, summarizes the main findings using Google Gemini, formats the output into styled HTML, and sends a clean email report to a specified address.
üöÄ What It Does
Collects news data from Bright Data's Reuters dataset.
Sorts and filters top 10 most recent news articles by publication_date.
Sends structured news data to Gemini Flash for summarization.
Converts Gemini's response (in Markdown) into styled HTML.
Delivers a concise news briefing via email, including clickable source links and topic highlights.
üõ†Ô∏è Step-by-Step Setup
User Form: Accepts a keyword from the user via an n8n form trigger.
Bright Data API: Posts a discover_new request to Bright Data's Reuters dataset using the keyword.
Snapshot Polling: Waits and checks for dataset readiness using the snapshot ID.
Data Retrieval: Downloads the news data once the snapshot is complete.
Parsing: Filters and sorts the latest 10 articles using a Python Code node.
AI Analysis: Google Gemini summarizes the filtered content into one briefing.
Markdown ‚Üí HTML: Converts AI response into styled HTML using Markdown + Code node.
Email Delivery: Sends the briefing as an email to a predefined recipient.
üß† How It Works
Polling Control: Uses Wait and If nodes to handle Bright Data snapshot readiness.
Date Sorting: Publication dates (ISO 8601 format) are parsed and used for sorting.
AI Summarization: Gemini condenses multiple articles into one cohesive summary.
Formatting: Clean HTML with readable styles is generated dynamically before sending.
üì® Final Output
The email includes:
A brief summary of the most important developments
Date range of the collected news
Topics covered
üîê Credentials Used
Bright Data API (replace YOUR_API_KEY in the HTTP nodes)
Google Gemini (Flash) API
Email SMTP (configured in Email Send node)
‚ö†Ô∏è Notes
You must replace all YOUR_API_KEY placeholders in Bright Data request headers with your actual Bright Data API key.
You can customize the keyword prompt and output style freely.
I would recommend to keep the sort = relevance option for best chronological results - sorting by date is handled later."
Real-time Chat Translation with DeepL,https://n8n.io/workflows/4532-real-time-chat-translation-with-deepl/,"Who is this for?
This workflow is intended for users, workers, creatives or students who want to translate languages quickly and automatically via text chat. translating a sentence will take time and seem impractical, with this the results will be practical and save time. This is also a form of devotion to the community at n8n and the n8n company, as well as devotion to users and students so that reality is no longer tiring and is able to answer the problems of existing reality.
How does it work?
Easy explanation:
Trigger Chat functions as a place or container for inputting text or sentences to be translated and the results come out directly here. Then the DeepL node will become the translation machine, here there are already parameters, the language can be adjusted, and the last node is useful for after the DeepL node no more operations appear And enjoy this workflow that suits you.
Setup instructions
Complete what is in the node as stated in the notes column.
You need a ""Credential Account"" on the DeepL node, then select the language. If you already have an account, just connect it, and if not, create it first, you can register it by following the guide from n8n, it's very easy.
Because here it is neatly and cleanly arranged. Then, save and run, test the workflow and activate the workflow. This workflow is ready to use.
Requirements
As a reminder:
There must be (if not, make sure it is registered) in each ""Credential Account"" by following the guide on how to create it, n8n's guide is very complete
Don't forget to save, and make sure the workflow is active.
How to customize this workflow to your needs
You can directly set the parameters in the language, so that the suitability is also high when working on tasks and answering them."
AI-Powered Document Chat with Nextcloud Files using LangChain and OpenAI,https://n8n.io/workflows/4465-ai-powered-document-chat-with-nextcloud-files-using-langchain-and-openai/,"Main Workflow ‚ÄúAI Nextcloud‚Äù
Entry point: A public chat-trigger greets the user; every incoming chat message starts the flow.
AI agent: A LangChain agent (‚ÄúAI Nextcloud‚Äù) uses the configured OpenAI model plus short-term memory to continue the dialogue in context.
Purpose: Answers questions about files stored in a Nextcloud folder. The user simply includes the folder path in their question.
Tool integration: Calls the sub-workflow ‚ÄúNextcloud Tool‚Äù whenever it needs to read files and pass their text back to the AI.
Sub-Workflow ‚ÄúNextcloud Tool‚Äù
Invocation: Triggered by other workflows with the input parameter path (folder path).
File listing: Retrieves every file in the specified folder via the Nextcloud API.
Filter: Allows only readable formats (PDF, Markdown, DOCX).
Download & text extraction
PDF ‚Üí Text via Extract From File
Markdown ‚Üí Raw text
DOCX ‚Üí Text via community node word2text
Aggregation: Combines all extracted text into a single output field and returns it.
Outcome: Each call yields the plain content of every supported file in a Nextcloud folder‚Äîproviding rich context for the AI agent to answer user questions accurately."
"Categorize Support Tickets with Gemini AI, Typeform, and Google Sheets Reporting",https://n8n.io/workflows/4447-categorize-support-tickets-with-gemini-ai-typeform-and-google-sheets-reporting/,"Transform chaotic support requests into organized, actionable insights automatically.
This intelligent workflow captures support tickets from forms, uses AI to categorize and analyze sentiment, stores everything in organized databases, and delivers comprehensive analytics reports to your team - eliminating manual sorting while providing valuable business intelligence.
üöÄ What It Does
Intelligent Ticket Processing: Automatically categorizes incoming support requests into Billing, Bug Reports, Feature Requests, How-To questions, and Complaints using advanced AI analysis.
Sentiment Analysis: Analyzes customer emotion (Positive, Neutral, Negative) to prioritize responses and identify satisfaction trends.
Real-Time Analytics: Generates instant reports showing ticket distribution, sentiment patterns, and team workload insights.
Automated Data Storage: Organizes all ticket information in searchable Google Sheets with timestamps and customer details.
Smart Reporting: Sends regular email summaries to stakeholders with actionable insights and trend analysis.
üéØ Key Benefits
‚úÖ Save 10+ Hours Weekly: Eliminate manual ticket sorting and categorization
‚úÖ Improve Response Times: Prioritize tickets based on category and sentiment
‚úÖ Boost Customer Satisfaction: Never miss urgent issues or complaints
‚úÖ Track Performance: Monitor support trends and team effectiveness
‚úÖ Scale Operations: Handle increasing ticket volume without additional staff
‚úÖ Data-Driven Decisions: Make informed improvements based on real patterns
üè¢ Perfect For
Customer Support Teams
SaaS companies managing user inquiries and bug reports
E-commerce stores handling order and product questions
Service businesses organizing client communications
Startups scaling support operations efficiently
Business Applications
Help Desk Management: Organize and prioritize incoming support requests
Customer Success: Monitor satisfaction levels and identify improvement areas
Product Development: Track feature requests and bug report patterns
Team Management: Distribute workload based on ticket categories and urgency
‚öôÔ∏è What's Included
Complete Workflow Setup: Ready-to-use n8n workflow with all nodes configured
AI Integration: Google Gemini-powered classification and sentiment analysis
Form Integration: Works with Typeform (easily adaptable to other platforms)
Data Management: Automated Google Sheets organization and storage
Email Reporting: Professional summary reports sent to your team
Documentation: Step-by-step setup and customization guide
üîß Technical Requirements
n8n Platform: Cloud or self-hosted instance
Google Gemini API: For AI classification (free tier available)
Typeform Account: For support form creation (alternatives supported)
Google Workspace: For Sheets data storage and Gmail reporting
SMTP Email: For automated report delivery
üìä Sample Output
Daily Support Summary Email:
üìß Support Ticket Summary - March 15, 2024

üìä TICKET BREAKDOWN:
‚Ä¢ Billing: 12 tickets (30%)
‚Ä¢ Bug Report: 8 tickets (20%)  
‚Ä¢ Feature Request: 6 tickets (15%)
‚Ä¢ How-To: 10 tickets (25%)
‚Ä¢ Complaint: 4 tickets (10%)

üòä SENTIMENT ANALYSIS:
‚Ä¢ Positive: 8 tickets (20%)
‚Ä¢ Neutral: 22 tickets (55%)
‚Ä¢ Negative: 10 tickets (25%)

‚ö° PRIORITY ACTIONS:
‚Ä¢ 4 complaints requiring immediate attention
‚Ä¢ 3 billing issues escalated to finance team
‚Ä¢ 6 feature requests for product backlog review
üé® Customization Options
Categories: Easily modify ticket categories for your specific business needs
Form Platforms: Adapt to Google Forms, JotForm, Wufoo, or custom webhooks
Reporting Frequency: Set daily, weekly, or real-time report delivery
Team Notifications: Configure alerts for urgent tickets or negative sentiment
Data Visualization: Create custom dashboards and charts in Google Sheets
Integration Extensions: Connect to CRM, project management, or chat platforms
üîÑ How It Works
Customer submits support request via your form
AI analyzes message content and assigns category + sentiment
Data is automatically stored in organized Google Sheets
System generates real-time analytics on all historical tickets
Professional report is emailed to your support team
Team can prioritize responses based on urgency and sentiment
üí° Use Case Examples
SaaS Company: Automatically route billing questions to finance, bugs to development, and feature requests to product team
E-commerce Store: Prioritize shipping complaints, categorize product questions, and track customer satisfaction trends
Consulting Firm: Organize client requests by service type, monitor project-related issues, and ensure timely responses
Healthcare Practice: Sort appointment requests, billing inquiries, and medical questions while maintaining HIPAA compliance
üìà Expected Results
80% reduction in manual ticket sorting time
50% faster initial response times through better prioritization
25% improvement in customer satisfaction scores
100% visibility into support trends and team performance
Unlimited scalability as your business grows
üìû Get Help & Learn More
üé• Free Video Tutorials
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
üíº Professional Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for implementation consulting
Share your automation success stories
Access exclusive templates and updates
üìß Direct Support
Email: Yaron@nofluff.online
Technical setup assistance
Custom workflow modifications
Integration with existing systems
Response within 24 hours
üèÜ Why Choose This Workflow
Proven Results: Successfully deployed across 100+ businesses worldwide
Expert Created: Built by automation specialist with 10+ years experience
Continuously Updated: Regular improvements and new features added
Money-Back Guarantee: Full refund if not satisfied within 30 days
Lifetime Support: Ongoing help and updates included with purchase"
"Generate AI Stock Images with Flux1-schnell, Metadata Tagging & Google Integration",https://n8n.io/workflows/4445-generate-ai-stock-images-with-flux1-schnell-metadata-tagging-and-google-integration/,"üì£ This template generates up to 2,000 AI-based stock images per day for under $3. It includes prompt generation, image creation, metadata enrichment, upload to Google Drive, and error logging ‚Äî fully automated with minimal setup.
üìå Who is this for?
This workflow is designed for:
Stock image creators and contributors
Agencies managing visual content at scale
Solo creators or marketers automating asset production
Anyone aiming to populate image libraries (e.g., Adobe Stock, Shutterstock)
‚ùì What problem is this workflow solving?
Manually generating, naming, tagging, and uploading thousands of stock images is time-consuming and expensive. This automation solves that by:
Generating photorealistic images via AI
Auto-tagging with rich metadata
Uploading them to structured Google Drive folders
Logging progress and capturing errors automatically
‚öôÔ∏è What this workflow does
‚úÖ Generates text prompts from idea topics using GPT
üé® Converts prompts into 4 images each via the Flux Schnell model
üìê Resizes and compresses images for preview and final use
üß† Uses GPT-4o-mini to generate metadata: filename, title, category, and 25 keywords
‚òÅÔ∏è Uploads to Google Drive, logs metadata to Google Sheets
üü¢ Marks each prompt as completed
üì≤ Sends Telegram alerts for success and errors
üõ† Setup
Google Sheets
Sheet 1: Ideas ‚Äî New prompt topics
Sheet 2: Generated Pmts ‚Äî Tracks generated prompts and status
Sheet 3: Error Logs ‚Äî Execution error records
Google Drive
A folder for image uploads
A folder for copied daily spreadsheets
API Access
OpenAI GPT (gpt-4o-mini)
PIAPI (Flux Schnell image model)
Telegram Bot (optional but recommended)
Real-time status notifications
üß© How to customize this workflow to your needs
üîÅ Change frequency of generation via trigger settings
üß† Swap models if using a different image API
üßæ Adjust metadata fields or format rules
üì§ Redirect outputs to Airtable, Notion, Dropbox, etc.
üß† Node Naming & Notes
‚úÖ All nodes are clearly renamed to reflect their purpose (e.g., Generate Image, Upload to Drive)
üóíÔ∏è Sticky notes are used to document setup and logic
üîó External links to Notion setup docs or Loom videos are recommended
üóÇÔ∏è Categories
AI
Marketing
Product
Building Blocks
üéÅ Bonus Feature
This template includes a bonus stock image renamer workflow ‚Äî ideal for renaming:
üì∏ Taken photos
üóÉÔ∏è Existing stock libraries
üñºÔ∏è Downloaded image batches
It applies SEO-friendly, consistent naming compatible with major platforms like Adobe Stock and Shutterstock."
Shopify VIP Alerts: AI Summary & Slack Notification for Big Orders,https://n8n.io/workflows/4384-shopify-vip-alerts-ai-summary-and-slack-notification-for-big-orders/,"üß® VIP Radar: Instantly Spot & Summarize High-Value Shopify Orders with AI + Slack Alerts
Automatically detect when a new Shopify order exceeds $200, fetch the customer‚Äôs purchase history, generate an AI-powered summary, and alert your team in Slack‚Äîso no VIP goes unnoticed.
üõ†Ô∏è Workflow Overview
Feature Description
Trigger Shopify ‚ÄúNew Order‚Äù webhook
Conditional Check Filters for orders > $200
Data Enrichment Pulls full order history for the customer from Shopify
AI Summary Uses OpenAI to summarize buying behavior
Notification Sends detailed alert to Slack with name, order total, and customer insights
Fallback Ignores low-value orders and terminates flow
üìò What This Workflow Does
This automation monitors your Shopify store and reacts to any high-value order (over $200). When triggered:
It fetches all past orders of that customer,
Summarizes the history using OpenAI,
Sends a full alert with context to your Slack channel.
No more guessing who‚Äôs worth a closer look. Your team gets instant insights, and your VIPs get the attention they deserve.
üß© Node-by-Node Breakdown
üîî 1. Trigger: New Shopify Order
Type: Shopify Trigger
Event: orders/create
Purpose: Starts workflow on new order
Pulls: Order total, customer ID, name, etc.
üî£ 2. Set: Convert Order Total to Number
Ensures the total_price is treated as a number for comparison.
‚ùì 3. If: Is Order > $200?
Condition: $json.total_price &gt; 200
Yes ‚Üí Continue
No ‚Üí End workflow
üîó 4. HTTP: Fetch Customer Order History
Uses the Shopify Admin API to retrieve all orders from this customer.
Requires your Shopify access token.
üßæ 5. Set: Convert Orders Array to String
Formats the order data so it's prompt-friendly for OpenAI.
üß† 6. LangChain Agent: Summarize Order History
Prompt: ""Summarize the customer's order history for Slack. Here is their order data: {{ $json.orders }}""
Model: GPT-4o Mini (customizable)
üì® 7. Slack: Send VIP Alert
Sends a rich message to a Slack channel.
Includes:
Customer name
Order value
Summary of past behavior
üß± 8. No-Op (Optional)
Used to safely end workflow if the order is not high-value.
üîß How to Customize
What How
Order threshold Change 200 in the If node
Slack channel Update channelId in the Slack node
AI prompt style Edit text in LangChain Agent node
Shopify auth token Replace shpat_abc123xyz... with your actual private token
üöÄ Setup Instructions
Open n8n editor.
Go to Workflows ‚Üí Import ‚Üí Paste JSON.
Paste this workflow JSON.
Replace your Shopify token and Slack credentials.
Save and activate.
Place a test order in Shopify to watch it work.
üí° Real-World Use Cases
üéØ Notify sales team when a potential VIP buys
üõéÔ∏è Prep support reps with customer history
üìà Detect repeat buyers and upsell opportunities
üîó Resources & Support
üë®‚Äçüíª Creator: Yaron Been
üì∫ YouTube: NoFluff with Yaron Been
üåê Website: https://nofluff.online
üì© Contact: Yaron@nofluff.online
üè∑Ô∏è Tags
#shopify, #openai, #slack, #vip-customers, #automation, #n8n, #workflow, #ecommerce, #customer-insights, #ai-summaries, #gpt4o"
Automate Your LinkedIn Engagement with AI-Powered Comments! üí¨‚ú®,https://n8n.io/workflows/4357-automate-your-linkedin-engagement-with-ai-powered-comments/,"Tutorial Video
This n8n workflow is your ultimate tool for smart LinkedIn interaction, leveraging the power of AI to generate witty comments and engage with posts via the Unipile API. Perfect for community managers, marketers, or anyone looking to scale their professional presence with intelligent automation!
How It Works:
Trigger by Chat Message: Simply send a chat message (e.g., from Telegram) containing a LinkedIn post ID. üì®
Extract Post ID with LLM: An intelligent Large Language Model (LLM) precisely extracts the LinkedIn post ID from your message, ready for action. üß†
Get Post Details: The workflow fetches all the juicy details of the target LinkedIn post from Unipile. üì•
AI-Crafted Comment: An OpenAI LLM, acting as your personal AI & startup expert, generates a unique, witty, and cheeky comment tailored to the post's content. No boring, generic replies here! ‚úíÔ∏èü§ñ
Publish & React: The generated comment is then published to the LinkedIn post via Unipile, and a reaction (like a 'like' or 'upvote') is automatically added to boost engagement. üëçüí¨
Confirmation to Telegram: Get instant feedback! A confirmation message is sent back to your Telegram, showing the post URL and the exact comment that was shared. ‚úÖ
Why You'll Love This Template:
Intelligent Engagement: Move beyond simple replies with AI-powered comments that resonate on a professional platform.
Time-Saving Automation: Automate repetitive LinkedIn tasks and free up your schedule for more strategic activities.
Scalable: Easily adapt and expand this workflow for various professional engagement types.
Customizable: Tweak the LLM prompts to match your professional brand's voice and desired tone.
Get Started:
Unipile API Key: Secure your UNIPILE_API_KEY and set it as an environment variable in n8n.
Unipile Account ID: The account_id (e.g., PXAEQeyiS2iSkSJCRuNcvg) is currently hardcoded within the HTTP Request nodes. For a production setup, consider making this dynamic or using n8n credentials if Unipile offers them.
OpenAI Credentials: Ensure your OpenAI API key is configured as an n8n credential.
Telegram Integration: Configure the Trigger: Chat Message Received node and the Telegram: Send Confirmation node with your Telegram Bot Token and Chat ID. The confirmation node is currently disabled; enable it to receive notifications.
Ready to supercharge your LinkedIn engagement? Give it a try! üöÄ"
Auto-Scrape TikTok User Data via Dumpling AI and Segment in Airtable,https://n8n.io/workflows/4326-auto-scrape-tiktok-user-data-via-dumpling-ai-and-segment-in-airtable/,"Who is this for?
This workflow is for social media agencies, influencer marketers, and brand managers who need to automatically qualify TikTok creators based on their follower metrics. It‚Äôs especially useful for teams managing influencer outreach campaigns or building talent databases.
What problem is this workflow solving?
Manually tracking TikTok user stats is time-consuming and inconsistent. This automation instantly pulls TikTok profile data and only saves creators who meet a defined follower threshold. It removes manual vetting, reduces spreadsheet work, and makes influencer qualification scalable.
What this workflow does
This workflow uses Airtable as the trigger, Dumpling AI to scrape TikTok profile information, and a logic condition to check if the profile has more than 100k followers. Qualified profiles are updated with full metrics and stored back in Airtable.
Setup
Airtable Setup
Create a table with a field named Tik tok username
Connect your Airtable account to n8n using a Personal Access Token
Set up a trigger to run when a new TikTok username is added
Dumpling AI
Sign up at Dumpling AI
Create a Dumpling AI credential in n8n using your API key
The HTTP node sends the TikTok handle to Dumpling‚Äôs /get-tiktok-profile endpoint
Configure Filter
The IF node checks if followerCount is greater than or equal to 100000
Airtable Update
If qualified, the record is updated with:
ID (TikTok ID)
followerCount
followingCount
heartCount
videoCount
How to customize this workflow to your needs
Change the follower count threshold to fit your campaign (e.g. 10K, 500K, 1M)
Add fields like engagement rate, niche tags, or scraped bio
Chain additional steps like sending approved creators to your CRM or triggering outreach messages
Add another filter to exclude private or inactive accounts"
Transform Raw Hiring Transcripts into Briefs & Scorecards with AI and Google Docs,https://n8n.io/workflows/4228-transform-raw-hiring-transcripts-into-briefs-and-scorecards-with-ai-and-google-docs/,"Who is this template for ?
Basically anyone involved in recurring recruiting processes and looking to save a considerable amount of time and energy (Talent acquisitions Managers, recruiting consultants, hiring managers, founders‚Ä¶etc)
What it does :
It takes a messy and raw transcript from an ‚Äúintake meeting‚Äù between a recruiter and a Hiring manager and turns it into a clean and exhaustive brief + scorecard templates for each interview rounds
It does it under 1 MINUTE while the usual ‚Äúmanual‚Äù process usually takes several hours
How to customize this workflow to your needs
Google doc is the default choice because it allows easy modification of the output, but you can choose to output this under any format and / or store it wherever you want
I strongly suggest to choose one of the latest LLM models for better output quality
Both LLM prompts can be revised to match your expectations better"
OpenAI Responses API Adapter for LLM and AI Agent Workflows,https://n8n.io/workflows/4218-openai-responses-api-adapter-for-llm-and-ai-agent-workflows/,"This n8n template demonstrates how to use OpenAI's Responses API with existing LLM and AI Agent nodes.
Though I would recommend just waiting for official support, if you're impatient and would like a round-about way to integrate OpenAI's responses API into your existing AI workflows then this template is sure to satisfy!
This approach implements a simple API wrapper for the Responses API using n8n's builtin webhooks. When the base url is pointed to these webhooks using a custom OpenAI credential, it's possible to intercept the request and remap for compatibility.
How it works
An OpenAI subnode is attached to our agent but has a special custom credential where the base_url is changed to point at this template's webhooks.
When executing a query, the agent's request is forwarded to our mini chat completion workflow.
Here, we take the default request and remap the values to use with a HTTP node which is set to query the Responses API.
Once a response is received, we'll need to remap the output for Langchain compatibility. This just means the LLM or Agent node can parse it and respond to the user.
There are two response formats, one for streaming and one for non-streaming responses.
How to use
You must activate this workflow to be able to use the webhooks.
Create the custom OpenAI credential as instructed.
Go to your existing AI workflows and replace the LLM node with the custom OpenAI credential. You do not need to copy anything else over to the existing template.
Requirements
OpenAI account for Responses API
Customising this workflow
Feel free to experiment with other LLMs using this same technique!
Keep up to date with the Responses API announcements and make modifications as required."
Customer Authentication for Chat Support with OpenAI and Redis Session Management,https://n8n.io/workflows/4216-customer-authentication-for-chat-support-with-openai-and-redis-session-management/,"This n8n template demonstrates one approach to customer authentication via chat agents.
Unlike approaches where you have to authenticate users prior to interacting with the agent, this approach allows guest users to authenticate at any time during the session or not at all.
Note about Security: this template is for illustration purposes only and requires much more work to be ready for production!
How it works
A conversational agent is used for this demonstration. The key component is the Redis node just after the chat trigger which acts as the session context.
For guests, the session item is blank. for customers, the session item is populated with their customer profile.
The agent is instructed to generate a unique login URL only for guests when appropriate or upon request.
This login URL redirects the guest user to a simple n8n form also hosted in this template. The login URL has the current sessionID as a query parameter as the way to pass this data to the form.
Once login is successful, the matching session item by sessionId is populated with the customer profile. The user can now return to the chat window.
Back to the agent, now when the user sends their next message, the Redis node will pick up the session item and the customer profile associated with it. The system prompt is updated with this data which let's the agent know the user is now a customer.
How to use
You'll need to update the ""auth URL"" tool to match the URL of your n8n instance. Better yet, copy the production URL of your form from the trigger.
Activate the workflow to turn on production mode which is required for this workflow.
Implement the authentication logic in step 3. This could be sending the user and pass to a postgreSQL data for validation.
Requirements
OpenAI for LLM (feel free to swap to any provider)
Redis for Cache/Sessions (again, feel free to swap this out for postgresql or other database)
Customising this workflow
Consider not populating the session item with the user data as it can become stale. Instead, just add the userId and instruct the agent to query using tools.
Extend the Login URL idea by experimenting with signup URLs or single-use Urls."
"Automate AI Phone Booking & CRM Updates with GPT-4, VAPI.ai, and GHL",https://n8n.io/workflows/3759-automate-ai-phone-booking-and-crm-updates-with-gpt-4-vapiai-and-ghl/,"Automate AI-driven appointment booking with phone call confirmations, real-time lead validation, CRM updates, and email notifications‚Äîpowered by n8n, OpenAI, VAPI.ai, and GoHighLevel.
Tools & Services Used
GoHighLevel (CRM & appointment management)
VAPI.ai (AI phone calls)
OpenAI (GPT-4 for lead validation and post-call analysis)
Email Service (transactional email notifications)
n8n (Self-hosted required for Community Nodes)
Workflow Overview
This automation performs the following steps:
Trigger: A new lead arrives via the GHL Webhook.
Validation: OpenAI (GPT-4) checks lead quality and relevance.
AI Phone Call: VAPI.ai initiates an automated call to confirm appointment details.
Post-Call Analysis: OpenAI interprets the call result (booking_made=YES/NO).
Conditional Actions:
If YES: Updates GoHighLevel CRM and sends confirmation emails to the client and owner.
If NO: Alerts the owner via email about the failed booking.
Prerequisites
Active accounts and API keys for:
GoHighLevel (webhook setup)
VAPI.ai (phone call API)
OpenAI (GPT-4 access)
Email service (e.g., SMTP, SendGrid, or Gmail)
How to Use This Template
Step 1: Import the Template
Import the JSON into your self-hosted n8n instance (requires Community Nodes like @n8n/n8n-nodes-langchain).
Step 2: Configure Credentials
GHL Webhook: Replace YOUR_WEBHOOK_ID with your GoHighLevel webhook ID.
OpenAI Nodes: Add your OpenAI API key (replace placeholder BxLbA94QZt0ifZsC).
VAPI Call Node: Ensure the VAPI.ai endpoint (https://api.vapi.ai/call/phone) has valid API credentials.
Email Nodes: Configure SMTP or API credentials for your email service.
Step 3: Enable Disabled Nodes
Activate the Post-call Analysis node if needed (disabled by default).
Finding Your GHL Webhook ID
In GoHighLevel, navigate to Automations ‚Üí Webhooks.
Create a new webhook and copy its unique ID into the GHL Webhook node.
Initial Test Run
Simulate a Lead: Manually trigger the GHL webhook with test lead data.
Debugging:
Verify OpenAI validates the lead.
Check if VAPI.ai initiates a call (use a test phone number).
Confirm CRM updates and emails are sent conditionally.
Production Prep:
Enable error-handling loops for failed calls.
Adjust GPT-4 prompts for stricter validation.
Use Cases
Medical Clinics: Reduce no-shows with automated appointment confirmations.
Salons/Spas: Streamline booking updates and client reminders.
Consultants: Sync client meetings to CRM in real time.
Disclaimer
Requires self-hosted n8n (Community Nodes are unsupported on n8n Cloud).
Test phone call and email nodes extensively before scaling."
Personalise Outreach Emails using Customer data and AI,https://n8n.io/workflows/3397-personalise-outreach-emails-using-customer-data-and-ai/,"This n8n template uses existing emails from customers as context to customise and ""finetune"" outreach emails to them using AI.
By now, it should be common knowledge that we can leverage AI to generate unique emails but in a way, they can remain generic as the AI lacks the customer context to be truly personalised. One way to solve this is by pulling in a source of customer data - and what better way then by using existing email correspondence.
How it works
Customers to target are pulled from Hubspot and each customer is then run in a loop. We're using a loop as the retrieved emails for each customer become separate items and a loop helps with item reference.
We connect to our Gmail account to pull all emails recieved from the customer.
The contents of the email will be suitable to build a short persona of the customer. We use the Information Extractor to get our AI model to pull out the key attributes of this persona such as decision making style and communication preferences.
With this persona, we can now pass this to our AI model to generate a personalised outreach email specifically for our customer.
Finally, a draft email is created for human review before sending. If you would rather send the email straight away, this is also possible.
How to use
Define the topic of the outreach email in the ""variables"" node. This directs the AI on what outreach email to generate.
Ensure the emails are pulled from the right account. If emails may contain sensitive data, adjust the filters and text parsing to ensure these are not leaked to the AI (which might then leak into the generated email).
Requirements
Hubspot for Contacts List
OpenAI for LLM
Gmail for Existing Emails and Sending Emails
Customising this workflow
Not using Hubspot? Any CRM would work just as well or even a simple text csv!
If you have customer past deals or engagements in your CRM, consider using this as additional context for the AI to use."
Automatically Create JIRA Issues from Outlook Email Support Requests,https://n8n.io/workflows/3893-automatically-create-jira-issues-from-outlook-email-support-requests/,"This n8n template watches an outlook shared inbox for support messages and creates an equivalent issue item in JIRA.
How it works
A scheduled trigger fetches recent Outlook messages from an shared inbox which collects support requests.
These support requests are filtered to ensure they are only processed once and their HTML body is converted to markdown for easier parsing.
Each support request is then triaged via an AI Agent which adds appropriate labels, assesses priority and summarises a title and description of the original request.
Finally, the AI generated values are used to create an issue in JIRA to be actioned.
How to use
Ensure the messages fetched are solely support requests otherwise you'll need to classify messages before processing them.
Specify the labels and priorities to use in the system prompt of the AI agent.
Requirements
Outlook for incoming support
OpenAI for LLM
JIRA for issue management
Customising this workflow
Consider automating more steps after the issue is created such as attempting issue resolution or capacity planning."
"Automate Employee Onboarding with Slack, Jira, and Google Workspace Integration",https://n8n.io/workflows/3860-automate-employee-onboarding-with-slack-jira-and-google-workspace-integration/,"Who is this for?
This template is ideal for HR teams, startup founders, operations leads, remote-first companies, and freelancers managing onboarding manually or across multiple tools.
Whether you‚Äôre hiring your first intern or streamlining onboarding for dozens of new team members, this workflow automates the entire trigger ‚Üí task creation ‚Üí onboarding delivery process using no-code tools connected via n8n.
What problem does this solve?
Employee onboarding is often fragmented and error-prone ‚Äî involving Jira tasks, Slack DMs, file sharing, and email templates spread across different apps.
This workflow connects it all, letting you create tasks, notify people, share folders, and message hires instantly ‚Äî right when they‚Äôre marked as ‚ÄúHired‚Äù in a Google Sheet or added to Slack.
No more bouncing between 5 tabs. This is onboarding as a system.
What this workflow does
‚úÖ Watches a Google Sheet (or Slack trigger) for new hires
üìã Creates a Jira onboarding Epic with role-based subtasks
üìÇ Generates a Google Drive folder and shares it with the new hire
üì¨ Sends a personalized, HTML-formatted welcome email via Gmail
üí¨ Posts a Slack message in the #onboarding channel + DM to the hire
üìä Logs onboarding activity back to the Sheet (status, links, etc.)
üîÅ Modular subtasks based on role/team (e.g. Dev vs Marketing vs Student)
üß† Expressions, fallback logic, and Slack‚ÜíJira user ID mapping built-in
Setup
Create API credentials for:
Slack
Google Sheets
Google Drive
Jira Cloud
Gmail (API or SMTP)
Import the .json workflow into your n8n instance (Cloud or self-hosted)
Replace placeholder values (e.g. Project ID, Issue Type ID, folder path)
Customize onboarding messages, email HTML, and task logic
Test with included mock data (sample hire, Slack user, Jira user)
Activate your flow and start onboarding with one click
üóí Color-coded workflow notes
This workflow uses Innovatio‚Äôs sticky note system to guide you visually:
üü© Green Notes ‚Üí Main automation steps
üü¶ Blue Notes ‚Üí What to customize (IDs, prompts, expressions)
üü® Yellow Notes ‚Üí Optional logic and future upgrades
üü´ Gray Notes ‚Üí Welcome + final ‚Äúnext steps‚Äù CTA
Every group includes pre-filled nodes, dynamic expressions, and mock outputs so you can test quickly and scale confidently.
How to customize this workflow?
üìÇ Swap Google Sheets for Airtable (better for teams or approvals)
üìÖ Add 30/60/90-day check-ins via Google Calendar
ü§ñ Plug in OpenAI to generate onboarding subtasks by role
üì¨ Auto-assign mentors or SlackBot reminders after 7 days
üìä Push Sheet logs into Notion or your HR dashboard
Need custom logic? Email me at velebit@innovatio.design
Final notes
This template was created by Velebit from Innovatio with modularity, scalability, and team experience in mind.
All links included are official. No affiliate tracking, no sponsored content.
üìú A separate commercial license applies to the purchase via Gumroad, which includes extended rights, usage for client work, and scaling strategies."
Google Trend Data Extract & Summarization with Bright Data & Google Gemini,https://n8n.io/workflows/3854-google-trend-data-extract-and-summarization-with-bright-data-and-google-gemini/,"Who this is for
The Google Trend Data Extract & Summarization workflow is ideal for trend researchers, digital marketers, content strategists, and AI developers who want to automate the extraction, summarization, and distribution of Google Trends data. This end-to-end solution helps transform trend signals into human-readable insights and delivers them across multiple channels.
It is built for:
Market Researchers - Tracking trends by topic or region
Content Strategists - Identifying content opportunities from trending data
SEO Analysts - Monitoring search volume and shifts in keyword popularity
Growth Hackers - Reacting quickly to real-time search behavior
AI & Automation Engineers - Creating automated trend monitoring systems
What problem is this workflow solving?
Google Trends data can provide rich insights into user interests, but the raw data is not always structured or easily interpretable at scale. Manually extracting, cleaning, and summarizing trends from multiple regions or categories is time-consuming.
This workflow solves the following problems:
Automates the conversion of markdown or scraped HTML into clean textual input
Transforms unstructured data into structured format ready for processing
Uses AI summarization to generate easy-to-read insights from Google Trends
Distributes summaries via email and webhook notifications
Persists responses to disk for archiving, auditing, or future analytics
What this workflow does
Receives input: Sets an URL for the data extraction and analysis.
Uses Bright Data‚Äôs Web Unlocker to extract content from relevant site.
Markdown to Textual Data Extractor: Converts markdown content into plaintext using n8n‚Äôs Function or Markdown nodes
Structured Data Extract: Parses the plaintext into structured JSON suitable for AI processing
Summarize Google Trends: Sends structured data to Google Gemini with a summarization prompt to extract key takeaways
Send Summary via Gmail: Composes an email with the AI-generated summary and sends it to a designated recipient
Persist to Disk: Writes the AI structured data to disk
Webhook Notification: Sends the summarized response to an external system (e.g., Slack, Notion, Zapier) using a webhook
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Set URL and Bright Data Zone for setting the brand content URL and the Bright Data Zone name.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Update Source : Update the workflow input to read from Google Sheet or Airbase etc.
Gemini Prompt Tuning :
Customize prompts to extract summaries like:
Summarize the most significant trend shifts
Generate content ideas from the trending search topics
Email Personalization : Configure Gmail node to:
Use dynamic subject lines like: Weekly Google Trends Summary ‚Äì {{date}}
Send to multiple stakeholders or mailing lists
File Storage Customization :
Save with timestamps, e.g., trends_summary_2025-04-29.json
Extend to S3 or cloud drive integrations
Webhook Use Cases :
Send summary to:
Internal dashboards
Slack channels
Automation tools like Make, Zapier etc."
Automate Etsy Data Mining with Bright Data Scrape & Google Gemini,https://n8n.io/workflows/3851-automate-etsy-data-mining-with-bright-data-scrape-and-google-gemini/,"Who this is for?
The Automate Etsy Data Mining with Bright Data Scrape & Google Gemini workflow is designed for eCommerce analysts, product researchers, and AI developers seeking to extract actionable insights from Etsy listings at scale.
It is ideal for:
eCommerce Entrepreneurs - Researching product demand and competition.
Market Analysts - Tracking pricing, reviews, and trends across Etsy categories.
Product Managers - Identifying niche opportunities and design inspirations.
Data Scientists & AI Engineers - Automating product intelligence pipelines.
Growth Hackers - Leveraging Etsy insights to refine product-market fit.
What problem is this workflow solving?
Manually browsing Etsy to analyze product listings, pricing, reviews, and seller activity is slow, inconsistent, and unscalable. Scraping Etsy requires unlocking JavaScript-heavy content and structuring noisy data for analysis.
This workflow solves:
Automated and scalable scraping of Etsy product listings using Bright Data‚Äôs infrastructure.
A fully paginated data structured Estry production data extraction via the Google Gemini LLM.
Enables faster decision-making for product research and competitive analysis via the fully automated paginated data extraction.
What this workflow does
Receives input: Sets the Esty URL for the data extraction and analysis.
Uses Bright Data's Web Unlocker to extract content from relevant sites.
Cleans and preprocesses the scraped content for readability.
Sends the content to Google Gemini for:
Enriched results including:
Data persistence over the disk.
Sends the response to a target system via Webhook notification.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the Set Esty Search Query for setting the brand content URL and the Bright Data Zone name.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
Input Sources : Replace the static URL with dynamic input from Google Sheets, Webhook, or Airtable to research multiple niches.
Prompt Customization : Adjust Gemini prompts to extract specific insights for example:
List key features of the product
Summarization of the review themes
Data Output Options : Update the Webhook notification to save data to:
Google Sheets
Notion or Airtable
SQL/NoSQL
Slack/Email"
"Generate AI Songs + Music Videos Using Suno API, Flux, Runway and Creatomate",https://n8n.io/workflows/3814-generate-ai-songs-music-videos-using-suno-api-flux-runway-and-creatomate/,"This automation is designed to help you generate AI-powered music tracks, cover art, and fully rendered music videos ‚Äî all triggered from a simple Telegram chat and managed via Google Sheets.
Sample Audio Generated: Youtube Link
‚ö†Ô∏è Note: This is only a summary. The full setup guide (with detailed steps, API setup, code snippets, and integrations) is included in a PDF on the Gumroad download page. Follow it to configure everything seamlessly.
‚öôÔ∏è What This Workflow Does
Lets users interact with an AI agent on Telegram to submit song ideas.
Automatically logs ideas in Google Sheets.
Generates lyrics, music, cover images, and looping video backgrounds using AI APIs.
Merges audio + video into a polished final clip.
Uploads everything to Google Drive and sends a final video link back to the Telegram user.
üîó Built with 3 Modular Workflows
Telegram & Sheet Logger
Collects ideas from users, stores them in a Google Sheet, and saves their chat ID for later notifications.
AI Generation Engine
Handles lyrics creation, music generation, image prompts, artwork, video background generation, and uploads.
Final Render + Delivery
Merges audio and video, uploads to cloud storage, updates the Sheet, and sends the finished video to Telegram.
üß© Integrations Used
Google Sheets
Telegram Bot API
Multiple AI APIs (for music, images, video, and compression)
Google Apps Script
Creatomate for video rendering
Cloud storage (via Google Drive)
üöÄ How to Set It Up
Import the .json file into your n8n instance.
Copy the included Google Sheets and Creatomate templates.
Set up the Google Apps Script using the included code.
Replace all placeholder API keys and folder IDs.
Test everything end-to-end using your Telegram bot.
‚úÖ Tip: No coding experience needed ‚Äî just follow the step-by-step documentation PDF provided in your Gumroad download. Everything is laid out clearly to get you running fast.
üí∞Costs Song generation costs (suno): $5 gets you 1000 credits, 1 song generation costs 12 credits, every api call for one song generates two versions by default. Check the song generation node to see the two versions. You can update the GSheet to append the two versions and send back to you to choose one. Flux Image gen cost is $0.04 per image. Runway cost is $0.16 per 10 seconds of video."
The Ultimate Instagram Automation for High-Quality Images & Text with GPT-Image,https://n8n.io/workflows/3741-the-ultimate-instagram-automation-for-high-quality-images-and-text-with-gpt-image/,"This n8n workflow revolutionizes Instagram content creation by automating everything from idea input to publishing high-quality, AI-generated posts with realistic images or infographics. Whether you're an entrepreneur, a content creator, or a marketer, this workflow lets you consistently deliver professional-grade posts without manual effort.
It leverages power of OpenAI Image Generator to generate engaging captions, create stunning visuals, and publish directly to Instagram ‚Äî fully automated!
It allows you to generate and post highly relevant images with statistics, graphs, and charts, hyper-realistic images, or any custom image style you want.
What is included?
‚úÖ 1 n8n Workflow (.json) file
‚úÖ 4 Video Guidance Tutorials:
Setup Tutorial: How to set up this workflow from scratch.
Instagram Connection Tutorial: How to connect n8n to Instagram (and all other Facebook products).
Google Cloud Storage Connection Tutorial: How to upload and host images on Google Cloud.
Google Product Integration Tutorial: How to connect n8n with all Google products.
Who is this for?
This template is ideal for:
Content creators who want to automate Instagram posting with AI assistance.
Entrepreneurs and brands aiming to build a consistent social media presence.
Social media managers seeking to save time while maintaining high-quality output.
Anyone looking to auto-generate professional posts without needing graphic design skills.
What problem is this workflow solving?
Building a consistent, high-quality Instagram feed is time-consuming.
This workflow solves key challenges by:
Automating research, writing, image generation, hosting, and publishing.
Saving hours of manual content creation work each week.
Allowing easy scalability of your Instagram marketing efforts.
Giving the option to create data-driven infographics or hyper-realistic images.
Ensuring posts stay engaging, informative, and visually appealing ‚Äî without creative burnout.
What this workflow does
This workflow automates the following steps:
Idea Input: Accepts a post idea through a form or scheduled posting based on a default niche.
Research & Caption Generation: Uses Web Search and GPT to research the topic and generate an engaging Instagram caption with trending hashtags.
Image Generation:
Option 1: Generate an infographic with statistics, graphs, and charts.
Option 2: Create a hyperrealistic, AI-generated photo based on real-world elements (fully customizable to any image style).
Publishing: Posts the image and caption automatically to Instagram via the Facebook Graph API.
Setup
Trigger Options:
By Schedule: Configure regular post publishing.
By Form: Submit a post idea anytime manually.
Choose Image Style:
Enable graphs/statistics for data-driven visuals.
Choose hyperrealistic images for lifestyle, travel, fashion, and more.
Customize Language and Niche:
Set your default language (English by default) and niche topic.
API Keys:
Insert your OpenAI API Key and Tavily API Key inside the workflow for activation.
Connect Your Accounts:
OpenAI for text and image generation.
Google Cloud Storage for image hosting (please refer to the video guidance).
Facebook Graph API for publishing to Instagram (please refer to the video guidance)..
How to Customize This Workflow
Post Style: Adjust the AI prompt settings to tweak the tone and style of your Instagram captions.
Image Look: Customize the image generation prompt to change image style, themes, and resolutions.
Frequency: Modify schedule triggers to post as frequently (or infrequently) as you like.
Category
Marketing | Social Media Automation | Content Creation"
"Scrape Books from URL with Dumpling AI, Clean HTML, Save to Sheets, Email as CSV",https://n8n.io/workflows/3701-scrape-books-from-url-with-dumpling-ai-clean-html-save-to-sheets-email-as-csv/,"üë• Who is this for?
This workflow is ideal for virtual assistants, researchers, developers, automation specialists, and data analysts who need to regularly extract and organize structured product information (like books) from a website. It‚Äôs especially useful for those working with catalog-based websites who want to automate extraction and delivery of clean, sorted data.
üß© What problem is this solving?
Manually copying product listings like book titles and prices from a website into a spreadsheet is slow and repetitive. This automation solves that problem by scraping content using Dumpling AI, extracting the right data using CSS selectors, and formatting it into a clean CSV file that is sent to your email‚Äîall triggered automatically when a new URL is added to Google Sheets.
‚öôÔ∏è What this workflow does
This template automates an entire content scraping and delivery process:
Watches a Google Sheet for new URLs
Scrapes the HTML content of the given webpage using Dumpling AI
Uses CSS selectors in the HTML node to extract each book from the page
Splits the HTML array into individual items
Extracts the book title and price from each HTML block
Sorts the books in descending order based on price
Converts the sorted data to a CSV file
Sends the CSV via email using Gmail
üõ†Ô∏è Setup
Google Sheets
Create a sheet titled something like URLs
Add your product listing URLs (e.g., http://books.toscrape.com)
Connect the Google Sheets trigger node to your sheet
Ensure you have proper credentials connected
Dumpling AI
Create an account at Dumpling AI - Generate your API key
Set the HTTP Method to POST and pass the URL dynamically from the Google Sheet
Use Header Auth to include your API key in the request header
Make sure ""cleaned"": ""True"" is included in the body for optimized HTML output
HTML Node
The first HTML node extracts the main book container blocks using:
.row &gt; li
The second HTML node parses out the individual fields:
title: h3 &gt; a (via the title attribute)
price: .price_color
Sort Node
Sorts books by price in descending order
Note: price is extracted as a string, ensure it's parsable if you plan to use numeric filtering later
Convert to CSV
The JSON data is passed into a Convert node and transformed into a CSV file
Gmail
Sends the CSV as an attachment to a designated email
üîÑ How to customize this workflow
Extract more data: Add more CSS selectors in the second HTML node to pull fields like author, availability, or product links
Switch destinations: Replace Gmail with Slack, Google Drive, Dropbox, or another platform
Adjust sorting: Sort alphabetically or based on another extracted value
Use a different source: As long as the site structure is consistent, this can scrape any listing-like page
Trigger differently: Use a webhook, form submission, or schedule trigger instead of Google Sheets
‚ö†Ô∏è Dependencies and Notes
This workflow uses Dumpling AI to perform the web scraping. This requires an API key and uses credits per request.
The HTML node depends on valid CSS selectors. If the site layout changes, the selectors may need to be updated.
Ensure you‚Äôre not scraping content from websites that prohibit automated scraping."
Transform Gmail Newsletters into Insightful LinkedIn Posts Using OpenAI,https://n8n.io/workflows/3509-transform-gmail-newsletters-into-insightful-linkedin-posts-using-openai/,"Who Is This For?
This workflow is perfect for content creators, marketers, and business professionals who receive regular newsletters and want to effortlessly convert them into engaging LinkedIn posts. By automating the extraction and repurposing process, you can save time and consistently share thoughtful updates with your network.
What Problem Does This Workflow Solve?
Manually reading newsletters, extracting the key points, and then formatting that content into professional, engaging LinkedIn posts can be time-consuming and error-prone. This workflow automates those steps by:
Filtering Emails: Uses the Gmail node to process only those emails from a specific sender (e.g., newsletter@example.com).
Extracting Content: Leverages OpenAI to identify and summarize the top news items in your newsletter.
Generating Posts: Crafts concise, insightful LinkedIn posts in a smart, deadpan style with a touch of subtle humor.
Publishing: Posts the generated content directly to LinkedIn.
What This Workflow Does
Filter Newsletters: The Gmail node is set up to only handle emails from your chosen sender, ensuring that only relevant newsletters are processed.
Extract Key Content: An OpenAI node analyzes the newsletter text to pull out the most important news items, including headlines and summaries.
Split Content: A Split Out node divides the extracted content so each news item is processed on its own.
Generate LinkedIn Posts: Another OpenAI node takes each news item's details and produces a well-structured LinkedIn post that delivers practical insights and ends with a reflective observation or question.
Publish to LinkedIn: The LinkedIn node publishes the crafted posts directly to your account.
Setup
Gmail Node: Rename it to ‚ÄúFilter Gmail Newsletter‚Äù and configure it to filter emails by your newsletter sender.
OpenAI Nodes: Ensure your OpenAI API credentials are set up correctly. Customize the prompt if needed to match your desired tone.
LinkedIn Node: Rename it to ‚ÄúPost to LinkedIn‚Äù and confirm that your LinkedIn OAuth2 credentials are properly configured.
How to Customize
OpenAI Prompts: Adjust the prompts in the OpenAI nodes to fine-tune the post tone and output formatting.
Email Filter: Change the Gmail filter to match the sender of your newsletters.
Post Processing: Optionally, add extra formatting (using Function nodes) to further enhance the readability of the generated LinkedIn posts.
This template offers an automated, hands-off solution to transform your newsletter content into engaging LinkedIn updates, keeping your audience informed and inspired with minimal effort."
Scrape Competitor Reviews & Generate Ad Creatives with Bright Data & OpenAI,https://n8n.io/workflows/3624-scrape-competitor-reviews-and-generate-ad-creatives-with-bright-data-and-openai/,"Scrape Competitor Reviews & Generate Ad Creatives with Bright data and OpenAI
How the Flow Runs
Fill the Form
Enter the Amazon product URL to analyze competitor reviews.
Trigger Bright Data Scraper
Bright Data scrapes Amazon reviews based on the provided URL.
Wait for Snapshot Completion
Periodically checks Bright Data until the scraping is complete.
Retrieve JSON Data
Collects the scraped review data in JSON format.
Save Reviews to Google Sheets
Automatically appends the scraped reviews to your Google Sheets.
Aggregate Reviews
Consolidates all reviews into a single summary for simpler analysis.
Analyze Reviews with OpenAI LLM
Sends the aggregated reviews to OpenAI (GPT-4o mini) to summarize competitors‚Äô main weaknesses clearly.
Generate Creative Ad Image
OpenAI generates a visually appealing 1080x1080 ad image addressing these identified pain points.
Send Ad Creative via Gmail
Automatically emails the creative and review summary to your media buying team for immediate use in Meta ads.
What You Need
Google Sheets: Template
Bright Data: Dataset and API key:
www.brightdata.com
OpenAI API Key: For GPT-4o mini or your preferred LLM
Automation Tool: Ensure it supports HTTP Requests, Wait, Conditional (If), Google Sheets integration, Form Trigger, OpenAI integration, and Gmail integration.
Form Fields to Fill
Amazon Product URL: Enter the competitor‚Äôs product URL from Amazon.
Setup Steps
Copy the provided Google Sheet template.
Import the JSON workflow into your automation tool.
Update credentials for Bright Data, Google Sheets, Gmail, and OpenAI.
Test manually by submitting the form and verifying functionality.
Optional: Set a schedule for regular workflow execution.
Bright Data Trigger Example
[
  {
    ""url"": ""https://www.amazon.com/example-product""
  }
]

Tips
Frequently update URLs to ensure fresh insights.

Allow more wait time for extensive data scrapes.

Focus on targeted products to optimize cost-efficiency.

Need Help?
Email: Yaron@nofluff.online

Resources:

YouTube:
https://www.youtube.com/@YaronBeen/videos

LinkedIn:
https://www.linkedin.com/in/yaronbeen/

Bright Data Documentation:
https://docs.brightdata.com/introduction"
"Document Analysis & Chatbot Creation with Llama Parser, Gemini LLM & Pinecone DB",https://n8n.io/workflows/3606-document-analysis-and-chatbot-creation-with-llama-parser-gemini-llm-and-pinecone-db/,"üìÑDescription
This automation workflow enables users to upload files via an N8N form, automatically analyzes the content using Google Gemini agents, and delivers the analyzed results via email along with a chatbot link. The system leverages Llama Cloud API, Google Gemini LLM, Pinecone vector database, and Gmail to provide a seamless, multilingual content analysis experience.
‚úÖ Prerequisites
Before setting up this workflow, ensure the following are in place:
An active N8N instance.
Access to Llama Cloud API.
Google Gemini LLM API keys (for Translator & Analyzer agents).
A Pinecone account with an active index.
A Gmail account with API access configured.
Basic knowledge of N8N workflow setup.
‚öôÔ∏è Setup Instructions
Deploy the N8N Form
Create a public-facing form using N8N.
Configure it to accept:
File uploads.
User email input.
File Preprocessing
Store the uploaded files temporarily.
Organize and preprocess them as needed.
Content Extraction using Llama Cloud API
Feed the files into the Llama Cloud API.
Extract and parse the content for further processing.
Translation (if required)
Use a Translator Agent (Google Gemini).
Check if the content is in English. If not, translate it.
Content Analysis
Forward the (translated) content to the Analyzer Agent (Google Gemini).
Perform deep analysis to extract insights.
Vector Storage in Pinecone
Store both:
The parsed and translated content.
The analyzed content.
Use Pinecone to store the content as embeddings for chatbot use.
User Notification via Gmail
Send the analyzed content and chatbot link to the user‚Äôs provided email using Gmail API.
üß© Customization Guidance
To add more languages: Update the translation logic to include additional language support.
To modify analysis depth: Adjust the prompts sent to the Gemini Analyzer Agent.
To change the chatbot behavior: Retrain or reconfigure the chatbot to utilize the new Pinecone index contextually.
üîÅ Workflow Summary
User uploads files and email via N8N form.
Files are parsed using Llama Cloud API.
Content is translated (if needed) using Gemini Translator Agent.
Translated content is analyzed by the Gemini Analyzer Agent.
Parsed and analyzed data is stored in Pinecone.
User receives email with analyzed results and a chatbot link."
Screen and Score Resumes from Gmail to Sheets with AI,https://n8n.io/workflows/3546-screen-and-score-resumes-from-gmail-to-sheets-with-ai/,"Description
This intelligent n8n automation streamlines the process of collecting, extracting, and scoring resumes sent to a Gmail inbox‚Äîmaking it an ideal solution for recruiters who regularly receive hundreds of applications. The workflow scans incoming emails with attachments, extracts relevant candidate information from resumes using AI, evaluates each candidate based on customizable criteria, and logs their scores alongside contact details in a connected Google Sheet.
Who Is This For?
Recruiters & Hiring Managers: Automate the resume screening process and save hours of manual work.
HR Teams at Startups & SMBs: Quickly evaluate talent without needing large HR ops infrastructure.
Agencies & Talent Acquisition Firms: Screen large volumes of resumes efficiently and with consistent criteria.
Solo Founders Hiring for Roles: Use AI to help score and shortlist top candidates from email applications.
What Problem Does This Workflow Solve?
Manually reviewing resumes is time-consuming, error-prone, and inconsistent. This workflow solves these challenges by:
Automatically detecting and extracting resumes from Gmail attachments.
Using OpenAI to intelligently extract candidate info from unstructured PDFs.
Scoring resumes using customizable evaluation criteria (e.g., relevant experience, skills, education).
Logging all candidate data (Name, Email, LinkedIn, Score) in a centralized, filterable Google Sheet.
Enabling faster, fairer, and more efficient candidate screening.
How It Works
1. Gmail Trigger
Runs on a scheduled interval (e.g., every 6 or 24 hours).
Scans a connected Gmail inbox (using OAuth credentials) for unread emails that contain PDF attachments.
2. Extract Attachments
Downloads the attached resumes from matching emails.
3. Parse Resume Text
Sends the PDF file to OpenAI's API (via GPT-4 or GPT-3.5 with file support or via base64 + PDF-to-text tool).
Prompts GPT with a structured format to extract fields like Name, Email, LinkedIn, Skills, and Education.
4. Score Resume
Evaluates the resume on predefined scoring logic using AI or logic inside the workflow (e.g., ""Has X skill = +10 points"").
5. Log to Google Sheets
Appends a new row in a connected Google Sheet, including:
Candidate Name
Email Address
LinkedIn URL
Resume Score
Setup
Accounts & API Keys
You‚Äôll need accounts and credentials for:
n8n (hosted or self-hosted)
Google Cloud Platform (for Gmail, Drive, and Sheets APIs)
OpenAI (for GPT model access)
Google Sheet
Make a Google Sheet and connect it via Google Sheets node in n8n. Columns should include:
Name
Email
LinkedIn
Score
Configuration
Google Cloud:
Enable Gmail API and Google Sheets API.
Set up OAuth 2.0 Credentials in Google Console.
Connect n8n Gmail, Drive, and Sheets nodes to these credentials.
OpenAI:
Generate an API Key.
Use the HTTP Request node or official OpenAI node to send prompt requests.
n8n Workflow:
Add Gmail Trigger.
Add extraction logic (e.g., filter PDFs).
Add OpenAI prompt for resume parsing and scoring.
Connect structured output to a Google Sheets node.
Requirements
Accounts:
n8n
Google (Gmail, Sheets, Drive, Cloud Console)
OpenAI
API Keys & Credentials:
OpenAI API Key
Google Cloud OAuth Credentials
Gmail Access Scopes (for reading attachments)
Configured Google Sheet
OpenAI usage (after free tier)
Google Cloud API usage (if exceeding free quota)"
Auto-Generate YouTube Chapters with Gemini AI & YouTube Data API v3,https://n8n.io/workflows/3450-auto-generate-youtube-chapters-with-gemini-ai-and-youtube-data-api-v3/,"Auto-Generate YouTube Chapters with AI-Powered Transcript Analysis
Overview
This workflow uses YouTube Data API v3 and Google Gemini 1.5 Flash AI to automatically generate timestamped chapters for videos by analyzing SRT captions. It enhances viewer navigation, improves SEO , and saves creators time by automating manual tasks.
Prerequisites
YouTube API Setup
Create a Google Cloud Project
Go to the Google Cloud Console.
Click Select a project > New Project and name it (e.g., ""YouTube Chapters Automation"") .
Enable YouTube Data API v3
Navigate to APIs & Services > Library.
Search for ""YouTube Data API v3"" and click Enable .
Configure OAuth Consent Screen
Go to APIs & Services > OAuth consent screen.
Select External (public) or Internal (testing), then add required details (app name, support email) .
Generate OAuth 2.0 Credentials
Under Credentials, click Create Credentials > OAuth client ID.
Choose Web app, then download the JSON key file .
Add Credentials to n8n
Other Requirements
Google Gemini API: Configure access for the gemini-1.5-flash-8b-exp-0924 model by getting the api key.
Workflow Steps
Set Video ID
Input the target video ID (e.g., r1wqsrW2vmE) using the Set Video ID node.
Fetch Video Metadata
Use the YouTube API node to retrieve the video‚Äôs title, category, and existing description .
Download SRT Captions
Get Caption ID: Call https://www.googleapis.com/youtube/v3/captions to fetch the caption track ID .
Download Transcript: Use the ID to retrieve SRT data via https://www.googleapis.com/youtube/v3/captions/{{ID}}?tfmt=srt .
Analyze Transcript with Gemini AI
Process the SRT file with Google Gemini AI to identify chapters using a prompt like:
""Classify this transcript into timestamped chapters (e.g., 00:00 - Introduction).""  
Validate output with a structured parser (e.g., Structured Captions node) .
Update Video Description
Append chapters to the description using the YouTube API‚Äôs videos.update method .
Value Proposition
Viewer Experience: Chapters improve navigation and reduce drop-off rates .
SEO Benefits: Structured descriptions enhance search visibility .
Time Savings: Eliminates manual chapter creation ."
‚úçÔ∏è AI Agent to Create Linkedin Posts for Blog Promotion with GPT-4o,https://n8n.io/workflows/3500-ai-agent-to-create-linkedin-posts-for-blog-promotion-with-gpt-4o/,"Tags: Automation, AI, Marketing, Content Creation
Context
I‚Äôm a Supply Chain Data Scientist and content creator who writes regularly about data-driven optimization, logistics, and sustainability. Promoting blog articles on LinkedIn used to be a manual task ‚Äî until I decided to automate it with N8N and GPT-4o.
This workflow lets you automatically extract blog posts, clean the content, and generate a professional LinkedIn post using an AI Agent powered by GPT-4o ‚Äî all in one seamless automation.
Save hours of repetitive work and boost your reach with AI.
üì¨ For business inquiries, you can add me on LinkedIn
Who is this template for?
This template is perfect for:
Bloggers and writers who want to promote their content on LinkedIn
Marketing teams looking to automate professional post-generation
Content creators using Ghost platforms
It generates polished LinkedIn posts with:
A hook
A quick summary
A call-to-action
A signature to drive readers to your contact page
How does it work?
This workflow runs in N8N and performs the following steps:
üöÄ Triggers manually or you can add a scheduler
üì∞ Pulls recent blog posts from your Ghost site (via API)
üßº Cleans the HTML content for AI input
ü§ñ Sends content to GPT-4o with a tailored prompt to create a LinkedIn post
üìÑ Records all data (post content + LinkedIn output) in a Google Sheet
What do I need to start?
You don‚Äôt need to write a single line of code.
Prerequisites:
A Ghost CMS account with blog content
A Google Sheet to store generated posts
An OpenAI API Key
Google Sheets API connected via OAuth2
Next Steps
Use the sticky notes in the workflow to understand how to:
Add your Ghost API credentials
Link your Google Sheet
Customize the AI prompt (e.g., change the author name or tone)
Optionally add auto-posting to LinkedIn using tools like Buffer or Make
üé• Watch My Tutorial
üöÄ Want to explore how automation can scale your brand or business?
üì¨ Let‚Äôs connect on LinkedIn
Notes
You can adapt this template for Twitter, Facebook, or even email newsletters by adjusting the prompt and output channel.
This workflow was built using n8n 1.85.4
Submitted: April 9th, 2025"
"Generate AI YouTube Shorts with Flux, Runway, Eleven Labs and Creatomate",https://n8n.io/workflows/3416-generate-ai-youtube-shorts-with-flux-runway-eleven-labs-and-creatomate/,"Automated Video Creation Workflow Using n8n
This workflow automates the creation and publishing of animated videos based on ideas listed in a Google Sheet. It processes one idea at a time, generating text prompts, images, animations, sound effects, and merging them into a final video before uploading it to YouTube.
Table of Contents
Pre-conditions and Requirements
Google Sheets Setup
Step-by-Step Workflow Explanation
Customization Guide
Pre-conditions and Requirements
1. API Keys Required
To run this workflow, you'll need API access to the following services:
Anthropic Claude or Google Gemini (for text prompt generation)
Flux AI (RapidAPI) (for AI-generated images)
RunwayML (API Documentation) (for AI video animation)
ElevenLabs (for AI-generated voiceovers and sound effects)
Creatomate (Website) (for video/audio merging and rendering)
YouTube API (for video upload and posting)
2. n8n Instance Setup
Use cloud (n8n.io) or Install and run n8n (Official Guide)
Set up credentials for each API in n8n‚Äôs settings
Google Sheets Setup
Before running the workflow, ensure your Google Sheet is structured as follows:
Column Name Description
title Video title (e.g., ""Elijah's Fiery Chariot"")
bibleverse Corresponding Bible verse reference
idea Brief description of the scene
style Animation style (e.g., cinematic, vibrant, etc.)
caption Suggested caption for social media
videoStatus Status of video creation (To Do, Created)
publishStatus Publishing status (Not Processed, Processed)
Step-by-Step Workflow Explanation
1. Extract an Idea from Google Sheets
The workflow retrieves the first row where videoStatus = ""To Do"".
Marks it as Processing to avoid duplicate processing.
2. Generate a Prompt for AI Image Creation and Sound Effects/Audio
Uses Anthropic Claude or Google Gemini to generate prompts.
3. Generate an AI Image with Flux AI
Sends the prompt to Flux AI to create a high-quality image.
4. Animate the Image Using RunwayML
The generated image is sent to RunwayML, which animates the image.
5. Generate Sound Effects and Voiceover with ElevenLabs
ElevenLabs produces a realistic narration based on the video content.
Background sound effects (e.g., storm sounds, fire crackling) are also generated.
6. Merge Video and Audio Using Creatomate
Creatomate compiles the animated video with the audio.
7. Upload and Publish to YouTube
The finalized video is automatically uploaded to YouTube using the YouTube API.
8. Update Google Sheet Status
Marks videoStatus as Created.
Marks publishStatus as Processed.
Customization Guide
Modifying the Animation Style
Update the style column in Google Sheets with custom animation preferences (e.g., cinematic, slow-motion).
Modify the prompt generation step in n8n to incorporate different styles.
Changing the Video Length
Adjust the RunwayML settings to control animation speed and length.
Modify the Creatomate rendering step to adjust clip duration.
Adding Subtitles
Modify the Creatomate step to include AI-generated subtitles from ElevenLabs' text output.
Publishing to Other Platforms
Add additional steps to post to TikTok, Instagram, or Facebook using their respective APIs.
This workflow ensures a fully automated video creation pipeline, reducing manual effort and optimizing content production. üöÄ"
"Create a Pizza Ordering Chatbot with GPT-3.5 - Menu, Orders & Status Tracking",https://n8n.io/workflows/3049-create-a-pizza-ordering-chatbot-with-gpt-35-menu-orders-and-status-tracking/,"Pizza Ordering Chatbot with OpenAI - Menu, Orders & Status Tracking
Introduction
This workflow template is designed to automate order processing for a pizza store using OpenAI and n8n. The chatbot acts as a virtual assistant to handle customer inquiries related to menu details, order placement, and order status tracking.
Features
The chatbot provides an interactive experience for customers by performing the following functions:
Menu Inquiry: When a customer asks about the menu, the chatbot responds with a list of available pizzas, prices, and additional options.
Order Placement: If a customer places an order, the chatbot confirms order details, provides a summary, informs the customer that the order is being processed, and expresses gratitude.
Order Status Tracking: If a customer asks about their order status, the chatbot retrieves details such as order date, pizza type, and quantity, providing real-time updates.
Prerequisites
Before setting up the workflow, ensure you have the following:
OpenAI account (Sign up here)
OpenAI API key to interact with GPT-3.5
n8n instance running locally or on a server (Installation Guide)
Configuration Steps
Step 1: Set Up OpenAI API Credentials
Log in to OpenAI's website.
Navigate to API Keys under your account settings.
Click Create API Key and copy the key for later use.
Step 2: Configure OpenAI Node in n8n
Open n8n and create a new workflow.
Click Add Node and search for OpenAI.
Select OpenAI from the list.
In the OpenAI node settings, click ""Create New"" under the Credentials section.
Enter a name for the credentials (e.g., ""PizzaBot OpenAI Key"").
Paste your API Key into the field.
Click Save.
Step 3: Set Up the Chatbot Logic
Connect the AI Agent Builder Node to the OpenAI Node and HTTP Request Node.
Configure the OpenAI Node with the following settings:
Model: gpt-3.5-turbo
Prompt: Provide dynamic text based on customer inquiries (e.g., ""List available pizzas,"" ""Place an order for Margherita pizza,"" ""Check my order status"").
Temperature: Adjust based on desired creativity (recommended: 0.7).
Max Tokens: Limit response length (recommended: 150).
Add multiple HTTP Request Node:
For Get Products: Fetch stored menu data and return details.
For Order Product: Capture order details, generate an order ID, and confirm with the customer.
For Get Order: Retrieve order details based on the order ID and display progress.
Step 4: Testing and Deployment
Click Execute Workflow to test the chatbot.
Open the Chat Message node, then copy the chat URL to access the chatbot in your browser.
Interact with the chatbot by asking different queries (e.g., ""What pizzas do you have?"" or ""I want to order a Pepperoni pizza"").
Verify responses and adjust prompts or configurations as needed.
Deploy the workflow and integrate it with a messaging platform (e.g., Telegram, WhatsApp, or a website chatbot).
Conclusion
This n8n workflow enables a fully functional pizza ordering chatbot using OpenAI's GPT-3.5. Customers can view menus, place orders, and track their order status efficiently. You can further customize the chatbot by refining prompts, adding new features, or integrating with external databases for order management.
üöÄ Happy automating!"
üóº AI Powered Supply Chain Control Tower with BigQuery and GPT-4o,https://n8n.io/workflows/3305-ai-powered-supply-chain-control-tower-with-bigquery-and-gpt-4o/,"Tags: Supply Chain, Logistics, Control Tower
Context
Hey! I‚Äôm Samir, a Supply Chain Engineer and Data Scientist from Paris, and the founder of LogiGreen Consulting.
We design tools to help companies improve their logistics processes using data analytics, AI, and automation‚Äîto reduce costs and minimize environmental impact.
Let‚Äôs use N8N to build smarter and more sustainable supply chains!
üì¨ For business inquiries, you can add me on LinkedIn
Who is this template for?
This workflow template is designed for logistics operations that need a monitoring solution for their distribution chains.
Connected to your Transportation Management Systems, this AI agent can answer any question about the shipments handled by your distribution teams.
How does it work?
The workflow is connected to a Google BigQuery table that stores outbound order data (customer deliveries).
Here‚Äôs what the AI agent does:
ü§î Receives a user question via chat.
üß† Understands the request and generates the correct SQL query.
‚úÖ Executes the SQL query using a BigQuery node.
üí¨ Responds to the user in plain English.
Thanks to the chat memory, users can ask follow-up questions to dive deeper into the data.
What do I need to get started?
This workflow requires no advanced programming skills.
You‚Äôll need:
A Google BigQuery account with an SQL table storing transactional records.
An OpenAI API key (GPT-4o) for the chat model.
Next Steps
Follow the sticky notes in the workflow to configure each node and start using AI to support your supply chain operations.

üé• Watch My Tutorial
üöÄ Curious how N8N can transform your logistics operations?
Notes
The chat trigger can easily be replaced with Teams, Telegram, or Slack for a better user experience.
You can also connect this to a customer chat window using a webhook.
This workflow was built using N8N version 1.82.1
Submitted: March 24, 2025"
"Startup Funding Research Automation with Claude, Perplexity AI, and Airtable",https://n8n.io/workflows/3107-startup-funding-research-automation-with-claude-perplexity-ai-and-airtable/,"Startup Funding Research Automation with Claude, Perplexity AI, and Airtable
How it works
This intelligent workflow automatically discovers and analyzes recently funded startups by:
Monitoring multiple news sources (TechCrunch and VentureBeat) for funding announcements
Using AI to extract key funding details (company name, amount raised, investors)
Conducting automated deep research on each company through perplexity deep research or jina deep search.
Organizing all findings into a structured Airtable database for easy access and analysis
Set up steps (10-15 minutes)
Connect your news feed sources (TechCrunch and VentureBeat). Could be extended. These were easy to scrape and this data can be expensive.
Set up your AI service credentials (Claude and Perplexity or jina which has generous free tier)
Connect your Airtable account and create a base with appropriate fields (can be imported from my base) or see structure below.
Airtable Base
Structure Funding Round Base
Field Name Data Type Description
website_url String URL of the company website
company_name String Name of the company
funding_round String The funding stage or round (e.g., Series A, Seed, etc.)
funding_amount Number The amount of funding received
lead_investor String The primary investor leading the funding round
market String The market or industry sector the company operates in
participating_investors String List of other investors participating in the funding round
press_release_url String URL to the press release about the funding
evaluation Number The company's valuation
Structure Company Deep Research Base
Field Name Data Type Description
website_url String URL of the company website
company_name String Name of the company
funding_round String The funding stage or round (e.g., Series A, Seed, etc.)
funding_amount Number The amount of funding received
currency String Currency of the funding amount
announcement_date String Date when the funding was announced
lead_investor String The primary investor leading the funding round
participating_investors String List of other investors participating in the funding round
industry String The industry sectors the company operates in
company_description String Description of the company's business
hq_location String Company headquarters location
founding_year Number Year the company was founded
founder_names String Names of the company founders
ceo_name String Name of the company CEO
employee_count Number Number of employees at the company
total_funding Number Total funding amount received to date
total_funding_currency String Currency of total funding
funding_purpose String Purpose or use of the funding
business_model String Company's business model
valuation Object Company valuation information
previous_rounds Object Information about previous funding rounds
source_urls String Source URLs for the funding information
original_report String Original report text about the funding
market String The market the company operates in
press_release_url String URL to the press release about the funding
evaluation Number The company's valuation
Notes
I found that by using perplexity via open router, we lose access to the sources, as they are not stored in the same location as the report itself so I opted to use perplexity API via HTTP node.
For using perplexity and or jina you have to configure header auth as described in Header Auth - n8n Docs
What you can learn
How to scrape data using sitemaps
How to extract strucutred data from unstructured text
How to execute parts of the workflow as subworkflow
How to use deep research in a practical scenario
How to define more complex JSON schemas"
AI-Powered Email Automation for Business: Summarize & Respond with RAG,https://n8n.io/workflows/2852-ai-powered-email-automation-for-business-summarize-and-respond-with-rag/,"This workflow is ideal for businesses looking to automate their email responses, especially for handling inquiries about company information. It leverages AI to ensure accurate and professional communication.
How It Works
Email Trigger:
The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new messages. When a new email arrives, it triggers the workflow.
Email Preprocessing:
The Markdown node converts the email's HTML content into plain text for easier processing by the AI models.
Email Summarization:
The Email Summarization Chain node uses an AI model (DeepSeek R1) to generate a concise summary of the email. The summary is limited to 100 words and is written in Italian.
Email Classification:
The Email Classifier node categorizes the email into predefined categories (e.g., ""Company info request""). If the email does not fit any category, it is classified as ""other"".
Email Response Generation:
The Write email node uses an AI model (OpenAI) to draft a professional response to the email. The response is based on the email content and is limited to 100 words.
The Review email node uses another AI model (DeepSeek) to review and format the drafted response. It ensures the response is professional and formatted in HTML (e.g., using &lt;br&gt;, &lt;b&gt;, &lt;i&gt;, &lt;p&gt; tags where necessary).
Email Sending:
The Send Email node sends the reviewed and formatted response back to the original sender.
Vector Database Integration:
The Qdrant Vector Store node retrieves relevant information from a vector database (Qdrant) to assist in generating accurate responses. This is particularly useful for emails classified as ""Company info request"".
The Embeddings OpenAI node generates embeddings for the email content, which are used to query the vector database.
Document Vectorization:
The workflow includes steps to create and refresh a Qdrant collection (Create collection and Refresh collection nodes).
Documents from Google Drive are downloaded (Get folder and Download Files nodes), processed into embeddings (Embeddings OpenAI1 node), and stored in the Qdrant vector store (Qdrant Vector Store1 node).
Set Up Steps
Configure Email Trigger:
Set up the Email Trigger (IMAP) node with the appropriate IMAP credentials to monitor the email inbox.
Set Up AI Models:
Configure the DeepSeek R1, OpenAI, and DeepSeek nodes with the appropriate API credentials for text summarization, response generation, and review.
Set Up Email Classification:
Define the categories in the Email Classifier node (e.g., ""Company info request"", ""Other"").
Ensure the OpenAI 4-o-mini node is configured to assist in classification.
Set Up Vector Database:
Configure the Qdrant Vector Store and Qdrant Vector Store1 nodes with the appropriate Qdrant API credentials and collection details.
Set up the Embeddings OpenAI and Embeddings OpenAI1 nodes to generate embeddings for the email content and documents.
Set Up Document Processing:
Configure the Get folder and Download Files nodes to access and download documents from Google Drive.
Use the Token Splitter and Default Data Loader nodes to process and split the documents into manageable chunks for vectorization.
Set Up Email Sending:
Configure the Send Email node with the appropriate SMTP credentials to send responses.
Test the Workflow:
Trigger the workflow manually using the When clicking ‚ÄòTest workflow‚Äô node to ensure all steps execute correctly.
Verify that emails are summarized, classified, and responded to accurately.
Activate the Workflow:
Once tested, activate the workflow to automate the process of handling incoming emails.
Key Features
Automated Email Handling: Automatically processes incoming emails, summarizes them, and generates professional responses.
AI-Powered Classification: Uses AI to classify emails into relevant categories for targeted responses.
Vector Database Integration: Retrieves relevant information from a vector database to enhance response accuracy.
Document Vectorization: Processes and stores documents from Google Drive in a vector database for quick retrieval.
Professional Email Formatting: Ensures responses are professionally formatted and concise.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Social Media Analysis and Automated Email Generation,https://n8n.io/workflows/2823-social-media-analysis-and-automated-email-generation/,"Social Media Analysis and Automated Email Generation
by Thomas Vie Thomas@pollup.net
Who is this for?
This template is ideal for marketers, lead generation specialists, and business professionals seeking to analyze social media profiles of potential leads and automate personalized email outreach efficiently.
What problem is this workflow solving?
Manually analyzing social media profiles and crafting personalized emails can be time-consuming and prone to errors. This workflow streamlines the process by integrating social media APIs with AI to generate tailored communication, saving time and increasing outreach effectiveness.
What this workflow does:
Google Sheets Integration: Start with a Google Sheet containing lead information such as LinkedIn URL, Twitter handle, name, and email.
Social Media Data Extraction: Automatically fetch profile and activity data from Twitter and LinkedIn using RapidAPI integrations.
AI-Powered Content Generation: Use OpenAI's Chat Model to analyze the extracted data and generate personalized email subject lines and cover letters.
Automated Email Dispatch: Send the generated email directly to the lead, with a copy sent to yourself for tracking purposes.
Progress Tracking: Update the Google Sheet to indicate completed actions.
Setup:
Google Sheets:
Create a sheet with the columns: LinkedIn URL, name, Twitter handle, email, and a ""done"" column for tracking.
Populate the sheet with your leads.
RapidAPI Accounts:
Sign up for RapidAPI and subscribe to the Twitter and LinkedIn API plans.
Configure API authentication keys in the workflow.
AI Configuration:
Connect OpenAI Chat Model with your API key for text generation.
Email Integration:
Add your email credentials or service (SMTP or third-party service like Gmail) for sending automated emails.
How to customize this workflow to your needs:
Modify the AI Prompt: Adapt the prompt in the AI node to better align with your tone, style, or specific messaging framework.
Expand Data Fields: Add additional data fields in Google Sheets if you require further personalization.
API Limits: Adjust API configurations to fit your usage limits or upgrade to higher tiers for increased data scraping capabilities.
Personalize Email Templates: Tweak email formats to suit different audiences or use cases.
Extend Functionality: Integrate additional social media platforms or CRM tools as needed.
By implementing this workflow, you‚Äôll save time on repetitive tasks and create more effective lead generation strategies."
Build an OpenAI Assistant with Google Drive Integration,https://n8n.io/workflows/2782-build-an-openai-assistant-with-google-drive-integration/,"Workflow Overview
This workflow automates the creation and management of a custom OpenAI Assistant for a travel agency (""Travel with us""), leveraging Google Drive for document storage.
How It Works
1. Create the OpenAI Assistant
Node: OpenAI
Action: Creates a custom assistant named ""Travel with us"" Assistant using the gpt-4o-mini model.
Instructions:
Respond only using the provided document (e.g., agency-specific info).
Stay friendly, brief, and focused on travel-related queries.
Ignore irrelevant questions politely.
Credentials: Requires OpenAI API key.
2. Upload Agency Document
Google Drive Node:
Action: Downloads a Google Doc as a PDF.
OpenAI2 Node:
Action: Uploads the PDF to OpenAI with purpose: ""assistants"".
Output: Generates a file_id.
3. Update the Assistant with the Document
OpenAI Node:
Action: Updates the assistant to include the uploaded file.
4. Chat Interaction
Chat Trigger:
Activates when a message is received (""When chat message received"").
OpenAI Assistant Node:
Action: Uses the updated assistant to respond to user queries.
Memory: Window Buffer Memory retains chat context for coherent conversations.
Set Up Steps
Prepare the Document:
Store your travel agency guide in Google Drive (e.g., as a Google Doc).
Update the Google Drive node with your document‚Äôs ID.
Configure Credentials:
Google Drive: Connect via OAuth2 (googleDriveOAuth2Api).
OpenAI: Add your API key to all OpenAI nodes.
Customize the Assistant:
Modify the instructions in the OpenAI node to reflect your agency‚Äôs needs.
Ensure the document includes FAQs, policies, and travel info.
Test the Workflow:
Trigger manually (""Test workflow"") to create the assistant and upload the file.
Send a chat message (e.g., ""What are your travel packages?"") to test responses.
Dependencies
Google Drive Account: To store and retrieve the agency document.
OpenAI API Access: For assistant creation and file uploads."
Proxmox AI Agent with n8n and Generative AI Integration,https://n8n.io/workflows/2749-proxmox-ai-agent-with-n8n-and-generative-ai-integration/,"Proxmox AI Agent with n8n and Generative AI Integration
This template automates IT operations on a Proxmox Virtual Environment (VE) using an AI-powered conversational agent built with n8n. By integrating Proxmox APIs and generative AI models (e.g., Google Gemini), the workflow converts natural language commands into API calls, enabling seamless management of your Proxmox nodes, VMs, and clusters.
Buy My Book:
Mastering n8n on Amazon
Full Courses & Tutorials:
http://lms.syncbricks.com
Watch Video on Youtube
How It Works
Trigger Mechanism
The workflow can be triggered through multiple channels like chat (Telegram, email, or n8n's built-in chat).
Interact with the AI agent conversationally.
AI-Powered Parsing
A connected AI model (Google Gemini or other compatible models like OpenAI or Claude) processes your natural language input to determine the required Proxmox API operation.
API Call Generation
The AI parses the input and generates structured JSON output, which includes:
response_type: The HTTP method (GET, POST, PUT, DELETE).
url: The Proxmox API endpoint to execute.
details: Any required payload parameters for the API call.
Proxmox API Execution
The structured output is used to make HTTP requests to the Proxmox VE API. The workflow supports various operations, such as:
Retrieving cluster or node information.
Creating, deleting, starting, or stopping VMs.
Migrating VMs between nodes.
Updating or resizing VM configurations.
Response Formatting
The workflow formats API responses into a user-friendly summary. For example:
Success messages for operations (e.g., ""VM started successfully"").
Error messages with missing parameter details.
Extensibility
You can enhance the workflow by connecting additional triggers, external services, or AI models. It supports:
Telegram/Slack integration for real-time notifications.
Backup and restore workflows.
Cloud monitoring extensions.
Key Features
Multi-Channel Input: Use chat, email, or custom triggers to communicate with the AI agent.
Low-Code Automation: Easily customize the workflow to suit your Proxmox environment.
Generative AI Integration: Supports advanced AI models for precise command interpretation.
Proxmox API Compatibility: Fully adheres to Proxmox API specifications for secure and reliable operations.
Error Handling: Detects and informs you of missing or invalid parameters in your requests.
Example Use Cases
Create a Virtual Machine
Input: ""Create a VM with 4 cores, 8GB RAM, and 50GB disk on psb1.""
Action: Sends a POST request to Proxmox to create the VM with specified configurations.
Start a VM
Input: ""Start VM 105 on node psb2.""
Action: Executes a POST request to start the specified VM.
Retrieve Node Details
Input: ""Show the memory usage of psb3.""
Action: Sends a GET request and returns the node's resource utilization.
Migrate a VM
Input: ""Migrate VM 202 from psb1 to psb3.""
Action: Executes a POST request to move the VM with optional online migration.
Pre-Requisites
Proxmox API Configuration
Enable the Proxmox API and generate API keys in the Proxmox Data Center.
Use the Authorization header with the format:
PVEAPIToken=&lt;user&gt;@&lt;realm&gt;!&lt;token-id&gt;=&lt;token-value&gt;
n8n Setup
Add Proxmox API credentials in n8n using Header Auth.
Connect a generative AI model (e.g., Google Gemini) via the relevant credential type.
Access the Workflow
Import this template into your n8n instance.
Replace placeholder credentials with your Proxmox and AI service details.
Additional Notes
This template is designed for Proxmox 7.x and above.
For advanced features like backup, VM snapshots, and detailed node monitoring, you can extend this workflow.
Always test with a non-production Proxmox environment before deploying in live systems.
Start with n8n
Learn n8n with Amjid
Get n8n Book
What is Proxmox"
üé® Interactive Image Editor with FLUX.1 Fill Tool for Inpainting,https://n8n.io/workflows/2747-interactive-image-editor-with-flux1-fill-tool-for-inpainting/,"Like this template? Connect with Eduard via LinkedIn.
This workflow is a prototype of an AI-powered image editing interface, similar to Photoshop's Generative Fill feature, but running entirely in the browser. It provides a web-based editor that allows users to:
Select areas in images using an adjustable brush tool
Input text prompts to guide the AI generation
Compare original and generated images side by side
Iterate on edits with different prompts and settings
Save or reuse generated images
üé® Perfect for product catalog management, seasonal content updates, and creative image editing tasks!
üìã Requirements
FLUX API Access: You'll need API credentials from FLUX to use this workflow.
Configure the HTTP Header Auth credential in n8n with your FLUX API key
üîß Key Components
FLUX Fill API for AI-powered image generation
Konva.js for canvas manipulation
img-comparison-slider for result visualization
Custom CSS/JS for editor functionality
Simple Editor Interface
HTML page with an editor is served on the Webhook call
Adjustable brush selection tool
Provides several mock examples and allows uploading custom images
Basic prompt and FLUX model parameter controls
Image Processing Pipeline
Handles image and mask separately
Processes FLUX Fill API requests
Delivers results back to the editor
Result Viewer
Split-screen comparison of original and generated images
Interactive slider for before/after comparison
Options to save or continue editing
Support for multiple iteration cycles
üéØ Use Cases
This prototype is particularly useful for:
Testing AI-powered image editing concepts
Quick product visualization experiments
Exploring creative image variations
Demonstrating inpainting capabilities
üí° Pro Tip: Save masks for frequently edited areas to quickly generate variations with different prompts!
The workflow can be extended to integrate with various data sources and can be customized for specific business needs."
API Schema Extractor,https://n8n.io/workflows/2658-api-schema-extractor/,"This workflow automates the process of discovering and extracting APIs from various services, followed by generating custom schemas. It works in three distinct stages: research, extraction, and schema generation, with each stage tracking progress in a Google Sheet.
üôè Jim Le deserves major kudos for helping to build this sophisticated three-stage workflow that cleverly automates API documentation processing using a smart combination of web scraping, vector search, and LLM technologies.
How it works
Stage 1 - Research:
Fetches pending services from a Google Sheet
Uses Google search to find API documentation
Employs Apify for web scraping to filter relevant pages
Stores webpage contents and metadata in Qdrant (vector database)
Updates progress status in Google Sheet (pending, ok, or error)
Stage 2 - Extraction:
Processes services that completed research successfully
Queries vector store to identify products and offerings
Further queries for relevant API documentation
Uses Gemini (LLM) to extract API operations
Records extracted operations in Google Sheet
Updates progress status (pending, ok, or error)
Stage 3 - Generation:
Takes services with successful extraction
Retrieves all API operations from the database
Combines and groups operations into a custom schema
Uploads final schema to Google Drive
Updates final status in sheet with file location
Ideal for:
Development teams needing to catalog multiple APIs
API documentation initiatives
Creating standardized API schema collections
Automating API discovery and documentation
Accounts required:
Google account (for Sheets and Drive access)
Apify account (for web scraping)
Qdrant database
Gemini API access
Set up instructions:
Prepare your Google Sheets document with the services information. Here's an example of a Google Sheet ‚Äì you can copy it and change or remove the values under the columns. Also, make sure to update Google Sheets nodes with the correct Google Sheet ID.
Configure Google Sheets OAuth2 credentials, required third-party services (Apify, Qdrant) and Gemini.
Ensure proper permissions for Google Drive access."
Agentic Telegram AI bot with with LangChain nodes and new tools,https://n8n.io/workflows/2592-agentic-telegram-ai-bot-with-with-langchain-nodes-and-new-tools/,"Create a Telegram bot that combines advanced AI functionalities with LangChain nodes and new tools.
Nodes as tools and the HTTP request tool are a new n8n feature that extend custom workflow tool and simplify your setup. We used the workflow tool in the previous Telegram template to call the Dalle-3 model.
In the new version, we've achieved similar results using the HTTP Request tool and the Telegram node tool instead. The main difference is that Telegram bot becomes more flexible. The LangChain Agent node can decide which tool to use and when. In the previous version, all steps inside the custom workflow tool were executed sequentially.
‚ö†Ô∏è Note that you'd need to select the Tools Agent to work with new tools.
Before launching the template, make sure to set up your OpenAI and Telegram credentials.
Here‚Äôs how the new Telegram bot works:
Telegram Trigger listens for new messages in a specified Telegram chat. This node activates the rest of the workflow after receiving a message.
AI Tool Agent receives input text, processes it using the OpenAI model and replies to a user. It addresses users by name and sends image links when an image is requested.
The OpenAI GPT-4o model generates context-aware responses. You can configure the model parameters or swap this node entirely.
Window buffer memory helps maintain context across conversations. It stores the last 10 interactions and ensures that the agent can access previous messages within a session. Conversations from different users are stored in different buffers.
The HTTP request tool connects with OpenAI's DALL-E-3 API to generate images based on user prompts. The tool is called when the user asks for an image.
Telegram node tool sends generated images back to the user in a Telegram chat. It retrieves the image from the URL returned by the DALL-E-3 model. This does not happen directly, however. The response from the HTTP request tool is first stored in the Agent‚Äôs scratchpad (think of it as a short-term memory). In the next iteration, the Agent sends the updated response to the GPT model once again. The GPT model will then create a new tool request to send the image back to the user. To pass the image URL, the tool uses the new $fromAI() expression.
Send final reply node sends the final response message created by the agent back to the user on Telegram. Even though the image was already passed to the user, the Agent always stops with the final response that comes from dedicated output.
‚ö†Ô∏è Note, that the Agent may not adhere to the same sequence of actions in 100% of situations. For example, sometimes it could skip sending the file via the Telegram node tool and instead just send an URL in the final reply. If you have a longer series of predefined steps, it may be better to use the ‚Äúold‚Äù custom workflow tool.
This template is perfect as a starting point for building AI agentic workflow. Take a look at another agentic Telegram AI template that can handle both text and voice messages."
Telegram AI bot assistant: ready-made template for voice & text messages,https://n8n.io/workflows/2534-telegram-ai-bot-assistant-ready-made-template-for-voice-and-text-messages/,"Free template for voice & text messages with short-term memory
This n8n workflow template is a blueprint for an AI Telegram bot that processes both voice and text messages. Ready to use with minimal setup.
The bot remembers the last several messages (10 by default), understands commands and provides responses in HTML.
You can easily swap GPT-4 and Whisper for other language and speech-to-text models to suit your needs.
Core Features
Text: send or forward messages
Voice: transcription via Whisper
Extend this template by adding LangChain tools.
Requirements
Telegram Bot API
OpenAI API (for GPT-4 and Whisper)
üí° New to Telegram bots? Check our step-by-step guide on creating your first bot and setting up OpenAI access.
Use Cases
Personal AI assistant
Customer support automation
Knowledge base interface
Integration hub for services that you use:
Connect to any API via HTTP Request Tool
Trigger other n8n workflows with Workflow Tool"
Scrape and summarize webpages with AI,https://n8n.io/workflows/1951-scrape-and-summarize-webpages-with-ai/,"This workflow integrates both web scraping and NLP functionalities. It uses HTML parsing to extract links, HTTP requests to fetch essay content, and AI-based summarization using GPT-4o. It's an excellent example of an end-to-end automated task that is not only efficient but also provides real value by summarizing valuable content.
Note that to use this template, you need to be on n8n version 1.50.0 or later."
Track Technology Stacks & Find Decision Makers with BuiltWith to Google Sheets,https://n8n.io/workflows/4784-track-technology-stacks-and-find-decision-makers-with-builtwith-to-google-sheets/,"Automated system to track and analyze technology stacks used by target companies, helping identify decision-makers and technology trends.
üöÄ What It Does
Tracks technology stack of target companies
Identifies key decision-makers (CTOs, Tech Leads)
Monitors technology changes and updates
Provides competitive intelligence
Generates actionable insights
üéØ Perfect For
B2B SaaS companies
Technology vendors
Sales and business development teams
Competitive intelligence analysts
Market researchers
‚öôÔ∏è Key Benefits
‚úÖ Identify potential customers
‚úÖ Stay ahead of technology trends
‚úÖ Target decision-makers effectively
‚úÖ Monitor competitor technology stacks
‚úÖ Data-driven sales strategies
üîß What You Need
BuiltWith API key
n8n instance
CRM integration (optional)
Email/Slack for alerts
üìä Data Tracked
Company technologies
Hosting providers
Frameworks and libraries
Analytics tools
Marketing technologies
üõ†Ô∏è Setup & Support
Quick Setup
Deploy in 20 minutes with our step-by-step guide
üì∫ Watch Tutorial
üíº Get Expert Support
üìß Direct Help
Gain a competitive edge by understanding the technology landscape of your target market."
"Automated Expense Approval System with GPT-4, Airtable & Pinecone Vector DB",https://n8n.io/workflows/4576-automated-expense-approval-system-with-gpt-4-airtable-and-pinecone-vector-db/,"Automate expense reviews with AI-powered CFO-level analysis. This workflow monitors Airtable expense submissions, uses GPT-4 to analyze expenses like an experienced CFO, flags suspicious expenses with detailed reasoning, and maintains comprehensive audit trails in Pinecone vector database.
üöÄ What It Does
Smart Monitoring: Watches Airtable for new expense submissions
AI CFO Analysis: GPT-4 applies financial expertise to review amounts, categories, and descriptions
Intelligent Flagging: Automatically identifies policy violations and suspicious patterns
Audit Trail: Stores all decisions in Pinecone for compliance and searchability
Auto Updates: Updates Airtable records with AI decisions and detailed reasoning
üéØ Perfect For
Finance teams needing intelligent expense oversight
CFOs wanting to automate expense policy enforcement
Growing companies scaling expense management
Businesses requiring compliance documentation
‚öôÔ∏è Key Benefits
‚úÖ 99% faster expense processing vs manual review
‚úÖ CFO-level intelligence applied to every expense
‚úÖ Complete audit trail for compliance
‚úÖ Real-time fraud detection and policy enforcement
‚úÖ Detailed explanations for every decision
üîß What You Need
Airtable base with expense data (template included)
OpenAI API for GPT-4 analysis
Pinecone account for audit trail storage
Basic expense submission process
üìä Sample Results
Input: $4,500 business class flight to Tokyo
AI Decision: ""Flagged - Amount exceeds typical travel thresholds. Requires verification against travel policies and client justification for premium travel.""
üõ†Ô∏è Setup & Support
Quick Setup: Deploy in 60 minutes with included templates and documentation
YouTube: https://www.youtube.com/@YaronBeen/videos
üíº Expert Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
üìß Direct Help
Email: Yaron@nofluff.online
Transform expense management from manual bottleneck to intelligent automation. Let AI handle policy compliance while your finance team focuses on strategy."
Export AI Agent Conversation Logs from Postgres to Google Sheets,https://n8n.io/workflows/4464-export-ai-agent-conversation-logs-from-postgres-to-google-sheets/,"Overview
This n8n workflow retrieves AI agent chat memory logs stored in Postgres and pushes them to Google Sheets, creating one sheet per session. It‚Äôs useful for teams building chat-based products or agents and needing to review or analyze session logs in a collaborative format.
Who is it for
Anyone with an AI Agent in Production storing the conversation logs in Postgres (or Supabase) who wants to see transcript and have control
Product teams building AI agents or assistants.
Teams that want to centralize conversation history for analysis or support.
Anyone managing AI chat memory and needing to explore it in a spreadsheet.
Prerequisites
A Postgres database with a n8n_chat_histories table with an AI Agent connected to it. If you need an example, you can follow this tutorial
Once done, you need to run the Postgresql query to add the created_at column (see Setup > Add a datetime column)
Google Sheets access and OAuth credentials connected to n8n.
A Google Sheets document set up as a template (see below).
Google Sheets Template
This workflow expects a Google Sheets file where each session will be stored in its own tab.
A basic tab layout is duplicated and renamed with the session ID.
üëâ Use this template as a starting point
Note: You can hide the template after the first tabs have been created
How it works
Trigger
The workflow can be launched manually or on a schedule (e.g. daily at noon).
Retrieve sessions
Runs a SQL query to get distinct session_id values from the n8n_chat_histories table.
Loop over sessions
For each session:
Clears the corresponding sheet (if it exists).
Duplicates the template tab.
Renames it with the current session_id.
Fetch messages
Selects all messages linked to the session from Postgres.
Append to sheet
Adds each message to the Google Sheet with columns:
Who: speaker role (user, assistant, etc.)
Message: text content
Date: timestamp from created_at, formatted yyyy-MM-dd hh:mm:ss
Notes
The sheet is cleared and rebuilt each run to ensure logs are up-to-date.
If a sheet for a session doesn‚Äôt exist, it will be created by duplicating the first tab (template)
You can group sessions under a persistent ID (like user_id) by overriding session_id in your memory config.
Works perfectly with Supabase by using PG credentials from the connection pooler.
üëâ If you're looking for a solution to better visualize and analyse conversations, reach out to us!"
Telegram Chat Summarizer with AI - using @telepilotco/n8n-nodes-telepilot,https://n8n.io/workflows/4461-telegram-chat-summarizer-with-ai-using-telepilotcon8n-nodes-telepilot/,"Template Description
Automatically generate intelligent summaries of your Telegram community discussions using AI, helping you stay updated on important conversations without reading every message.
This powerful n8n automation monitors your Telegram chats and creates periodic AI-powered summaries, highlighting key trends, new ideas, and important updates. Perfect for busy community managers, team leads, and active group participants who want to stay informed without information overload.
Key Features
ü§ñ AI-Powered Analysis
Smart Summarization: Uses OpenRouter AI models to analyze conversation patterns
Trend Detection: Identifies emerging topics and discussion themes
Context Awareness: Maintains conversation history through MongoDB memory
Intelligent Filtering: Focuses on meaningful content while skipping noise
‚è∞ Flexible Scheduling
Automated Execution: Runs every 2 hours automatically
Manual Triggers: Execute on-demand for instant summaries
Time-Based Filtering: Analyzes only recent messages (last 2 hours)
Customizable Intervals: Easy to adjust frequency based on chat activity
üì± Advanced Telegram Integration
TelePilot Integration: Full Telegram userbot capabilities
Chat Access: Can monitor any public or private chat you have access to
Message History: Retrieves up to 100 recent messages per execution
Real-time Processing: Handles various message types and formats
üß† Persistent Memory System
MongoDB Storage: Maintains conversation context across executions
Chat-Specific Memory: Separate memory for each monitored chat
Historical Awareness: Avoids repeating previously summarized content
Scalable Architecture: Handles multiple chats simultaneously
How It Works
Chat Monitoring
Connects to specified Telegram chat using TelePilot
Retrieves recent message history (configurable limit)
Filters messages from the last 2 hours
Message Processing
Extracts text content from various message types
Filters out empty messages and system notifications
Structures data for AI analysis (sender_id, text, timestamp)
AI Analysis
Aggregates filtered messages into conversation context
Applies AI summarization with custom system prompts
Identifies trends, new ideas, and important updates
Maintains awareness of previous summaries to avoid repetition
Summary Delivery
Posts AI-generated summary back to the chat
Includes conversation trends and key highlights
Formatted for easy reading and quick understanding
Setup Requirements
TelePilot Plugin Installation
Before using this template, install the TelePilot community node:
Go to Settings ‚Üí Community modules in your n8n instance
Select ""Install Community node""
Specify the name: @telepilotco/n8n-nodes-telepilot
Check the risk acknowledgment checkbox and click ""Install""
Required Integrations
TelePilot Account: For Telegram userbot functionality (Full Documentation)
OpenRouter API: For AI summarization capabilities
MongoDB Database: For persistent conversation memory
Target Telegram Chat: Public or private chat access
TelePilot Authentication
After installation, you'll need to authenticate your Telegram account:
Click the ""Chat"" button at the bottom of the n8n interface
Type /start in the chat window and follow instructions
Keep your smartphone or desktop Telegram app ready to receive the authentication code
üìñ Complete TelePilot Setup Guide
Configuration Options
Chat Selection: Easily change target chat by username
Schedule Frequency: Adjust from 2-hour default to your needs
Message Limit: Control how many messages to analyze
Summary Style: Customize AI prompts for different summary formats
Perfect For
Community Managers: Keep track of multiple active groups
Team Leaders: Stay updated on team discussions across time zones
Active Group Members: Catch up on missed conversations quickly
Content Creators: Monitor audience discussions and feedback
Research Teams: Track ongoing project conversations
Global Teams: Bridge communication gaps across different schedules
Key Benefits
‚úÖ Never Miss Important Updates: Stay informed without constant monitoring
‚úÖ Reduce Information Overload: Get concise summaries instead of hundreds of messages
‚úÖ Maintain Context: AI remembers previous summaries to avoid repetition
‚úÖ Flexible Scheduling: Automated or manual execution options
‚úÖ Multi-Chat Support: Monitor multiple communities simultaneously
‚úÖ Trend Analysis: Identify emerging topics and discussion patterns
‚úÖ Time-Efficient: Spend minutes instead of hours catching up
‚úÖ Scalable Solution: Works for groups of any size
Use Cases
Daily Team Standup Summaries: Automated updates for distributed teams
Community Highlight Reels: Weekly summaries for large communities
Project Status Updates: Track ongoing discussions and decisions
Customer Feedback Analysis: Monitor support and feedback channels
Event Planning Updates: Stay current on planning discussions
Learning Community Insights: Track educational discussions and resources
What's Included
üîß Complete Workflow: Ready to deploy with minimal configuration
üìã Setup Instructions: Step-by-step TelePilot integration guide
üéØ Smart Filtering: Pre-configured message processing logic
ü§ñ AI Optimization: Fine-tuned prompts for quality summaries
üíæ Memory Management: MongoDB integration for context persistence
‚öôÔ∏è Error Handling: Robust workflow with fallback mechanisms
Transform your chat monitoring from overwhelming to insightful with this intelligent automation solution!
Documentation: TelePilot Plugin Documentation
Tags: #Telegram #AI #ChatSummary #CommunityManagement #TelePilot #OpenRouter #MongoDB #Automation #TeamCommunication #ContentCuration"
BeyondPresence Sales Intelligence ‚Üí Real-Time Lead Scoring,https://n8n.io/workflows/4454-beyondpresence-sales-intelligence-real-time-lead-scoring/,"Monitor BeyondPresence video agent conversations in real-time to automatically score leads (0-100+) based on buying signals and send instant Slack alerts when hot opportunities or competitors are mentioned. This template helps sales teams prioritize leads immediately, never miss competitor mentions, and respond to high-intent prospects while they're still engaged.
How it works
Real-time webhook processes each user message as it happens during calls
Scoring engine analyzes for buying signals (+points) and objections (-points)
Competitor detection instantly identifies when alternatives are mentioned
Smart routing sends alerts to different Slack channels based on urgency
Hot leads (70+ score) trigger immediate notifications with recommendations
Call summary (Optional) provides final qualification score when conversation ends
Set up steps
Connect Slack OAuth2 - Use n8n's built-in Slack integration (no webhooks needed!)
Create Slack channels - Set up #sales-hot-leads, #sales-competitors, #sales-qualified
Add webhook to BeyondPresence - Copy URL from n8n to BeyondPresence Settings ‚Üí Webhooks
Customize competitors - Edit the scoring node to add your specific competitor names
Adjust scoring weights (optional) - Tune point values for your sales process
Setup time: 10-15 minutes
Requirements: BeyondPresence account, Slack workspace admin access"
Command-based Telegram Bot for Article Summarization & Image Prompts with OpenAI,https://n8n.io/workflows/4392-command-based-telegram-bot-for-article-summarization-and-image-prompts-with-openai/,"Telegram AI Assistant: Summarize Links & Generate Images On Demand
This workflow turns any Telegram chat into a smart assistant. By typing simple commands like /summary or /img, users can trigger powerful AI actions‚Äîdirectly from Telegram.
‚ú® What It Does
This automation listens for specific commands in Telegram messages:
/help: Sends a help menu explaining available commands.
/summary &lt;link&gt;: Fetches a webpage, extracts its content, and summarizes it using OpenAI into 10‚Äì12 bullet points.
/img &lt;prompt&gt;: Sends the image prompt to OpenAI and replies that the request has been received (designed for future integration with image APIs).
üì¶ Features
‚úÖ Works instantly in Telegram
üß† Uses OpenAI for text summarization and image prompt processing
üåê Scrapes and cleans raw article text before summarizing
üì§ Replies directly to the same Telegram thread
üîß Easily expandable to support more commands
üîß Use Cases
Research Summaries: Quickly condense articles or reports shared in chat.
Content Review: Get team-friendly TL;DRs of long blog posts or product pages.
Creative Brainstorming: Share visual ideas via /img and get quick prompts logged.
Customer Support: Offer instant answers in group chats (with further extension).
Daily Digest Bot: Connect to news feeds and auto-summarize updates.
üöÄ Getting Started
Clone this workflow and connect your Telegram Bot.
Insert your OpenAI credentials.
Deploy and test by messaging /summary https://example.com in your Telegram group or DM.
Expand with new commands or connect Stability.ai or other services for real image generation.
üîó Author & Resources
Built by Yaron Been
Follow more automations at nofluff.online"
Evaluation Metric: Summarization,https://n8n.io/workflows/4428-evaluation-metric-summarization/,"This n8n template demonstrates how to calculate the evaluation metric ""Summarization"" which in this scenario, measures the LLM's accuracy and faithfulness in producing summaries which are based on an incoming Youtube transcript.
The scoring approach is adapted from https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates#pointwise_summarization_quality
How it works
This evaluation works best for an AI summarization workflows.
For our scoring, we simple compare the generated response to the original transcript.
A key factor is to look out information in the response which is not mentioned in the documents.
A high score indicates LLM adherence and alignment whereas a low score could signal inadequate prompt or model hallucination.
Requirements
n8n version 1.94+
Check out this Google Sheet for a sample data https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing"
Evaluate AI Agent Response Relevance using OpenAI and Cosine Similarity,https://n8n.io/workflows/4425-evaluate-ai-agent-response-relevance-using-openai-and-cosine-similarity/,"This n8n template demonstrates how to calculate the evaluation metric ""Relevance"" which in this scenario, measures the relevance of the agent's response to the user's question.
The scoring approach is adapted from the open-source evaluations project RAGAS and you can see the source here
https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_relevance.py
How it works
This evaluation works best for Q&A agents.
For our scoring, we analyse the agent's response and ask another AI to generate a question from it. This generated question is then compared to the original question using cosine similarity.
A high score indicates relevance and the agent's successful ability to answer the question whereas a low score means agent may have added too much irrelevant info, went off script or hallucinated.
Requirements
n8n version 1.94+
Check out this Google Sheet for a sample data https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing"
AI-Powered Calendar & Meeting Digest with Gmail and GPT-4o/Claude - Daily Brief,https://n8n.io/workflows/4385-ai-powered-calendar-and-meeting-digest-with-gmail-and-gpt-4oclaude-daily-brief/,"üõ† How It Works
The Daily Calendar Brief is an automated n8n workflow designed to prep you each morning with a rich, executive-style email summary of your day. It pulls data from your calendar, email, and external contact sources to deliver a contextualized, prioritized briefing.
Daily Trigger
Runs every weekday morning at 7:00 AM using a schedule node.
Calendar Parsing
Grabs all Google Calendar events for the day and extracts details like:
‚Ä¢ Event title, time, and duration
‚Ä¢ Full attendee list
‚Ä¢ Meeting description
Attendee Intelligence
Filters for external attendees and:
‚Ä¢ Scrapes recent email threads with them
‚Ä¢ Uses Hunter and LinkedIn to enrich attendee info with roles, bios, locations, and recent activity
Brief Assembly
Combines:
‚Ä¢ Event metadata
‚Ä¢ Attendee research
‚Ä¢ Recent conversations
Email Delivery
Renders the Markdown as HTML and sends a polished email to your inbox with:
‚Ä¢ Meeting timeline & attendees
‚Ä¢ Key takeaways & extra context
‚Ä¢ Conflicts & FYIs
Setup Steps
Prerequisites
‚Ä¢ n8n instance (self-hosted or cloud)
‚Ä¢ Google Calendar and Gmail OAuth credentials
‚Ä¢ OpenRouter key (for GPT-4o or Claude 3.7)
Configuration
1. Authorize Credentials
‚Ä¢ Connect Google Calendar and Gmail nodes with OAuth2
‚Ä¢ Set up OpenRouter credentials for AI processing
2. Set Your Email
Update the Send Email node with your preferred destination address (default is you@yourcompany.com).
3. Set Your Domain Filter
In the ‚ÄúIdentify External Attendees‚Äù node, adjust the filter value yourcompany.com to your actual domain.
4. Customize Prompts (Optional)
You can fine-tune tone, formatting, or limits in the two language model nodes:
‚Ä¢ Research and Develop Brief (attendee context + email summary)
‚Ä¢ Summarize Schedule (overall Markdown brief formatting)
5. Activate Workflow
Enable the workflow and test it manually once to validate your setup. Confirm that the email lands correctly."
"Automate Job Search and Matching with Adzuna API, GPT-3.5, and Google Sheets",https://n8n.io/workflows/4230-automate-job-search-and-matching-with-adzuna-api-gpt-35-and-google-sheets/,"Job Search Automation Workflow
Streamline Job Discovery with API-Powered Intelligence
This workflow template transforms the job search process by leveraging the Adzuna API and AI to automatically find, summarize, and evaluate job opportunities. Unlike traditional RSS-based job search tools, this workflow connects directly to Adzuna's comprehensive job API for real-time, high-quality job data.
Workflow Overview
The Job Search Automation Workflow follows a streamlined process:
Define job titles to search for
Fetch current job listings from Adzuna's API
Split individual job results for processing
Use AI to extract and summarize key job information
Score each job against candidate profiles
Organize results for easy review
This automation eliminates manual searching across job boards and provides objective evaluation of opportunities, saving valuable time for both job seekers and professionals who support them.
Technical Implementation
The workflow is built with a focus on efficiency and extensibility:
API Integration: Direct connection to Adzuna's job API ensures access to current listings with comprehensive details
AI Processing: Leverages OpenAI's GPT models to intelligently summarize job descriptions and evaluate candidate fit
Modular Design: Each node serves a specific purpose, making the workflow easy to understand and customize
Scalable Architecture: Can be expanded to include additional job sources or evaluation criteria
Use Cases
This template serves multiple user groups:
Job Seekers: Automate discovery of relevant positions matching your skills
Recruiters: Quickly identify quality positions for your candidate pool
Career Coaches: Systematically help clients find suitable positions
AI Automation Builders: Learn how to integrate job APIs with AI evaluation systems
Prerequisites
To use this workflow, you'll need:
An Adzuna API key (available at Adzuna.com)
n8n instance (cloud or self-hosted)
OpenAI API credentials (or compatible alternative)
Getting Started
Import this template to your n8n instance
Configure your Adzuna API credentials
Set up your OpenAI API connection
Customize job titles and candidate profiles
Execute the workflow to start discovering opportunities
Customization Options
The workflow can be easily adapted to your specific needs:
Modify job search parameters (location, industry, etc.)
Adjust AI prompts for different summarization styles
Customize scoring criteria based on specific skills or requirements
Add nodes to integrate with other tools in your workflow
Why This Approach Matters
Traditional job search methods often rely on RSS feeds, which can be limited in detail and frequency of updates. By connecting directly to the Adzuna API, this workflow ensures you're always working with the most current job data available.
The AI-powered evaluation provides objective scoring of how well each position matches candidate profiles, helping prioritize applications for positions with the highest potential fit.
Whether you're a job seeker looking to streamline your search, a professional helping others find opportunities, or an automation enthusiast interested in API integrations, this template provides a solid foundation for more efficient job discovery and evaluation."
"Create AI-Ready Vector Datasets from Web Content with Claude, Ollama & Qdrant",https://n8n.io/workflows/4219-create-ai-ready-vector-datasets-from-web-content-with-claude-ollama-and-qdrant/,"AI-Powered Web Data Pipeline with n8n
How It Works
This n8n workflow builds an AI-powered web data pipeline that automates the entire process of:
Extraction
Structuring
Vectorization
Storage
It integrates multiple advanced tools to transform messy web pages into clean, searchable vector databases.
Integrated Tools
Scrapeless
Bypasses JavaScript-heavy websites and anti-bot protections to reliably extract HTML content.
Claude AI
Uses LLMs to analyze unstructured HTML and generate clean, structured JSON data.
Ollama Embeddings
Generates local vector embeddings from structured text using the all-minilm model.
Qdrant Vector DB
Stores semantic vector data for fast and meaningful search capabilities.
Webhook Notifications
Sends real-time updates when workflows complete or errors occur.
From messy webpages to structured vector data ‚Äî this pipeline is perfect for building intelligent agents, knowledge bases, or research automation tools.
Setup Steps
1. Install n8n
Requires Node.js v18 / v20 / v22
npm install -g n8n
n8n
After installation, access the n8n interface via:
URL: http://localhost:5678
2. Set Up Scrapeless
Register at: Scrapeless
Copy your API token
Paste the token into the HTTP Request node labeled ""Scrapeless Web Request""
3. Set Up Claude API (Anthropic)
Sign up at Anthropic Console
Generate your Claude API key
Add the API key to the following nodes:
Claude Extractor
AI Data Checker
Claude AI Agent
4. Install and Run Ollama
macOS
brew install ollama
Linux
curl -fsSL https://ollama.com/install.sh | sh
Windows
Download the installer from: https://ollama.com
Start Ollama Server
ollama serve
Pull Embedding Model
ollama pull all-minilm
5. Install Qdrant (via Docker)
docker pull qdrant/qdrant

docker run -d \
  --name qdrant-server \
  -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant
Test if Qdrant is running:
curl http://localhost:6333/healthz
6. Configure the n8n Workflow
Modify the Trigger (Manual or Scheduled)
Input your Target URLs and Collection Name in the designated nodes
Paste all required API Tokens / Keys into their corresponding nodes
Ensure your Qdrant and Ollama services are running
Ideal Use Cases
Custom AI Chatbots
Private Search Engines
Research Tools
Internal Knowledge Bases
Content Monitoring Pipelines"
Auto-Repost TikTok Videos to YouTube Shorts with Google Sheets & Telegram Alerts,https://n8n.io/workflows/4199-auto-repost-tiktok-videos-to-youtube-shorts-with-google-sheets-and-telegram-alerts/,"TikTok to YouTube Parser Video
Generate traffic and grow your audience effortlessly with this plug-and-play n8n automation template.
Automatically fetch TikTok videos, upload them to YouTube Shorts, save logs to Google Sheets, and trigger uploads via Telegram ‚Äî no coding required.
Perfect for creators, marketers, and automation enthusiasts.
What This Workflow Does
This complete n8n automation:
‚úÖ Fetches the latest TikTok videos from any user
‚úÖ Avoids duplicate uploads with intelligent filtering
‚úÖ Uploads to YouTube Shorts via OAuth2
‚úÖ Saves video metadata to Google Sheets
‚úÖ Sends Telegram notifications on completion
‚úÖ Lets you trigger uploads directly from Telegram
‚úÖ Supports custom titles, tags, descriptions, and privacy settings
Use Cases
üìà Repurpose TikTok content to grow your YouTube channel
üì≤ Automate content curation from influencers
üïê Schedule regular syncing (daily/hourly)
üë©‚Äçüíº Perfect for agencies, content managers, or solo creators
What‚Äôs Included
Intelligent filter to avoid re-uploading already-processed videos (via staticData)
Dynamic title extraction from TikTok video
You control the video description (use for ads, credits, or SEO)
Upload scheduling (via Schedule Trigger or Telegram Trigger)
Append upload logs to a connected Google Sheets document
Telegram Trigger support ‚Äî send a username to your bot and watch the magic happen
Easily adjustable through Set nodes (number of uploads, tags, etc.)
üé• See It in Action
Want to know exactly how it works?
Watch the video demo https://www.youtube.com/watch?v=Aci40EpeGvA to see the workflow in action ‚Äî from fetching TikToks to uploading YouTube Shorts and logging everything in Google Sheets.
üì≤ Telegram trigger included ‚Äî you'll see how easy it is to run the whole process from your phone.
Requirements
An n8n instance (self-hosted or cloud)
YouTube API credentials (OAuth2 setup included)
Google Sheets API enabled
Telegram bot token (optional)"
"Find & Verify Business Emails Automatically with OpenRouter, Serper & Prospeo",https://n8n.io/workflows/3817-find-and-verify-business-emails-automatically-with-openrouter-serper-and-prospeo/,"Who is this template for?
Growth teams, SDRs, recruiters, or anyone who‚ÄØroutinely hunts for hard‚Äëto‚Äëfind business emails and would rather spend time reaching out than guessing formats.
What problem does this workflow solve?
Manually piecing together email patterns, cross‚Äëchecking them in a verifier, and updating a tracking sheet is slow and error‚Äëprone. This template automates the‚ÄØentire loop‚Äîresearch, guess, verify, and log‚Äîso you hit Start and watch rows fill up with ready‚Äëto‚Äësend addresses.
What this workflow does
Pull fresh leads ‚Äì Grabs only the rows in your Google‚ÄØSheet where Status‚ÄØ=‚ÄØFALSE.
Find the company pattern ‚Äì Queries Serper.dev for snippets and feeds them to Gemini‚ÄØFlash (via OpenRouter) to spot the dominant email format.
Build the address ‚Äì Constructs a likely email for every first/last name.
Verify in real time ‚Äì Pings Prospeo by default (API) or lets you bulk‚Äëclean in Sparkle.io.
Write it back ‚Äì Updates the sheet with pattern, email, confidence, verification status, and flips Status to‚ÄØTRUE.
Loop until done ‚Äì Runs batch‚Äëby‚Äëbatch so you never hit API limits.
üÜì Work free‚Äëtier magic (up to ~2,500 contacts/month)
Service Free allowance How this template uses it
Serper.dev 2,500 searches/mo Scrapes three public email snippets per domain to learn the pattern
Sparkle.io 10,000 bulk verifications/day Manual upload‚Äëdownload option‚Äîperfect to clean your first 2.5k emails at zero cost
Prospeo 75 API calls/mo Built‚Äëin if you prefer fully automated verification
Quick Sparkle workflow:
Let the template generate emails.
Export the ‚ÄúEmail‚Äù column to‚ÄØCSV ‚Üí upload to Sparkle.io.
Download the results and paste the ""verification_status"" back into the sheet (or add a small n8n import sub‚Äëflow).
Setup (5‚ÄØminutes)
Copy the Google‚ÄØSheet linked in the sticky note and paste its ID into the Get Rows and Update Rows nodes.
Add credentials for Google‚ÄØSheets, Serper (X‚ÄëAPI‚ÄëKEY), OpenRouter, and optionally Prospeo.
Hit Execute Workflow‚Äîthat‚Äôs it.
How to customise
Prefer Sparkle for volume: Skip the Prospeo node, export emails in one click, bulk‚Äëverify in Sparkle, and re‚Äëimport results.
Swap the search source: Replace the Get Email Pattern HTTP node with Bing, Brave, etc.
Extend enrichment: Add phone look‚Äëups or LinkedIn scrapers before the Update Rows node.
Auto‚Äërun: Replace the Manual Trigger with a Cron node so the sheet cleans itself every morning.
Additional‚ÄØresources
Tool Purpose Link
Prospeo ‚Äì API‚Äëready email verification<br><sub>Special offer: 20‚ÄØ% free credits for the first‚ÄØ3‚ÄØmonths on any plan using this link!</sub> Real‚Äëtime, single‚Äëcall mailbox validation prospeo.io
Sparkle.io ‚Äì high‚Äëvolume bulk verifier (manual upload) Free daily quota of 10‚ÄØ000 verifications app.sparkle.io/sign‚Äëup
OpenRouter ‚Äì API gateway for Gemini Flash & other LLMs One key unlocks multiple frontier models openrouter.ai
Serper.dev ‚Äì Google Search API 2‚ÄØ500 searches/month on the free tier serper.dev
Add the relevant keys or signup details from these links, drop them into the matching n8n credentials, and you‚Äôre all set to enrich your first 2‚ÄØ500 contacts at zero cost. Happy building!"
"Stacey ‚Äì Your Telegram AI Assistant (Powered by MCP, Gemini & Google Tools)",https://n8n.io/workflows/4025-stacey-your-telegram-ai-assistant-powered-by-mcp-gemini-and-google-tools/,"This n8n template builds Stacey, an AI assistant that runs inside Telegram. Stacey listens to your messages, understands what you want using AI, and intelligently routes commands to MCP-connected tools ‚Äî like Gmail, Google Calendar, a blog writer, and more.
For optimal performance, we recommend using OpenAI‚Äôs GPT-4o model. In this template, Google Gemini is used as a free alternative.
üí° Who is this for?
This workflow is designed for:
AI tool creators and automation builders
Entrepreneurs who want an intelligent Telegram assistant
Support and scheduling teams who use Google tools
Agencies that build & resell AI automations
Users looking to automate everyday actions like emails, scheduling, blog writing, and contact lookups
üß† What this workflow does
Listens to Telegram messages (text and voice)
Transcribes audio using Whisper (optional)
Uses Stacey, an AI agent powered by Gemini (or GPT-4o if you upgrade), to:
Understand the user's intent
Choose the correct tool using MCP logic
Execute tasks using Gmail, Google Calendar, blog writer, and more
Responds to the user naturally with confirmations and outputs
‚öôÔ∏è Prerequisites
Before using this workflow, make sure you have:
A self-hosted or cloud-based n8n instance
A Telegram Bot Token from @BotFather
Google OAuth2 credentials for:
Gmail
Google Calendar
Optional: OpenAI or Whisper API key for voice transcription
Optional: Tavily API key for live web search
Gemini (Google AI) is preconfigured in the template but can be swapped
üöÄ Step-by-Step Setup
‚úÖ Step 1: Add Required Credentials in n8n
Go to Settings ‚Üí Credentials and add:
Telegram API: Your bot token from BotFather
Google OAuth2:
Gmail: Scope https://www.googleapis.com/auth/gmail.modify
Calendar: Scope https://www.googleapis.com/auth/calendar
Gemini / Palm API: Used for the language model
(Optional) OpenAI Whisper: For voice transcription
(Optional) Tavily: For real-time internet searches
‚úÖ Step 2: Import the Workflow
Go to n8n
Click Workflows ‚Üí Import from File
Upload ai_assistant.json
Connect your saved credentials to the correct nodes:
Telegram Trigger & Sender
Gmail, Calendar, Tavily, Gemini, Whisper
‚úÖ Step 2.5: Import the Content Creator Sub-Workflow
The Content Creator is implemented as a modular sub-workflow and invoked through the Map Server as part of the MCP logic.
To set it up:
Go to Workflows ‚Üí Import from File
Upload content_creator_tool.json (provided in your files)
Save it with a name like ‚ÄúContent Creator Tool‚Äù
üîó Integration with MCP:
This tool is triggered via the MCP Map Server using an Execute Workflow node
The AI agent chooses this tool when the user request involves writing blog posts, emails, product descriptions, etc.
Make sure the tool ID or name in the Map Server matches what the AI agent uses in its logic
You can customize this sub-workflow to:
Adjust writing prompts (e.g., tone, format, target audience)
Add branching for different content types (e.g., blog vs. email)
Send outputs directly to Gmail, Google Docs, or Sheets
‚úÖ Step 3: Set Up Your Telegram Bot
Talk to @BotFather
Use /newbot to create a bot and get your token
Paste this token into:
Telegram Trigger node (Telegram Trigger1)
Telegram Send Message node (Response1)
Make sure your bot‚Äôs privacy is set correctly (use /setprivacy)
‚úÖ Step 4: Customize Your Assistant‚Äôs Personality
Open the ‚ÄúAI Agent‚Äù node
In the systemMessage field, you'll find a prompt that defines Stacey:
Her role is to delegate user requests to the right MCP tool
Includes examples, tone, rules, and logic
You can customize Stacey‚Äôs:
Name
Behavior
Supported tools
‚úÖ Step 5: Define Your MCP Tools (if extending)
This template includes:
Send Email
Reply to Email
Get Emails
Label Emails
Create/Update/Delete Events
Content Creator
Search Web with Tavily
Calculator
To extend:
Add a new tool node
Link it to MCP Server Trigger
Reference it in the prompt in AI Agent node
‚úÖ Step 6: Test the Workflow
Open Telegram and message your bot:
‚ÄúSend an email to John about the new budget‚Äù
‚ÄúSchedule a meeting Friday at 3 PM with Alex‚Äù
‚ÄúWrite a blog post about solar energy‚Äù
‚ÄúWhat‚Äôs in my inbox?‚Äù
‚ÄúTranslate this message‚Äù (if extended with translation tools)
The bot will:
Interpret the intent
Ask for any missing data
Trigger the right tool
Send confirmation via Telegram
‚ú® Customization Ideas
‚úèÔ∏è Add Voice Transcription
Enable the Download File and Transcribe nodes
Requires OpenAI Whisper API key
üß† Upgrade to GPT-4o
Replace the Gemini node with an OpenAI Chat node
Connect GPT-4o for improved reasoning and language understanding
üß© Add More Tools
Notion, Slack, Salesforce, Hubspot, WhatsApp, and more can be added
Just route them via MCP and update the AI prompt
üß™ Troubleshooting
Telegram not responding?
Ensure correct bot token and webhook connection
Make sure the bot is not in privacy mode if needed
Gmail actions not working?
Double-check your OAuth scopes
Ensure Gmail API is enabled in your Google Cloud project
AI not responding or behaving poorly?
Consider upgrading to OpenAI GPT-4o for better reasoning
Revisit and refine your system prompt
üßæ Summary
Name: Stacey ‚Äì AI Telegram Assistant
Built with: n8n + Gemini + Google + MCP Logic
Telegram acts as the front-end
Gemini or GPT-4o powers intelligence
MCP routes user intent to the right tool
Fully extensible and no-code friendly
üåü Credits & License
Created by David Olusola
Free to use, modify, and resell with attribution.
If this helped you, please rate the template or follow me on the n8n Creator Page."
"AI Chatbot Call Center: Demo Call Center (Production-Ready, Part 2)",https://n8n.io/workflows/4045-ai-chatbot-call-center-demo-call-center-production-ready-part-2/,"Workflow Name: ‚òéÔ∏è Demo Call Center
Template was created in n8n v1.90.2
Skill Level: High
Categories: n8n, Chatbot
Stacks
Execute Sub-workflow Trigger node
Chat Trigger node
Redis node
Postgres node
AI Agent node
If node, Switch node, Code node, Edit Fields (Set)
Prerequisite
Execute Sub-workflow Trigger: Telegram Call In Workflow (or your own node)
Sub-workflow: Taxi Service (or your own node)
Sub-workflow: Taxi Booking Worker (or your own node)
Sub-workflow: Demo Call Back (or your own node)
Production Features
Scaling Design for n8n Queue mode in production environment
Optional Rate Limit design to prevent overused
Optional Long Terms Memory design
Multi-Service design
Testing Flow with or without dependance on other workflow.
Error Management
What this workflow does?
This is a n8n Demo Call Center Workflow demo. It is the main entry node for a Multiple Services Chatbot. It will receive message from the Call In Workflow, and decide which service should be use, or which service provider should be process the selected result.
How it works
The Flow Trigger node will wait for the message from the Call In Workflow or other Sub-workflow.
When message is received, it will first check for the Rate Limit.
If ok, load the Session Data from Cache.
Next, check the current Session for the channel_no (default is chat).
if channel_no is chat, continue to the AI Agent for chit-chat.
if channel_no is taxi or others, pass to the Service Input (i.e. Taxi Service) or Service Worker (i.e. Taxi Booking Worker) to handle it directly.
The AI Agent should decide which service (i.e. taxi) will be used at some point and update the channel_no in Session, and pass to the Service Node (i.e. Taxi Service) at once.
In case of any error, reply the error in Call Back.
Set up instructions
Pull and Set up the required SQL from our Github repository.
Create you Redis credentials, refer to n8n integration documentation for more information.
Select your Credentials in Rate Limit, Session, Provider and New Session.
Create you Postgres credentials, refer to n8n integration documentation for more information.
Select your Credentials in Postgres Chat Memory, Load User Memory and Save User Memory.
Modify the AI Agent prompt to fit your need
How to adjust it to your needs
In Session, we have a timestamp fields which is created at the Input node. The usage of this is combined to use with the session id to create a unique session, since some media, such as Telegram, do not have a unique session along with the chat.
You can use any AI Model for the AI Agent node
Learn we use the prompt for the Load/Save User Memory on demand.
Include is our prompt for the taxi service. You can add more service similar to this."
Build a Pipedrive MCP Server with Google Gemini AI,https://n8n.io/workflows/4040-build-a-pipedrive-mcp-server-with-google-gemini-ai/,"This n8n workflow leverages the power of AI and automation to streamline Pipedrive's CRM operations using natural language commands. It allows you to interact with your Deals, Leads, Persons, and Organizations simply by sending a message.
How it Works
The workflow initiates the process by sending the incoming message to an AI Agent. The AI Agent is powered by Google Gemini Chat Model and utilizing n8n's Simple Memory to maintain context, interprets the natural language command. Based on the interpreted command, the AI Agent instructs the MCP Client node to perform specific actions. The MCP Client then interacts with various nodes from Pipedrive designed to manage Deals, Leads, Persons, and Organizations within your CRM. These nodes can perform actions like creating, getting, updating, or searching for data in each category. Finally, a Gmail node sends a summary compiling the actions executed.
Set Up Steps
Set up your MCP Client: Paste the MCP URL from your MCP Trigger node into the MCP Client node.
Configure the AI Agent: Connect your Google Gemini Chat Model credentials or your desired LLM.
Configure Pipedrive: The individual nodes for Deals, Leads, Persons, and Organizations will need to be connected to your specific CRM. Add your Oath credentials for Pipedrive by creating a private app.
Configuring each specific node (Create Deal, Get Lead, Update Person, Search Organization, etc.) to perform the desired action.
Configure Send Summary: Set up the Gmail node to send the summary email.
Benefits
Significant Time Saving: Automates data entry, retrieval, and updates in your CRM, freeing up valuable time for sales teams and managers.
Increased Efficiency: Perform multiple CRM actions with a single natural language command.
Simplified CRM Interaction: Interact with your CRM using natural language instead of navigating complex interfaces.
Reduced Errors: Automation minimizes manual data entry errors.
Suggested Enhancements
Integrate More CRM Operations: Add nodes for additional CRM functionalities like logging activities, managing tasks, or working with other modules and tools.
Connect to Other Tools: Integrate with other sales or marketing tools to create a more comprehensive workflow.
Advanced AI Capabilities: Explore fine-tuning the AI model for better understanding of specific industry jargon or complex requests. Should you add more nodes and tools, you might want to insert a specific prompt to the AI agent to give more context about the tools and actions to perform.
User Management and Permissions: Customize the private app's permissions on your Pipedrive account to limit access.
Error Handling and Notifications: Add more robust error handling and notification systems to be alerted if a workflow run fails.
Need help?
Feel free to contact us at 1 Node.
Get instant access to a library of free resources we created."
Scrape & Analyse Meta Ad Library Image Ads with Apify and OpenAI,https://n8n.io/workflows/4003-scrape-and-analyse-meta-ad-library-image-ads-with-apify-and-openai/,"Meta Image Ads Analyzer
This n8n template builds an automated system to scrape, analyze, and extract insights from Meta advertising content. The workflow uses AI to perform deep analysis of image ads and organize the results in a structured format.
How it works
The workflow connects to Facebook's Ad Library to scrape image ads based on a specified page ID.
Images are filtered by reach and days running, then processed through OpenAI's GPT-4o to analyze their content.
Each image ad is systematically analyzed to extract key components: visual description, hook elements, main offer, call-to-action, and psychological triggers.
Results are processed through an AI agent that structures the data into standardized fields.
Original images are saved to Google Drive for reference.
All analysis and metadata are saved to a Google Sheet for easy access and further processing.
How to use
Once you've set up your credentials and configured the output:
Enter the Meta Ad Library URL you want to analyze in the Settings node.
Adjust the maximum number of ads to scrape and analyze based on your needs (defaults: 300 to scrape, 10 to analyze).
Configure the Google Drive folder to save the original ad images.
Ensure your Google Sheets connection is set up to receive the structured analysis.
Click ""Test Workflow"" to start the analysis process.
Requirements
Apify account (for Meta Ad Library scraping)
OpenAI API key (for image analysis)
Google Drive and Google Sheets access
Customizing this workflow
Modify the AI prompts in the ""Analyze Image Contents"" node to extract different information from the images.
Adjust the output formats in the Structured Output Parser node.
Change the Google Sheets mapping to match your desired output structure.
Increase the number of ads analyzed for more comprehensive research.
Filter ads based on different criteria by modifying the ""Filter only Image Ads"" node."
"Smart Job Search: Resume Scoring & Tailoring with OpenAI, Apify, and Airtable",https://n8n.io/workflows/3724-smart-job-search-resume-scoring-and-tailoring-with-openai-apify-and-airtable/,"Who is this for?
This workflow is designed for job seekers who want to automate their job application research and resume optimization. It's ideal for professionals who want to match their CVs to new job postings daily, improving the chance of landing interviews without manual work.
Use case
Problem: Manually searching for jobs, matching resumes, and updating application records is time-consuming and inefficient.
Use Case: Automatically fetches new job listings based on user preferences, scores them against the user's existing CV, generates a revamped CV tailored for each job, and stores everything neatly into an Airtable database for easy tracking.
What this workflow does?
Fetches user job preferences from Google Sheets daily.
Searches for jobs matching those preferences using Apify‚Äôs scraping.
Filters job posts that are fresh (posted within 24-48 hours).
Scores each job against the user‚Äôs current CV using an OpenAI agent.
Generates a revamped CV tailored to each job.
Stores the job listing, compatibility score, match reason, and revamped CV into Airtable for future use.
API Credentials Required
Google Sheets API Credentials ‚Äî for reading user-defined job preferences.
Apify API Key ‚Äî to scrape job postings (e.g., Indeed Scraper Actor).
OpenAI API Key ‚Äî for AI scoring and CV enhancement.
Airtable API Key ‚Äî for job listing and tracking.
Setup
Google Sheets: Store your job preferences (like titles, locations, etc.).
Apify API: Set up a scraper for LinkedIn, Indeed, or other job boards.
OpenAI API: Provide access to a GPT model (ideally GPT-4 Turbo) to handle CV scoring and revamping.
Airtable: Create two tables:
One for archived jobs (old jobs >48 hours).
One for current processed jobs with AI scores and revamped CVs.
Columns for Airtable:
job_title,company,location,date_posted,job_type,description,link,compatibilityScore,,,,.
n8n: Deploy the full workflow with nodes for triggers, loops, API calls, parsing, and storage.
How to customize it for your needs
Edit Job Preferences: Add or update the fields in Google Sheets (Columns: job_title, job_location) to search.
Fine-tune AI Prompts: Adjust the scoring criteria (e.g., favor remote roles, leadership experience, certifications).
Customize CV Style: Configure the AI to generate shorter, more detailed, or industry-specific resumes.
Change Storage Destination: Replace Airtable with Notion, Google Sheets, a CRM system, or even send yourself Slack updates.
Expand Job Sources: Easily swap the job scraper to pull listings from your favorite niche job boards.
Why Use This Template?
Saves 10+ hours/week on manual job search.
Instantly tailor CVs to each application.
Centralizes all data across Google Sheets and Airtable.
Flexible ‚Äî customize AI prompts, scoring logic, or expand to multiple users!
Need Assistance?
For setup guidance, customization, or business inquiries,
Email: ashish060921@gmail.com"
Convert Images to 3D Models with Fal AI Trellis and Store in Google Drive,https://n8n.io/workflows/3894-convert-images-to-3d-models-with-fal-ai-trellis-and-store-in-google-drive/,"This workflow allows users to convert a 2D image into a 3D model by integrating multiple AI and web services. The process begins with a user uploading or providing an image URL, which is then sent to a generative AI model capable of interpreting the content and generating a 3D representation in .glb format. The model is then stored and a download link is returned to the user.
Main Steps
Trigger Node: Initiates the workflow either via HTTP request, webhook, or manual execution.
Image Upload or Input: The image is acquired via direct upload or URL input.
API Integration: The image is sent to a 3D generation API (e.g., a service like Kaedim, Luma Labs, or a custom AI model).
Model Generation: The external API processes the image and creates a 3D model.
File Storage: The resulting 3D model is stored in cloud storage (e.g., S3, Google Drive, or a local server).
Response to User: A download link for the 3D model is returned to the user via the same communication channel (HTTP response, email, or chat).
Advantages
Automation: Eliminates the need for manual 3D modeling, saving time for artists, developers, and designers.
AI-Powered: Leverages AI to generate realistic and usable 3D models from simple 2D inputs.
Scalability: Can be triggered automatically and scaled up to handle many requests via n8n's automation.
Integration-Friendly: Easily extendable with other services like Discord, Telegram, or marketplaces for 3D assets.
No-Code Configuration: Built with n8n‚Äôs visual interface, making it editable without programming knowledge.
How It Works
Trigger: The workflow can be started manually (""When clicking ‚ÄòTest workflow‚Äô"") or automatically at scheduled intervals (""Schedule Trigger"").
Data Retrieval: The ""Get new image"" node fetches data from a Google Sheet, including the model image, product image, and product ID.
3D Image Creation: The ""Create 3D Image"" node sends the image data to the Fal.run API (Trellis) to generate a 3D model.
Status Check: The workflow periodically checks the request status (""Get status"" and ""Wait 60 sec."") until the job is marked as ""COMPLETED.""
Result Processing: Once completed, the 3D model URL is retrieved (""Get Url 3D image""), the file is downloaded (""Get File 3D image""), and uploaded to Google Drive (""Upload 3D Image"").
Sheet Update: The final 3D model URL is written back to the Google Sheet (""Update result"").
Set Up Steps
Prepare Google Sheet:
Create a Google Sheet with columns: IMAGE MODEL and 3D RESULT (empty).
Example sheet: Google Sheet Template.
Obtain Fal.run API Key:
Sign up at Fal.ai and get an API key.
Configure the Authorization header in the ""Create 3D Image"" node with Key YOURAPIKEY.
Configure Workflow Execution:
Run manually via the Test workflow button.
For automation, set up the Schedule Trigger node (e.g., every 5 minutes).
Verify Credentials: Ensure Google Sheets, Google Drive, and Fal.run API credentials are correctly set in n8n.
Once configured, the workflow processes new entries in the Google Sheet, generates 3D models, and updates the results automatically.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Discover Business Ideas from HackerNews Posts with GPT-4.1 Analysis and Google Sheets,https://n8n.io/workflows/3877-discover-business-ideas-from-hackernews-posts-with-gpt-41-analysis-and-google-sheets/,"What it is
This n8n workflow monitors top Hacker News posts (in the Front Page) and identifies business-related pain points using AI.
How It Works
It filters posts by engagement (comments > 80, points > 200, and recent date), extracts key metadata, fetches the article content, and uses a GPT-4.1-based agent to detect and summarize pain points.
Results are appended to a Google Sheet for further analysis.
Setup
To run this workflow, you need to set up credentials for:
OpenRouter: Uses an API Key. Generate this key directly from your OpenRouter account settings. (YT Tutorial : https://youtu.be/Cq5Y3zpEhlc)
Google Sheets: Uses OAuth 2.0. Requires setup in Google Cloud Console (enable Sheets API, create OAuth Client ID with n8n redirect URI) to get a Client ID & Secret.
Ensure these credentials are created and selected in the respective n8n nodes (Get Posts, OpenRouter Chat Model nodes, Output The Results)."
AI Agent Web Search using SearchAPI & LLM,https://n8n.io/workflows/3829-ai-agent-web-search-using-searchapi-and-llm/,"ü§ñ AI Agent Web Search using SearchApi & LLM
Who is this for?
This workflow is ideal for anyone conducting online research, including students, researchers, content creators, and professionals looking for accurate, up-to-date, and verifiable information. It also serves as an excellent foundation for building more sophisticated AI-driven applications.
What problem does this workflow solve? / Use case
This workflow automates web searches by enabling an AI agent to efficiently retrieve and summarize external, verifiable information, ensuring accuracy through source citations.
What this workflow does
Connects an AI agent node to SearchApi.io as an integrated search tool.
Empowers the AI agent to perform real-time web searches using various SearchApi engines (e.g., Google, Bing).
Allows the AI agent to dynamically determine search parameters based on user interaction, delivering contextually relevant results.
Ensures responses include clearly cited sources for validation and further exploration.
Setup
Install the SearchApi community node:
Open Settings ‚Üí Community Nodes inside your self‚Äëhosted n8n instance.
Fill npm Package Name with @searchapi/n8n-nodes-searchapi.
Accept the risk prompt, and hit Install.
It should now appear as a node when you search for it.
API Configuration:
Set up your SearchApi.io credentials in n8n.
Add your preferred LLM provider credentials (e.g., OpenRouter API).
Input Requirements:
Provide the YouTube video ID (e.g., wBuULAoJxok).
Connect LLM Integration:
Configure the summarization chain with your chosen model and parameters for text splitting.
How to customize this workflow to your needs
Integrate additional nodes to structure or store search results (e.g., saving to databases, Notion, Google Sheets).
Extend chatbot capabilities to integrate with messaging platforms (Slack, Discord) or email notifications.
Adjust search parameters and filters within the AI agent node to tailor information retrieval.
Example Usage
Input: User asks, ""What are the latest developments in AI regulation?""
Output: AI retrieves, summarizes, and cites recent, authoritative articles and news sources from the web."
Automatic WhatsApp Response with Groq LLM and Conversation Memory,https://n8n.io/workflows/3707-automatic-whatsapp-response-with-groq-llm-and-conversation-memory/,"Who is this for?
This workflow is intended for individual users or teams who want to automate their small business on WhatsApp practically by automatically replying about your business products or services using the groq model. Based on small businesses, there is a task to answer many customers or clients on WhatsApp, in reality answering one by one is very time-consuming and tiring, moreover we have to compose sentences first or click on templates and send them. This is also a form of dedication to the community at n8n and n8n company, as well as dedication to small businesses so that reality is no longer tiring and able to answer the problems of existing reality.
How it works?
Easy explanation:
Whatsapp Trigger is used to receive whatsapp messages from other numbers, this is also a place to input data and send it to the next node or to the AI Agent
If node is used to direct the workflow flow under certain conditions. This will be like checking whether a condition is met or not, directing the action to the next node, until it meets the criteria.
Then it is directed to the AI Agent to be able to use the groq model and storage using simple memory, why? Because it minimizes the costs used later but still works as expected.
The Groq chat model will think and carry out its tasks and store data in simple memory, and carry out its tasks to answer/send messages to whatsapp customers/clients on the AI Agent node bond.
And enjoy this workflow working for you.
Set up instructions
Complete what is in the nodes as stated in the notes column.
You need a ""Credential Account"" On the first node, namely as a WhatsApp trigger, you can register it by following the guide from n8n
You need an AI Agent to carry out the objectives of these tasks. You can change it to define below in ‚ÄúSource for Prompt (User Message)>Fix and ‚ÄúPrompt (User Message)‚Äù to text body>Expression.
You need a Groq model to be able to think and carry out tasks, you have to set this up by creating a ‚ÄúCredential Account‚Äù and just follow the steps on n8n and select the model.
After that, for practical and simple storage, add memory to the Nodes AI agent, namely ‚ÄúSimple Memory‚Äù, select Define below in ‚ÄúSession ID‚Äù and in ‚ÄúKey‚Äù is used to store incoming chat receipts.
After that, add nodes to produce action output on WhatsApp, namely Message Actions>Send message. After adding this set up nodes according to the existing notes, and I have set it up.
Save and run, test the workflow and activate the workflow. And this Workflow is ready to use.
Requirements
As a reminder:
Must set up in nodes, such as what your business description is, also according to your small business conditional, so that the AI Agent is in accordance with your business knowledge base
Must have (if not, make sure you have registered) on each ""Credential Account"" by following the guide on how to do it n8n the guidelines are very complete
Do not forget to save, and make sure the workflow is active.
How to customize this workflow to your needs
You can directly set up your business knowledge base on the nodes, so that the accuracy is also high when carrying out tasks and answering them."
Generate 360¬∞ Virtual Try-on Videos for Clothing with Kling API (unofficial),https://n8n.io/workflows/3411-generate-360-virtual-try-on-videos-for-clothing-with-kling-api-unofficial/,"What's the workflow used forÔºü
Leverage this Kling API (unofficial) provided by PiAPI workflow to streamline virtual try-on video creation. This tool is designed for e-commerce platforms, fashion brands, content creators and content influencers. By uploading model and clothing images and linking PiAPI account, users can swiftly generate a realistic video of the model sporting the outfit with a 360¬∞ turn, offering an immersive viewing experience.
Step-by-step Instruction
For basic settings of virtual try-on, check API doc to get best practice.
Fill in your X-API-Key of your PiAPI account in Preset Parameters node.
Upload the model photo and provide target clothing image urls.
Click Test Workflow to generate virtual try-on image.
Get the video output in the final node.
Param Settings
If you want to change into a dress, input the model_input URL and the dress_input URL in the parameters.
If you want to change into separates, input model_input URL, upper_input URL and lower_input URL in Preset Parameters.
Use Case
Input imagesÔºö

Output Video
<video src=""https://static.piapi.ai/n8n-instruction/virtual-try-on/example1.mp4"" controls />
The output demonstrates that the model is wearing the clothing from the specified image and showcases a rotating runway-style view.
This workflow enables you to efficiently test garment-on-model presentation effects while reducing business model validation costs to a certain extent."
"Generate Graphic Wallpaper with Midjourney, GPT-4o-mini and Canvas APIs",https://n8n.io/workflows/3627-generate-graphic-wallpaper-with-midjourney-gpt-4o-mini-and-canvas-apis/,"Who is the template for?
This workflow is specifically designed for content creators and social media professionals, enabling Instagram and X (Twitter) influencers to produce highly artistic visual posts, empowering marketing teams to quickly generate event promotional graphics, assisting blog authors in creating featured images and illustrations, and helping knowledge-based creators transform key insights into easily shareable card visuals.
Set up Instructions
Fill in your API key from PiAPI.
Fill in Basic Params Node following the sticky note guidelines.
Set up a design template in Canvas Switchboard.
Make a simple template in Switchboard.
Click Crul and get the API code to fill in JSON of Design in Canvas.
Click Test Workflow and get a url result.
Use Case
Here we will provide some setting examples to help users find a proper way to use this workflow. User could change these settings based on specific purposes.
Basic Params Setting:
theme: Hope
scenario: Don't know about the future, confused and feel lost with tech-development.
style: Cinematic Grandeur, Sci-Tech Aesthetic, 3D style
example: 1. March. Because of your faith, it will happen. 2. Something in me will save me. 3. To everyone carrying a heavy heart in silence. You are going to be okay. 4. Tomorrow will be better.
image prompt: A cinematic sci-fi metropolis where Deep Neural Nets control a hyper-connected society. Holographic interfaces glow in the air as robotic agents move among humans, symbolizing Industry 4.0. The scene contrasts organic human emotion with cold machine precision, rendered in a hyper-realistic 3D style with futuristic lighting. Epic wide shots showcase the grandeur of this civilization‚Äôs industrial evolution.
Output Image:
More Example Results for Reference"
MCP Supabase Agent ‚Äì Manage Your Database with AI,https://n8n.io/workflows/3641-mcp-supabase-agent-manage-your-database-with-ai/,"Hi, I‚Äôm Amanda üå∑
This workflow was built with love to help you manage your Supabase database using natural language, powered by the MCP (Multi-Channel Protocol) AI Agent on n8n.
With just a message like ‚Äúupdate the status to active where city is New York‚Äù, your agent will know exactly what to do ‚Äî safely, step by step, and always asking for confirmation before deleting anything.
It‚Äôs ideal for developers who want a smart assistant to create, update, delete or search database records without writing SQL or opening Supabase.
What this workflow does
Receives messages through a chat interface or webhook
Translates them into actions using GPT-4o (via LangChain Agent)
Interacts with Supabase using a custom MCP tool
Supports create, update, delete, search (single or all rows)
Handles confirmations, validations, and missing data checks
Setup (quick and easy üí´)
Connect your Supabase credentials
Adjust your table and field names
Link the MCP webhook to your AI frontend (Typebot, WhatsApp, etc.)
Customize the system message to reflect your tone of voice
Start chatting ‚Äî your agent is ready to assist!
‚úÖ Works on n8n Cloud and Self-Hosted
üß† Built with GPT-4o + LangChain + Supabase + MCP Trigger
üí° No code or SQL required
Want it tailored to your project?
‚ù§Ô∏è Buy Workflows: https://iloveflows.gumroad.com
üí¨ Hire My Services: +5517991557874 (WhatsApp)"
Build your own Github MCP server,https://n8n.io/workflows/3635-build-your-own-github-mcp-server/,"This n8n demonstrates how to build your own Github MCP server to personalise it to your organisation's repositories, issues and pull requests.
This n8n implementation, though not as fully featured as the official MCP server offered by Github, allows you to control precisely what access and/or functionality is granted to users which can make MCP use simpler and in some cases, more secure. The use-case in this template is to simply view and comment on issues within a specific repository but can be extended to meet the needs of your team.
This MCP example is based off an official MCP reference implementation which can be found here https://github.com/modelcontextprotocol/servers/tree/main/src/github
How it works
A MCP server trigger is used and connected to 3 custom workflow tools. We're using custom workflow tools as there is quite a few nodes required for each task.
Behind these tools are regular Github nodes although preconfigured with credentials and targeted repository.
The ""Get Issue Comments"" and ""Create Issue Comment"" tools depend on obtaining an Issue Number first. The agent should call the ""Get Latest Issues"" tool for this.
How to use
This Github MCP server allows any compatible MCP client to view and comment on Github Issues. You will need to have a Github account and repository access available before you can use this server.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""Can you get me the latest issues about MCP?""
""What is the current progress on Issue 12345?""
""Please can you add a comment to Issue 12345 that they should try installing the latest version and see if that works?""
Requirements
Github for account and repository access. The repository need not be your own but you'll still need to ensure you have the correct permissions.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
Extend this template to interactive with pull requests or workflows within your own company's Github repositories. Alternatively, pull in metrics and generate reports for programme managers.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
"Ultimate AI Assistant: Automate Email, Calendar, WebSearch, Notion, RAG & X",https://n8n.io/workflows/3629-ultimate-ai-assistant-automate-email-calendar-websearch-notion-rag-and-x/,"This is an ultimate AI assistant: Handle emails, schedule meetings, search the web, take notes, post to social media, and retrieve information from your knowledge base, all through simple Telegram commands (text, voice, or image).
What is included:
1 Ready-to-use n8n workflow file (.json)
4 Instructional videos demonstrating how to connect n8n with:
Google Products (Gmail, Calendar, Sheets)
Telegram
Pinecone Vector Store
Notion
Who is this for?
This workflow is ideal for:
Individuals seeking to automate personal and professional tasks.
Professionals needing efficient management of emails, calendars, and contacts.
Content creators or researchers requiring quick information retrieval and drafting capabilities.
Anyone wanting a centralized AI assistant.
What problem is this workflow solving?
Managing numerous digital tasks across different platforms can be time-consuming and fragmented. This workflow consolidates control into Telegram, leveraging AI to understand your needs and automate actions across email, calendar, contacts, web search, note-taking (Notion), social media (X), and your personal knowledge base (Pinecone RAG), saving you time and effort. It ensures tasks are handled correctly using a built-in verification step.
What this workflow does:
This workflow acts as a central AI Agent that:
Receives Input via Telegram: Accepts text messages, voice notes (transcribes them using OpenAI), or images (analyzes them using OpenAI).
Understands Requests: Uses an AI Agent (powered by models like GPT) to interpret the user's instructions.
Orchestrates Tools: Selects and uses the appropriate tools based on the request:
Email (Gmail): Send, read, reply, draft, and label emails.
Calendar (Google Calendar): Create, get, update, and delete events, managing attendees if needed.
Contacts (Google Sheets): Look up and add contact information.
Web Search (Tavily): Search the internet for up-to-date information.
Knowledge Retrieval (Pinecone): Query your personal vector database (RAG).
Note Taking (Notion): Create notes in a specified Notion database.
Social Media (X/Twitter): Post tweets directly to X.
Maintains Context: Uses Window Buffer Memory to remember recent interactions.
Ensures Accuracy: Employs a ""Think"" tool to double-check tool usage and task completion.
Responds via Telegram: Sends the results or confirmation back to the user in the Telegram chat.
Setup:
Telegram: Connect your Telegram bot credentials.
AI Model: Connect your OpenAI API key.
Email: Connect your Gmail account.
Calendar: Connect your Google Calendar account.
Contacts: Connect your Google Sheets account.
Knowledge Base (RAG): Connect Pinecone.
Note Taking: Connect Notion and configure Notion Database.
Web Search: Add Tavily API key.
Social Media: Connect your X (Twitter) account.
(Refer to the included videos for detailed setup guidance)
How to customize this workflow:
AI Prompts: Modify the system messages and prompts within the ""AI Agent"" node.
AI Models: Swap out the language model.
Tools: Add, remove, or replace tool nodes.
Category:
Automation, AI Agent, Personal Assistant, Productivity, Task Management, Telegram"
Convert Text to Speech with Local KOKORO TTS,https://n8n.io/workflows/3547-convert-text-to-speech-with-local-kokoro-tts/,"Disclaimer
The Execute Command node is only supported on self-hosted (local) instances of n8n.
Introduction
KOKORO TTS - Kokoro TTS is a compact yet powerful text-to-speech model, currently available on Hugging Face and GitHub. Despite its modest size‚Äîtrained on less than 100 hours of audio‚Äîit delivers impressive results, consistently topping the TTS leaderboard on Hugging Face. Unlike larger systems, Kokoro TTS offers the advantage of running locally, even on devices without GPUs, making it accessible for a wide range of users.
Who will benefit from this integration?
This will be useful for video bloggers, TikTokers, and it will also enable the creation of a free voice chat bot. Currently, TTS models are mostly paid, but this integration will allow for fully free voice generation. The possibilities are limited only by your imagination.
Note
Unfortunately, we can't interact with the KOKORO API via browser URL (GET/POST),
but we can run a Python script through n8n and pass any variables to it.
In the tutorial, the D drive is used, but you can rewrite this for any paths, including the C drive.
Step 1
You need to have Python installed. link
Also, download and extract the portable version of KOKORO from GitHub.
Create a file named voicegen.py with the following code in the KOKORO folder: (C:\KOKORO). As you can see, the output path is: (D:\output.mp3).
import sys
import shutil
from gradio_client import Client

# Set UTF-8 encoding for stdout
sys.stdout.reconfigure(encoding='utf-8')

# Get arguments from command line
text = sys.argv[1] # First argument: input text
voice = sys.argv[2] # Second argument: voice
speed = float(sys.argv[3]) # Third argument: speed (converted to float)

print(f""Received text: {text}"")
print(f""Voice: {voice}"")
print(f""Speed: {speed}"")

# Connect to local Gradio server
client = Client(""http://localhost:7860/"")

# Generate speech using the API
result = client.predict(
text=text,
voice=voice,
speed=speed,
api_name=""/generate_speech""
)

# Define output path
output_path = r""D:\output.mp3""

# Move the generated file
shutil.move(result[1], output_path)

# Print output path
print(output_path)
Step 2
Go to n8n and create the following workflow.
Step 3
Edit Field Module.
{
  ""voice"": ""af_sarah"",
  ""text"": ""Hello world!""
}
Step 4
We‚Äôll need an Execute Command module with the command: python
C:\KOKORO\voicegen.py ‚Äú{{ $json.text }}‚Äù ‚Äú{{ $json.voice }}‚Äù 1
Step 5
The script is already working, but to listen to it, you can connect a Binary module with the path to the generated MP3 file
D:/output.mp3
Step 6
Click ‚ÄúText workflow‚Äù and enjoy the result.
There are more voices and accents than in ChatGPT, plus it‚Äôs free.
P.S.
If you want, there is a detailed tutorial on my blog."
Automatically Create Cinematic Quote Videos with AI and Upload to YouTube,https://n8n.io/workflows/3438-automatically-create-cinematic-quote-videos-with-ai-and-upload-to-youtube/,"‚ö†Ô∏è Important Disclaimer:
This template is only compatible with a self-hosted n8n instance using a community node.
Who is this for?
This workflow is ideal for digital content creators, marketers, social media managers, and automation enthusiasts who want to produce fully automated vertical video content featuring inspirational or motivational quotes. Specifically tailored for Thai language, it effectively demonstrates integration of AI-generated imagery, video, ambient sound, and visually appealing quote overlays.
What problem is this workflow solving?
Manually creating high-quality, vertically formatted quote videos is often repetitive, time-consuming, and involves multiple tedious steps like selecting suitable visuals, editing audio tracks, and correctly overlaying text. Additionally, manual uploading to platforms like YouTube and maintaining accurate content records are prone to errors and inefficiencies.
What this workflow does:
Fetches a quote, author, and scenic background description from a Google Sheet.
Automatically generates a vertical background image using the Flux AI (txt2img) API.
Transforms the AI-generated image into a subtly animated cinematic vertical video using the Kling video-generation API.
Generates an immersive, ambient background sound using ElevenLabs‚Äô sound generation API.
Dynamically overlays the selected Thai-language quote and author text onto the generated video using FFmpeg, ensuring visually appealing typography (e.g., Kanit font).
Automatically uploads the final video to YouTube.
Updates the resulting YouTube video URL back to the Google Sheet, keeping your content records current and well-organized.
Setup
Requirements:
This workflow requires a self-hosted n8n instance, as the execution of FFmpeg commands is not supported on n8n Cloud. Ensure FFmpeg is installed on your self-hosted environment.
API keys and accounts setup for Flux, Kling, ElevenLabs, Google Sheets, Google Drive, and YouTube.
Google Sheets Setup:
Your Google Sheet must include these columns:
Index Unique identifier for each quote
Quote (Thai) Quote text in Thai language (or your chosen language)
Pen Name (Thai) Author or pen name of the quote's creator
Background (EN) Short English description of the scene (e.g., ""sunrise over mountains"")
Prompt (EN) Detailed English prompt describing the image/video scene (e.g., ""peaceful sunrise with misty mountains"")
Background Image URL of AI-generated image (updated automatically)
Background Video URL of generated video (updated automatically)
Music Background URL of generated ambient audio (updated automatically)
Video Status YouTube URL (updated automatically after upload)
A ready-to-use Google Sheets template is provided [here (provide your actual link)].
To help you get started quickly, you can use this template spreadsheet.
Next steps:
Authenticate Google Sheets, Google Drive, YouTube API, Flux AI, Kling API, and ElevenLabs API within n8n.
Ensure FFmpeg supports fonts compatible with your chosen language (for Thai, ""Kanit"" font is recommended).
Prepare your Google Sheets with desired quotes, authors, and image/video prompts.
How to customize this workflow to your needs:
Fonts: Adjust font type, size, color, and positioning within the provided FFmpeg commands in the workflow‚Äôs code nodes. Verify that selected fonts properly support your target language.
Media Customization: Customize the scene descriptions in your Google Sheet to change image/video backgrounds automatically generated by AI.
Quote Management: Easily manage, add, or update quotes and associated details directly via Google Sheets without workflow modifications.
Audio Ambiance: Customize or adjust the ambient sound prompt for ElevenLabs within the workflow‚Äôs HTTP Request node to match your video's desired mood.
Benefits of using AI-generated content and localized fonts:
Leveraging AI-generated visual and audio elements along with localized fonts greatly enhances audience engagement by creating visually appealing, professional-quality content tailored specifically for your target audience. This automated workflow drastically reduces production time and manual effort, enabling rapid, consistent content creation optimized for platforms such as YouTube Shorts, Instagram Reels, and TikTok."
Generate SEO Keywords with AI: Topic to Keyword List in Seconds,https://n8n.io/workflows/3544-generate-seo-keywords-with-ai-topic-to-keyword-list-in-seconds/,"Who is this template for?
This AI Keyword Generator workflow template is designed for marketers, SEO specialists, and content creators who need to quickly generate high-quality keyword lists for their content strategy. Instead of spending hours researching keywords manually, this AI-powered tool delivers targeted keyword suggestions based on your specific criteria.
What problem does this workflow solve?
Keyword research is a time-consuming but essential part of SEO and content marketing. Many professionals struggle with:
Finding relevant keywords that match specific search intents
Balancing between short-tail and long-tail keywords
Generating comprehensive keyword lists that cover different aspects of a topic
Consistently identifying high-potential keywords for content creation
What this workflow does
This n8n workflow leverages AI to automatically generate a customized list of 15-20 high-potential keywords based on three simple inputs:
Topic - The main subject area you want keywords for
Search Intent - Choose between Navigational, Informational, Commercial, or Transactional
Keyword Type - Select Short-Tail or Long-Tail keywords
The workflow processes your input through an AI language model that follows SEO best practices to generate relevant keywords. It then formats the results and delivers them directly to your email inbox, ready for use in your SEO strategy.
Setup
Setting up this workflow is straightforward:
Add your credentials for the AI language model in the ""Select your Chat Model"" node
Click on the node and connect your Groq account (and choose any LLM you want, like: OpenAI, Claude AI or Llama) or replace with another LLM provider
Configure email delivery in the ""Send Result"" node
Update the ""sendTo"" parameter with your email address
Add your Gmail credentials or replace with your preferred email service
Test your workflow by clicking the ""Test Workflow"" button
Use the form to enter your topic, search intent, and keyword type
Check your email for the generated keyword report
Activate the workflow once testing is complete
How to customize this workflow
The template is highly adaptable to fit your specific needs:
Replace the email node with a database or spreadsheet node to store keywords
Modify the AI prompts in the ""AI Keyword Agent"" to adjust the keyword generation strategy
Add additional filtering nodes to further refine keywords based on custom criteria
Integrate with other SEO tools to analyze competition or search volume for generated keywords
This workflow serves as a powerful starting point for automating your keyword research process, saving you valuable time while delivering consistent, high-quality results."
Parse Gmail Inbox and Transform into Todoist tasks with Solve Propositions,https://n8n.io/workflows/3507-parse-gmail-inbox-and-transform-into-todoist-tasks-with-solve-propositions/,"Who is it for?
If you are getting a lot of emails into your Gmail inbox, then probably some of those can be solved easly by replying or by doing specific short tasks. But analyzing whole email thread content just to catch up with multiple threads can be very wasteful. So by using AI you can actually get simple propositions of what should be done before closing this specific email and actual proposed answer to that email.
This is especially useful if you need to do some actions before replying to email. In that case you can simply assign task to specific person, await until it's done, copy-paste AI answer when it's done, and close.
Another good use would be if on one inbox there are working multiple people. It can make the process much more streamlined.
How It Works?
Script runs on your selected trigger. If you are using section ""Read and Star"", then you may use ""Email Trigger"".
Automation is looking for exiting open Todoist tasks, that have the same title as email
If task does not exist, then we are asking AI to analyze thread and give output that is Todoist-API-ready:
having summary of email content
having proposed actions to be taken
having proposed answer to this email
If email was unstarred for some reason but task was not closed, then task is being closed automatically.
Script FOR PURPOSE is not trying to unstar messagess which have closed tasks, because this could lead to some inconsistencies.
How to set up?
Select and setup your triggers, depending on your needs
Setup connections using N8N instructions. You will need:
Gmail
Todoist
AI (in this workflow OpenAI is used)
(Optional) Remove ""Read and Star"" section if you don't want tasks automatically read and starred.
(Optional) Adjust AI node - especially useful if you want to use different model or have response in different language
NOTE Chat does not heave memory attached on purpose. The purpose is that it should analyze each inbox message separately, not in thread. When using memory, it can get lost easily.
NOTE2 You might want to adjust limits on nodes ""Get Unread From Inbox"", ""Get Starred From Inbox"" and ""Get Open Tasks"", especially if having issues with model complying to output structure.
And that's it. I hope that this automation will make your Gmail <-> Todoist process much more streamlined!
What's More?
There is actually more that you could do with this automation, but it really depends on your needs. For example, you could add Form trigger to handle incoming support requests. Another thing is that you could replace Todoist with Asana or any database (like NocoDB) if you are using it for your task management."
"Analyze DEX Liquidity, Trades & Spot Pairs with CoinMarketCap AI Agent",https://n8n.io/workflows/3424-analyze-dex-liquidity-trades-and-spot-pairs-with-coinmarketcap-ai-agent/,"Gain full visibility into decentralized exchanges using CoinMarketCap‚Äôs DEXScan API‚Äîpowered by AI.
This workflow is part of the CoinMarketCap AI Analyst system and delivers real-time and historical insights on spot trading pairs, DEX liquidity, trading activity, and OHLCV data across chains like Ethereum, Polygon, Solana, and more.
Use this workflow as a sub-agent triggered by a parent supervisor workflow, or run it manually with inputs sessionId and message.
üîß Supported Tools (8 Total)
DEX Metadata ‚Üí Static info (name, launch date, logo, URLs)
DEX Networks List ‚Üí All supported DEX chains + network metadata
DEX Listings Quotes ‚Üí Ranked list of DEXs with live trading volume, market share
DEX Pair Quotes (Latest) ‚Üí Real-time liquidity, price, and buy/sell stats
DEX OHLCV Historical ‚Üí Time-series data (daily/hourly/1m)
DEX OHLCV Latest ‚Üí Today‚Äôs price, volume, open/close for pairs
DEX Trades Latest ‚Üí Up to 100 recent trades for any DEX pair
DEX Spot Pairs Latest ‚Üí Active token pairs across DEXs + filters (volume, liquidity, volatility)
Agent Architecture
AI Model: gpt-4o-mini
Context Memory: Window buffer using sessionId
Trigger Input: message, sessionId
Execution: Via Execute Workflow or parent AI supervisor
Design: Tool-based LangChain agent with CMC DEXScan endpoints
üí° Use Cases
üîπ Find top DEXs by 24h volume
üîπ Get spot pairs with highest liquidity on a specific network
üîπ Track historical OHLCV for Uniswap pairs
üîπ View latest trades for SOL/USDC pool
üîπ Analyze tax, pooled % and holders for specific pairs
üîπ Filter pairs by 24h volume, percent change, liquidity, or number of transactions
‚úÖ Example Queries
‚úÖ ""Top 5 DEXs by 24h volume on Ethereum""
‚úÖ ""Get historical OHLCV for SOL-USDC on Solana""
‚úÖ ""Latest trades for a PancakeSwap pair""
‚úÖ ""Show all spot pairs with over $500K in liquidity on Polygon""
‚úÖ ""Retrieve metadata for Uniswap and SushiSwap""
üõ†Ô∏è Setup Instructions
Get a CoinMarketCap API Key
Sign up at: https://coinmarketcap.com/api/
Add API Key to Credentials in n8n
Use HTTP Header Auth method
Trigger from Parent Workflow (Optional)
Use Execute Workflow and pass message and sessionId
Test Prompt Ideas
Try: ""Compare liquidity of Uniswap and Curve pairs on Ethereum""
Sticky Notes Included
DEXScan Agent Guide ‚Äì Workflow architecture + supported tools
Usage & API Call Examples ‚Äì Prompts, test inputs, setup flow
Error Codes + Licensing ‚Äì 400/401/429/500 troubleshooting, IP rights
‚úÖ Final Notes
This agent is part of the CoinMarketCap AI Analyst System, which includes multiple specialized agents for cryptocurrencies, exchanges, and community data. Visit my Creator profile to find the full suite of tools.
Master DEX analytics with AI‚Äîget powerful liquidity, trading, and pair insights in seconds."
Automated Voice Appointment Reminders with Google Calendar GPT ElevenLabs Gmail,https://n8n.io/workflows/3194-automated-voice-appointment-reminders-with-google-calendar-gpt-elevenlabs-gmail/,"This workflow automates voice reminders for upcoming appointments by generating a professional audio message and sending it to clients via email with the voice file attached.
It integrates Google Calendar to track appointments, ElevenLabs to generate high-quality voice messages, and Gmail to deliver them efficiently.
Who Needs Automated Voice Appointment Reminders?
This automated voice appointment reminder system is ideal for businesses that rely on scheduled appointments. It helps reduce no-shows, improve client engagement, and streamline communication.
Medical Offices & Clinics ‚Äì Ensure patients receive timely appointment reminders.
Real Estate Agencies ‚Äì Keep potential buyers and renters informed about property visits.
Service-Based Businesses ‚Äì Perfect for salons, consultants, therapists, and coaches.
Legal & Financial Services ‚Äì Help clients remember important meetings and consultations.
If your business depends on scheduled appointments, this workflow saves time and enhances client satisfaction. üöÄ
Why Use This Workflow?
Ensures clients receive timely reminders.
Reduces appointment no-shows and scheduling issues.
Automates the process with a personalized voice message.
Step-by-Step: How This Workflow Automates Voice Reminders
Trigger the Workflow ‚Äì The system runs manually or on a schedule to check upcoming appointments in Google Calendar.
Retrieve Appointment Data ‚Äì It fetches event details (client name, time, and location) from Google Calendar.
The workflow uses the summary, start.dateTime, location, and attendees[0].email fields from Google Calendar to personalize and send the voice reminders.
Generate a Voice Reminder ‚Äì Using ElevenLabs, the workflow converts the appointment details into a natural-sounding voice message.
Send via Email ‚Äì The generated audio file is attached to an email and sent to the client as a reminder.
Customization: Tailor the Workflow to Your Business Needs
Adjust Trigger Frequency ‚Äì Modify the scheduling to run daily, hourly, or at specific intervals.
Customize Voice Message Format ‚Äì Change the script structure and voice tone to match your business needs.
Change Notification Method ‚Äì Instead of email, integrate SMS or WhatsApp for delivery.
üîë Prerequisites
Google Calendar Access ‚Äì Ensure you have access to the calendar with scheduled appointments.
ElevenLabs API Key ‚Äì Required for generating voice messages (you can start for free).
Gmail API Access ‚Äì Needed for sending reminder emails.
n8n Setup ‚Äì The workflow runs on an n8n instance (self-hosted or cloud).
üöÄ Step-by-Step Installation & Setup
Set Up Google Calendar API
Go to Google Cloud Console.
Create a new project and enable Google Calendar API.
Generate OAuth 2.0 credentials and save them for n8n.
Get an ElevenLabs API Key
Sign up at ElevenLabs.
Retrieve your API key from the dashboard.
Configure Gmail API
Enable Gmail API in Google Cloud Console.
Create OAuth credentials and authorize your email address for sending.
Deploy n8n & Install the Workflow
Install n8n (Installation Guide).
Add the required Google Calendar, ElevenLabs, and Gmail nodes.
Import or build the workflow with the correct credentials.
Test and fine-tune as needed.
‚ö† Important:
The LangChain Community node used in this workflow only works on self-hosted n8n instances. It is not compatible with n8n Cloud. Please ensure you are running a self-hosted instance before using this workflow.
Summary
This workflow ensures a professional and seamless experience for your clients, keeping them informed and engaged. üöÄüîä
Phil | Inforeole"
Build Custom AI Agent with LangChain & Gemini (Self-Hosted),https://n8n.io/workflows/3326-build-custom-ai-agent-with-langchain-and-gemini-self-hosted/,"Overview
This workflow leverages the LangChain code node to implement a fully customizable conversational agent. Ideal for users who need granular control over their agent's prompts while reducing unnecessary token consumption from reserved tool-calling functionality (compared to n8n's built-in Conversation Agent).
Setup Instructions
Configure Gemini Credentials: Set up your Google Gemini API key (Get API key here if needed). Alternatively, you may use other AI provider nodes.
Interaction Methods:
Test directly in the workflow editor using the ""Chat"" button
Activate the workflow and access the chat interface via the URL provided by the When Chat Message Received node
Customization Options
Interface Settings: Configure chat UI elements (e.g., title) in the When Chat Message Received node
Prompt Engineering:
Define agent personality and conversation structure in the Construct & Execute LLM Prompt node's template variable
‚ö†Ô∏è Template must preserve {chat_history} and {input} placeholders for proper LangChain operation
Model Selection: Swap language models through the language model input field in Construct & Execute LLM Prompt
Memory Control: Adjust conversation history length in the Store Conversation History node
Requirements:
‚ö†Ô∏è This workflow uses the LangChain Code node, which only works on self-hosted n8n.
(Refer to LangChain Code node docs)"
"Self-Learning AI Assistant with Permanent Memory | GPT,Telegram & Pinecone RAG",https://n8n.io/workflows/3183-self-learning-ai-assistant-with-permanent-memory-or-gpttelegram-and-pinecone-rag/,"Your AI secretary that self-learning every day and remembers everything you said (text, audio, image).
Imagine having a personal AI secretary accessible right from your Telegram, ready to assist you with information and remember everything you discuss. This n8n workflow transforms Telegram into your intelligent assistant, capable of understanding text, audio, and images, and continuously learning from your interactions. It integrates RAG's offline data ingestion and online querying functionalities, letting you save inspiration and key information permanently in real-time, and giving you an AI assistant that remembers all your dialogues and information. It builds and queries a powerful vector database in real-time, ensuring relevant and accurate responses. Video guidance on how to set up Telegram integration is also included.
Who is this for?
This template is ideal for:
Individuals seeking a personal AI assistant for quick information retrieval and note-taking.
Professionals who need to keep track of important conversations and insights.
Anyone interested in leveraging the power of Retrieval-Augmented Generation (RAG) and vector databases for personal knowledge management.
Users who want a self-learning AI that improves over time based on their interactions.
What problem is this workflow solving?
This workflow integrates RAG's offline data ingestion and online querying functionalities, letting you save inspiration and key information permanently in real-time, and giving you an AI assistant that remembers all your dialogues and information. This workflow addresses the challenge of information overload and the need for an easily accessible, personalized knowledge base. It eliminates the need to manually organize notes and search through past conversations. By automatically storing and retrieving information from a vector database, this workflow makes it effortless to access the knowledge you need, when you need it. It also provides a way to retain information from various media types like voice notes and images.
What this workflow does:
This workflow automates the following steps:
Instant Information Capture: Receives text messages, audio notes (transcribed), and images (with content analysis) directly from your Telegram.
Intelligent Question Answering: When you ask a question, the AI searches its knowledge base (Pinecone vector store) for relevant information and provides a comprehensive answer. It even considers your recent conversations for context.
Automatic Knowledge Storage: When you make a statement or provide information, the AI extracts key details and saves them in a Google Docs ""memory palace.""
Daily Self-Learning: Every day, the workflow automatically takes all the information stored in the Google Docs, converts it into a vector representation, and adds it to its knowledge base (Pinecone vector store). This ensures the AI continuously learns and remembers everything you've shared.
Image Understanding: Extracts text and information from images you send.
Audio Transcription: Automatically transcribes your voice notes into text for processing and storage.
Short-Term Memory: Remembers recent interactions within a session for more context-aware conversations.
Setup:
To get started, you'll need to connect the following services to your n8n instance:
Telegram: Connect your Telegram bot API credentials. A video guidance is included for telegram integration setup.
OpenAI: Provide your OpenAI API key for audio transcription and image analysis.
Pinecone: Set up a Pinecone account and provide your API key and environment. Create a namespace in Pinecone.
Google Docs: Connect your Google account with access to Google Docs. You'll need to create a Google Doc that will serve as the daily ""memory palace"" and provide its ID in the workflow.
How to customize this workflow:
Adjust the AI Agent's Personality: Modify the system prompt in the ""AI Agent"" node to tailor the AI's tone and behavior.
Expand Knowledge Sources: Integrate other data sources into the daily learning process, such as emails or other documents, by adding more nodes to the scheduled trigger workflow.
Add More Tools for the AI Agent: Integrate additional tools into the AI Agent, such as web search or other APIs, to further enhance its capabilities.
Modify the Daily Schedule: Adjust the schedule trigger to run at a different time or interval."
CoinMarketCap Telegram Price Bot,https://n8n.io/workflows/3158-coinmarketcap-telegram-price-bot/,"Get real-time cryptocurrency prices directly in Telegram! This workflow integrates CoinMarketCap API with Telegram, allowing users to request live crypto prices simply by sending a message to the bot. Ideal for crypto traders, analysts, and enthusiasts who need quick and easy access to market data.
How It Works
A Telegram bot listens for user input (e.g., ""BTC"" for Bitcoin).
The workflow sends a request to the CoinMarketCap API to fetch the latest price.
The response is processed using an AI-powered language model (GPT-4o-mini) for structured messaging.
The workflow logs session data using a memory buffer for better response tracking.
The latest price is sent back to the user via Telegram.
Set Up Steps
Create a Telegram Bot
Use @BotFather on Telegram to create a bot and obtain an API token.
Get a CoinMarketCap API Key
Sign up at CoinMarketCap and retrieve your API key.
Configure API Credentials in n8n
Add the CoinMarketCap API key under HTTP Header Auth.
Add your Telegram bot token under Telegram API credentials.
Deploy and Test
Send a message (e.g., ""BTC"") to your Telegram bot and receive live price updates instantly!
Automate your crypto price tracking today with this powerful Telegram bot!"
Split Test Different Agent Prompts with Supabase and OpenAI,https://n8n.io/workflows/2992-split-test-different-agent-prompts-with-supabase-and-openai/,"Split Test Agent Prompts with Supabase and OpenAI
Use Case
Oftentimes, it's useful to test different settings for a large language model in production against various metrics. Split testing is a good method for doing this.
What it Does
This workflow randomly assigns chat sessions to one of two prompts, the baseline and the alternative. The agent will use the same prompt for all interactions in that chat session.
How it Works
When messages arrive, a table containing information regarding session ID and which prompt to use is checked to see if the chat already exists
If it does not, the session ID is added to the table and a prompt is randomly assigned
These values are then used to generate a response
Setup
Create a table in Supabase called split_test_sessions. It needs to have the following columns: session_id (text) and show_alternative (bool)
Add your Supabase, OpenAI, and PostgreSQL credentials
Modify the Define Path Values node to set the baseline and alternative prompt values.
Activate the workflow and test by sending messages through n8n's inbuilt chat
Experiment with different chat sessions to test see both prompts in action
Next Steps
Modify the workflow to test different LLM settings such as temperature
Add a method to measure the efficacy of the two alternative prompts"
üåêü™õ AI Agent Chatbot with Jina.ai Webpage Scraper,https://n8n.io/workflows/2943-ai-agent-chatbot-with-jinaai-webpage-scraper/,"The üåêü§ñ AI Agent Chatbot with Jina.ai Webpage Scraper workflow is a powerful automation designed to integrate real-time web scraping capabilities into an AI-driven chatbot. Here's how it works and why it's important:
How It Works
üí¨ Chat Trigger: The workflow begins when a user sends a chat message, triggering the ""When chat message received"" node.
üß† AI Agent Processing: The input is passed to the ""Jina.ai Web Scraping Agent,"" which uses advanced AI logic to interpret the user‚Äôs query and determine the information needed.
üåê Web Scraping: The agent utilizes the ""HTTP Request"" node to scrape real-time data from a user-provided URL, enabling the chatbot to fetch and analyze live website content.
üóÇÔ∏è Memory Management: The ""Window Buffer Memory"" node ensures context retention by storing and managing conversational history, allowing for seamless interactions.
ü§ñ Language Model Integration: The scraped data is processed using the ""gpt-4o-mini"" language model, which generates clear, accurate, and contextually relevant responses for the user.
Why It's Cool
‚è±Ô∏è Real-Time Information Retrieval: This workflow empowers users to access up-to-date web content directly through a chatbot, eliminating manual web searches.
‚ú® Enhanced User Experience: By combining web scraping with conversational AI, it delivers precise answers tailored to user queries in real time.
üîÑ Versatility: It can be applied across various domains, such as customer support, research, or data analysis, making it a valuable tool for businesses and individuals alike.
‚öôÔ∏è Automation Efficiency: Automating web scraping and response generation saves time and effort while ensuring accuracy."
AI-powered email processing autoresponder and response approval (Yes/No),https://n8n.io/workflows/2861-ai-powered-email-processing-autoresponder-and-response-approval-yesno/,"How it Works
This workflow is designed to automate the process of handling incoming emails, summarizing their content, generating appropriate responses, and obtaining approval before sending replies. Below are the key operational steps:
Email Reception and Summarization:
The workflow starts with an Email Trigger (IMAP) node that listens for new emails in a specified inbox.
Once an email is received, its HTML content is processed by a Markdown node to convert it into plain text if necessary, followed by an Email Summarization Chain node which uses AI to create a concise summary of the email's content using prompts tailored for this purpose.
Response Generation and Approval:
A Write email node generates a professional response based on the summarized content, utilizing predefined templates and guidelines such as keeping responses under 100 words and ensuring they're formatted correctly in HTML.
Before sending out any automated replies, the system sends these drafts via Gmail for human review and approval through a Gmail node configured with double-approval settings. If approved (Approve?), the finalized email is sent back to the original sender using the Send Email node; otherwise, it loops back for further edits or manual intervention.
Set Up Steps
To replicate this workflow within your own n8n environment, follow these essential configuration steps:
Configuration:
Begin by setting up an n8n instance either locally or via cloud services offered directly from their official site.
Import the provided JSON configuration file into your workspace, making sure all required credentials like IMAP, SMTP, OpenAI API keys, etc., are properly set up under Credentials section since multiple nodes rely heavily on external integrations for functionalities like reading emails, generating summaries, crafting replies, and managing approvals.
Customization:
Adjust parameters according to specific business needs, including but not limited to adjusting the conditions used during conditional checks performed by nodes like Approve?.
Modify the template messages given to AI models so they align closely with organizational tone & style preferences while maintaining professionalism expected in business communications. Ensure correct mappings between fields when appending data to external systems like Google Sheets or similar platforms where records might need tracking post-interaction completion."
"Automate SIEM Alert Enrichment with MITRE ATT&CK, Qdrant & Zendesk in n8n",https://n8n.io/workflows/2840-automate-siem-alert-enrichment-with-mitre-attandck-qdrant-and-zendesk-in-n8n/,"n8n Workflow: Automate SIEM Alert Enrichment with MITRE ATT&CK & Qdrant
Who is this for?
This workflow is ideal for:
Cybersecurity teams & SOC analysts who want to automate SIEM alert enrichment.
IT security professionals looking to integrate MITRE ATT&CK intelligence into their ticketing system.
Organizations using Zendesk for security incidents who need enhanced contextual threat data.
Anyone using n8n and Qdrant to build AI-powered security workflows.
What problem does this workflow solve?
Security teams receive large volumes of raw SIEM alerts that lack actionable context. Investigating every alert manually is time-consuming and can lead to delayed response times. This workflow solves this problem by:
‚úî Automatically enriching SIEM alerts with MITRE ATT&CK TTPs.
‚úî Tagging & classifying alerts based on known attack techniques.
‚úî Providing remediation steps to guide the response team.
‚úî Enhancing security tickets in Zendesk with relevant threat intelligence.
What this workflow does
1Ô∏è‚É£ Ingests SIEM alerts (via chatbot or ticketing system like Zendesk).
2Ô∏è‚É£ Queries a Qdrant vector store containing MITRE ATT&CK techniques.
3Ô∏è‚É£ Extracts relevant TTPs (Tactics, Techniques, & Procedures) from the alert.
4Ô∏è‚É£ Generates remediation steps using AI-powered enrichment.
5Ô∏è‚É£ Updates Zendesk tickets with threat intelligence & recommended actions.
6Ô∏è‚É£ Provides structured alert data for further automation or reporting.
Setup Guide
Prerequisites
n8n instance (Cloud or Self-hosted).
Qdrant vector store with MITRE ATT&CK data embedded.
OpenAI API key (for AI-based threat processing).
Zendesk account (for ticket enrichment, if applicable).
Clean Mitre Data Python Script
Cleaned Mitre Data
Full Mitre Data
Steps to Set Up
1Ô∏è‚É£ Embed MITRE ATT&CK data into Qdrant
This workflow pulls MITRE ATT&CK data from Google Drive and loads it into Qdrant.
The data is vectorized using OpenAI embeddings for fast retrieval.
2Ô∏è‚É£ Deploy the n8n Chatbot
The chatbot listens for SIEM alerts and sends them to the AI processing pipeline.
Alerts are analyzed using an AI agent trained on MITRE ATT&CK.
3Ô∏è‚É£ Enrich Zendesk Tickets
The workflow extracts MITRE ATT&CK techniques from alerts.
It updates Zendesk tickets with contextual threat intelligence.
The remediation steps are included as internal notes for SOC teams.
How to Customize This Workflow
üîß Modify the chatbot trigger: Adapt the chatbot node to receive alerts from Slack, Microsoft Teams, or any other tool.
üîß Change the SIEM input source: Connect your workflow to Splunk, Elastic SIEM, or Chronicle Security.
üîß Customize remediation steps: Use a custom AI model to tailor remediation responses based on organization-specific security policies.
üîß Extend ticketing integration: Modify the Zendesk node to also work with Jira, ServiceNow, or another ITSM platform.
Why This Workflow is Powerful
‚úÖ Saves time: Automates alert triage & classification.
‚úÖ Improves security posture: Helps SOC teams act faster on threats.
‚úÖ Leverages AI & vector search: Uses LLM-powered enrichment for real-time context.
‚úÖ Works across platforms: Supports n8n Cloud, Self-hosted, and Qdrant.
üöÄ Get Started Now!
üìñ Watch the Setup Video
üí¨ Have Questions? Join the Discussion in the YouTube Comments!"
Extract and process information directly from PDF using Claude and Gemini,https://n8n.io/workflows/2764-extract-and-process-information-directly-from-pdf-using-claude-and-gemini/,"Overview
This workflow helps you compare Claude 3.5 Sonnet and Gemini 2.0 Flash when extracting data from a PDF
This workflow extracts and processes the data within a PDF in one single step, instead of calling an OCR and then an LLM‚Äù
How it works
The initial 2 steps download the PDF and convert it to base64.
This base64 string is then sent to both Claude 3.5 Sonnet and Gemini 2.0 Flash to extract information.
This workflow is made to let you compare results, latency, and cost (in their dedicated dashboard).
How to use it
Set up your Google Drive if not already done
Select a document on your Google Drive
Modify the prompt in ""Define Prompt"" to extract the information you need and transform it as wanted.
Get a Claude API key and/or Gemini API key
Note that you can deactivate one of the 2 API calls if you don't want to try both
Test the Workflow"
AI-Powered RAG Workflow For Stock Earnings Report Analysis,https://n8n.io/workflows/2741-ai-powered-rag-workflow-for-stock-earnings-report-analysis/,"This n8n workflow creates a financial analysis tool that generates reports on a company's quarterly earnings using the capabilities of OpenAI GPT-4o-mini, Google's Gemini AI and Pinecone's vector search. By analyzing PDFs of any company's earnings reports from their Investor Relations page, this workflow can answer complex financial questions and automatically compile findings into a structured Google Doc.
How it works:
Data loading and indexing
Fetches links to PDF earnings document from a Google Sheet containing a list of file links.
Downloads the PDFs from Google Drive.
Parses the PDFs, splits the text into chunks, and generates embeddings using the Embeddings Google AI node (text-embedding-004 model).
Stores the embeddings and corresponding text chunks in a Pinecone vector database for semantic search.
Report generation with AI agent
Utilizes an AI Agent node with a specifically crafted system prompt. The agent orchestrates the entire process.
The agent uses a Vector Store Tool to access and retrieve information from the Pinecone database.
Report delivery
Saves the generated report as a Google Doc in a specified Google Drive location.
Set up steps
Google Cloud Project & Vertex AI API:
Create a Google Cloud project.
Enable the Vertex AI API for your project.
Google AI API key:
Obtain a Google AI API key from Google AI Studio.
Pinecone account and API key:
Create a free account on the Pinecone website.
Obtain your API key from your Pinecone dashboard.
Create an index named company-earnings in your Pinecone project.
Google Drive - download and save financial documents:
Go to a company you want to analize and download their quarterly earnings PDFs
Save the PDFs in Google Drive
Create a Google Sheet that stores a list of file URLs pointing to the PDFs you downloaded and saved to Google Drive
Configure credentials in your n8n environment for:
Google Sheets OAuth2
Google Drive OAuth2
Google Docs OAuth2
Google Gemini(PaLM) Api (using your Google AI API key)
Pinecone API (using your Pinecone API key)
Import and configure the workflow:
Import this workflow into your n8n instance.
Update the List Of Files To Load (Google Sheets) node to point to your Google Sheet.
Update the Download File From Google Drive to point to the column where the file URLs are
Update the Save Report to Google Docs node to point to your Google Doc where you want the report saved."
AI-Powered Social Media Amplifier,https://n8n.io/workflows/2681-ai-powered-social-media-amplifier/,"Reach out to me for any setup help/consulting.
Automate the curation and sharing of trending GitHub discussions from Hacker News to Twitter and LinkedIn. This workflow leverages AI to generate engaging posts, streamlining your social media content creation and distribution.
How it Works
Crawl Hacker News for GitHub Posts: The workflow fetches trending GitHub-related discussions from Hacker News.
Extract Key Information: Relevant data such as post titles, URLs, and metadata are extracted and filtered to focus only on unposted content.
Fetch Additional Details: For each GitHub post, the workflow retrieves extra information from the GitHub repository page to enrich the post content.
Generate Social Media Posts: Using AI, the workflow automatically generates tailored posts for Twitter and LinkedIn based on the collected data.
Post to Twitter & LinkedIn: The generated content is posted to your Twitter and LinkedIn accounts.
Track and Log Posts: Each post is logged in Airtable for tracking, and its status is updated to ensure no duplicate posts are made.
Telegram Notification: After posting, a summary of the posts is sent to your Telegram chat for real-time updates.
Requirements
n8n Account: Set up and configured. Sign up here.
API Credentials: Valid keys for LinkedIn, Twitter, Airtable, OpenAI, and Telegram.
Airtable Base: Configured with fields such as Title, URL, Post Content, Status, and Timestamp. Get started with Airtable.
Telegram Chat ID: For receiving real-time notifications.
Set Up Steps
Clone the Workflow: Import the workflow into your n8n environment using the provided JSON.
Configure API Credentials: Enter your API keys for LinkedIn, Twitter, Airtable, OpenAI, and Telegram into the respective nodes.
Set Up Airtable Base: Create an Airtable base with fields such as Title, URL, Post Content, Status, and Timestamp.
Customize Telegram Chat ID: Modify the 'Ping me' node with your Telegram chat ID to receive notifications.
Run the Workflow: Activate the workflow to start the automated content curation and posting process.
Additional Resources
n8n AI Agentic Workflows Guide
n8n AI Workflow Tutorial
n8n Community Tutorial on Building an AI-Powered Telegram Bot
Note: Chat GPT prompt should/can be tweaked in the step to give the desired behaviour.
Sample Posts from my X and LinkedIn -"
Automate Blog Creation in Brand Voice with AI,https://n8n.io/workflows/2648-automate-blog-creation-in-brand-voice-with-ai/,"This n8n template demonstrates a simple approach to using AI to automate the generation of blog content which aligns to your organisation's brand voice and style by using examples of previously published articles.
In a way, it's quick and dirty ""training"" which can get your automated content generation strategy up and running for very little effort and cost whilst you evaluate our AI content pipeline.
How it works
In this demonstration, the n8n.io blog is used as the source of existing published content and 5 of the latest articles are imported via the HTTP node.
The HTML node is extract the article bodies which are then converted to markdown for our LLMs.
We use LLM nodes to (1) understand the article structure and writing style and (2) identify the brand voice characteristics used in the posts.
These are then used as guidelines in our final LLM node when generating new articles.
Finally, a draft is saved to Wordpress for human editors to review or use as starting point for their own articles.
How to use
Update Step 1 to fetch data from your desired blog or change to fetch existing content in a different way.
Update Step 5 to provide your new article instruction. For optimal output, theme topics relevant to your brand.
Requirements
A source of text-heavy content is required to accurately breakdown the brand voice and article style. Don't have your own? Maybe try your competitors?
OpenAI for LLM - though I recommend exploring other models which may give subjectively better results.
Wordpress for blog but feel free to use other preferred publishing platforms.
Customising this workflow
Ideally, you'd want to ""train"" your agent on material which is similar to your output ie. your social media post may not get the best results from your blog content due to differing formats.
Typically, this brand voice extraction exercise should run once and then be cached somewhere for reuse later. This would save on generation time and overall cost of the workflow."
Extract text from PDF and image using Vertex AI (Gemini) into CSV,https://n8n.io/workflows/2614-extract-text-from-pdf-and-image-using-vertex-ai-gemini-into-csv/,"Case Study
I'm too lazy to record every transaction for my expense tracking. Since all my expenses are digital, I just extract the transactions from bank PDF statements and screenshots into CSV to import into my budgeting software.
Read more -> How I used A.I. to track all my expenses
What this workflow does
Upload your PDF or screenshots into Google Drive
It then passes the PDF/image to Vertex Gemini to do some A.I. image recognition
It then sends the transactions as CSV and stores it into another Google Drive folder
Setup
Set up 2 google drive folders. 1 for uploading and 1 for the output.
Input your Google Drive crendtials
Input your Vertex Gemini credentials
How to adjust it to your needs
You can upload other types of documents for information extraction.
You can extract any text data from any image or PDF
You can adjust the A.I. prompt to do different things"
Conversational Interviews with AI Agents and n8n Forms,https://n8n.io/workflows/2566-conversational-interviews-with-ai-agents-and-n8n-forms/,"This n8n template combines an AI agent with n8n's multi-page forms to create a novel interaction which allows automated question-and-answer sessions. One of the more obvious use-cases of this interaction is what I'm calling the AI interviewer.
You can read the full post here: https://community.n8n.io/t/build-your-own-ai-interview-agents-with-n8n-forms/62312
Live demo here: https://jimleuk.app.n8n.cloud/form/driving-lessons-survey
How it works
A form trigger is used to start the interview and a new session is created in redis to capture the transcript.
An AI agent is then tasked to ask questions to the user regarding the topic of the interview. This is setup as a loop so the questions never stop unless the user wishes to end the interview.
Each answer is recorded in our session set up earlier between questions.
When the user requests to end the interview we break the loop and show the interview completion screen.
Finally, the session is then saved in a Google Sheet which can then be shared with team members and for the purpose of data analysis.
How to use
You'll need to be on a n8n instance that is accessible to your target audience. Not technical enough to setup your own server? Try out n8n cloud and instantly deploy template!
Remember to activate the workflow so the form trigger is published and available for users to use.
Requirements
Groq LLM for AI agent. Feel free to swap this out for any other LLM.
Redis(-compatible) storage for capturing sessions
Customising this workflow
The next step would be adding tools! AI interviews with knowledge retrieval could definitely open up other possibilities. Eg. An onboarding wizard generating questions by pulling facts from internal knowledgebase."
Generate SQL queries from schema only - AI-powered,https://n8n.io/workflows/2508-generate-sql-queries-from-schema-only-ai-powered/,"This workflow is a modification of the previous template on how to create an SQL agent with LangChain and SQLite.
The key difference ‚Äì the agent has access only to the database schema, not to the actual data. To achieve this, SQL queries are made outside the AI Agent node, and the results are never passed back to the agent.
This approach allows the agent to generate SQL queries based on the structure of tables and their relationships, without having to access the actual data.
This makes the process more secure and efficient, especially in cases where data confidentiality is crucial.
üöÄ Setup
To get started with this workflow, you‚Äôll need to set up a free MySQL server and import your database (check Step 1 and 2 in this tutorial).
Of course, you can switch MySQL to another SQL database such as PostgreSQL, the principle remains the same. The key is to download the schema once and save it locally to avoid repeated remote connections.
Run the top part of the workflow once to download and store the MySQL chinook database schema file on the server.
With this approach, we avoid the need to repeatedly connect to a remote db4free database and fetch the schema every time. As a result, we reach greater processing speed and efficiency.
üó£Ô∏è Chat with your data
Start a chat: send a message in the chat window.
The workflow loads the locally saved MySQL database schema, without having the ability to touch the actual data. The file contains the full structure of your MySQL database for analysis.
The Langchain AI Agent receives the schema, your input and begins to work.
The AI Agent generates SQL queries and brief comments based solely on the schema and the user‚Äôs message.
An IF node checks whether the AI Agent has generated a query. When:
Yes: the AI Agent passes the SQL query to the next MySQL node for execution.
No: You get a direct answer from the Agent without further action.
The workflow formats the results of the SQL query, ensuring they are convenient to read and easy to understand.
Once formatted, you get both the Agent answer and the query result in the chat window.
üåü Example queries
Try these sample queries to see the schema-driven AI Agent in action:
Would you please list me all customers from Germany?
What are the music genres in the database?
What tables are available in the database?
Please describe the relationships between tables. - In this example, the AI Agent does not need to create the SQL query.
And if you prefer to keep the data private, you can manually execute the generated SQL query in your own environment using any database client or tool you trust üóÑÔ∏è
üí≠ The AI Agent memory node does not store the actual data as we run SQL-queries outside the agent. It contains the database schema, user questions and the initial Agent reply. Actual SQL query results are passed to the chat window, but the values are not stored in the Agent memory."
Ultimate Scraper Workflow for n8n,https://n8n.io/workflows/2431-ultimate-scraper-workflow-for-n8n/,"What this template does
The Ultimate Scraper for n8n uses Selenium and AI to retrieve any information displayed on a webpage. You can also use session cookies to log in to the targeted webpage for more advanced scraping needs.
‚ö†Ô∏è Important: This project requires specific setup instructions. Please follow the guidelines provided in the GitHub repository: n8n Ultimate Scraper Setup : https://github.com/Touxan/n8n-ultimate-scraper/tree/main.
The workflow version on n8n and the GitHub project may differ; however, the most up-to-date version will always be the one available on the GitHub repository : https://github.com/Touxan/n8n-ultimate-scraper/tree/main.
How to use
Deploy the project with all the requirements and request your webhook.
Example of request:
curl -X POST http://localhost:5678/webhook-test/yourwebhookid \
-H ""Content-Type: application/json"" \
-d '{
  ""subject"": ""Hugging Face"",
  ""Url"": ""github.com"",
  ""Target data"": [
    {
      ""DataName"": ""Followers"",
      ""description"": ""The number of followers of the GitHub page""
    },
    {
      ""DataName"": ""Total Stars"",
      ""description"": ""The total numbers of stars on the different repos""
    }
  ],
  ""cookie"": []
}'
Or to just scrap a url :
curl -X POST http://localhost:5678/webhook-test/67d77918-2d5b-48c1-ae73-2004b32125f0 \
-H ""Content-Type: application/json"" \
-d '{
  ""Target Url"": ""https://github.com"",
  ""Target data"": [
    {
      ""DataName"": ""Followers"",
      ""description"": ""The number of followers of the GitHub page""
    },
    {
      ""DataName"": ""Total Stars"",
      ""description"": ""The total numbers of stars on the different repo""
    }
  ],
  ""cookies"": []
}'"
AI agent that can scrape webpages,https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/,"‚öôÔ∏èüõ†Ô∏èüöÄü§ñü¶æ
This template is a PoC of a ReAct AI Agent capable of fetching random pages (not only Wikipedia or Google search results).
On the top part there's a manual chat node connected to a LangChain ReAct Agent. The agent has access to a workflow tool for getting page content.
The page content extraction starts with converting query parameters into a JSON object. There are 3 pre-defined parameters:
url ‚Äì an address of the page to fetch
method = full / simplified
maxlimit - maximum length for the final page. For longer pages an error message is returned back to the agent
Page content fetching is a multistep process:
An HTTP Request mode tries to get the page content.
If the page content was successfuly retrieved, a series of post-processing begin:
Extract HTML BODY; content
Remove all unnecessary tags to recuce the page size
Further eliminate external URLs and IMG scr values (based on the method query parameter)
Remaining HTML is converted to Markdown, thus recuding the page lengh even more while preserving the basic page structure
The remaining content is sent back to an Agent if it's not too long (maxlimit = 70000 by default, see CONFIG node).
NB:
You can isolate the HTTP Request part into a separate workflow.
Check the Workflow Tool description, it guides the agent to provide a query string with several parameters instead of a JSON object.
Please reach out to Eduard is you need further assistance with you n8n workflows and automations!
Note that to use this template, you need to be on n8n version 1.19.4 or later."
Starred Slack Messages to Notion Database with AI Auto-Tagging,https://n8n.io/workflows/4502-starred-slack-messages-to-notion-database-with-ai-auto-tagging/,"Who is this for?
Teams that want to capture important Slack messages in Notion with smart categorization.
Perfect for knowledge workers, community managers, or any team that needs to preserve valuable conversations from Slack and organize them automatically in a Notion database.
What problem does this solve?
Important Slack messages get buried in chat history and are hard to find later.
This workflow monitors your Slack channel and automatically saves starred messages to Notion with AI-generated titles and smart tags. No more manually copying messages or losing track of important discussions.
How it works
Trigger - Schedule Trigger fires every 10 minutes to check for new messages.
Get Slack Messages - fetches recent messages from your specified Slack channel within the last 10 minutes.
Star Filter - only processes messages that have been reacted to with a ‚≠ê emoji (configurable to any emoji).
Get Message Link - retrieves the permanent link to the starred Slack message.
Choose Notion DB - connects to your target Notion database and loads available tag options.
Set Tags - prepares the available tags from your Notion database for AI processing.
AI Processing - uses OpenAI GPT-4o-mini to:
Generate a concise title (under 50 characters)
Automatically categorize the message with existing tags (90%+ confidence threshold)
Create Notion Page - saves the message to Notion with:
AI-generated title
Auto-selected tags
Original message content
Permanent link back to Slack
Setup steps
1. Import and connect credentials
Import this template into n8n
Connect your Slack API credentials (Slack Bot Setup Guide)
Connect your Notion API token (Notion Integration Guide)
Connect your OpenAI API credentials
2. Configure Slack integration
Create a Slack bot and add it to your target channel
Update the channelId in ""Get Slack Messages"" node with your channel ID
Ensure your bot has these permissions:
channels:history - to read channel messages
reactions:read - to detect star reactions
chat:write.public - to get message permalinks
Slack Bot Setup Resources:
Creating a Slack App
Bot Token Scopes
Adding Apps to Channels
3. Set up Notion database
Create or use existing Notion database with required properties:
Tags (Multi-select) - add tag options with descriptions for better AI accuracy
Link (URL) - for storing Slack permalink
Title (Title) - will be auto-populated by AI
Update the databaseId in ""Choose Notion DB"" and ""Create Notion Page"" nodes
4. Customize the workflow
Change trigger emoji: Edit the IF condition to use a different emoji (e.g., bookmark, pushpin)
Adjust timing: Modify the Schedule Trigger interval (remember to update the ""oldest"" filter in Get Slack Messages to match)
Improve AI prompts: Edit the prompt in ""Write Title & Tag"" node for better categorization
5. Test the setup
Star a message in your Slack channel
Wait for the next 10-minute trigger or manually execute the workflow
Check your Notion database for the new entry
Example output
Slack message:
""Hey team, just found this amazing tool for automating our design workflow. We should definitely consider it for next sprint: https://example.com/design-tool""
Generated Notion page:
Title: ""Design Tool Recommendation for Next Sprint""
Tags: ""Tools"", ""Design"", ""Sprint Planning""
Content: Full message text with Slack permalink
Link: Direct link back to original Slack message
Extending the flow
Multi-channel support - duplicate the ""Get Slack Messages"" node for different channels and merge the outputs.
Enhanced AI processing - modify the prompt to extract additional metadata like:
Priority levels
Action items
Mentioned team members
Due dates
Rich content preservation - add logic to handle Slack attachments, images, or threaded replies.
Notification system - add nodes to notify team members when important messages are archived.
Sentiment analysis - incorporate additional AI processing to categorize message sentiment or urgency.
This template provides a lean setup for intelligent Slack-to-Notion archiving. Star important messages, let AI handle the organization, and never lose track of valuable conversations again."
"Monitor Elderly Health Vitals & Send Alerts with Apple Health, Twilio & Gmail",https://n8n.io/workflows/4563-monitor-elderly-health-vitals-and-send-alerts-with-apple-health-twilio-and-gmail/,"What is Elderwatch
Elder Watch is a simple system that checks daily vitals ‚Äî like heart rate, oxygen, and walking symmetry ‚Äî using data from an iPhone or Apple Watch.
If something looks off ‚Äî say oxygen drops or heart rate spikes ‚Äî it flags that as ‚Äúattention required.‚Äù
And depending on that status, it can either:
Email a daily report to a caregiver
Or if there‚Äôs an alert ‚Äî trigger a phone call via Twilio
Why do we need this
Elder Watch can help older people living alone for children or care givers to keey an eye on without obsessively checking apps.
It‚Äôs useful for clinics that run home-care programs.
Requirements
Self hosted or cloud N8N
Apple health vis iphone/watch
Twilio VOIP phone number (to place a call)
Workflows
Core workflow for getting health data, processing and making a phone call.
Twilio workflow to invoke Calls API to place an outbound voice call.
twilio workflow
{
  ""name"": ""Twilio Bridge Caller copy"",
  ""nodes"": [
    {
      ""parameters"": {
        ""httpMethod"": ""POST"",
        ""path"": ""twilio-call"",
        ""responseMode"": ""responseNode"",
        ""options"": {}
      },
      ""type"": ""n8n-nodes-base.webhook"",
      ""typeVersion"": 2,
      ""position"": [
        0,
        0
      ],
      ""id"": ""ca3e6c69-3e7f-4d28-b699-4789a6fa2a6d"",
      ""name"": ""Webhook"",
      ""webhookId"": ""eb3d63df-800c-401d-931a-c6fba7d834ae""
    },
    {
      ""parameters"": {
        ""respondWith"": ""text"",
        ""responseBody"": ""={{ $json.body }}"",
        ""options"": {
          ""responseCode"": 200,
          ""responseHeaders"": {
            ""entries"": [
              {
                ""name"": ""Content-Type"",
                ""value"": ""text/xml""
              }
            ]
          }
        }
      },
      ""type"": ""n8n-nodes-base.respondToWebhook"",
      ""typeVersion"": 1.1,
      ""position"": [
        580,
        0
      ],
      ""id"": ""6587b7e2-ace8-4e2b-9f4b-ed028a363c25"",
      ""name"": ""Respond to Webhook""
    },
    {
      ""parameters"": {
        ""jsCode"": ""const summary = $input.first().json.query.summary || 'No summary, check mail for critical health info';\n\nreturn [\n  {\n    json: {\n      body: `&lt;Response&gt;\n  &lt;Say voice=\""alice\""&gt;${summary}&lt;/Say&gt;\n&lt;/Response&gt;`\n    }\n  }\n];\n""
      },
      ""type"": ""n8n-nodes-base.code"",
      ""typeVersion"": 2,
      ""position"": [
        340,
        0
      ],
      ""id"": ""0d4abf87-daf3-4533-8811-64ae61265f5d"",
      ""name"": ""Voice Twilio response""
    }
  ],
  ""pinData"": {
    ""Webhook"": [
      {
        ""json"": {
          ""headers"": {
            ""host"": ""n8n.domain.com"",
            ""user-agent"": ""curl/8.7.1"",
            ""content-length"": ""0"",
            ""accept"": ""*/*"",
            ""accept-encoding"": ""gzip, br""
            
          },
          ""params"": {},
          ""query"": {
            ""lead"": "" 44711111111111""
          },
          ""body"": {},
          ""webhookUrl"": ""https://n8n.domain.com/webhook/twilio-call"",
          ""executionMode"": ""production""
        }
      }
    ]
  },
  ""connections"": {
    ""Webhook"": {
      ""main"": [
        [
          {
            ""node"": ""Voice Twilio response"",
            ""type"": ""main"",
            ""index"": 0
          }
        ]
      ]
    },
    ""Voice Twilio response"": {
      ""main"": [
        [
          {
            ""node"": ""Respond to Webhook"",
            ""type"": ""main"",
            ""index"": 0
          }
        ]
      ]
    }
  },
  ""active"": true,
  ""settings"": {
    ""executionOrder"": ""v1""
  },
  ""versionId"": ""b58c5a12-75be-4b1d-b144-8c7251468021"",
  ""meta"": {
    ""instanceId"": ""8dc0e8a0878d0086b2f46ef04bb00ae07186c936d82d0f0a67563e9652996d33""
  },
  ""id"": ""RHaKqf8Wqt7fIuGH"",
  ""tags"": []
}
Samples

Resources
https://www.youtube.com/watch?v=HYk5_jtMlgc
Questions/Support
Contact me on info@pankstr.com."
üè§ Scrapping of European Union Events with Google Sheets,https://n8n.io/workflows/4561-scrapping-of-european-union-events-with-google-sheets/,"Tags: Scrapping, Events, European Union, Networking
Context
Hey! I‚Äôm Samir, a Supply Chain Engineer and Data Scientist from Paris, and the founder of LogiGreen Consulting.
We use AI, automation, and data to support sustainable and data-driven operations across all types of organizations.
This workflow is part of our networking strategy (as a business) to track official EU events that may relate to topics we cover.
Want to stay ahead of critical EU meetings and events without checking the website every day?
This n8n workflow automatically scrapes the EU‚Äôs official event portal and logs the latest entries with clean metadata including date, location, category, and link.
üì¨ For collaborations, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is useful for:
Policy & public affairs teams following institutional activities
Sustainability teams watching for relevant climate-related summits
NGOs and researchers interested in event calendars
Data teams building dashboards on public event trends
What does it do?
This n8n workflow:
üåê Scrapes the EU events portal for new meetings and conferences
üìÖ Extracts event metadata (title, date, location, type, and link)
üîÅ Handles pagination across multiple pages
üö´ Checks for duplicates already stored
üìä Saves new records into a connected Google Sheet
How it works
Triggered daily via cron
HTTP node loads the event listing HTML
Extract HTML blocks for each event article
Parse event name, link, type, location, and full date
Concatenate and clean dates for easy tracking
Store non-duplicate entries in Google Sheets
The workflow uses static data to track pagination and ensure only new events are stored, making it ideal for building up a clean dataset over time.
What do I need to get started?
You‚Äôll need:
A Google Sheet connected to your n8n instance
No code or AI tools needed ‚Äî just n8n and this template
Follow the Guide!
Sticky notes are included directly inside the workflow to guide you step-by-step through setup and customisation.

üé• Watch My Tutorial
Notes
This is ideal for analysts and consultants who want clean, structured data from the EU portal
You can add filtering, email alerts, or AI classifiers later
This workflow was built using n8n version 1.93.0
Submitted: June 1, 2025"
AI Testimonial Extractor Agent: Feedback to Marketing Gold,https://n8n.io/workflows/4449-ai-testimonial-extractor-agent-feedback-to-marketing-gold/,"Transform raw customer feedback into powerful testimonial quotes automatically.
This intelligent workflow monitors feedback forms, uses AI to identify and extract the most emotionally engaging testimonial content, and organizes everything into a searchable database for your marketing campaigns - turning every piece of customer feedback into potential marketing assets.
üöÄ What It Does
Smart Feedback Monitoring: Automatically detects new customer feedback submissions from Google Forms and triggers testimonial extraction within minutes.
AI-Powered Quote Extraction: Uses Google Gemini to analyze feedback and extract short, emotionally engaging testimonial quotes while filtering out neutral or irrelevant content.
Marketing-Ready Output: Focuses on impactful phrases and statements that work perfectly for websites, social media, ads, and sales materials.
Automated Database Building: Creates and maintains a searchable testimonial library in Google Sheets with customer details and extracted quotes.
Instant Team Notifications: Sends immediate email alerts to your marketing team with new testimonials, ensuring no valuable social proof goes unused.
üéØ Key Benefits
‚úÖ Never Miss Marketing Gold: Automatically extract value from every feedback submission
‚úÖ Save 8+ Hours Weekly: Eliminate manual review of feedback for testimonials
‚úÖ Build Social Proof Library: Create searchable database of customer quotes
‚úÖ Boost Conversion Rates: Use authentic testimonials across marketing campaigns
‚úÖ Identify Happy Customers: Spot satisfied clients for case studies and referrals
‚úÖ Scale Content Creation: Generate testimonials faster than customers submit feedback
üè¢ Perfect For
Businesses Needing Social Proof
E-commerce stores showcasing product satisfaction
SaaS companies highlighting user success stories
Service businesses building trust and credibility
Coaches and consultants demonstrating client results
Marketing Applications
Website Content: Populate testimonial sections automatically
Social Media: Create quote posts and success story content
Sales Materials: Include powerful customer quotes in proposals
Email Marketing: Add authentic testimonials to campaigns
‚öôÔ∏è What's Included
Complete Workflow Setup: Ready-to-deploy n8n workflow with all integrations configured
Google Forms Integration: Automatically processes new feedback submissions
AI Quote Extraction: Google Gemini identifies most impactful testimonial content
Database Management: Organized Google Sheets storage with customer information
Team Notifications: Instant email alerts to marketing team members
Setup Documentation: Complete configuration and customization guide
üîß Technical Requirements
n8n Platform: Cloud or self-hosted instance
Google Workspace: For Forms, Sheets, and Gmail integration
Google Gemini API: For AI-powered testimonial extraction (free tier available)
Customer Feedback: Existing or new feedback collection process
üìä Before & After Examples
Before (Raw Customer Feedback):
""I was really struggling with managing my team's projects and keeping track of all the deadlines. Everything was scattered across different tools and I was spending way too much time just trying to figure out what everyone was working on. Since we started using your project management software about 6 months ago, it's been a complete game changer. Now I can see everything at a glance, our team communication has improved dramatically, and we're actually finishing projects ahead of schedule. The reporting features are amazing too - I can finally show my boss concrete data about our team's productivity. I honestly don't know how we managed without it. The customer support team has been fantastic as well, always quick to help when we had questions during setup.""
After (AI Extracted Testimonial):
""Complete game changer - now I can see everything at a glance, our team communication has improved dramatically, and we're actually finishing projects ahead of schedule.""
Healthcare Example:
Raw Feedback:
""I had been dealing with chronic back pain for over 3 years and had tried everything - physical therapy, medication, different doctors. Nothing seemed to help long-term. When I found Dr. Martinez, I was honestly pretty skeptical because I'd been disappointed so many times before. But after our first consultation, I felt hopeful for the first time in years. She really listened to me and explained everything clearly. The treatment plan she developed was comprehensive but manageable. Within just 2 months, I was experiencing significant pain reduction, and now after 6 months, I'm practically pain-free. I can play with my kids again, sleep through the night, and even started hiking on weekends. Dr. Martinez didn't just treat my symptoms - she helped me get my life back.""
Extracted Testimonial:
""Within just 2 months, I was experiencing significant pain reduction, and now I'm practically pain-free. Dr. Martinez didn't just treat my symptoms - she helped me get my life back.""
üé® Customization Options
Industry-Specific Extraction: Tailor AI prompts for healthcare, technology, finance, retail terminology
Quote Length Control: Adjust extraction for short punchy quotes vs longer detailed testimonials
Sentiment Targeting: Focus on specific emotions like excitement, relief, satisfaction, transformation
Multi-Channel Forms: Connect multiple feedback sources to one testimonial database
Approval Workflows: Add human review step before testimonials go live
CRM Integration: Connect extracted testimonials to customer records
üîÑ How It Works
Customer submits feedback via your Google Form
Workflow detects new submission within 1 minute automatically
AI analyzes feedback content to identify most impactful statements
Testimonial quote is extracted and formatted for marketing use
Quote is saved to database with customer details and timestamp
Marketing team receives email with new testimonial content
üí° Use Case Examples
SaaS Company: Automatically extract user success quotes from feature feedback surveys for website testimonials
E-commerce Store: Turn product review submissions into powerful testimonial quotes for product pages and ads
Healthcare Practice: Extract patient satisfaction quotes from feedback forms for website and marketing materials
Consulting Firm: Convert client project feedback into testimonials highlighting business transformation results
üìà Expected Results
300% increase in testimonial collection vs manual methods
90% time savings on testimonial creation and organization
50% improvement in marketing content authenticity
25% boost in conversion rates using extracted testimonials
Unlimited scalability as feedback volume grows
üõ†Ô∏è Setup & Support
Quick Deployment: Complete setup in 20 minutes with included guide
Pre-Built Prompts: AI extraction prompts optimized for different industries
Template Library: Ready-to-use feedback forms and testimonial layouts
Video Tutorial: Complete walkthrough from setup to first extracted testimonial
üìû Get Help & Learn More
üé• Free Video Tutorials
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup and configuration guide
üíº Professional Support
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for testimonial marketing strategy consulting
Share your social proof automation success stories
Access exclusive templates for different business types
üìß Direct Support
Email: Yaron@nofluff.online
Technical setup assistance and customization help
AI prompt optimization for your specific business
Integration with existing marketing and CRM systems
Response within 24 hours"
Promote YouTube Videos on Reddit with AI-Generated Comments and Email Digest,https://n8n.io/workflows/4433-promote-youtube-videos-on-reddit-with-ai-generated-comments-and-email-digest/,"Motivation
Are you a small YT channel looking to grow to 1000 subs ?
This workflow has helped me grow to 80 subs via Reddit Marketing.
Check out the YT Tutorial.
What it is
This n8n workflow automates the process of promoting your YouTube videos on Reddit. It identifies relevant Reddit posts, generates humanized comments tailored to your video content, and delivers a weekly digest of opportunities directly to your inbox.
How It Works
A user submits their YouTube video URL and email through a form.
The workflow analyzes the YouTube video's title, description, and tags to extract relevant keywords and create a custom AI classification prompt.
It searches Reddit for posts containing those keywords and filters them based on engagement metrics
upvotes > 15
non-empty text content
posted within the last 14 days
upvote ratio > 0.95
An AI agent (using OpenRouter's GPT-4.1-mini) classifies each filtered Reddit post to determine its relevance to your YouTube video.
For relevant posts, another AI agent (using OpenRouter's Gemini-2.0-flash-lite-001) generates a humanized, authentic comment designed to subtly reference your video and add value to the Reddit discussion.
Relevant posts and their proposed comments are stored in Google Sheets.
A professional HTML email digest is generated, summarizing the marketing opportunities.
The email is sent to the user's provided email address.
Setup
To run this workflow, you need to set up credentials in n8n for:
YouTube: Uses OAuth 2.0. Connect by authenticating your YouTube account.
Reddit: Uses OAuth 2.0. Requires creating an app on Reddit to get a Client ID & Secret. (YT Tutorial)
OpenRouter: Generate API key from your OpenRouter account. (YT Tutorial)
Google Sheets: Recommend OAuth2 - just connect by authenticating.
Gmail: Recommend OAuth 2.0. like Google Sheets
n8n account setup
If you do not have a n8n account, follow the YT Tutorial to get started."
AI-Powered MIS Agent,https://n8n.io/workflows/4341-ai-powered-mis-agent/,"The AI-powered MIS Agent is an intelligent, automated system built using n8n that streamlines email-based data collection and document organization for businesses. It classifies incoming emails, extracts and processes attachments or Drive links, and routes them to the correct destination folders in Google Drive. Additionally, it provides advanced file operations like cleaning, merging, joining, and transforming data.
Advantages
üì• Automated Email and File Management
Detects and processes emails containing attachments or Drive links, ensuring seamless classification and routing of business-critical files.
üß† AI-Based Classification
Uses LLMs (like GPT-4o Mini) to classify emails into categories such as Daily Sales, Customer Info, and Address based on their content.
üìÇ Smart File Routing and Upload
Recognizes whether a file is a direct attachment or a Google Drive link, extracts the file ID if necessary, and uploads it to predefined folders.
üìä Powerful Data Operations
Supports operations like append, join, group by, aggregation, and standardization of data directly from spreadsheets using Python and Pandas within the workflow.
üîÅ Scheduled and Triggered Automation
Supports scheduled runs and real-time email triggers, making it highly reliable and timely.
üîß Fully Modular and Scalable
Easily expandable with more logic, new folders, or different workflows. Clean architecture and annotations make maintenance simple.
How It Works
Email Trigger
The system uses a Gmail trigger to monitor incoming emails with specific labels or attachments.
Classification
An LLM-based text classifier identifies the purpose of the email (e.g., sales data, address list, customer details).
Conditional Logic
Regex-based conditions check if the email contains Google Drive links or attachments.
File Handling
If it's a Drive link, it extracts the file ID and copies it to the correct folder. If it's an attachment, it uploads directly.
Scheduled Data Management
Periodically moves or logs files from predefined folders using a schedule trigger.
Data Cleaning and Processing
Performs data cleaning and transformation tasks like replacing missing values, standardizing formats, and joining datasets based on criteria provided by the user.
Final Output
Cleaned and processed files are saved in designated folders with their public links shared back through the system.
Set Up Steps
Configure Nodes:
Gmail Trigger: Detects relevant incoming emails.
Text Classifier: Uses OpenAI model to categorize email content.
Regex Conditions: Determine whether a link or attachment is present.
Google Drive Operations: Upload or copy files to categorized folders.
Python Nodes: Handle data manipulation using Pandas.
Google Sheets Nodes: Extract, clean, and write structured data.
LLM-based Chat Models: Extract and apply cleaning configurations.
Connect Nodes:
Seamlessly connect Gmail inputs, classification, file processing, and data logic.
Output links or processed files are uploaded back to Drive and ready to share.
Credentials:
Ensure OAuth credentials for Gmail, Google
Drive, and OpenAI are correctly set.
Ideal For
Sales & CRM teams managing large volumes of email-based reports.
Data teams needing structured pipelines from unstructured email inputs.
Businesses looking to automate classification, storage, and transformation of routine data.
Testing and Additional customization
If you want to test this bot capability
before purchasing the workflow.
ask me on my mail
kumar.shivam19oce@gmail.com
I will share the chat url and the links of
associated google drives to see the result
once you are satisfied then we are good to
go.
I have just kept $1 for testing purposes
because of paid open ai .
-If there is any customization needed like charts and other request like adding databases feel free to let me know i can do it accordingly.
This is the first version i will come with more advancements based on the request and responses.
Use it and let me know on kumar.shivam19oce@gmail.com"
"Find, Scrape & Analyze Twitter Posts by Name with Bright Data & Gemini",https://n8n.io/workflows/4325-find-scrape-and-analyze-twitter-posts-by-name-with-bright-data-and-gemini/,"This n8n workflow template automates the process of collecting and analyzing Twitter (X) posts for any public profile, then generates a clean, AI-powered summary including key metrics, interests, and activity trends.
üöÄ What It Does
Accepts a user's full name and date range through a public form.
Automatically finds the person‚Äôs X (formerly Twitter) profile using a Google search.
Uses Bright Data to retrieve full post data from the X.com profile.
Extracts key post metrics like views, likes, reposts, hashtags, and mentions.
Uses Google Gemini (PaLM) to generate a personalized summary: tone, themes, popularity, and sentiments.
Stores both raw data and the AI summary into a connected Google Sheet for further review or team collaboration.
üõ†Ô∏è Step-by-Step Setup
Deploy the public form to collect full name and date range.
Build a Google search query using the name to find their X profile.
Scrape the search results via Bright Data (Web Unlocker zone).
Parse the page content using the HTML node.
Use Gemini AI to extract the correct X profile URL.
Pull full post data via Bright Data dataset snapshot API.
Transform post data into clean structured fields:
date_posted, description, hashtags, likes, views,
quoted_post.date_posted, quoted_post.description,
replies, reposts, quotes, and tagged_users.profile_name.
Analyze all posts using Google Gemini for interest detection and persona generation.
Save results to a Google Sheet: structured post data + AI-written summary.
Show success or fallback messages depending on profile detection or scraping status.
üß† How It Works: Workflow Overview
Trigger: When user submits form
Search & Match: Google search ‚Üí HTML parse ‚Üí Gemini filters matching X profile
Data Gathering: Bright Data ‚Üí Poll for snapshot completion ‚Üí Fetch post data
Transformation: Extract and restructure key fields via Code node
AI Summary: Use Gemini to analyze tone, interests, and trends
Export: Save results to Google Sheet
Fallback: Display custom error message if no X profile found
üì® Final Output
A record in your Google Sheet with:
Clean post-level data
Profile-level engagement summary
An AI-written overview including tone, common topics, and post popularity
üîê Credentials Used
Bright Data account (for search & post scraping)
Google Gemini (PaLM) or Gemini Flash via -
OpenAI/Google Vertex API
Google Sheets (OAuth2) account (for result storage)
‚ö†Ô∏èCommunity Node Dependency
This workflow uses a custom community node:
n8n-nodes-brightdata
Install it via UI (Settings ‚Üí Community Nodes ‚Üí Install)."
Reliable AI Agent Output Without Structured Output Parser - w/ OpenAI & Switch,https://n8n.io/workflows/4316-reliable-ai-agent-output-without-structured-output-parser-w-openai-and-switch/,"This workflow serves as a solid foundation when you need an AI Agent to return output in a specific JSON schema, without relying on the often-unreliable Structured Output Parser.
What It Does
The example workflow takes a simple input (like a food item) and expects a JSON-formatted output containing its nutritional values.
Why Use This Instead of Structured Output Parser?
The built-in Structured Output Parser node is known to be unreliable when working with AI Agents.
While the n8n documentation recommends using a ‚ÄúBasic LLM Chain‚Äù followed by a Structured Output Parser, this alternative workflow completely avoids using the Structured Output Parser node.
Instead, it implements a custom loop that manually validates the AI Agent's output.
This method has proven especially reliable with OpenAI's gpt-4.1 series (gpt-4.1, gpt-4.1-mini, gpt-4.1-nano), which tend to produce correctly structured JSON on the first try, as long as the System Prompt is well defined.
In this template, gpt-4.1-nano is set by default.
How It Works
Instead of using the Structured Output Parser, this workflow loops the AI Agent through a manual schema validation process:
A custom schema check is performed after the AI Agent response.
A runIndex counter tracks the number of retries.
A Switch node:
If the output does not match the expected schema, it routes back to the AI Agent with an updated prompt asking it to return the correct format. The process allows up to 4 retries to avoid infinite loops.
If the output does match the schema, it continues to a Set node that serves as chat response (you can customize this part to fit your use case).
This approach ensures schema consistency, offers flexibility, and avoids the brittleness of the default parser."
Evaluation metric example: Correctness (judged by AI),https://n8n.io/workflows/4271-evaluation-metric-example-correctness-judged-by-ai/,"AI evaluation in n8n
This is a template for n8n's evaluation feature.
Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow.
By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't.
How it works
This template shows how to calculate a workflow evaluation metric: whether an output matches an expected output (i.e. has the same meaning).
The workflow takes questions about the causes of historical events and compares them with the reference answers in the dataset.
We use an evaluation trigger to read in our dataset
It is wired up in parallel with the regular chat trigger so that the workflow can be started from either one. More info
If we're evaluating (i.e. the execution started from the evaluation trigger), we calculate the correctness metric using AI
We pass this information back to n8n as a metric
If we're not evaluating we avoid calculating the metric, to reduce cost"
Enrich Pipedrive CRM Contact Data with LinkedIn Profiles using GPT & Multi-CRM Support,https://n8n.io/workflows/4242-enrich-pipedrive-crm-contact-data-with-linkedin-profiles-using-gpt-and-multi-crm-support/,"‚ö†Ô∏è DISCLAIMER: This workflow uses the HDW LinkedIn community node, which is only available on self-hosted n8n instances. It will not work on n8n.cloud.
Overview
This n8n workflow automates the enrichment of CRM contact data with professional insights from LinkedIn profiles. The workflow integrates with both Pipedrive and HubSpot CRMs, finding LinkedIn profiles that match your contacts and updating your CRM with valuable information about their professional background and recent activities.
Key Features
Multi-CRM Support: Works with both Pipedrive and HubSpot
AI-Powered Data Enrichment: Uses an advanced AI agent to analyze and summarize professional information
Automated Triggers: Activates when new contacts are added or when enrichment is requested
Comprehensive Profile Analysis: Captures LinkedIn profile summaries and post activity
How It Works
Triggers
The workflow activates in three scenarios:
When a new contact is created in CRM
When a contact is updated in CRM with an enrichment flag
LinkedIn Data Collection Process
Email Lookup: First tries to find the LinkedIn profile using the contact's email
Advanced Search: If email lookup fails, uses name and company details to find potential matches
Profile Analysis: Collects comprehensive profile information
Post Analysis: Gathers and analyzes the contact's recent LinkedIn activity
CRM Updates
The workflow updates your CRM with:
LinkedIn profile URL
Professional summary (skills, experience, background)
Analysis of recent LinkedIn posts and activity
Setup Instructions
Requirements
Self-hosted n8n instance with the HDW LinkedIn community node installed
API access to OpenAI (for GPT-4o)
Pipedrive and/or HubSpot account
HDW API key https://app.horizondatawave.ai
Installation Steps
Install the HDW LinkedIn Node:
npm install n8n-nodes-hdw
Follow the detailed instructions at: https://www.npmjs.com/package/n8n-nodes-hdw
Configure Credentials:
OpenAI: Add your OpenAI API key
Pipedrive: Connect your Pipedrive account (if using)
HubSpot: Connect your HubSpot account (if using)
HDW LinkedIn: Add your API key from https://app.horizondatawave.ai
CRM Custom Fields Setup:
For Pipedrive:
Go to Settings ‚Üí Data Fields ‚Üí Contact Fields ‚Üí + Add Field
Create the following custom fields:
LinkedIn Profile: Field type - Large text
Profile Summary: Field type - Large text
LinkedIn Posts Summary: Field type - Large text
Need Enrichment: Field type - Single option (Yes/No)
Detailed instructions for creating custom fields in Pipedrive:
https://support.pipedrive.com/en/article/custom-fields
For HubSpot:
Go to Settings ‚Üí Properties ‚Üí Create property
Create the following properties for Contact object:
linkedin_url: Field type - Single-line text
profile_summary: Field type - Multi-line text
linkedin_posts_summary: Field type - Multi-line text
need_enrichment: Field type - Checkbox (Boolean)
Detailed instructions for creating properties in HubSpot:
https://knowledge.hubspot.com/properties/create-and-edit-properties
Import the Workflow:
Import the ""HDW_CRM_Enrichment.json"" file into your n8n instance
Activate Webhooks:
Enable the webhook triggers for your CRM to ensure the workflow activates correctly
Customization Options
AI Agent Prompts
You can modify the system prompts in the ""Data Enrichment AI Agent"" nodes to:
Change the focus of profile analysis
Adjust the tone and detail level of summaries
Customize what information is extracted from posts
CRM Field Mapping
The workflow is pre-configured to update specific custom fields in Pipedrive and HubSpot. Update the field/property mappings in:
""Update data in Pipedrive"" nodes
""Update data in HubSpot"" node
Troubleshooting
Common Issues
LinkedIn Profile Not Found: Check if the contact's email is their work email; consider adjusting the search parameters
Webhook Not Triggering: Verify webhook configuration in your CRM
Missing Custom Fields: Ensure all required custom fields are created in your CRM with correct names
Rate Limits
Be aware of LinkedIn API rate limits (managed by HDW LinkedIn node)
Consider implementing delays if processing large batches of contacts
Best Practices
Use enrichment flags to selectively update contacts rather than enriching all contacts
Review and clean contact data in your CRM before enrichment
Periodically review the AI-generated summaries to ensure quality and relevance"
Create AI-Powered WhatsApp Quiz Bot with GPT-4o-mini and Supabase Storage,https://n8n.io/workflows/4114-create-ai-powered-whatsapp-quiz-bot-with-gpt-4o-mini-and-supabase-storage/,"Quiz Assistant via WhatsApp with Supabase and OpenAI
Create a quiz assistant that helps users study a topic of their choice through WhatsApp. Using Supabase and OpenAI, this workflow captures missing user data, stores it, and delivers dynamic quizzes tailored to each topic.
Main Use Cases
Guide users through personalized study sessions
Collect and store user preferences (name + topic)
Automate quiz creation with AI
Deliver interactive content via WhatsApp
How It Works
This workflow is composed of three main paths:
1. User Info Collection
Triggered by an incoming WhatsApp message
Retrieves existing user data from Supabase
Checks if the name and topic are already defined
If not, sends WhatsApp prompts to collect missing info
Updates Supabase with new entries
2. AI Quiz Generation
Once name and topic are confirmed, the Merge node consolidates all inputs
Data is sent to the AI Agent (OpenAI Chat + Memory)
The agent generates a quiz based on the selected topic
3. Response Delivery
The quiz is sent back to the user via WhatsApp
Flow ends, ready to restart with the next interaction"
Multi-platform Video Publishing from Google Sheets to 9 Social Networks via Blotato API,https://n8n.io/workflows/4227-multi-platform-video-publishing-from-google-sheets-to-9-social-networks-via-blotato-api/,"This workflow automates the process of publishing content from a Google Sheet to multiple social media platforms using the Blotato API. It retrieves content details (caption and Google Drive video URL) from a Google Sheet, uploads the media to Blotato, and then distributes it to Instagram, Facebook, LinkedIn, TikTok, YouTube, Threads, Twitter, Bluesky, and Pinterest. It also includes a separate branch for generating and publishing an AI-created image to Pinterest.
Tools & Services Used
Content Source: Google Sheets (for captions, status, and Google Drive video URLs)
Media Hosting: Google Drive (for source videos)
Social Media Publishing API: Blotato API (for media upload and posting to all platforms)
AI Image Generation:
OpenAI (DALL-E for generating an image for Pinterest)
Target Social Platforms:
Instagram
Facebook (Pages)
LinkedIn
TikTok
YouTube
Threads
Twitter (X)
Bluesky
Pinterest
Workflow Overview
This automation performs the following steps:
Trigger & Content Retrieval:
The Schedule Trigger node initiates the workflow on a defined schedule (e.g., every interval).
The Google Sheets node reads data from a specified sheet (Sheet1 of ""Publish to 9 Social Platforms""). It filters for rows where the ""Status"" column is ""Ready to Post"" and returns the first match. This provides the caption and the Google Drive URL for the video content.
The Get Google Drive ID node (a Set node) extracts the unique Google Drive file ID from the video URL obtained from the sheet.
Configuration & Media Preparation for Blotato:
The Setup Social Accounts node (a Set node) defines placeholders for the Blotato API key and various social media account IDs required by the Blotato API. These need to be manually filled with your actual credentials and IDs.
The Upload to Blotato node (HTTP Request) uploads the video content to Blotato's media endpoint. It constructs the downloadable Google Drive URL using the extracted file ID and sends it to Blotato along with the API key. Blotato then fetches and stores the media, returning a Blotato media URL.
Content Distribution to Social Platforms via Blotato (Parallel Branches from ""Upload to Blotato""):
The Blotato media URL (from the previous step) and the caption (from Google Sheets) are used to make individual POST requests to the Blotato API (/v2/posts) for each target platform. Each request is an HTTP Request node configured for a specific platform:
[Instagram] Publish via Blotato (Disabled by default): Posts to the configured Instagram account.
Publish to Facebook:
Posts to the configured Facebook Page.
Publish to LinkedIn:
Posts to the configured LinkedIn account.
Publish to Tiktok:
Posts to the configured TikTok account, with specific privacy and feature settings.
Publish to Youtube:
Uploads the video as a public YouTube short/video with a title and notifying subscribers.
Publish to Threads:
Posts to the configured Threads account (caption sliced to 500 chars).
Publish to Twitter:
Posts to the configured Twitter (X) account (caption sliced to 280 chars).
Publish to Bluesky:
Posts text-only (no media URL used in the example body) to the configured Bluesky account (caption sliced to 280 chars).
AI Image Generation & Pinterest Publishing (Separate Branch from ""Upload to Blotato""):
This branch demonstrates an alternative content type.
The OpenAI node generates an image based on a prompt (""Image of a manatee staring in the mirror at its AI avatar"") and returns image URLs.
The Upload to Blotato - Image node (HTTP Request) takes one of these generated image URLs and uploads it to Blotato's media endpoint.
The [Pinterest] Publish via Blotato node then posts this AI-generated image to the configured Pinterest account and board, along with the caption from the Google Sheet and a link."
LinkedIn Auto-Connect & Personalized Messaging for Sales,https://n8n.io/workflows/4190-linkedin-auto-connect-and-personalized-messaging-for-sales/,"Who is it for?
This template is for individuals or businesses who want to sell on LinkedIn and grow their network.
Use case
This workflow can be used to generate leads on LinkedIn, engage them with personalized messages, and qualify them based on their responses.
How this workflow works
You fill out a form specifying your target audience.
The system searches for your target audience on LinkedIn.
It checks if the found profiles are already connections.
It sends connection requests to those who are not connections.
After a profile connects back, it sends them a personalized message using their LinkedIn profile data.
How to set it up
Create a Browserflow account and connect it to your LinkedIn account.
Enter all your credentials.
Set a limit on the number of connection requests.
Start using the workflow through the n8n form or change the trigger as needed.
Browswerflow link - https://browserflow.io"
"Save Telegram Text, Voice & Audio to Notion with DeepSeek & OpenAI Summaries",https://n8n.io/workflows/4188-save-telegram-text-voice-and-audio-to-notion-with-deepseek-and-openai-summaries/,"Tired of manually copying and pasting Telegram messages into Notion? This n8n workflow solves that!
What it does:
This powerful workflow automates the process of saving your Telegram activity to Notion. Whether it's text chats, important voice memos, or shared audio files, ""TeleNotion Scribe"" captures it all. But it doesn't stop there! It also leverages AI to generate clear, concise summaries of your messages, giving you instant context and saving you time.
Key Features:
Seamless Telegram Integration: Automatically triggers on new Telegram messages.
Versatile Content Capture: Saves text messages, voice notes, and audio files.
AI-Powered Summarization: Get instant summaries of your chats with advanced language models.
Notion Database Automation: Creates organized entries in your Notion database.
Customizable: Easily adapt the workflow to your specific Notion database structure.
Time-Saving: Eliminate manual data entry and streamline your workflow.
Improved Organization: Keep all your Telegram information neatly organized in Notion.
Who is this for?
Project Managers: Track team communications and decisions.
Researchers: Log observations and data from chat groups.
Note-Takers: Capture meeting discussions and action items.
Anyone who wants to save and organize their Telegram chats!
What you'll get:
The complete n8n workflow JSON file.
Stop letting valuable information slip through the cracks. Invest in ""TeleNotion Scribe"" and transform your Telegram chats into actionable data!
Requirements:
n8n instance (cloud or self-hosted)
Telegram API credentials
Notion API integration token
OpenAI API key
DeepSeek API key"
Natural Language Task Management with Todoist and GPT-4o,https://n8n.io/workflows/4186-natural-language-task-management-with-todoist-and-gpt-4o/,"Turn plain-language chat like ‚ÄúTomorrow 9 AM: write blog post‚Äù into neatly organised Todoist tasks with GPT-4o and n8n‚Äîzero code.
ü™Ñ Ultimate Personal Todoist Agent
Turn natural-language requests into perfectly-organized Todoist tasks‚Äîall on autopilot inside n8n.
‚ÄúAdd Finish quarterly report by Friday afternoon‚Äù ‚Üí the agent creates the task, sets the due date & priority, and even drops it into the right project. ‚ú®
üåü Why this workflow rocks
All-in-one Todoist super‚Äëpowers ‚Äì create, update, complete, move, archive‚Ä¶ every major Todoist endpoint is wired up (tasks, projects, sections, labels, comments).
LLM‚Äëpowered intent detection ‚Äì an OpenAI model interprets plain-English (or emoji‚Äëfilled!) messages so you don‚Äôt have to remember slash‚Äëcommands.
Minimal setup ‚Äì just two credentials and you‚Äôre live.
Battle‚Äëtested building block ‚Äì use it as‚Äëis, or plug the Todoist Agent node into your own agents & chatbots.
üõ†Ô∏è What you‚Äôll need
Credential Where it‚Äôs used How to set it up
OpenAI API Orchestrator & LLM nodes Paste your OpenAI secret key into an OpenAI credential in n8n.
Todoist OAuth2 Todoist node and HTTP Request node Log in Todoist from your browser to set up credential in n8n.
That‚Äôs it‚Äîno webhooks, no extra secrets.
Tested with gpt‚Äë4o‚Äëlatest ‚Äì the fastest & most accurate model in our trials.
‚ö° Quick‚Äëstart (5‚ÄØminutes)
Import the JSON template (hit ‚ñ∂Ô∏è Try it out on the n8n template page or drag‚Äëdrop the file into your canvas).
Select your credentials in the two credential dropdowns.
Click Test workflow. In the sample Function node, tweak the message field (e.g. ‚ÄúTomorrow at 9‚ÄØam: write blog post‚Äù). Run ‚Üí watch your new Todoist task appear.
(Optional) Swap the Function node for your favourite chat trigger (Telegram, Slack, WhatsApp, Discord, you name it).
Boom‚Äîyour personal Todoist genie is alive! üßû‚Äç‚ôÇÔ∏è
üß© How it works (under the hood)
[Trigger / Chat message]
        ‚îÇ
        ‚ñº
[üóÇÔ∏è Orchestrator Agent]  ‚Üê OpenAI Chat Model + Short‚Äëterm Memory
        ‚îÇ                 ‚Ü≥ Parses intent & entities
        ‚îÇ
        ‚ñº
[ü§ñ Todoist Agent]       ‚Üê 15+ Todoist endpoints
        ‚îÇ                 ‚Ü≥ Executes the right call (create, update, complete, etc.)
        ‚ñº
[Done ‚úÖ  ]
The Orchestrator is an example. In production you can drop it and simply expose the Todoist Agent as a tool for any other agent workflow.
üéõÔ∏è Customising & extending
Idea How to do it
Notion / Sheets sync After the Todoist Agent node, add a Notion or Google Sheets node to log completed items.
Voice commands Swap the chat trigger for a Speech‚Äëto‚ÄëText node (e.g. Whisper).
ü§ù Need custom automations?
Want me to build or tweak something for you?
‚Üí Email maxemelyanenko@gmail.com and let‚Äôs make it happen!
‚ö†Ô∏è What‚Äôs not included (yet)
Shared projects & other Todoist Pro/Business endpoints.
File attachments in the comments.
Editing comments.
Pull requests welcome! üôå"
Auto-Ticket Maker: Convert Slack Conversations into Structured Project Tickets,https://n8n.io/workflows/4153-auto-ticket-maker-convert-slack-conversations-into-structured-project-tickets/,"Workflow: Auto-Ticket Maker
‚ö° About the Creators
This workflow was created by Varritech Technologies, an innovative agency that leverages AI to engineer, design, and deliver software development projects 500% faster than traditional agencies. Based in New York City, we specialize in custom software development, web applications, and digital transformation solutions. If you need assistance implementing this workflow or have questions about content management solutions, please reach out to our team.
üèóÔ∏è Architecture Overview
This workflow transforms your Slack conversations into complete project tickets, effectively replacing the need for a dedicated PM for task creation:
Slack Webhook ‚Üí Captures team conversation
Code Transformation ‚Üí Parses Slack message structure
AI PM Agent ‚Üí Analyzes requirements and creates complete tickets
Memory Buffer ‚Üí Maintains conversation context
Slack Output ‚Üí Returns formatted tickets to your channel
Say goodbye to endless PM meetings just to create tickets! Simply describe what you need in Slack, and our AI PM handles the rest, breaking down complex projects into structured epics and tasks with all the necessary details.
üì¶ Node-by-Node Breakdown
flowchart LR
A[Webhook: Slack Trigger] --> B[Code: Parse Message]
B --> C[AI PM Agent]
C --> D[Slack: Post Tickets]
E[Memory Buffer] --> C
F[OpenAI Model] --> C
Webhook: Slack Trigger
Type: HTTP Webhook (POST /slack-ticket-maker)
Purpose: Captures messages from your designated Slack channel.
Code Transformation
Function: Parses complex Slack payload structure
Extracts: User ID, channel, message text, timestamp, thread information
AI PM Agent
Inputs: Parsed Slack message
Process:
Evaluates project complexity
Requests project name if needed
Asks clarifying questions (up to 2 rounds)
Breaks down into epics and tasks
Formats with comprehensive structure
Ticket Structure:
Title
Description
Objectives/Goals
Definition of Done
Requirements/Acceptance Criteria
Implementation Details
Risks & Challenges
Testing & Validation
Timeline & Milestones
Related Notes & References
Open Questions
Memory Buffer
Type: Window Buffer Memory
Purpose: Maintains context across conversation
Slack Output
Posts fully-formatted tickets back to your channel
Uses markdown for clean, structured presentation
üîç Design Rationale & Best Practices
Replace Your PM's Ticket Creation Time
Let your PM focus on strategy while AI handles the documentation. Cut ticket creation time by 90%.
Standardized Quality
Every ticket follows best practices with consistent structure, detail level, and formatting.
No Training Required
Describe your needs conversationally - the AI adapts to your communication style.
Seamless Integration
Works within your existing Slack workflow - no new tools to learn."
"Automate AI News Videos with GPT-4o, Heygen Avatars, and Blotato",https://n8n.io/workflows/4080-automate-ai-news-videos-with-gpt-4o-heygen-avatars-and-blotato/,"Turn your ideas into engaging videos, talking avatars, and scheduled posts ‚Äî all on autopilot.
üöÄ How It Works: 3 Steps to AI-Generated Video Posts
ü§ñ AI Researches & Creates Your Video Script
Automatically fetches trending AI/LLM stories from Hacker News
Generates a 30-second viral video script with:
Attention-grabbing hook
Balanced analysis with statistics
Call-to-action (""Hit follow to stay ahead in AI!"")
Writes two caption versions:
Long (50 words + hashtags)
Short (2 sentences for platforms like Twitter/Threads)
2. üé• Heygen Generates the AI Avatar Video
Uses your preferred Heygen avatar/voice (configure in ""Setup Heygen"")
Applies a professional background video (included)
Processes the video in ~8 minutes (wait time adjustable)
3. üì£ Auto-Publishes to 10+ Platforms via Blotato
Uploads video to Blotato‚Äôs CDN
Publishes natively to:
‚úÖ Instagram | ‚úÖ YouTube | ‚úÖ TikTok | ‚úÖ LinkedIn
‚úÖ Twitter/X | ‚úÖ Facebook | ‚úÖ Threads (coming soon)
Platform-specific optimizations:
Instagram/YouTube: Long captions + hashtags
Twitter/Threads: Short captions
TikTok: Vertical video formatting
‚è±Ô∏è Full Automation Timeline
AI Research ‚Üí Scriptwriting ‚Üí Video Generation ‚Üí Multi-Platform Publishing
(Runs daily at 10 AM or your custom schedule)
üõ†Ô∏è Key Features
No Manual Work: Fully hands-off after setup
Customizable: Adjust AI prompts, avatars, or add platforms"
Sort Gmail Emails with GPT-4o into Action Required and No Action Labels,https://n8n.io/workflows/4053-sort-gmail-emails-with-gpt-4o-into-action-required-and-no-action-labels/,"üßæ Description:
This automation uses GPT-4o to scan unread Gmail emails and intelligently classify them as:
Action ‚Üí Requires your attention (reply, review, schedule, or respond)

No Action ‚Üí Informational or promotional; no action needed
The result? You eliminate inbox noise and gain a clear daily routine: only check what's in Action Required.
‚öôÔ∏è How It Works:
Trigger: Runs on a customizable schedule

Fetch Emails: Pulls unread messages from Gmail

Classify via GPT-4o: Determines if each email needs action or not

Sort Emails:

    Labels actionable emails as Action Required

    Labels non-actionable ones as No Action

    Removes the Inbox label to clean your primary inbox view

    ‚úÖ Emails stay in your account‚Äîjust better organized
üöÄ How to Use:
Import the workflow into your n8n instance

Set up Gmail and OpenAI credentials

Create Gmail labels:

    Action Required

    No Action

Activate the workflow

Start your day by checking only the Action Required label
üì¶ Requirements:
n8n (self-hosted or cloud)

Gmail OAuth2 account

OpenAI API key (GPT-4o or GPT-4o-mini)

Gmail labels: Action Required, No Action
üí° Why It Matters:
Stop manually filtering emails.
This workflow helps you focus only on what matters while keeping everything else out of your way‚Äîwithout deleting or archiving anything."
"AI Chatbot Call Center: General Exception Flow (Production-Ready, Part 8)",https://n8n.io/workflows/4052-ai-chatbot-call-center-general-exception-flow-production-ready-part-8/,"Workflow Name: üëª Exception Flow
Template was created in n8n v1.90.2
Skill Level: Low
Categories: n8n, Chatbot
Stacks
Error Trigger
Slack node
What this workflow does?
This is a n8n Error Workflow. It will trigger when there is an error in another workflow. When this happens, it then tries to send an error notification to the preset Slack channel.
How it works
The Error Trigger node will trigger when there is an error in another workflow, as long as that workflow is set up to do so.
A error notification will be sent to the Slack Channel.
Set up instructions
Create you Slack credentials, refer to n8n integration documentation for more information.
Set up the Channel in üëª Exception Alert node.
For any n8n workflows to trigger this, switch to that workflow, select menu > settings, and set the Error Workflow to üëª Exception Flow.
How to adjust it to your needs
Although this workflow template is part of the AI Chatbot Call Center Series, it could be used with any n8n workflows.
Update the Channel in üëª Exception Alert to your own channel
https://chatpaylabs.com/blog/part-8-build-your-own-ai-chatbot-call-center-general-exception-flow-production-ready-n8n-workflow-free-download-"
"Automate Sprint Planning with OpenAI, Google Calendar, and Gmail for Agile Teams",https://n8n.io/workflows/4038-automate-sprint-planning-with-openai-google-calendar-and-gmail-for-agile-teams/,"üë§ Who is this for?
This workflow is designed for Scrum Masters and Agile Coaches who prepare and coordinate Sprint Planning sessions, using Google Calendar, Google Sheets, and OpenAI.
üß© What problem is this workflow solving?
It solves the manual and time-consuming task of collecting, validating, and preparing backlog items for sprint planning‚Äîespecially when coordinating with distributed teams or large product backlogs.
‚öôÔ∏è What this workflow does
Every week, the workflow:
Detects if a Sprint Planning event is coming up.
Retrieves relevant backlog items marked as ‚ÄúReady for Sprint Planning‚Äù or ‚ÄúActive in Sprint.‚Äù
Checks each user story against the Definition of Ready (DoR) using AI.
Adds AI-generated feedback per story in the backlog.
Drafts a personalized preparation email for the team and sends it after Scrum Master approval.
Optionally, generates Sprint Goal suggestions for the Product Owner.
üöÄ Setup
Connect your Google Calendar, Sheets, and Gmail accounts.
Then configure variables like event names, sheet names, and email addresses in the ‚ÄúStart Here‚Äù node.
üõ†Ô∏è How to customize this workflow to your needs
Swap Google Sheets with Jira or another tool.
Adjust status filters and column mappings.
Tweak AI prompts for tone, language, or preferred practices.
Change email logic for different approval flows.
üî• Unique Selling Points (USPs)
Validated by a Scrum Master with 10+ years of experience
AI-driven backlog validation and email drafting.
Weekly automation with human approval loops.
Clear, structured output aligned with agile best practices.
Fully customizable for any Scrum environment."
"AI-Powered Blog Post Promoter for Instagram, Facebook & X with GPT",https://n8n.io/workflows/4033-ai-powered-blog-post-promoter-for-instagram-facebook-and-x-with-gpt/,"AI Social Media Promoter ‚Äì Automated Blog Sharing Workflow
This workflow is built for bloggers, creators, and marketing teams who want to automatically promote new blog content across Instagram, Facebook, and X (Twitter). Its core purpose is to detect new blog posts from your RSS feed and instantly create platform-specific social media posts ‚Äî complete with AI-generated captions and visuals ‚Äî to drive traffic from social media back to your blog. Everything runs on autopilot: from detecting a new article to posting eye-catching content on each platform, while logging actions to Google Sheets and sending post previews by email. The result is a consistent and professional online presence with zero manual effort.
How it works
Once a new blog post appears in your RSS feed, the workflow kicks in. It checks Google Sheets to ensure the post hasn‚Äôt already been published. Then it calls GPT-4 to generate unique captions for Instagram, Facebook, and X, based on the post content and your brand‚Äôs tone. Simultaneously, OpenAI's GPT-Image generates a custom visual in the right format (square, portrait, or landscape). The workflow then publishes the complete posts using the Meta Graph API (for IG/FB) and X API (Twitter). All actions are logged in a Google Sheet, and a formatted email summary with images is sent to your inbox for review.
How to set up
To get started, connect the required APIs: OpenAI (for text and images), ImgBB or Cloudinary (for image hosting), Meta (Instagram/Facebook), X (Twitter), Google Sheets, and Gmail. Replace placeholder values in HTTP request nodes ‚Äî such as your RSS URL, API keys, or spreadsheet ID. You can personalize the prompt content with your blog name, tone of voice, and call-to-action. Once connected, the system is fully automated. You can run it manually, or schedule it to check your blog daily with a Schedule Trigger node."
Generate Recipes from Ingredients with Ollama AI Chef Agent,https://n8n.io/workflows/4007-generate-recipes-from-ingredients-with-ollama-ai-chef-agent/,"What It Does
The Chef Agent is your AI-powered kitchen companion‚Äîready to turn leftover ingredients into meal inspiration. It's a simple, fun n8n automation that:
Accepts a list of ingredients via webhook
Uses Ollama AI to suggest 5 creative recipes or food ideas
Recommends up to 3 missing ingredients to improve the dish
Returns a fallback message if the AI is unavailable
Includes setup notes for beginners
Requirements
An active n8n instance (local or hosted)
Ollama AI running locally (or another LLM via HTTP request)
A webhook endpoint (defaults to /lets-cook)
Why You‚Äôll Love It
Fully customizable for your use case or favorite LLM
Great intro to AI + workflow automation
Comes with playful Clown Mutiny flair:
‚ÄúPowered by Clown Mutiny‚Äôs taste-bud liberation division.‚Äù
Installation
Import the provided JSON template into your n8n workspace.
Configure your AI node to match your local Ollama instance.
Trigger the flow by sending a POST request to the webhook:
{
  ""ingredients"": ""eggs, rice, spinach""
}"
Generate Written Content with GPT Recursive Writing & Editing Agents,https://n8n.io/workflows/3503-generate-written-content-with-gpt-recursive-writing-and-editing-agents/,"Who is this for?
Content creators, writers, and automation enthusiasts experimenting with recursive AI workflows for content generation and refinement. Ideal for those exploring AI agents that collaborate in cycles of writing and editing.
What problem does this solve?
This template introduces a fully automated, recursive writing‚Äëediting loop using multi‚Äëagent collaboration. A ‚ÄúWriting Agent‚Äù generates content based on an input topic. An ‚ÄúEditing Agent‚Äù reviews it, suggests improvements, and determines whether the work is complete. The loop continues until the editor is satisfied‚Äîallowing for high‚Äëquality, iterative AI‚Äëassisted writing with minimal human input.
How it works
This template is a foundational setup to help you build custom recursive writing workflows:
Trigger: Activated by an n8n chat message containing a topic. You can customize this to work with webhooks, forms, or other input sources.
Edit Handler: A code node checks for previous edits and sets a default empty string if none are found.
Writing Agent: Generates a blurb based on the topic and any edits. Customize the prompt in this node by editing the user/system instructions to fit your tone, domain, or style preferences.
Editing Agent: Suggests specific edits and outputs a structured JSON object:
{
  ""status"": ""incomplete"",
  ""edits"": ""Replace passive voice with active voice in the second sentence. Clarify the main idea in the opening line.""
}
You can adjust the JSON format or editing criteria in the prompt field. Customize the prompt in this node by editing the user/system instructions to fit your tone, domain, or style preferences.
Recursive Loop: If the status is ‚Äúincomplete,‚Äù the edits are passed back to the Writing Agent, which revises the blurb.
Completion: Once the Editing Agent outputs a status of ‚Äúcomplete,‚Äù the workflow ends, and the final blurb is returned to the n8n chat.
Setup Steps
Import the Template into your n8n workspace.
Configure API Credentials: Link your OpenAI API key (or your preferred LLM like Claude or Gemini) in the credentials section.
Customize the Prompts (Optional but recommended):
In the Writing Agent, you can instruct it to mimic a specific tone, format, or genre.
In the Editing Agent, specify your editing standards (e.g., concise, persuasive, technical).
Modify the JSON output structure in the Structured Output Parser node if needed.
Test and Iterate: Run a test by sending a topic via the chat trigger and observe the loop behavior.
Example Output
Input Topic: ‚ÄúThe future of remote work‚Äù
Final Blurb: ‚ÄúRemote work is here to stay. As companies embrace flexible setups, productivity and employee satisfaction are reaching new highs. The challenge now is to build culture and collaboration tools that keep up.‚Äù
This template offers a powerful starting point for recursive AI writing. Expand it with additional agents, tone shifts, formatting layers, or sentiment analysis as needed."
Automated Daily Customer Win-Back Campaign with AI Offers,https://n8n.io/workflows/4001-automated-daily-customer-win-back-campaign-with-ai-offers/,"Proactively retain customers predicted to churn with this automated n8n workflow. Running daily, it identifies high-risk customers from your Google Sheet, uses Google Gemini to generate personalized win-back offers based on their churn score and preferences, sends these offers via Gmail, and logs all actions for tracking.
What does this workflow do?
This workflow automates the critical process of customer retention by:
Running automatically every day on a schedule you define.
Fetching customer data from a designated Google Sheet containing metrics like predicted churn scores and preferred categories.
Filtering to identify customers with a high churn risk (score > 0.7) who haven't recently received a specific campaign (based on the created_campaign_date field - you might need to adjust this logic).
Using Google Gemini AI to dynamically generate one of three types of win-back offers, personalized based on the customer's specific churn score and preferred product categories:
Informational: (Score 0.7-0.8) Highlights new items in preferred categories.
Bonus Points: (Score 0.8-0.9) Offers points for purchases in a target category (e.g., Books).
Discount Percentage: (Score 0.9-1.0) Offers a percentage discount in a target category (e.g., Books).
Sending the personalized offer directly to the customer via Gmail.
Logging each sent offer or the absence of eligible customers for the day in a separate 'SYSTEM_LOG' Google Sheet for monitoring and analysis.
Who is this for?
CRM Managers & Retention Specialists: Automate personalized outreach to at-risk customers.
Marketing Teams: Implement data-driven retention campaigns with minimal manual effort.
E-commerce Businesses & Subscription Services: Proactively reduce churn and increase customer lifetime value.
Anyone using customer data (especially churn prediction scores) who wants to automate personalized retention efforts via email.
Benefits
Automated Retention: Set it up once, and it runs daily to engage at-risk customers automatically.
AI-Powered Personalization: Go beyond generic offers; tailor messages based on churn risk and customer preferences using Gemini.
Proactive Churn Reduction: Intervene before customers leave by addressing high churn scores with relevant offers.
Scalability: Handle personalized outreach for many customers without manual intervention.
Improved Customer Loyalty: Show customers you value them with relevant, timely offers.
Action Logging: Keep track of which customers received offers and when the workflow ran.
How it Works
Daily Trigger: The workflow starts automatically based on the schedule set (e.g., daily at 9 AM).
Fetch Data: Reads all customer data from your 'Customer Data' Google Sheet.
Filter Customers: Selects customers where predicted_churn_score &gt; 0.7 AND created_campaign_date is empty (verify this condition fits your needs).
Check for Eligibility: Determines if any customers passed the filter.
IF Eligible Customers Found:
Loop: Processes each eligible customer one by one.
Generate Offer (Gemini): Sends the customer's predicted_churn_score and preferred_categories to Gemini. Gemini analyzes these and the defined rules to create the appropriate offer type, value, title, and detailed message, returning it as structured JSON.
Log Sent Offer: Records action_taken = SENT_WINBACK_OFFER, the timestamp, and customer_id in the 'SYSTEM_LOG' sheet.
Send Email: Uses the Gmail node to send an email to the customer's user_mail with the generated offer_title as the subject and offer_details as the body.
IF No Eligible Customers Found:
Set Status: Creates a record indicating system_log = NOT_FOUND.
Log Status: Records this 'NOT_FOUND' status and the current timestamp in the 'SYSTEM_LOG' sheet.
n8n Nodes Used
Schedule Trigger
Google Sheets (x3 - Read Customers, Log Sent Offer, Log Not Found)
Filter
If
SplitInBatches (Used for Looping)
Langchain Chain - LLM (Gemini Offer Generation)
Langchain Chat Model - Google Gemini
Langchain Output Parser - Structured
Set (Prepare 'Not Found' Log)
Gmail (Send Offer Email)
Prerequisites
Active n8n instance (Cloud or Self-Hosted).
Google Account with access to Google Sheets and Gmail.
Google Sheets API Credentials (OAuth2): Configured in n8n.
Two Google Sheets:
'Customer Data' Sheet: Must contain columns like customer_id, predicted_churn_score (numeric), preferred_categories (string, e.g., [""Books"", ""Electronics""]), user_mail (string), and potentially created_campaign_date (date/string).
'SYSTEM_LOG' Sheet: Should have columns like system_log (string), date (string/timestamp), and customer_id (string, optional for 'NOT_FOUND' logs).
Google Cloud Project with the Vertex AI API enabled.
Google Gemini API Credentials: Configured in n8n (usually via Google Vertex AI credentials).
Gmail API Credentials (OAuth2): Configured in n8n with permission to send emails.
Setup
Import the workflow JSON into your n8n instance.
Configure Schedule Trigger: Set the desired daily run time (e.g., Hours set to 9).
Configure Google Sheets Nodes:
Select your Google Sheets OAuth2 credentials for all three Google Sheets nodes.
1. Fetch Customer Data...: Enter your 'Customer Data' Spreadsheet ID and Sheet Name.
5b. Log Sent Offer...: Enter your 'SYSTEM_LOG' Spreadsheet ID and Sheet Name. Verify column mapping.
3b. Log 'Not Found'...: Enter your 'SYSTEM_LOG' Spreadsheet ID and Sheet Name. Verify column mapping.
Configure Filter Node (2. Filter High Churn Risk...):
Crucially, review the second condition: {{ $json.created_campaign_date.isEmpty() }}. Ensure this field and logic correctly identify customers who should receive the offer based on your campaign strategy. Modify or remove if necessary.
Configure Google Gemini Nodes: Select your configured Google Vertex AI / Gemini credentials in the Google Gemini Chat Model node. Review the prompt in the 5a. Generate Win-Back Offer... node to ensure the offer logic matches your business rules (especially category names like ""Books"").
Configure Gmail Node (5c. Send Win-Back Offer...): Select your Gmail OAuth2 credentials.
Activate the workflow.
Ensure your 'Customer Data' and 'SYSTEM_LOG' Google Sheets are correctly set up and populated. The workflow will run automatically at the next scheduled time.
This workflow provides a powerful, automated way to engage customers showing signs of churn, using personalized AI-driven offers to encourage them to stay. Adapt the filtering and offer logic to perfectly match your business needs!"
Access Control for AI Agents (RBAC) using Airtable and Telegram,https://n8n.io/workflows/3988-access-control-for-ai-agents-rbac-using-airtable-and-telegram/,"Purpose
This workflow allows granular control over the access to tools connected to AI Agents (including Multi-Agent setups) using Role Based Access Control.
Demo & Explanation
How it works
User permissions are managed in Airtable where every restricted AI tool is listed by name and connected via roles to users
Requests to the Main Agent can be sent through a Telegram message (can be replaced by Whatsapp, IMAP or similar)
On every request the Telegram username is used to query a list of all allowed tools which are linked in Airtable
A LangChain Code node is used to compare that list against the connected tools
Every tool which is not permitted to be used is being replaced by a tool, which has a status response, telling the Agent to return a message to the user, that he is not authorized to use the tool
Otherwise allowed tools are passed through to the Agent, as if they were connected directly to the Agent
The parameters can also be passed to a sub-agent called as a sub-workflow where permissions can be checked the same way
Every response is sent back to the same Telegram conversation
Setup
Clone the workflow and select the belonging credentials. You'll need an OpenAI and Airtable Account as well as a Telegram Bot (refer to the docs for the Telegram credentials).
Copy this Airtable Template into your workspace
Follow the instructions given in the yellow sticky notes
Activate the workflow
How to use
Try this example:
Create a new line in Airtable under ‚ÄúUsers‚Äù containing your Telegram username and your full name
Set the roles ‚Äúbasic‚Äù and ‚Äúinfo‚Äù
Consider temporarily disconnecting or resetting the chat memories so they do not remember previous confirmations
Start a new chat, asking about your permitted roles - you should get a list of those
Ask about the current weather in your city - you should be informed, that you do not have permission to access that information
Back in Airtable add the role ‚Äúweather‚Äù to your user
Now ask the Agent the same question again - It should give you a proper answer this time
From here on you can add tools and create roles to your likings.
Disclaimer
Please note, that this workflow can only run on self-hosted n8n instances, since it requires the LangChain Code Node."
Auto-Generate And Post Tweet Threads Based On Google Trends Using Gemini AI,https://n8n.io/workflows/3978-auto-generate-and-post-tweet-threads-based-on-google-trends-using-gemini-ai/,"AI-Powered Twitter Automation using n8n to Check BigQuery Latest Google Trend and Publish
Who is this for?
This template is designed for:
Content creators
Tech influencers
Educators and marketers
who want to automatically tweet trend-based threads using AI and stay relevant without spending time on manual research and content writing.
What problem does this workflow solve?
Creating daily engaging Twitter threads takes time and effort. This workflow:
Finds trending search terms in your region (via Google Trends)
Selects the most relevant topic using AI
Generates a thread of five value-packed tweets
Publishes them automatically on X (Twitter)
You get high-quality, niche-specific content on autopilot ‚Äî ideal for boosting visibility, brand awareness, and engagement.
What this workflow does
‚è∞ Runs on a scheduled basis (daily at your preferred hour)
üåç Uses BigQuery to fetch the top 25 trending search terms
üéØ AI Agent selects one niche-relevant trend
‚úçÔ∏è AI generates a 5-part tweet thread using Gemini or OpenAI
üê¶ Tweets are published to X (Twitter) in sequence
üîÅ Works with any niche ‚Äì simply modify the AI prompt
Setup Instructions
üîß Google Cloud Setup
Create a project in Google Cloud Console
Enable BigQuery API
Use the included SQL to fetch Google Trends
üê¶ Twitter Developer Setup
Go to developer.twitter.com
Create an app and generate OAuth 2.0 credentials
üß† AI Credentials
Add either Gemini API Key or OpenAI Key in n8n
‚ú® Customize
Modify the AI prompts to match your niche (e.g., marketing, health, finance)
Adjust tweet tone, call to action, hashtags, or output format
How to customize this workflow to your needs
üåç Change Google Trends region or country in the SQL query
üß† Switch to another AI model (Claude, GPT-4o, DeepSeek, etc.)
üì¢ Replace Twitter with LinkedIn, Telegram, or Slack integration
üìä Add metrics or logging for tweet performance (advanced)
üé• Watch How It Works
In this tutorial, I walk you through the full automation ‚Äì from pulling Google Trends to posting your AI-generated tweet thread using n8n.
Example Use Cases
üîç Daily tweet series on trending tools or technologies
üíπ Market insights or productivity tips based on daily trends
üéì Educational content scheduled automatically
üß† Personal brand building using consistent, trend-aware posts
Dependencies
n8n (self-hosted or cloud)
Google Cloud project with BigQuery enabled
Twitter Developer Account (OAuth 2.0 setup)
AI provider (Gemini or OpenAI)
Keywords
n8n twitter automation, tweet scheduler, X automation, auto post tweets, AI tweet writer, trending topics Twitter bot, Google Trends automation, AI content workflow, tweet generator using GPT, Twitter bot with Gemini, twitter + n8n
Support & Credit
Created by Amjid Ali ‚Äî
üèÜ Global CIO200 | Founder of Syncbricks
If this template saved you time, consider supporting:
‚û°Ô∏è paypal.me/pmptraining
üìö Learn more:
Enroll in the full n8n course
LMS
More automation templates
Watch all YouTube tutorials"
Automated Facebook Comment Management with GPT-4o and LangChain,https://n8n.io/workflows/3949-automated-facebook-comment-management-with-gpt-4o-and-langchain/,"ü§ñ Facebook AI Agent with MCP Server ‚Äì Built for Smart Engagement and Automation
Hi! I‚Äôm Amanda ü•∞üòò ‚Äî I build intelligent automations with n8n and Make.
This powerful workflow was designed to help teams automatically handle Facebook page interactions with AI. Using Meta Graph API, LangChain, MCP Server, and GPT-4o, it allows your AI agent to search for posts, read captions, fetch comments, and even reply or message followers, all through structured tools.
üîß What the workflow does
Searches for recent media using Facebook Page ID and access token
Reads and extracts captions or media URLs
Fetches comments and specific replies from each post
Replies to comments automatically with GPT-generated responses
Sends direct messages to followers who commented
Maps user input and session to keep memory context via LangChain
Communicates via Server-Sent Events (SSE) using your MCP Server URL
üß∞ Nodes & Tech Used
LangChain Agent + Chat Model with GPT-4o
Memory Buffer for session memory
toolHttpRequest to search media, comments, and send replies
MCP Trigger and MCP Tool (custom SSE connection)
Set node for input and variable assignment
Webhook and JSON for Facebook API structure
‚öôÔ∏è Setup Instructions
Create your Facebook App in Meta Developer Portal
Add your Facebook Page ID and Access Token in the Set node
Update the MCP Server Tool URL in the MCP Facebook node
Use your n8n server URL (e.g. https://yourdomain.com/mcp/server/facebook/sse)
Trigger the workflow using the included LangChain Chat Trigger
Interact via text to ask the agent to:
‚ÄúGet latest posts‚Äù
‚ÄúReply to comment X with this message‚Äù
‚ÄúSend DM to this user about...‚Äù
üë• Who this is for
Social media teams managing multiple Facebook pages
Brands automating engagement with followers
Agencies creating smart, autonomous digital assistants
Developers building conversational bots for Facebook
‚úÖ Requirements
Meta Graph API access
Facebook Page (with permissions)
n8n instance (Cloud or Self-hosted)
MCP Server configured (SSE Endpoint enabled)
OpenAI API Key (for GPT-4o + LangChain)
üåê Want to use this workflow?
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda"
Send Personalized WhatsApp Templates Triggered by KlickTipp with Auto-Responses,https://n8n.io/workflows/3937-send-personalized-whatsapp-templates-triggered-by-klicktipp-with-auto-responses/,"Community Node Disclaimer: As this workflow relies on a community node, it is limited to self-hosted environments.
How It Works
This workflow automates personalized WhatsApp message template delivery triggered by events in KlickTipp or by messages sent to the Whatsapp Business account. When a contact triggers an Outbound, the workflow uses a pre-approved WhatsApp message template to send dynamic, real-time messages through the WhatsApp Business Cloud API. When receiving messages it checks whether a cancellation should be processed or if a auto-response is sent.
This setup is ideal for time-sensitive campaigns such as:
Birthday greetings
Discount or promo notifications
Follow-ups on product or service interest
Key Features
KlickTipp Trigger
Starts the workflow when a specific outbound is triggered
Typical use case: subcriber receives activation Tag and triggers an Outbound which sends a webhook call to trigger WhatsApp messaging.
WhatsApp Business Cloud - Message Trigger
Listens to messages from the contact and processes answers with answering auto-responder or by tagging the contact in KlickTipp.
WhatsApp Business Cloud - Sending Template Messages
Sends WhatsApp message templates using a pre-approved template.
Template placeholders are filled with data from KlickTipp custom fields.
Setup Instructions
Set up the KlickTipp and Whatsapp nodes in your n8n instance.
Authenticate your WhatsApp and KlickTipp accounts.
Create the necessary custom fields to match the data structure
Verify and customize field assignments in the workflow to align with your specific form and subscriber list setup.
Field Label Field Type
Whatsapp_Produkt/Dienstleistung Single line
Whatsapp_Name/Unternehmen Single line
Whatsapp_Link_Endung Single line
Testing & Deployment
Use a real test contact with all required fields filled.
Trigger the Outbound in KlickTipp using the activation tag and answer with a message to the template.
Run the scenario once in n8n to verify successful delivery of the whatsapp message template to your test contact as well as the reception of the auto-responder and the subscription and tagging in KlickTipp to stop further messages.
Campaign Expansion Ideas
Connect campaign to process keywords like ""STOP"" from WhatsApp messages
Pair WhatsApp with welcome email series for onboarding.
Use tags like product_interest_X for precise segmentation.
A/B test templates with different CTA formats or timings.
Monitor CTRs via dynamic URLs in WhatsApp templates.
Benefits
Multi-channel engagement: Adds WhatsApp to your marketing toolkit.
Dynamic content: Personalizes messages using contact data.
-KlickTipp campaign control Whatsapp contacts can for example signal with messages like ""STOP"" to receive the according Tag in KlickTipp in order to start/end automations.
üí° Pro Tip: Customize the domain link ending per campaign or product line. This allows targeted redirects, e.g., meinshop.de/ProduktA or `mein"
"Collect LinkedIn Profiles with AI Processing using SerpAPI, OpenAI, and NocoDB",https://n8n.io/workflows/3920-collect-linkedin-profiles-with-ai-processing-using-serpapi-openai-and-nocodb/,"What problem does this solve?
It fetches LinkedIn profiles for a multitude of purposes based on a keyword and location via Google search and stores them in an Excel file for download and in a NocoDB database.
It tries to avoid using costly services and should be n8n beginner friendly.
It uses the serpapi.com to avoid being blocked by Google Search and to process the data in an easier way.
What does it do?
Based on criteria input, it searches LinkedIn profiles
It discards unnecessary data and turns the follower count into a real number
The output is provided as an Excel table for download and in a NocoDB database
How does it do it?
Based on criteria input, it uses serpAPI.com to conduct Google search of the respective LinkedI profiles
With OpenAI.com the name of the respective company is being added
With OpenAI.com the follower number e.g., 300+ is turned into a real number: 300
All unnecessary metadata is being discarded
As an output an Excel file is being created
The output is stored in a nocodb.com table
Step-by-step instruction
Import the Workflow:
Copy the workflow JSON from the ""Template Code"" section below.
Import it into n8n via ""Import from File"" or ""Import from URL"".
Set up a free account at serpapi.com and get API credentials to enable good Google search results
Set up an API account at openai.com and get API key
Set up a nocodb.com account (or self-host) and get the API credentials
Create the credentials for serpapi.com, opemnai.com and nocodb.com in n8n.
Set up a table in NocoDB with the fields indicated in the note above the NocoDB node
Follow the instructions as detailed in the notes above individual nodes
When the workflow is finished, open the Excel node and click download if you need the Excel file"
Analyze Google Sheets Data with OpenAI-powered Data Agent,https://n8n.io/workflows/3835-analyze-google-sheets-data-with-openai-powered-data-agent/,"Welcome to Ozki Your Data Analyst Agent V1.
The Ozki Data Analyst Agent is designed to analyze data from Google Sheets. To use it, you'll need to provide the URL of your Google Sheet file. The agent will then process the data and provide you with analysis results, including key performance indicators (KPIs).
Configuration:
Configure your credentials on the OpenAI model or select the n8n free OpenAI credits.
Set up your agent memory. Use Simple Memory as default.
Set your credentials to Google Sheets. Log in with the Google Sheet tool.
Instructions:
Start with a ""Hi"" to get the instructions.
Ozki needs your Google Sheet URL, which you can get from the address bar of your browser when you have the file open.
Follow the conversation with Ozki for your data analysis results."
AI-Powered Research Assistant with Perplexity Sonar API,https://n8n.io/workflows/3673-ai-powered-research-assistant-with-perplexity-sonar-api/,"Name:
AI-Powered Research Agent using Perplexity Sonar
Description:
This workflow acts as an AI-powered research assistant using the Perplexity Sonar model. When triggered by another workflow, it sends a user-defined prompt to the Perplexity API to retrieve up-to-date search results. The response is then parsed into a clean format for downstream processing.
How it Works:
Trigger: Activated from another workflow via Execute Workflow Trigger.
Prompt Setup: Sets a system role message and user query dynamically.
API Call: Sends a POST request to Perplexity's /chat/completions endpoint with your credentials.
Response Handling: Extracts the message content from the API response.
Output: Returns the result, ready for display or further processing.
Requirements:
A Perplexity AI API Key
Set up authentication via Header Auth with Bearer token
Ensure your account allows outbound HTTP requests in n8n
Customization Tips:
Modify the system prompt to suit your research domain
Chain this workflow with other automation like blog creation, summaries, etc.
Replace the output handling logic to fit into Google Sheets, Notion, or Telegram"
"Optimize Amazon Ads with GPT-4o for Bid, Budget & Keyword Recommendations",https://n8n.io/workflows/3793-optimize-amazon-ads-with-gpt-4o-for-bid-budget-and-keyword-recommendations/,"Overview
This template is designed for Amazon sellers and advertisers who want to automate their campaign performance analysis and bidding strategy. It solves the common challenge of manually reviewing Sponsored Products reports and guessing how to adjust keywords, placements, and budgets. By combining Amazon Advertising reports with OpenAI's GPT-4o, this workflow delivers real-time, personalized optimization instructions ‚Äî automatically.
Features
üì• Automatically downloads Sponsored Products reports from Google Drive
üß† Uses AI to analyze campaign, keyword, placement, targeting, and budget performance
üìä Supports both .csv and .xlsx report formats
üîÅ Handles multiple ASINs and scales easily across ad accounts
üìß Sends structured optimization recommendations to your inbox via Gmail
üóÇ Built-in logic to normalize filenames and correctly map reports
üßπ Includes error handling and formatting cleanup for AI-ready input
Requirements
To use this workflow, you‚Äôll need:
An Amazon Ads account with access to Sponsored Products reports
A Google Drive folder where Amazon Ads reports are delivered (manually or via Gmail automation)
A Gmail account (for sending summaries)
An OpenAI API key with access to GPT-4o
Optional: a developer account for the Amazon Ads API to fully automate report generation in the future
Setup Instructions
üìÇ Connect your Amazon Ads reports folder in the Google Drive node
üîê Add your credentials to the OpenAI and Gmail nodes
üìù Schedule five reports in the Amazon Ads Console:
Search Term Report ‚Üí Detailed
Targeting Report ‚Üí Detailed
Campaign Report ‚Üí Summary
Placement Report ‚Üí Summary
Budget Report ‚Üí Summary
Use ‚ÄúLast 30 Days‚Äù, ‚ÄúDaily‚Äù, and .xlsx or .csv format
üîÅ (Optional) Automate report ingestion using Gmail + Drive workflows
üß™ Test with one account, then replicate across additional ad accounts as needed
‚è±Ô∏è Setup time: 15‚Äì30 minutes
üìå All field-specific guidance is included in workflow notes`"
Email Assistant: Convert Natural Language to SQL Queries with Phi4-mini and PostgreSQL,https://n8n.io/workflows/3761-email-assistant-convert-natural-language-to-sql-queries-with-phi4-mini-and-postgresql/,"Who is this for?
üßëüèªü´±üèª‚Äçü´≤üèªü§ñ Humans and Robots alike.
This workflow can be used as a Chat Trigger, as well as a Workflow Trigger.
It will take a natural language request, and then generate a SQL query. The resulting query parameter will contain the query, and a sqloutput parameter will contain the results of executing such query.
What's the use case?
This template is most useful paired with other workflows that extract e-mail information and store it in a structured Postgres table, and use LLMs to understand inquiries about information contained in an e-mail inbox and formulate questions that needs answering.
Plus, the prompt can be easily adapted to formulate SQL queries over any kind of structured database.
Privacy and Economics
As LLM provider I'm using Ollama locally, as I consider my e-mail extremely sensitive information. As model, phi4-mini does an excellent job balancing quality and efficiency.
Setup
Upon running for the first time, this workflow will automatically trigger a sub-section to read all tables and extract their schema into a local file.
Then, either by chatting with the workflow in n8n's interface or by using it as a sub-workflow, you will get a query and a sqloutput response.
Customizations
If you want to work with just one particular table yet keep edits at bay, append a condition to the List all tables in a database step, like so:
WHERE table_schema='public' AND table_name='my_emails_table_name'
To repurpose this workflow to work with any other data corpus in a structured database, inspect the AI Agent user and system prompts and edit them accordingly."
Create Animated Illustrations from Text Prompts with Midjourney and Kling API,https://n8n.io/workflows/3626-create-animated-illustrations-from-text-prompts-with-midjourney-and-kling-api/,"What does the workflow do?
This workflow is primarily designed to generate animated illustrations for content creators and social media professionals with Midjourney (unoffcial) and Kling (unofficial) API served by PiAPI.
PiAPI is an API platform which provides professional API service. With service provided by PiAPI, users could generate a fantastic animated artwork simply using workflow on n8n without complex settings among various AI models.
What is animated illustration?
An animated illustration is a digitally enhanced artwork that combines traditional illustration styles with subtle, purposeful motion to enrich storytelling while preserving its original artistic essence.
Who is this workflow for?
Social Media Content Creators: Produces animated illustrations for social media posts.
Digital Marketers: Generates marketing materials with motion graphics.
Independent Content Producers: Creates animated content without specialized animation skills.
Step-by-step Setting Instructions
To simplify workflow settings, usually users just need to change basic prompt of the image and the motion of the final video following the instrution below:
Sign in your PiAPI account and get your X-API-Key.
Fill in your X-API-Key of PiAPI account in Midjourney and Kling nodes.
Enter your desired image prompt in the Prompt node.
Enter the motion prompt in Kling Video Generator node.
For more complex or customization settings, users could also add more nodes to get more output images and generate more videos. Also, they could change the target image to gain a better result. As for recommendation, users could change the video models for which we would recommend live-wallpaper LoRA of Wanx. Users could check API doc to see more use cases of video models and image models for best practice.
Use Case
Input Prompt
A gentle girl and a fluffy rabbit explore a sunlit forest together, playing by a sparkling stream. Butterflies flutter around them as golden sunlight filters through green leaves. Warm and peaceful atmosphere, 4K nature documentary style. --s 500 --sref 4028286908 --niji 6
Output Video
<video src=""https://static.piapi.ai/n8n-instruction/motion-illustration/example1.mp4"" controls />
When there is troubleshooting
Check if the X-API-Key has been filled in nodes needed.
Check your task status in Task History in PiAPI to get more details about task status.
More Generation Case for Reference
<video src=""https://static.piapi.ai/n8n-instruction/motion-illustration/example2.mp4"" controls />"
WordPress Content Automation Machine with HUMAN-IN-THE-LOOP & DEEP RESEARCH,https://n8n.io/workflows/3725-wordpress-content-automation-machine-with-human-in-the-loop-and-deep-research/,"HUMAN-IN-THE-LOOP Content Automation Pro with Deep Research & Airtable for Power Users - Collaborative AI for WordPress Blog Automation
The BEST n8n Content Automation workflow in the market is getting even better, now enabling you to have full control over the entire process. It‚Äôs time to unlock the power of true collaborative creation with GenAI!
Want to leverage AI automation but don't want to give up full control over the whole content creation process? Get the best of both worlds - get ‚ÄúHuman-in-the-Loop‚Äù with this supreme n8n workflow template!
This template blends the power of AI with the crucial oversight of human expertise and intervention, all managed through a central, streamlined Airtable interface. Say goodbye to generic content and hello to high-quality, deeply researched articles published directly to your WordPress site with full control.
Check out my Youtube channel for a full walkthrough.



What problem is this workflow solving? / Use cases
Fully automated content generation tools often lack the nuance, accuracy, and strategic control necessary for truly impactful content. This workflow solves that critical gap by integrating powerful AI capabilities into a structured, human-supervised content creation process managed via a simple interface. It automates the most repetitive and time-consuming tasks ‚Äì like research gathering, chapter ideations, domain copywriting ‚Äì while strategically embedding essential human review points at critical junctures.
Instead of simply providing a topic and receiving a finished article of uncertain quality, you actively guide the process step-by-step within an interface. You review, refine, and approve AI suggestions for chapters and content before finalizing and publishing. This approach makes it perfect for creating high-quality, deeply researched, and brand-aligned blog posts for WordPress more efficiently, without ever sacrificing essential editorial control.
Who is this for?
This workflow template is meticulously designed for individuals and teams deeply involved in content creation who seek a powerful blend of AI-driven efficiency and essential human-controlled quality assurance:
Content Creators & Bloggers: Who want to leverage AI for drafting and research but maintain full editorial control.
Marketing Teams: Looking to scale content production efficiently while ensuring brand voice and quality consistency.
SEO Specialists: Needing to produce well-researched, structured, and SEO-optimized content regularly.
Agencies: Managing content creation for multiple clients with a structured, repeatable process.
Anyone who prefers a ""human-in-the-loop"" approach over fully automated ""black box"" content generators.
How this workflow works
This template automates significant portions of the WordPress content creation process, utilizing Airtable as the central hub for management and user interaction, while leveraging AI for research and copywriting. The system is composed of four interconnected n8n flows, each handling a distinct stage of the content lifecycle. In addition, there is a sub-workflow dedicated to the research tool (Perplexity).
Content Creation Process in Airtable
The entire journey from idea to published post is managed within your Airtable base, offering transparency and control. Progress through each tab sequentially to complete the content creation process and produce a high-quality article that fulfills your requirements.
Configure Settings (‚ÄúSettings‚Äù Table): Define global parameters (website details, target audience, writing style, category IDs, about us, CTA etc.) in the ‚ÄúSettings‚Äù table. These serve as default inputs for the AI.
Create Topics (‚ÄúCreate Topics‚Äú Table): Add a record to the ‚ÄúCreate Topics‚Äù table for each new blog post idea. Mark the topic as ready by setting its 'Status' field to ‚ÄúTo Do‚Äù and ticking the 'Execute Flow' checkbox to initiate the first flow.
Generate Chapters (‚ÄúGenerate Chapters‚Äú Table): The first flow calls to the sub-workflow to do online research and generates potential chapter outlines based on your topic and settings, saving them to the ‚ÄúGenerate Chapters‚Äù table.
Select Chapters to Write (‚ÄúSelect Chapters‚Äú Table): Review the suggested chapters in the ‚ÄúSelect Chapters‚Äú Table. Select the chapters you want to proceed with by setting their 'Status' to ‚ÄúTo Do‚Äù and ticking the 'Execute Flow' checkbox.
Generate Content for selected Chapters (‚ÄúGenerate Content‚Äú Table): The second flow calls to the sub-workflow to research each selected chapter and generates draft content, saving it to the ‚ÄúGenerate Content‚Äù table, where you have the opportunity to review and modify if necessary.
Select Chapter Content (‚ÄúSelect Content‚Äú Table): Review and edit the AI-generated text directly in Airtable. Approve the content for each chapter. Once all necessary content is approved, trigger the third flow for the final assembly by setting the status to ‚ÄúTo Do‚Äù and ticking the 'Execute Flow' checkbox.
Finalize Post (‚ÄúFinalize Post‚Äú Table): The third flow aggregates the approved content, generates the title, SEO metadata, tags, chapter images and featured image, placing all compiled information in the ‚ÄúFinalize Post‚Äù table for your final review. When the final article is ready to be published on your website, mark the status as ""To Do"" and check the ""Ready to Publish"" box to activate the fourth and final flow.
Publish & Backup Post (‚ÄúBackup Post‚Äú Table): the fourth flow publishes the post to the website and creates a backup record in the ‚ÄúBackup Post‚Äù table, archiving key details including the live URL of the published article.
n8n Workflow Functions (JSON Files)
The automation is handled by four separate n8n flows, each triggered by specific changes in Airtable:
Flow 1 (Topic Initiation & Chapter Generation):
Trigger: Monitors the ‚ÄúCreate Topics‚Äù table; activates when a record's 'Status' is ""To Do"" AND 'Execute Flow' checkbox is ticked.
Action: Fetches settings from Settings. Performs initial research (e.g., via Perplexity). Uses AI to generate chapter outlines. Saves suggestions as new records in the ‚ÄúGenerate Chapters‚Äù table.
Flow 2 (Content Generation & Internal Linking):
Trigger: Monitors the ‚ÄúSelect Chapters‚Äù table; activates when a record's 'Status' is ""To Do"" AND 'Execute Flow' checkbox is ticked.
Action: Fetches chapter details and settings. Gathers internal links (e.g., by parsing sitemap). Performs online/ deep research per chapter. Uses AI to write draft content, incorporating research and links. Saves drafts into corresponding records in the ‚ÄúGenerate Content‚Äù table.
Flow 3 (Post Assembly & Image Generation):
Trigger: Monitors the ‚ÄúSelect Content‚Äù table; activates when all content for a topic is approved and final assembly is triggered when a record's 'Status' is ""To Do"" AND 'Execute Flow' checkbox is ticked.
Action: Fetches all content from the ‚ÄúGenerate Content‚Äù table. Aggregates text. Uses AI for title, SEO metadata, tags, and image prompts. Generates chapter images and a featured cover image (e.g., via AI image model DALL-E). Converts content to HTML. Saves compiled data and image info into the ‚ÄúFinalize Post‚Äù table.
Flow 4 (WordPress Publishing & Backup):
Trigger: Monitors the ‚ÄúFinalize Post‚Äù table; activates when a record is marked Status' is ""To Do"" AND ‚ÄòPost to Website‚Äô checkbox is ticked.
Action: Fetches finalized data. Uploads images to WordPress. Creates WordPress post (status set to ‚Äúpublished‚Äù by default) with content, metadata, tags, and image. Creates a record in the ‚ÄúBackup Post‚Äù table, with a link to the post on the website.
This modular approach ensures each stage is handled distinctly, orchestrated via updates on Airtable interface.
Unique Features
Human-in-the-Loop Design: Guarantees quality and relevance through mandatory user review and approval stages within Airtable for chapters, content, and the final post.
Collaborative AI Partnership: Leverages AI as a powerful assistant for research, structuring, and drafting, significantly boosting productivity while relying on human expertise for refinement and final judgment.
Airtable as Interface & Control Center: Utilizes Airtable as an intuitive interface and central database to manage the entire content lifecycle, offering clear progress tracking and facilitating team collaboration.
Deep Research Integration: Incorporates external research tools (like Perplexity) to ensure content is based on fresh, up-to-date information, crucial for accuracy and authority.
Modular n8n Structure: Simplifies complexity by dividing the process into four manageable, interconnected flows plus one sub-flows, making the system easier to understand, troubleshoot, and customize.
Rate-limit Aware: Includes built-in Wait nodes and logic designed to mitigate API rate limit issues from AI and research tools, enhancing workflow reliability during intensive use.
Multiple-Image Generation: Generates and adds images for each chapter, as well as a featured image for the article.
Direct WordPress Integration: Streamlines publication by automatically sending finalized, formatted content, metadata, tags, and the featured image directly to your WordPress site.
Internal Linking: Embeds internal website links strategically within each chapter and throughout the article, boosting SEO and enhancing user navigation.
Integrated Backup: Provides data security by automatically archiving key post details in Airtable and optionally saving text and image files to Google Drive.
Future Features
‚ÄúDeeper Research‚Äù: Plans for multi-level research capabilities to enable the creation of even more comprehensive reports or e-book length content.
Content Sample: Provide content samples for the AI to learn from and mimic in terms of writing style and guidelines.
Category-Specific Styling: Aiming to allow users to define styles per category for the AI to learn and imitate, ensuring greater brand consistency.
Advanced Link Insertion: Developing more sophisticated strategies for inserting relevant internal and external links, including automatic source citation and dynamic internal link density adjustments.
Requirements
Airtable Account: create a Airtable account for free.
WordPress Website: Requires an Application Password with permissions to create posts and upload media. Must have the REST API enabled.
OpenAI API Key: Or a key from another compatible AI provider for text and image generation (DALL-E is the default for images).
Perplexity API Key: Or an API key for your chosen online research tool.
Google Drive: for the optional backup feature to Google Drive.
Setup Step-by-Step
Import n8n Workflows: Download the .json workflow files and import them into your n8n instance. There is one main/ ‚Äúmonth‚Äù workflow and one sub/ ‚Äúchild‚Äù workflow. Name the workflows to your needs.
Get Airtable Base: Duplicate the companion Airtable Base template.
Configure Credentials: Add new credentials in n8n for Airtable, WordPress (use Application Password), OpenAI API, PerplexityAI API, and Google (for Drive backup).
Connect Workflow Nodes: Open each workflow in n8n and assign the correct credential to every node that requires authentication (Airtable, WordPress, AI, HTTP Request, Google nodes).
Link the main workflow to the sub-workflow: Access the Research Tool nodes and establish a connection between the main workflow to sub-workflow.
Configure Airtable Nodes: Update all Airtable nodes across all workflows. Verify that all 'Table' and 'Field' names in the nodes exactly match your duplicated base.
Populate Airtable Settings: Fill in your specific details in the Settings table of your Airtable base (Website URL, Audience, Style, Category IDs, CTA, etc.).
Test Connections: Run manual executions on key nodes within each flow to ensure connections and basic configurations are correct before activating.
Activate Workflow: Toggle the ""Active"" switch ON for the main/ ‚Äúmother‚Äù workflow in your n8n list. No need to activate the sub/ ‚Äúchild‚Äù workflow.
Start Creating: Go to the Create Topics table in Airtable, add a new topic, go through the process and get amazed by AI magic!
Airtable Database Explanation
Airtable is the central hub, acting as the database, user interface, and trigger mechanism for this workflow.
Structure: The base uses linked tables to manage the process:
Settings: Global configurations.
Create Topics: Initiates new content pieces.
Generate Chapters: Stores AI-generated chapter outlines for review.
Select Chapters: Facilitates user selection of chapters to write.
Generate Content: Stores AI-generated draft content for selected chapters.
Select Content: Manages user approval of drafted content before final assembly.
Finalize Post: Holds the compiled post (text, metadata, image info) for final review and publish trigger.
Backup Post: Archives data (including live URL) of successfully published posts.
Workflow Interaction:
User updates trigger fields in Airtable (e.g., sets Status='To Do' & Execute Flow=true).
n8n Airtable Trigger nodes detect these specific changes.
The corresponding n8n flow runs, fetching data from Airtable.
AI and other tools process data and generate outputs.
Results are written back to the appropriate Airtable tables/fields.
User reviews results in Airtable and triggers the next stage via status AND checkbox updates.
Setup: Using the exact duplicated Airtable base template is mandatory, as n8n workflows expect specific table and field names.
Tips for Pros
AI Prompts: Modify prompts in the AI nodes to refine tone, style, format, or incorporate specific instructions.
AI Models: the workflow is optimized for OpenAI's GPT-4o model due to its consistent performance. While you can replace the AI models in the nodes with alternatives (if you have the credentials for them), keep in mind that the workflow may not function.
AI Research Model: PerplexityAI is the tool of choice for research. The ""sonar"" model is the default for research in this workflow due to its speed and cost-effectiveness. However, you are welcome to explore Perplexity's other models, such as the ""sonar-deep-research"" model, which is specifically designed for in-depth research.
AI Image Model: The default AI model for image generation is OpenAI‚Äôs Dall-E. However, the outputs of this model are not impressive. While OpenAI's 4o multimodal is expected to be available via API soon, better/ superior image quality can be achieved in the meantime by using alternative AI image models (such as FLUX.1).
Airtable Base: Add or modify fields/tables to match your specific tracking needs, but remember to update corresponding n8n nodes accordingly.
3rd-party Integrations: Replace research tools, image generators, or modify the final step to publish to a different CMS or add post-publication steps like social sharing.
Error Handling: For production reliability, incorporate an ‚ÄúError Handling‚Äù workflow in n8n for this automation to catch failures and send notifications
Important Considerations
API Rate Limits: AI and research tools have usage limits. High volume processing might cause errors. The included Wait nodes help, but may need adjustment based on your API plans.
Testing Costs: API calls to AI models aren't free. To ensure the workflow runs correctly, start testing with OpenAI's GPT-4o. Once you've confirmed functionality, explore using other AI models which may be cheaper to keep costs down. Be aware that the workflow may not be as reliable with models other than GPT-4o. Keep an eye on your API provider's billing dashboard to monitor usage and costs.
Sequential Processing: The workflow processes one topic fully at a time to ensure stability and manage API limits. For fully automated, parallel processing from a list, consider the alternative template: ‚Äú[n8n] Content Automation Pro ‚Äì with DEEP RESEARCH ‚Äì WordPress Blog Automation‚Äù."
Automated Lead Generation & Contact Enrichment with Hunter.io and Perplexity AI,https://n8n.io/workflows/3616-automated-lead-generation-and-contact-enrichment-with-hunterio-and-perplexity-ai/,"Who is this for?
This template is ideal for B2B founders, solopreneurs, growth marketers, SDRs, or anyone looking to scale their lead generation and enrichment with no-code tools to low-code tools.
Whether you're building your first lead pipeline or upgrading a manual spreadsheet, LeadAIgen automates the entire discovery ‚Üí enrichment ‚Üí logging process using AI and verified email data.
What problem does this solve?
Lead sourcing and enrichment are time-consuming, inconsistent, and often require multiple tools or manual scraping.
This workflow solves that by chaining AI + contact discovery, validating everything, and storing clean, structured data in Google Sheets ‚Äî ready for outreach or CRM sync.
What this workflow does:
‚úÖ Accepts an industry/topic prompt via chat trigger or Telegram,
ü§ñ Uses OpenRouter (Perplexity) to find 15 relevant companies with domains,
üì§ Parses the results, filters duplicates based on your Google Sheet,
üì© Enriches each domain with up to 3 real personal emails using Hunter.io,
üí° If no emails found ‚Üí Fallback AI tries to locate general email info,
üìä Updates your Google Sheet with all valid enriched leads (status: ‚ÄúEnriched‚Äù),
üîÅ Runs in safe batches of 10 to protect your API limits,
üõ†Ô∏è Includes logic to clean data, validate JSON, and slice leads.
Setup:
Create Google Sheets, Hunter.io, and OpenRouter credentials in your n8n instance
Import the JSON workflow
Set your OpenRouter model (default = Perplexity Sonar Large)
Connect your Google Sheet (must contain a header row with Domain, Status, etc.)
Paste your Hunter API key into the Hunter.io node
(Optional) Add Telegram Bot trigger to chat with your lead generator
üóí Color-coded workflow notes:
To make customization easier, this template includes Innovatio's signature visual sticky note system inside the n8n canvas.
You‚Äôll find:
üü© Green Notes ‚Üí Main Steps
The core logic blocks ‚Äî from company generation to AI fallback and Google Sheets update.
üü¶ Blue Notes ‚Üí Personalization Tips
How to adapt filters, prompts, and data mappings for your business needs.
üü® Yellow Notes ‚Üí Optional / Advanced
Expand with Telegram chat triggers, Airtable integration, CRM push, or lead scoring logic.
üü´ Gray Notes ‚Üí Welcome, Outro & Upgrade Suggestions
Clear guidance and CTAs if you want help scaling this further.
üëâ Each node includes comments or setup notes so you can learn the system as you use it.
How to customize this workflow?
üéØ Swap Google Sheets for Airtable for relational views and tagging,
ü§ñ Replace Perplexity with GPT-4, Claude, or Mixtral inside OpenRouter,
üì¨ Auto-send enriched leads to Gmail, Slack, or your CRM,
üó£Ô∏è Connect this to Telegram for on-the-go company generation,
üîÅ Want dynamic lead scoring or auto-categorization? We can help ‚Äî velebit@innovatio.design
Final notes
This template was designed by Velebit from Innovatio.
External links (e.g. documentation or support email) lead only to official sources with no affiliate tracking or paid placements.
A separate license applies to the paid version on Gumroad, which includes commercial use rights, extended fallback logic, and advanced upgrade tips."
"Indeed Data Scraper & Summarization with Airtable, Bright Data & Google Gemini",https://n8n.io/workflows/3703-indeed-data-scraper-and-summarization-with-airtable-bright-data-and-google-gemini/,"Who this is for?
Indeed Data Scraper & Summarization with Airtable, Bright Data and Google Gemini is an automated workflow that extracts company profile information from Indeed using Bright Data Web Unlocker, transforms the data using Google Gemini's LLM, and forward the transformed response with the summary to a specified webhook for downstream use.
This workflow is tailored for:
Recruiters and HR teams who want quick summaries of companies listed on Indeed.
Market researchers and analysts needing structured insights into businesses.
Founders, investors, and consultants scouting potential competitors, partners, or clients.
No-code enthusiasts looking to automate data extraction and enrichment pipelines without manual scraping or parsing.
What problem is this workflow solving?
Manually gathering structured information about companies on Indeed is time-consuming and inconsistent. Pages vary in structure, and extracting clean, digestible summaries can require technical scraping expertise.
This workflow automates:
Extracting company data from Indeed reliably using Bright Data Web Unlocker.
Cleaning and summarizing the extracted content using Google Gemini LLM.
Storing structured insights directly into Airtable for easy access and further workflows.
Eliminates manual research, saves hours, and produces AI-enhanced, easily searchable records.
What this workflow does
Triggers on-demand.
Pulls company page URLs from Airtable.
Scrapes content from each Indeed company profile using Bright Data Web Unlocker.
Sends the raw HTML to Google Gemini for extraction and summarization.
Sends the summarized data to other platforms via a Webhook notification mechanism.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials for Bright Data.

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the Airtable Personal Access Token account under Credentials.
Update the Webhook Notifier with the Webhook endpoint of your choice.
How to customize this workflow to your needs
This workflow is built to be flexible - whether you're a company or a market researcher, entrepreneur, or data analyst. Here's how you can adapt it to fit your specific use case:
Extend the scraper: Modify Bright Data targets to pull job listings, salaries, or employee reviews via the Airtable data source.
Customize the summary prompt: Ask Gemini to extract different attributes hiring trends, practices etc.
Routing the output to different destinations:
Send summaries or transformed response to Google Sheets, Airtable, or CRMs like HubSpot or Salesforce etc."
Extract Amazon Best Seller Electronic Info with Bright Data and Google Gemini,https://n8n.io/workflows/3681-extract-amazon-best-seller-electronic-info-with-bright-data-and-google-gemini/,"Who this is for?
Extract Amazon Best Seller Electronic Info is an automated workflow that extracts best seller data from Amazon's Electronics section using Bright Data Web Unlocker, transform it into structured JSON using Google Gemini's LLM, and forwards a fully structured JSON response to a specified webhook for downstream use.
This workflow is tailored for:
eCommerce Analysts
Who need to monitor Amazon best-seller trends in the Electronics category and track changes in real-time or on a schedule.
Product Intelligence Teams
Who want structured insights on competitor offerings, including rankings, prices, ratings, and promotions.
AI-powered Chatbot Developers
Who are building assistants capable of answering product-related queries with fresh, structured data from Amazon.
Growth Hackers & Marketers
Looking to automate competitive research and surface trending product data to inform pricing strategies.
Data Aggregators and Price Trackers
Who need reliable and smart scraping of Amazon data enriched with AI-driven parsing.
What problem is this workflow solving?
Keeping up with Amazon's best sellers in Electronics is a time-consuming, error-prone task when done manually.This workflow automates the process, ensuring:
Automating Data Extraction from Amazon Best Sellers using Bright Data, ensuring reliable access to real-time, structured data.
Enhancing Raw Data with Google Gemini, turning product lists into structured JSON using the Google Gemini LLM.
Sending Results to a Webhook, enabling seamless integration into dashboards, databases, or chatbots.
What this workflow does
The workflow performs the following steps:
Extracts Amazon Best Seller Electronics page info using Bright Data's Web Unlocker API.
Processes the unstructured content using Google Gemini's Flash Exp model to extract structured product data.
Sends the structured information to a webhook endpoint.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
Update the Amazon URL with the Bright Data zone by navigating to the Amazon URL with the Bright Data Zone node.
Update the Webhook HTTP Request node with the Webhook endpoint of your choice.
How to customize this workflow to your needs
This workflow is built to be flexible - whether you're a market researcher, e-commerce entrepreneur, or data analyst. Here's how you can adapt it to fit your specific use case:
Change the Amazon Category
Update the Amazon URL with the topic of your interest such as Computers & Accessories, Home Audio, etc.
Customize the Gemini Prompt
Update the Gemini prompt to get different styles of output ‚Äî comparison tables, summaries, feature highlights, etc.
Send Output to Other Destinations
Replace the Webhook URL to forward output to:
Google Sheets
Airtable
Slack or Discord
Custom API endpoints"
Google Calendar MCP server for AI Agent with Dynamic Scheduling,https://n8n.io/workflows/3677-google-calendar-mcp-server-for-ai-agent-with-dynamic-scheduling/,"Google Calendar AI Agent with Dynamic Scheduling
Version: 1.0.0
n8n Version: 1.88.0+
Author: Koresolucoes
License: MIT
Description
An AI-powered workflow to automate Google Calendar operations using dynamic parameters and MCP (Model Control Plane) integration. Enables event creation, availability checks, updates, and deletions with timezone-aware scheduling [[1]][[2]][[8]].
Key Features:
üìÖ Full Calendar CRUD: Create, read, update, and delete events in Google Calendar.
‚è∞ Availability Checks: Verify time slots using AVALIABILITY_CALENDAR node with timezone support (e.g., America/Sao_Paulo).
ü§ñ AI-Driven Parameters: Use $fromAI() to inject dynamic values like Start_Time, End_Time, and Description [[3]][[4]].
üîó MCP Integration: Connects to an MCP server for centralized AI agent control [[5]][[6]].
Use Cases
Automated Scheduling: Book appointments based on AI-recommended time slots.
Meeting Coordination: Sync calendar events with CRM/task management systems.
Resource Management: Check room/equipment availability before event creation.
Instructions
1. Import Template
Go to n8n > Templates > Import from File and upload this workflow.
2. Configure Credentials
Add Google Calendar OAuth2 credentials under Settings > Credentials.
Ensure the calendar ID matches your target (e.g., ODONTOLOGIA group calendar).
3. Set Up Dynamic Parameters
Use $fromAI('Parameter_Name') in nodes like CREATE_CALENDAR to inject AI-generated values (e.g., event descriptions).
4. Activate & Test
Enable the workflow and send test requests to the webhook path /mcp/:tool/calendar.
Tags
Google Calendar Automation MCP AI Agent Scheduling CRUD
Screenshots
License
This template is licensed under the MIT License.
Notes:
Extend multi-tenancy by adding :userId to the webhook path (e.g., /mcp/:userId/calendar) [[7]].
For timezone accuracy, always specify options.timezone in availability checks [[8]].
Refer to n8n‚Äôs Google Calendar docs for advanced field mappings."
Automated Real Estate Property Lead Scoring with BatchData,https://n8n.io/workflows/3664-automated-real-estate-property-lead-scoring-with-batchdata/,"How It Works
This workflow automates the real estate lead qualification process by leveraging property data from BatchData. The automation follows these steps:
When a new lead is received through your CRM webhook, the workflow captures their address information
It then makes an API call to BatchData to retrieve comprehensive property details
A sophisticated scoring algorithm evaluates the lead based on property characteristics like:
Property value (higher values earn more points)
Square footage (larger properties score higher)
Property age (newer constructions score higher)
Investment status (non-owner occupied properties earn bonus points)
Lot size (larger lots receive additional score)
Leads are automatically classified into categories (high-value, qualified, potential, or unqualified)
The workflow updates your CRM with enriched property data and qualification scores
High-value leads trigger immediate follow-up tasks for your team
Notifications are sent to your preferred channel (Slack in this example)
The entire process happens within seconds of receiving a new lead, ensuring your sales team can prioritize the most valuable opportunities immediately..
Who It's For
This workflow is perfect for:
Real estate agents and brokers looking to prioritize high-value property leads
Mortgage lenders who need to qualify borrowers based on property assets
Home service providers (renovators, contractors, solar installers) targeting specific property types
Property investors seeking specific investment opportunities
Real estate marketers who want to segment audiences by property value
Home insurance agents qualifying leads based on property characteristics
Any business that bases lead qualification on property details will benefit from this automated qualification system.
About BatchData
BatchData is a comprehensive property data provider that offers detailed information about residential and commercial properties across the United States. Their API provides:
Property valuation and estimates
Ownership information
Property characteristics (size, age, bedrooms, bathrooms)
Tax assessment data
Transaction history
Occupancy status (owner-occupied vs. investment)
Lot details and dimensions
By integrating BatchData with your lead management process, you can automatically verify and enrich leads with accurate property information, enabling more intelligent lead scoring and routing based on actual property characteristics rather than just contact information.
This workflow demonstrates how to leverage BatchData's property API to transform your lead qualification process from manual research into an automated, data-driven system that ensures high-value leads receive immediate attention."
Find High-Intent Sales Leads by Scraping Glassdoor with Bright Data & GPT,https://n8n.io/workflows/3607-find-high-intent-sales-leads-by-scraping-glassdoor-with-bright-data-and-gpt/,"üîç Scrape Glassdoor with Bright Data
Designed for sales teams, recruiters, and marketers aiming to automate job discovery and prospecting.
This workflow scrapes Glassdoor job listings using Bright Data and automatically generates targeted pitches using AI, streamlining lead identification and outreach.
üß© How It Works
This automation leverages n8n, Bright Data, Google Sheets, and OpenAI:
1. Trigger
Starts with a custom form input (Location, Keyword, Country).
2. Bright Data Job Scrape
Triggers a Bright Data dataset snapshot via HTTP Request.
Polls snapshot progress using a Wait node, ensuring data readiness.
Retrieves full job listings dataset once ready.
3. Google Sheets Integration
Writes detailed job data (company, role, location, overview, metrics) into a Google Sheet.
Uses a pre-built template for organized data storage.
4. Automated Pitch Generation (AI)
Splits listings into actionable parts: company name, title, and description.
Sends data to OpenAI (via LangChain) to generate relevant pitches or icebreakers.
Saves generated content back into the same sheet for easy access.
‚úÖ Requirements
Ensure you have the following:
Google Sheets
Google account
Template Sheet with columns for job details and AI-generated pitches
Bright Data
Active account with Dataset API access
API key and dataset ID
OpenAI
Valid OpenAI API key for GPT models
n8n Environment
Nodes: HTTP Request, Wait, If, Google Sheets, Split Out, LangChain (OpenAI)
Credentials:
Google Sheets OAuth2
Bright Data API credentials
OpenAI API key
‚öôÔ∏è Setup Instructions
Step 1: Prepare Google Sheets
Copy the provided Google Sheets template
Do not change headers
Step 2: Import & Configure Workflow in n8n
Import the workflow JSON file
Set Google Sheets node:
Link to your copied sheet
Confirm correct tab name
Step 3: Configure Bright Data
Replace &lt;YOUR_BRIGHT_DATA_API_KEY&gt; with your real key
Set your dataset ID in all HTTP Request nodes
Step 4: Configure OpenAI (LangChain)
Connect OpenAI API key to the LangChain node
Customize prompt to match tone and outreach style
Step 5: Testing & Scheduling
Test via manual form trigger
Schedule runs or leave form enabled for on-demand use
üß† Tips & Best Practices
Use specific keywords and locations for better results
Adjust polling intervals based on dataset size
Refine AI prompts regularly to improve pitch quality
Clean unused columns from your sheet to boost performance
üí¨ Support & Feedback
For help or customization:
üìß Email: Yaron@nofluff.online
üì∫ YouTube: @YaronBeen
üîó LinkedIn: linkedin.com/in/yaronbeen
üìö Bright Data Docs: docs.brightdata.com/introduction"
üßë‚Äçü¶ØImprove your Website Accessibility with GPT-4o and Google Sheet,https://n8n.io/workflows/3640-improve-your-website-accessibility-with-gpt-4o-and-google-sheet/,"Tags: Accessibility, SEO, Blogging, Marketing, Automation, AI, Web Auditing
Context
Hey! I‚Äôm Samir, a Supply Chain Engineer and Data Scientist from Paris, and the founder of LogiGreen Consulting.
In my personal blog, I share insights on how to use AI, automation, and data analytics to improve logistics, operations, and digital sustainability practices.
Have you heard about accessibility?
In this workflow, I use n8n to improve the quality of alternative texts for images on my personal website.
üì¨ For business inquiries, you can connect with me on LinkedIn
Who is this template for?
This workflow is for:
Bloggers and website owners who want to improve accessibility
SEO professionals looking to boost page performance
Web developers and product teams automating web audits
What does it do?
This n8n workflow:
üîç Downloads the HTML of a blog or web page
üñºÔ∏è Extracts all &lt;img&gt; tags and their alt attributes
üìâ Detects missing or too-short alt texts
ü§ñ Sends those images to GPT-4o (with vision) to generate new alt descriptions
üìÑ Saves the results into a Google Sheet, updating the alt text when needed
How it works
Set a page URL using the Set node
Download HTML content
Extract image src and alt using a Code node
Store results in a Google Sheet
Filter images with altLength &lt; 50
Send image URL to GPT-4o
Update the Google Sheet with the newly generated newAlt text
The AI alt texts are concise, descriptive, and accessibility-compliant.
What do I need to get started?
You‚Äôll need:
A Google Sheet to store the audit results
An OpenAI account with GPT-4o access
Follow the Guide!
Follow the sticky notes in the workflow or check my tutorial to configure each node and start using AI to improve the accessibility of your website.
üé• Watch My Tutorial
Notes
GPT-generated alt texts are limited to ~125‚Äì150 characters for best results
Use this to comply with WCAG and improve Google indexing
Easily adapt it to audit multiple domains or e-commerce catalogues
This workflow was built using n8n version 1.85.4
Submitted: April 21, 2025"
Build your own Qdrant Vector Store MCP server,https://n8n.io/workflows/3636-build-your-own-qdrant-vector-store-mcp-server/,"This n8n demonstrates how to build your own Qdrant MCP server to extend its functionality beyond that of the official implementation.
This n8n implementation exposes other cool API features from Qdrant such as facet search, grouped search and recommendations APIs. With this, we can build an easily customisable and maintainable Qdrant MCP server for business intelligence.
This MCP example is based off an official MCP reference implementation which can be found here - https://github.com/qdrant/mcp-server-qdrant
How it works
A MCP server trigger is used and connected to 5 custom workflow tools. We're using custom workflow tools as there is quite a few nodes required for each task.
We use a mix of n8n supported Qdrant nodes for simple operations such as insert documents and similarity search, and HTTP node to hit the Qdrant API directly for Facet search, group search and recommendations.
We use ""Edit Field"" and ""Aggregate"" nodes to return suitable responses to the MCP client.
How to use
This Qdrant MCP server allows any compatible MCP client to manage a Qdrant Collection by supporting select and create operations. You will need to have a collection available before you can use this server. Use the Prerequisite manual steps to get started!
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""Can you help me list the available companies in the collection?""
""What do customers say about product deliveries from company X?""
""What do customers of company X and company Y say about product ease of use?""
Requirements
Qdrant for vector store. This can be an a cloud-hosted instance or one you can self-host internally.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
Depending on what queries you'll receive, adjust the tool inputs to make it easier for the agent to set the right parameters.
Not interested in Reviews? The techniques shared in this template can be used for other types of collections.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
"Travel Planning Assistant with MongoDB Atlas, Gemini LLM and Vector Search",https://n8n.io/workflows/3577-travel-planning-assistant-with-mongodb-atlas-gemini-llm-and-vector-search/,"Building agentic AI workflows often requires multiple moving parts: memory management, document retrieval, vector similarity, and orchestration.
Until now, these pieces had to be custom-wired.
But with the new native n8n nodes for MongoDB Atlas, we reduce that overhead dramatically.
With just a few clicks:
Store and recall long-term memory from MongoDB
Query vector embeddings stored in Atlas Vector Search
Use these results in your LLM chains and automation logic
In this example we present an ingestion and AI Agent flows that focus around Travel Planning. The different interest points that we want the agent to know about can be ingested into the vector store.
The AI Agent will use the vector store tool to get relevant context about those points of interest if it needs to.
Prerequisites
MongoDB Atlas project and Cluster
OpenAI Valid API Key for embeddings (can be other provider)
Gemini API Key for the LLM (can be other provider)
How it works:
There are 2 main flows.
One is ingesting flow:
Gets a document from a webhook and use MongoDB Vector Atlas to embed the document title and description into points_of_interest collection.
Embeddings are stored in a field named embedding
Embeddings used are OpenAI's but it can be any type of supported embedders.
Second flow is an AI Agent node with Chat Memory Stored in MongoDB Atlas and a Vector Search node as a tool:
Chat Message Trigger: Chatting with the AI Agent will trigger the conversation store in the MongoDB Chat Memory node.
When data is necessary like a location search or details it will go to the ""Vector Search"" tool.
Vector Search Tool - uses Atlas Vector Search index created on the points_of_interest collection:
// index name : ""vector_index""
// If you change an embedding provider make sure the numDimensions correspond to the model.
{
  ""fields"": [
    {
      ""type"": ""vector"",
      ""path"": ""embedding"",
      ""numDimensions"": 1536,
      ""similarity"": ""cosine""
    }
  ]
}
Additional Resources
MongoDB Atlas Vector Search
n8n Atlas Vector Search docs"
Scrape Indeed Job Listings for Hiring Signals Using Bright Data and LLMs,https://n8n.io/workflows/3601-scrape-indeed-job-listings-for-hiring-signals-using-bright-data-and-llms/,"Scrape Indeed Job Listings for Hiring Signals Using Bright Data and LLMs
How the flow runs
Fill the form with job position you're hunting for.
Bright data's scraper will scrape Indeed based on your requirments.
Workflow waits for the snapshot.
Data returns as JSON.
Jobs append to Google Sheets.
Each row goes to an LLM to analyze if you're a good fit for the job (based on your prompts).
The LLMswrites YES or NO next to each job opportunity, helping you find job posts that are relevant to you.
What you need
Google Sheets with our template.
Bright Data dataset and API key.
OpenAI key for GPT‚Äë4o mini (or any other LLM).
n8n with required nodes.
Form fields To Fill
Job Location ‚Äì city or region.
Keyword ‚Äì role or skills.
Country ‚Äì two‚Äëletter code.
Setup steps
Copy the sheet template link.
Import the JSON workflow.
Add your credentials in nodes.
Test the form manually.
Add a schedule if desired.
Bright Data filter example
[
  {
    ""country"": ""US"",
    ""domain"": ""indeed.com"",
    ""keyword_search"": ""Growth Marketer"",
    ""location"": ""Miami"",
    ""date_posted"": ""Last 24 hours""
  }
]

**Tips**
-Choose Last 24 hours often.

-Increase wait time for big snapshots.

-Narrow keywords to save credits.


**Need help?
**Email me anytime: 
Yaron@nofluff.online
YouTube: @YaronBeen
- LinkedIn: https://www.linkedin.com/in/yaronbeen/
=======================================
Bright Data Docs: https://docs.brightdata.com/introduction"
LINE Chatbot with Google Sheets Memory and Gemini AI,https://n8n.io/workflows/3600-line-chatbot-with-google-sheets-memory-and-gemini-ai/,"Main Use Case
This workflow enables automated, AI-assisted replies to users messaging a LINE Official Account, while storing and referencing chat history from Google Sheets to maintain context. Ideal for businesses or support teams that want to provide smart, personalized customer interactions using AI with memory.
How It Works (Step-by-Step)
Connect to LINE Official Account's API
A Webhook listens for incoming messages from users on LINE.
When a message is received, it triggers the workflow.
Prepare the Data
An Edit Fields module structures incoming data (e.g. extracts user ID, message content).
This ensures data is clean and usable downstream.
Retrieve Chat History
The user‚Äôs previous conversations are fetched from a Google Sheet.
This ensures the AI has memory and can continue conversations contextually.
Prepare Prompt
The retrieved chat history is combined with the new message to form a complete prompt for the AI.
Example format: ‚ÄúUser previously said X. Now they said Y. How should we respond?‚Äù
AI Agent: Google Gemini
The formatted prompt is passed to an AI Agent (Google Gemini Chat Model).
The AI generates a response based on the message + history.
Tools used: Chat ModeMemory, ToolOutputParser for accurate replies.
Split & Clean History
The conversation history is split into smaller chunks for cleaning and storage.
This ensures the Google Sheet remains readable and manageable over time.
Save Chat History
The cleaned new message and AI reply are saved to Google Sheets.
This updates the chat history for future context.
Send Reply to LINE
The AI-generated reply is sent back to the user via a POST HTTP Request to the LINE Messaging API.
How to Set Up
Prerequisites:
LINE Official Account
Google Sheet to store chat history
Google Gemini API or AI agent with context memory
Automation platform (e.g., n8n, as this seems visually similar)
Step-by-Step:
Create a Webhook on LINE:
Set the webhook URL to your automation service.
Enable webhook events.
Design Your Google Sheet:
Create a sheet with columns: User ID, Timestamp, Message, AI Reply.
Set Up Modules in Automation Platform:
Webhook: receives user messages.
Edit Fields: extract user ID and message.
Google Sheets Read: fetch message history.
Prompt Composer: format prompt using past history + new message.
AI Agent: connect to Google Gemini for smart replies.
Split & Clean: clean and chunk history if needed.
Google Sheets Write: save the updated conversation.
HTTP Request: send reply to LINE via Messaging API.
Test Your Workflow:
Send a message from LINE.
Watch the full loop: receive ‚Üí process ‚Üí AI ‚Üí store ‚Üí reply.
Deploy & Monitor:
Ensure error handling is in place (e.g., for blank messages or failed API calls).
Regularly check your Google Sheets for storage limits. (If limits reached, you can increase the history row.)
üì¶ Benefits
Maintains context in conversations
Personalized, AI-driven responses
Easy history tracking via Google Sheets
Fully automated and scalable"
Log meal nutrients from Telegram to Google Sheets using an AI agent,https://n8n.io/workflows/3599-log-meal-nutrients-from-telegram-to-google-sheets-using-an-ai-agent/,"Who is this for?
This workflow is ideal for individuals focused on nutrition tracking, meal planning, or diet optimization‚Äîwhether you‚Äôre a health-conscious individual, fitness coach, or developer working on a healthtech app. It also fits well for anyone who wants to capture their meal data via voice or text, without manually entering everything into a spreadsheet.
What problem is this workflow solving?
Manually logging meals and breaking down their nutritional content is time-consuming and often skipped. This workflow automates that process using Telegram for input, OpenAI for natural language understanding, and Google Sheets for structured tracking. It enables users to record meals by typing or sending voice messages, which are transcribed, analyzed for nutrients, and automatically stored for tracking and review.
What this workflow does
This n8n automation lets users send either a text or voice message to a Telegram bot describing their meal. The workflow then:
Receives the Telegram message
Checks if it‚Äôs a voice message
‚Ä¢ If yes: Downloads the audio file and transcribes it using OpenAI
‚Ä¢ If no: Uses the text input directly
Sends the meal description to OpenAI to extract a structured list of ingredients and nutritional details
Parses and stores the results in Google Sheets
Responds via Telegram with a personalized confirmation message
A testing interface also allows you to simulate prompts and view structured outputs for development or debugging.
Setup
Create a Telegram bot via BotFather and note the API token.
Create an empty Google Sheet and store the sheet ID in the environment.
Set up your OpenAI credentials in the n8n credential manager.
Customize the ‚ÄúList of Ingredients and Nutrients‚Äù node with your prompt if needed.
(Optional) Use the ‚ÄúTesting‚Äù section to simulate messages and refine outputs before going live.
How to customize this workflow to your needs
‚Ä¢ Enhance prompts in the OpenAI node to improve the structure and accuracy of responses.
‚Ä¢ Add new fields in the Google Sheet and corresponding logic in the parser if you want more detail.
‚Ä¢ Adjust the Telegram response to provide motivational feedback, dietary tips, or summaries.
‚Ä¢ Upgrade to the ‚ÄúPro‚Äù version mentioned in the contact section for USDA database integration and complete nutrient breakdowns.
This is a lightweight, AI-powered meal logging automation that transforms voice or text into actionable nutrition data‚Äîperfect for making healthy eating easier and more data-driven.
See my other workflows here"
Build Lists of Profiles from Any Platform using Airtop and Google Sheets,https://n8n.io/workflows/3479-build-lists-of-profiles-from-any-platform-using-airtop-and-google-sheets/,"About The List Building Automation
This automation will guide you on how to automate list building using Airtop. You‚Äôll have a streamlined workflow that can reduce your research time by up to 90% while improving the accuracy of your target lists.
How to automate list building
It can be challenging to spend too much time on tasks like compiling lists of potential investors, customers, job candidates, industry influencers, or key decision-makers. Verifying contact details often requires significant effort, whether building an outreach list, tracking thought leaders, or researching potential customers. Not anymore. With Airtop's List Building Automation, turn hours of tedious research into clean, reliable and accurate lists, built in just minutes.
Check this out:
What You'll Need
A free Airtop API Key
Target audience parameters (persona and which network. i.e. ""AI Influencers on LinkedIn)
Make a copy of this template to start
Understanding the Process
This automation leverages Airtop's advanced data processing capabilities powered by AI to scan multiple unstructured sources and compile accurate, targeted lists based on your specific requirements. The magic lies in its ability to understand context and verify information across different platforms.
This workflow:
Handles multi-source data collection and consolidation
Manages automatic verification of social profiles and domains
Automates the filtering and ranking of results based on relevance
Setting Up Your Automation
Enter your search criteria in the ""Parameters"" node:
Who: Your target audience (e.g., ""Angel investors in Europe,"" ""Top AI influencers"")
Where: The platform or domain to focus on (e.g., ""LinkedIn,"" ""TikTok"")
Configure your Airtop API Key
Create one for free at the Airtop Portal
In the last node, select the spreadsheet that you copied earlier
Run the workflow
Customization Options
While our template works out of the box, you might want to customize it for your specific needs:
Add custom filtering criteria for more targeted results
Implement automatic data enrichment from additional sources
Set up automatic exports to your preferred CRM or database
Real-World Applications
Here's how businesses can use this automation:
A VC firm could use this automation to build a comprehensive EU angel investors database. What previously required their analysts to work 15 hours per week now runs automatically in the background, providing fresh leads daily.
A PR agency could automate its influencer discovery process across multiple platforms, reducing its research time from 10 hours to 30 minutes per client while increasing the relevance of its outreach lists.
Best Practices
To get the most out of this automation:
Start with specific, well-defined parameters to ensure relevant results
Regularly update your parameters to keep your lists fresh and relevant
Combine multiple runs with different parameters for comprehensive coverage
What's Next?
Now that you've automated your list building, you might be interested in:
Setting up automated outreach sequences
Creating dynamic lead scoring systems
Implementing automatic list updating and maintenance
Happy automating!"
Create Daily Israeli Economic Newsletter using RSS and GPT-4o,https://n8n.io/workflows/3564-create-daily-israeli-economic-newsletter-using-rss-and-gpt-4o/,"Daily Economic News Brief for Israel (Hebrew, RTL, GPT-4o)
Overview
Stay ahead of the curve with this AI-powered workflow that delivers a daily economic summary tailored for professionals tracking the Israeli economy.
At 8:00 PM Israel Time, this workflow:
Retrieves the latest articles from Calcalist and Mako via RSS
Filters duplicates and irrelevant stories
Uses OpenAI‚Äôs GPT-4o to identify the 5 most important stories of the day
Summarizes each article in concise, readable Hebrew
Generates a fully styled, responsive HTML email (with proper RTL layout)
Sends it to your inbox using your preferred SMTP email provider
Perfect for economists, analysts, investors, or policymakers who want an actionable and personalized news digest -- no distractions, no fluff.
Setup Instructions
Estimated setup time:
10 minutes
Required credentials:
OpenAI API Key
SMTP credentials (for email delivery)
Steps:
Import this template into your n8n instance.
Add your OpenAI API Key under credentials.
Configure the SMTP Email node with:
Host (e.g. smtp.gmail.com)
Port (465 or 587)
Username (your email)
Password (app-specific password or login)
Set your target email address in the last node.
(Optional) Customize the GPT prompt to adjust tone or audience (e.g. general public, policy makers).
Activate the workflow and receive daily updates straight to your inbox.
Customization Tips
Change the RSS sources to pull from other Hebrew or international news websites
Modify the summarization prompt to fit different sectors (e.g. tech, health, politics)
Add integrations like Notion, Airtable, or Telegram for logging or distribution
Apply your branding to the HTML output (logos, footer, colors)
Why Use This?
This is more than a news digest. It‚Äôs an intelligent economic assistant that filters noise, highlights what matters, and keeps you informed-automatically.
You can set it up in 10 minutes and benefit every single day."
"Convert YouTube Videos into SEO Blog Posts with GPT-4o, Dumpling AI, and Flux",https://n8n.io/workflows/3531-convert-youtube-videos-into-seo-blog-posts-with-gpt-4o-dumpling-ai-and-flux/,"Workflow Description
This workflow helps content creators automatically repurpose YouTube videos into SEO-friendly blog posts. It extracts the video transcript, uses AI to generate a full blog post with a relevant image, and sends the complete package via email, ready for publication.
Prerequisites/Requirements
This workflow relies on external AI services. You will need:
OpenAI Account: Used for generating the blog post text (specifically mentioned using GPT-4o in the workflow notes).
Credentials: Requires an API key from OpenAI.
Cost: OpenAI API usage is typically paid based on the amount of text processed (tokens). Check OpenAI's current pricing.
Setup: Sign up at OpenAI and obtain your API key.
Dumpling AI Account: Used for retrieving YouTube video transcript and generating the blog post image.
Credentials: Requires an API key from Dumpling AI.
Cost: Dumpling AI offers 250 free credits to start with and different plans for different levels of usage. Check the pricing page for more details.
Setup: Sign up at Dumpling AI and obtain your API key/credentials.
Email Account: Credentials for the email service (e.g., Gmail) used to send the final result.
How it works
Input Video Details: You provide the YouTube video URL and your email address.
Get Transcript: The workflow fetches the transcript of the specified YouTube video.
Generate Content: An AI model crafts a blog post (title, description, body) based on the transcript.
Create Image: Another AI model generates a suitable image for the blog post.
Format & Package: The blog post is converted to HTML, and the image is prepared for sending.
Email Result: The final HTML blog post and image are emailed to you.
Set up steps
Configure Variables: Enter the specific YouTube video URL and the recipient email address in the ""Set Variables"" node.
Connect Credentials: Add your credentials for the services used (e.g., OpenAI for text generation, Dumpling AI for YouTube Transcript and AI image generation service).
Connect Email Credentials: Authenticate your Gmail account (or chosen email provider) to allow the workflow to send the email.
Take it to the next level
Direct Publishing: Instead of emailing the result, connect directly to your CMS (like WordPress, Ghost, Webflow) to automatically create a draft or publish the blog post.
AI Agent Integration: Replace the single ""Generate Blog Post"" step with an AI Agent for more sophisticated content generation, potentially researching topics or structuring the post section by section based on the transcript.
Social Media Snippets: Add steps to generate companion social media posts (e.g., for Twitter, LinkedIn) summarizing the blog post.
Batch Processing: Modify the trigger to read multiple YouTube URLs from a spreadsheet or database to convert videos in bulk.
Enhanced SEO: Refine the AI prompts to specifically target keywords or incorporate SEO best practices more deeply into the generated content.
Multiple Image Options: Generate several image variations and include them in the email or draft post for selection."
Conversing with Data: Transforming Text into SQL Queries and Visual Curves,https://n8n.io/workflows/3497-conversing-with-data-transforming-text-into-sql-queries-and-visual-curves/,"Conversational Data Retrieval and Visualization Workflow
This workflow enables users to interact with a PostgreSQL database using natural language. It translates text inputs into SQL queries, retrieves the corresponding data, and generates visualizations using QuickChart, facilitating seamless data analysis without manual query writing.
Table of Contents
Pre-conditions and Requirements
Database Schema Setup
Step-by-Step Workflow Explanation
Customization Guide
Pre-conditions and Requirements
1. API Keys and Services Required
To operate this workflow, access to the following services is necessary:
DeepSeek API: For converting natural language into SQL queries.
API Key: Obtain from your DeepSeek account.
QuickChart: For generating data visualizations.
Service URL: https://quickchart.io/chart
2. n8n Instance Setup
n8n Installation: Install and run n8n using the Official Guide.
Credential Configuration:
DeepSeek API: Set up DeepSeek credentials in n8n by adding your API key.
PostgreSQL Database:
Local Database Access: If your PostgreSQL database is hosted locally and needs to be accessed over the internet (e.g., by n8n running on a different machine or in the cloud), you can expose it using ngrok:
Install ngrok: Download and install ngrok from ngrok.com.
Start ngrok Tunnel: Run the command ngrok tcp 5432 to expose your local PostgreSQL server.
This will provide a forwarding address like tcp://0.tcp.ngrok.io:12345 that can be used to connect to your local database remotely.
Update n8n Credentials: In n8n, configure the PostgreSQL node to use the ngrok forwarding address, ensuring remote access to your local database.
Database Schema Setup
Before initiating the workflow, ensure that the database schema is extracted and saved:
Extract Schema: Retrieve the database schema, including table names and column details.
Save Schema: Store the extracted schema in a JSON file for reference during query generation.
Step-by-Step Workflow Explanation
User Input Handling
The workflow begins by receiving a natural language query from the user.
Schema Retrieval
Loads the previously saved database schema from the JSON file.
AI-Based SQL Generation
Combines the user's query with the database schema.
Utilizes the DeepSeek API to translate the natural language query into a SQL statement.
SQL Query Execution
Executes the generated SQL query against the PostgreSQL database.
Retrieves the data corresponding to the query.
Data Visualization
Formats the retrieved data into a structure compatible with QuickChart.
Sends the data to QuickChart to generate a visual representation.
Example: To create a bar chart, construct a URL with the chart configuration:
https://quickchart.io/chart?c={type:'bar',data:{labels:['Label1','Label2'],datasets:[{label:'Dataset1',data:[10,20]}]}}
This URL returns an image of the chart.
Response Delivery
Presents the generated visualizations and data insights to the user.
Customization Guide
Modifying the AI Model
Alternative AI Services: Replace DeepSeek with other AI models by adjusting the API call configurations in the workflow.
Changing Visualization Services
Visualization Tools: Swap QuickChart with other visualization services by modifying the data processing and visualization steps.
Expanding Database Support
Additional Databases: Adapt the workflow to support other databases (e.g., MySQL, MongoDB) by configuring the respective database credentials and query execution nodes.
This workflow streamlines the process of data retrieval and visualization, allowing users to interact with their database using natural language, thereby enhancing accessibility and efficiency in data analysis."
Get Exchange & Sentiment Insights with CoinMarketCap AI Agent,https://n8n.io/workflows/3423-get-exchange-and-sentiment-insights-with-coinmarketcap-ai-agent/,"Analyze exchange data, market indexes, and community sentiment from CoinMarketCap‚Äîpowered by AI.
This sub-agent provides access to exchange listings, token holdings, metadata, and high-level metrics like the CMC 100 Index and the Fear & Greed Index. It‚Äôs designed for use within your larger CoinMarketCap AI Analyst system or as a standalone workflow.
This agent can be triggered by a supervisor or manually used with message and sessionId inputs.
Supported Tools (5 Total)
üîç Exchange Map
Get CoinMarketCap IDs, names, and slugs for exchanges (used as lookup before deeper queries).
üßæ Exchange Info
Metadata including launch date, social links, country, and operational status.
üí∞ Exchange Assets
Token balances, wallet addresses, and total USD value held by a specific exchange.
üìà CoinMarketCap 100 Index
Constituents and weights of the CMC 100 Index, updated live.
üò± Fear & Greed Index
Market sentiment score updated daily, ranging from Extreme Fear to Extreme Greed.
What You Can Do with This Agent
üîπ Map exchanges to retrieve their ID and slug
üîπ Analyze exchange holdings by token and blockchain
üîπ Pull metadata for major CEXs like Binance or Coinbase
üîπ Compare global sentiment using the Fear & Greed Index
üîπ Access index data to understand CMC‚Äôs top 100 crypto asset breakdown
Example Queries You Can Use
‚úÖ ""What is the latest Fear and Greed Index reading?""
‚úÖ ""Get a list of all exchanges on CoinMarketCap.""
‚úÖ ""What tokens are held by Binance?""
‚úÖ ""Retrieve metadata for Coinbase.""
‚úÖ ""Show me the top assets in the CMC 100 Index.""
Agent Architecture
AI Brain: GPT-4o-mini
Memory: Window buffer memory using sessionId
Tools: 5 API-connected nodes
Trigger: External input via message and sessionId
Setup Instructions
Get a CoinMarketCap API Key
Apply here: https://coinmarketcap.com/api/
Configure n8n Credentials
Use HTTP Header Auth to store your CoinMarketCap API key.
Optional: Trigger from a Supervisor
Connect to a parent agent using Execute Workflow with message and sessionId inputs.
Test Sample Prompts
‚ÄúGet all exchanges‚Äù, ‚ÄúFetch CMC index‚Äù, ‚ÄúShow Binance token holdings‚Äù
Sticky Notes Included
Exchange & Community Guide ‚Äì Explains agent purpose and component connections
Usage & Examples ‚Äì Walkthrough for sample use cases
Error Handling & Licensing ‚Äì Includes API error code reference and licensing details
‚úÖ Final Notes
This agent is part of a broader CoinMarketCap AI Analyst System. Visit my Creator profile to download all available sub-agents and supervisor flows.
Understand exchange behavior and community sentiment‚Äîautomated with AI and CoinMarketCap."
Transcribe audio files with Google Gemini and Telegram,https://n8n.io/workflows/3388-transcribe-audio-files-with-google-gemini-and-telegram/,"Transcribe audio messages from Telegram using Google Gemini for free.
Send an audio file to your Telegram bot and get a full transcription using Gemini‚Äôs free model.
Who is this template for?
Anyone who needs quick and accurate transcriptions of audio messages.
Perfect for:
Creators and podcasters
Coaches or educators
People who receive a lot of audio on Telegram
You can also adapt this workflow to transcribe audio from other sources, like WhatsApp, Google Drive, or direct uploads.
How it works
You send an audio file to your Telegram bot
n8n downloads the file and converts it to text using Google Gemini
The transcript is sent back to you on Telegram
How to set up
You‚Äôll just need:
A Telegram bot token
A Google Gemini API key (free tier supported)
The rest works out by itself.
Check out my other templates
üëâ https://n8n.io/creators/solomon/"
Automate Audio/Video Transcription in Any Language with the New ElevenLabs Model,https://n8n.io/workflows/3105-automate-audiovideo-transcription-in-any-language-with-the-new-elevenlabs-model/,"How it works üó£Ô∏è> üìñ
I set up this workflow to convert any audio or video file into structured text using the new ElevenLabs Scribe model, one of the best Speech-to-Text AIs, available in 99+ languages. This workflow integrates seamlessly with n8n and leverages the ElevenLabs Scribe API to:
This workflow seamlessly integrates with n8n to:
‚úÖ Upload audio/video files automatically
‚úÖ Transcribe them with industry-leading accuracy in any language
‚úÖ Export the text for further processing (summaries, subtitles, SEO content, etc.)
üëâ Try the new ElevenLabs Scribe model now: Convert speech to text instantly
Business Cases
üîπ Podcast Transcriptions ‚Äì Convert podcast episodes into blog posts for SEO and accessibility
üîπ YouTube Subtitles ‚Äì Generate captions automatically for increased engagement
üîπ Legal & Compliance ‚Äì Accurately transcribe meetings, interviews, or customer calls
üîπ E-learning ‚Äì Turn lectures and webinars into structured course notes
üîπ SEO & Content Marketing ‚Äì Repurpose videos into articles, quotes, and social media content
üí° Boost your productivity with the new Scribe model ‚Üí Start with ElevenLabs Scribe
Set up steps
üöÄ Quick & simple setup in n8n ‚Äì Upload your file, select the model (scribe_v1), and let the AI handle the rest via the ElevenLabs API.
‚∏ª
üì¢ Why I Chose the New ElevenLabs Scribe Model?
I wanted the most accurate and reliable transcription tool for my workflow. After testing different options, Scribe outperformed Google Gemini & OpenAI Whisper in independent benchmarks. It delivers high-quality transcriptions, even in underserved languages like Serbian, Mongolian, and many more.
‚úÖ Transcribes in 99+ languages
‚úÖ Fast, accurate, and easy to integrate
‚úÖ Suitable for content creators, businesses, and professionals
üîó Get started now and revolutionize your workflow with the new Scribe model ‚Üí Try Scribe AI today üöÄ
Phil | Inforeole"
"Create Product Satisfaction Surveys with Telegram, Google Sheets and AI",https://n8n.io/workflows/3115-create-product-satisfaction-surveys-with-telegram-google-sheets-and-ai/,"This n8n template uses a Telegram chatbot to conduct a Product Satisfaction Survey and fetches questions and stores answers in a Google sheet. It augments an AI Agent to ask follow-up questions to engage the user and uncover more insights in their responses.
This template is intended to demonstrate how you'd realistically approach a workflow where there is structured conversation (static questions) but you still want to include an free-form element (follow-up questions) which can only be accomplished via AI.
Check out an example Survey results: https://docs.google.com/spreadsheets/d/e/2PACX-1vQWcREg75CzbZd8loVI12s-DzSTj3NE_02cOCpAh7umj0urazzYCfzPpYvvh7jqICWZteDTALzBO46i/pubhtml?gid=0&single=true
How it works
A chat session is started with the user who needs to enter the bot command ""/next"" to start the survey.
Once started, the template pulls in questions from a google sheet to ask the user. Questions are asked in sequence from left column to right column.
When the user answers the question, a text classifier node is used to determine
if a follow-up question could be asked.
If so, a mini conversation is initiated by the AI agent to get more details.
If not, the survey proceeds to the next question.
All answers and mini-conversations are recorded in the Google Sheet under the respective question.
When all questions are answered, the template will stop the survey and give the user a chance to restart.
How to use
You'll need to setup a Telegram bot (see docs)
Create a google sheet with an ID column. Populate the rest of the columns with your survey questions (see sample)
Ensure you have a Redis instance to capture state. Either self-host or sign-up to Upstash for a free account.
Update the ""Set Variable"" node with your google sheet ID and survey title.
Share your bot to allow others to participate in your survey.
Requirements
Telegram for Chatbot
Google Sheets for Survey questions and answers
Redis for State Management and Chat Memory
Community+ license and above for Execution data node - you can remove this node if you don't have this licence.
Customising this workflow
Not using Telegram? This template technically works with other chat apps such as Whatsapp, wechat and even n8n's hosted chat!
This state management pattern can also be applied to other use-cases and scenarios. Try it for other types of surveys!"
Automatically Create YouTube Metadata with AI,https://n8n.io/workflows/2976-automatically-create-youtube-metadata-with-ai/,"This n8n workflow automates YouTube video metadata generation using AI. It extracts video transcripts, analyzes content, and produces optimized titles, descriptions, tags, hashtags, and call-to-action elements. Additionally, the workflow integrates affiliate and promotional links to enhance overall video performance.
Key Features
Automated Metadata Generation
Utilizes an AI agent integrated with OpenAI GPT-4 to generate engaging metadata based on the provided video transcript.
SEO and Engagement Optimization
Creates keyword-rich, well-structured content that boosts search engine visibility and audience engagement.
Affiliate and Promotional Integration
Retrieves pre-set promotional and affiliate links using a Google Docs integration.
Direct YouTube Update
Automatically updates video details on YouTube via the YouTube API.
Customization
Allows you to modify the AI prompt to tailor metadata for your specific niche.
Workflow Breakdown
User Submission
Users supply the YouTube video link, transcript, and optionally, focus keywords.
Video ID Extraction
The workflow converts the YouTube URL into a video ID to streamline automation.
Link Retrieval
Affiliate and course links are fetched from a designated Google Docs file.
AI-Powered Metadata Generation
The AI agent generates the video title, description, tags, hashtags, and call-to-action elements.
Metadata Formatting and Update
The generated metadata is structured and directly updated on YouTube.
Confirmation
A success message is displayed upon completion of the update process.
Setup and Configuration
Deploying the Workflow
Deploy the workflow in n8n and ensure all integrations are properly set up.
Configuring Integrations
Google Docs: Configure credentials to retrieve affiliate and promotional links.
OpenAI (GPT-4): Set up credentials for AI-powered metadata generation.
YouTube API: Enter your API credentials to enable automatic video updates.
User Input Requirements
Provide a valid YouTube video link and its corresponding transcript. Optionally, include focus keywords to further enhance metadata accuracy.
Ideal For
YouTube Content Creators: Automate video descriptions and boost SEO.
Digital Marketers: Enhance content for improved search rankings and audience engagement.
Affiliate Marketers: Simplify the insertion of promotional and affiliate links.
AI & Automation Enthusiasts: Explore the integration of AI into automated workflows.
Additional Resources
For further guidance, refer to the tutorial video on this workflow. More courses and resources are available on the SyncBricks website. For support or inquiries, contact Amjid Ali at info@syncbricks.com. You can also support this work via PayPal donations and subscribe for additional AI and automation workflows.
Watch the Tutorial: YouTube Video on This Workflow
More Courses & Resources:
SyncBricks LMS
Full Course on ERPNext & AI Automation
Connect:
Email: info@syncbricks.com
Website: SyncBricks
YouTube: SyncBricks Channel
LinkedIn: Amjid Ali
Support & Subscribe:
Donate via PayPal
Subscribe for More AI & Automation Workflows"
RAG:Context-Aware Chunking | Google Drive to Pinecone via OpenRouter & Gemini,https://n8n.io/workflows/2871-ragcontext-aware-chunking-or-google-drive-to-pinecone-via-openrouter-and-gemini/,"Workflow based on the following article.
https://www.anthropic.com/news/contextual-retrieval
This n8n automation is designed to extract, process, and store content from documents into a Pinecone vector store using context-based chunking. The workflow enhances retrieval accuracy in RAG (Retrieval-Augmented Generation) setups by ensuring each chunk retains meaningful context.
Workflow Breakdown:
üîπ Google Drive - Retrieve Document:
The automation starts by fetching a source document from Google Drive. This document contains structured content, with predefined boundary markers for easy segmentation.
üîπ Extract Text Content - Once retrieved, the document‚Äôs text is extracted for processing. Special section boundary markers are used to divide the text into logical sections.
üîπ Code Node - Create Context-Based Chunks:
A custom code node processes the extracted text, identifying section boundaries and splitting the document into meaningful chunks. Each chunk is structured to retain its context within the entire document.
üîπ Loop Node - Process Each Chunk:
The workflow loops through each chunk, ensuring they are processed individually while maintaining a connection to the overall document context.
üîπ Agent Node - Generate Context for Each Chunk:
We use an Agent node powered by OpenAI‚Äôs GPT-4.0-mini via OpenRouter to generate contextual metadata for each chunk, ensuring better retrieval accuracy.
üîπ Prepend Context to Chunks & Create Embeddings - The generated context is prepended to the original chunk, creating context-rich embeddings that improve searchability.
üîπ Google Gemini - Text Embeddings:
The processed text is passed through Google Gemini text-embedding-004, which converts the text into semantic vector representations.
üîπ Pinecone Vector Store - Store Embeddings:
The final embeddings, along with the enriched chunk content and metadata, are stored in Pinecone, making them easily retrievable for RAG-based AI applications.
Use Case:
This automation enhances RAG retrieval by ensuring each chunk is contextually aware of the entire document, leading to more accurate AI responses. It‚Äôs perfect for applications that require semantic search, AI-powered knowledge management, or intelligent document retrieval.
By implementing context-based chunking, this workflow ensures that LLMs retrieve the most relevant data, improving response quality and accuracy in AI-driven applications."
"Simple Expense Tracker with n8n Chat, AI Agent and Google Sheets",https://n8n.io/workflows/2819-simple-expense-tracker-with-n8n-chat-ai-agent-and-google-sheets/,"Use Case
It is very convenient to add expenses via simple chat message. This workflow attempts to do exactly this using AI-powered n8n magic!

Send message to a chat, something like ""car wash; 59.3 usd; 25 jan 2024""
And get a response:
Your expense saved, here is the output of save sub-workflow:{""cost"":59.3,""descr"":""car wash"",""date"":""2024-01-25"",""msg"":""car wash; 59.3 usd; 25 jan 2024""}
LLM will smartly parse your message to structured JSON and save the expense as a new row into Google Sheet!
Installation
1. Set up Google Sheets:
Clone this Sheet:
https://docs.google.com/spreadsheets/d/1D0r3tun7LF7Ypb21CmbTKEtn76WE-kaHvBCM5NdgiPU/edit?gid=0#gid=0
(File -> Make a copy)
Choose this sheet into ""Save expense into Google Sheets"" node.
2. Fix sub-workflow dropdown:
open ""Parse msg and save to Sheets"" node (which is an n8n sub-workflow executor tool) and make sure the SAME workflow is chosen in the dropdown. it will allow n8n to locate and call ""Workflow Input Trigger"" properly when needed.
3. Activate the workflow to make chat work properly.
Sent message to chat, something like ""car wash; 59.3 usd; 25 jan 2024""
you should get a response:
Your expense saved, here is the output of save sub-workflow:{""cost"":59.3,""descr"":""car wash"",""date"":""2024-01-25"",""msg"":""car wash; 59.3 usd; 25 jan 2024""}
and new row in Google sheets should be inserted!"
"Scrape Trustpilot Reviews with DeepSeek, Analyze Sentiment with OpenAI",https://n8n.io/workflows/2792-scrape-trustpilot-reviews-with-deepseek-analyze-sentiment-with-openai/,"Workflow Overview
This workflow automates the process of scraping Trustpilot reviews, extracting key details, analyzing sentiment, and saving the results to Google Sheets. It uses OpenAI for sentiment analysis and HTML parsing for review extraction.
How It Works
1. Scrape Trustpilot Reviews
HTTP Request:
Fetches review pages from Trustpilot (https://it.trustpilot.com/review/{{company_id}}).
Paginates through pages (up to max_page limit).
HTML Parsing:
Extracts review URLs using CSS selectors
Splits the URLs into individual review links.
2. Extract Review Details
Information Extractor:
Uses DeepSeek to extract structured data from the review:
Author: Name of the reviewer.
Rating: Numeric rating (1-5).
Date: Review date in YYYY-MM-DD format.
Title: Review title.
Text: Full review text.
Total Reviews: Number of reviews by the user.
Country: Reviewer‚Äôs country (2-letter code).
3. Sentiment Analysis
Sentiment Analysis Node:
Uses OpenAI to classify the review text as Positive, Neutral, or Negative.
Example output:
{  
  ""category"": ""Positive"",  
  ""confidence"": 0.95  
}  
4. Save to Google Sheets
Google Sheets Node:
Appends or updates the extracted data to a Google Sheet
Set Up Steps
1. Configure Trustpilot Scraping
Edit Fields1 Node:
Set company_id to the Trustpilot company name
Set max_page to limit the number of pages scraped.
2. Configure Google Sheets
Google Sheets Node:
Update the documentId with your Google Sheet ID
Ensure the sheet has the required columns (Id, Data, Nome, etc.).
3. Configure OpenAI
OpenAI Chat Model Node:
Add your OpenAI API key.
Sentiment Analysis Node:
Ensure the categories match your desired sentiment labels (Positive, Neutral, Negative).
Key Components
Nodes:
HTTP Request/HTML: Scrape and parse Trustpilot reviews.
Information Extractor: Extract structured review data using DeepSeek.
Sentiment Analysis: Classify review sentiment.
Google Sheets: Save and update review data.
Credentials:
OpenAI API key.
DeepSeek API key.
Google Sheets OAuth2."
"Zoom AI Meeting Assistant creates mail summary, ClickUp tasks and follow-up call",https://n8n.io/workflows/2800-zoom-ai-meeting-assistant-creates-mail-summary-clickup-tasks-and-follow-up-call/,"Update 19-04-2025
Change from OpenAI to Claude 3.7 Sonnet module
Adding the Think Tool
The update enables significantly better results to be achieved. This is particularly noticeable during longer meetings!
What this workflow does
This workflow retrieves the Zoom meeting data from the last 24 hours. The transcript of the last meeting is then retrieved, processed, a summary is created using AI and sent to all participants by email.
AI is then used to create tasks and follow-up appointments based on the content of the meeting.
Important: You need a Zoom Workspace Pro account and must have activated Cloud Recording/Transcripts!
This workflow has the following sequence:
manual trigger (Can be replaced by a scheduled trigger or a webhook)
retrieval of of Zoom meeting data
filter the events of the last 24 hours
retrieval of transcripts and extract of the text
creating a meeting summary, format to html and send per mail
create tasks and follow-up call (if discussed in the meeting) in ClickUp/Outlook (can be replaced by Gmail, Airtable, and so forth) via sub workflow
Requirements:
Zoom Workspace (via API and HTTP Request): Documentation
Microsoft Outlook: Documentation
ClickUp: Documentation
AI API access (e.g. via OpenAI, Anthropic, Google or Ollama)
SMTP access data (for sending the mail)
You must set up the individual sub-workflows as separate workflows. Then set the ‚ÄúExecute workflow trigger‚Äù here. Then select the corresponding sub-workflow in the AI Agent Tools.
You can select the number of domains yourself. If the data queries are not required, simply delete the corresponding tool (e.g. ‚ÄúAnalytics_Domain_5).
Feel free to contact me via LinkedIn, if you have any questions!"
Personal Shopper Chatbot for WooCommerce with RAG using Google Drive and openAI,https://n8n.io/workflows/2784-personal-shopper-chatbot-for-woocommerce-with-rag-using-google-drive-and-openai/,"This workflow combines OpenAI, Retrieval-Augmented Generation (RAG), and WooCommerce to create an intelligent personal shopping assistant. It handles two scenarios:
Product Search: Extracts user intent (keywords, price ranges, SKUs) and fetches matching products from WooCommerce.
General Inquiries: Answers store-related questions (e.g., opening hours, policies) using RAG and documents stored in Google Drive.
How It Works
1. Chat Interaction & Intent Detection
Chat Trigger:
Starts when a user sends a message (""When chat message received"").
Information Extractor:
Uses OpenAI to analyze the message and determine if the user is searching for a product or asking a general question.
Extracts:
search (true/false).
keyword, priceRange, SKU, category (if product-related).
Example:
{  
  ""search"": true,  
  ""keyword"": ""red handbags"",  
  ""priceRange"": { ""min"": 50, ""max"": 100 },  
  ""SKU"": ""BAG123"",  
  ""category"": ""women's accessories""  
}  
2. Product Search (WooCommerce Integration)
AI Agent:
If search: true, routes the request to the personal_shopper tool.
WooCommerce Node:
Queries the WooCommerce store using extracted parameters (keyword, priceRange, SKU).
Filters products in stock (stockStatus: ""instock"").
Returns matching products (e.g., ""red handbags under ‚Ç¨100"").
3. General Inquiries (RAG System)
RAG Tool:
If search: false, uses the Qdrant Vector Store to retrieve store information from documents.
Google Drive Integration:
Documents (e.g., store policies, FAQs) are stored in Google Drive.
Downloaded, split into chunks, and embedded into Qdrant for semantic search.
OpenAI Chat Model: Generates answers based on retrieved documents (e.g., ""Our store opens at 9 AM"").
Set Up Steps
1. Configure the RAG System
Google Drive Setup:
Upload store documents .
Update the Google Drive2 node with your folder ID.
Qdrant Vector Database:
Clean the collection (update Qdrant Vector Store node with your URL).
Use Embeddings OpenAI to convert documents into vectors.
2. Configure OpenAI & WooCommerce
OpenAI Credentials:
Add your API key to all OpenAI nodes (OpenAI Chat Model, Embeddings OpenAI, etc.).
WooCommerce Integration:
Connect your WooCommerce store (credentials in the personal_shopper node).
Ensure product data is synced and accessible.
3. Customize the AI Agent
Intent Detection:
Modify the Information Extractor‚Äôs system prompt to align with your store‚Äôs terminology.
RAG Responses:
Update the tool description to reflect your store‚Äôs documents.
Notes
This template is ideal for e-commerce businesses needing a hybrid assistant for product discovery and customer support.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Basic Automatic Gmail Email Labelling with OpenAI and Gmail API,https://n8n.io/workflows/2740-basic-automatic-gmail-email-labelling-with-openai-and-gmail-api/,"Description
This workflow automates email categorization in Gmail using the Gmail API and OpenAI's language model. It periodically checks for new emails, reads their content, and categorizes them based on existing Gmail labels. If no matching label is found, the workflow creates a new label and assigns it to the email.
Key Features
Polling for Emails: The workflow triggers every 5 minutes to check for new emails using the Gmail Trigger node.
Reading Labels: Existing Gmail labels are fetched to determine the most relevant match for email categorization.
Dynamic Labeling: If no existing label matches, a new label is created dynamically based on the email's content.
OpenAI Integration: The workflow uses OpenAI's Chat model to analyze email content and suggest or create appropriate labels.
Email Categorization: Labels are applied to emails, ensuring they are organized in Gmail's structure. The workflow also removes less relevant emails (e.g., ads) from the inbox.
Nodes in Use
Gmail Trigger: Polls Gmail every 5 minutes for new emails.
Gmail - Read Labels: Fetches all existing Gmail labels.
Gmail - Get Message: Retrieves the full content of a specific email.
Gmail - Add Label to Message: Assigns a chosen label to the email.
Gmail - Create Label: Creates a new label if necessary.
OpenAI Chat Model: Analyzes email content for categorization.
Memory Buffer: Retains context for email analysis across multiple iterations.
Wait Node: Adds a buffer period to manage email processing.
Prerequisites
Gmail API Setup: Ensure Gmail OAuth2 credentials are configured in n8n.
OpenAI API Key: Configure OpenAI credentials for email analysis.
Labeling Standards: Maintain a consistent Gmail label structure for better organization.
Instructions
Add your Gmail API credentials to the Gmail nodes.
Add your OpenAI API credentials to the OpenAI Chat Model node.
Activate the workflow. It will start polling for new emails every 5 minutes.
Monitor and refine the categorization logic if necessary to ensure alignment with Gmail's organizational needs.
Use Case
Ideal for individuals or teams handling high email volumes who want to maintain an organized inbox and automate repetitive categorization tasks.
Note: You can improve the prompt to get better results from the agent by giving it more personal rules on how to categorize."
AI Agent to chat with Airtable and analyze data,https://n8n.io/workflows/2700-ai-agent-to-chat-with-airtable-and-analyze-data/,"Video Guide
I prepared a detailed guide that shows the entire process of building an AI agent that integrates with Airtable data in n8n. This template covers everything from data preparation to advanced configurations.
Youtube Link
Who is this for?
This workflow is designed for developers, data analysts, and business owners who want to create an AI-powered conversational agent integrated with Airtable datasets. It is particularly useful for users looking to enhance data interaction through chat interfaces.
What problem does this workflow solve?
Engaging with data stored in Airtable often requires manual navigation and time-consuming searches. This workflow allows users to interact conversationally with their datasets, retrieving essential information quickly while minimizing the need for complex queries.
What this workflow does
This workflow enables an AI agent to facilitate chat interactions over Airtable data. The agent can:
Retrieve order records, product details, and other relevant data.
Execute mathematical functions to analyze data such as calculating averages and totals.
Optionally generate maps for geographic data visualization.
Dynamic Data Retrieval: The agent uses user prompts to dynamically query the dataset.
Memory Management: It retains context during conversations, allowing users to engage in a more natural dialogue.
Search and Filter Capabilities: Users can perform tailored searches with specific parameters or filters to refine their results.
Set up steps
Separate workflows:
Create additional workflow and move there Workflow 2.
Replace credentials:
Replace connections and credentials in all nodes.
Start chat:
Ask questions and don't forget to mention required base name."
AI Agent for realtime insights on meetings,https://n8n.io/workflows/2651-ai-agent-for-realtime-insights-on-meetings/,"Video Guide
I prepared a detailed guide explaining how to build an AI-powered meeting assistant that provides real-time transcription and insights during virtual meetings.
Youtube Link
Who is this for?
This workflow is ideal for business professionals, project managers, and team leaders who require effective transcription of meetings for improved documentation and note-taking. It's particularly beneficial for those who conduct frequent virtual meetings across various platforms like Zoom and Google Meet.
What problem does this workflow solve?
Transcribing meetings manually can be tedious and prone to error. This workflow automates the transcription process in real-time, ensuring that key discussions and decisions are accurately captured and easily accessible for later review, thus enhancing productivity and clarity in communications.
What this workflow does
The workflow employs an AI-powered assistant to join virtual meetings and capture discussions through real-time transcription. Key functionalities include:
Automatic joining of meetings on platforms like Zoom, Google Meet, and others with the ability to provide real-time transcription.
Integration with transcription APIs (e.g., AssemblyAI) to deliver seamless and accurate capture of dialogue.
Structuring and storing transcriptions efficiently in a database for easy retrieval and analysis.
Real-Time Transcription: The assistant captures audio during meetings and transcribes it in real-time, allowing participants to focus on discussions.
Keyword Recognition: Key phrases can trigger specific actions, such as noting important points or making prompts to the assistant.
Structured Data Management: The assistant maintains a database of transcriptions linked to meeting details for organized storage and quick access later.
Setup
Preparation
Create Recall.ai API key
Setup Supabase account and table
create table
  public.data (
    id uuid not null default gen_random_uuid (),
    date_created timestamp with time zone not null default (now() at time zone 'utc'::text),
    input jsonb null,
    output jsonb null,
    constraint data_pkey primary key (id),
  ) tablespace pg_default;
Create OpenAI API key
Development
Bot Creation:
Use a node to create the bot that will join meetings. Provide the meeting URL and set transcription options within the API request.
Authentication:
Configure authentication settings via a Bearer token for interacting with your transcription service.
Webhook Setup:
Create a webhook to receive real-time transcription updates, ensuring timely data capture during meetings.
Join Meeting:
Set the bot to join the specified meeting and actively listen to capture conversations.
Transcription Handling:
Combine transcription fragments into cohesive sentences and manage dialog arrays for coherence.
Trigger Actions on Keywords:
Set up keyword recognition that can initiate requests to the OpenAI API for additional interactions based on captured dialogue.
Output and Summary Generation:
Produce insights and summary notes from the transcriptions that can be stored back into the database for future reference."
Auto Categorise Outlook Emails with AI,https://n8n.io/workflows/2454-auto-categorise-outlook-emails-with-ai/,"Automate your email management with this workflow, designed for freelancers and business professionals who receive high volumes of emails. By leveraging AI-powered categorisation and dynamic email processing, this template helps you organise your inbox and streamline communication for better efficiency and productivity.
Check out the YouTube video for step-by-step set up instructions!
How it works:
Fetch & Filter Emails: The workflow retrieves emails from your Microsoft Outlook account, filtering out flagged emails and those already categorised.
Content Preparation: Each email is cleaned up and converted to a structured format using Markdown, making it easier for AI processing.
AI Categorization: The content is analysed using an AI model, which categorises the emails into predefined categories (e.g., Action, Junk, Business, SaaS) based on the context and content.
Email Categorization & Folder Management: The categorised emails are updated in Microsoft Outlook and moved to respective folders such as ""Junk Email"" or ""Receipts"" based on the AI's classification.
Conditional Processing & Final Checks: Additional checks and conditions ensure that only unread emails are processed, and errors are gracefully managed to maintain workflow stability.
Set up steps:
Connect Microsoft Outlook: Link your Microsoft Outlook account using the built-in credentials node to enable email fetching, updating, and folder management.
Configure AI Model (Ollama API): Set up the AI model by connecting to the Ollama API and choosing your desired language model for categorisation.
Modify Email Categories (Optional): Customize the categories and subcategories within the workflow to suit your unique email management needs.
Set Up Error Handling: Review the error handling node settings to ensure smooth workflow execution.
This template offers a robust solution for managing and organising your inbox, helping you save time and keep your focus on important emails."
Notion AI Assistant Generator,https://n8n.io/workflows/2415-notion-ai-assistant-generator/,"This n8n workflow template lets teams easily generate a custom AI chat assistant based on the schema of any Notion database. Simply provide the Notion database URL, and the workflow downloads the schema and creates a tailored AI assistant designed to interact with that specific database structure.
Set Up
Watch this quick set up video üëá
Key Features
Instant Assistant Generation: Enter a Notion database URL, and the workflow produces an AI assistant configured to the database schema.
Advanced Querying: The assistant performs flexible queries, filtering records by multiple fields (e.g., tags, names). It can also search inside Notion pages to pull relevant content from specific blocks.
Schema Awareness: Understands and interacts with various Notion column types like text, dates, and tags for accurate responses.
Reference Links: Each query returns direct links to the exact Notion pages that inform the assistant‚Äôs response, promoting transparency and easy access.
Self-Validation: The workflow has logic to check the generated assistant, and if any errors are detected, it reruns the agent to fix them.
Ideal for
Product Managers: Easily access and query product data across Notion databases.
Support Teams: Quickly search through knowledge bases for precise information to enhance support accuracy.
Operations Teams: Streamline access to HR, finance, or logistics data for fast, efficient retrieval.
Data Teams: Automate large dataset queries across multiple properties and records.
How It Works
This AI assistant leverages two HTTP request tools‚Äîone for querying the Notion database and another for retrieving data within individual pages. It‚Äôs powered by the Anthropic LLM (or can be swapped for GPT-4) and always provides reference links for added transparency."
Notion knowledge base AI assistant,https://n8n.io/workflows/2413-notion-knowledge-base-ai-assistant/,"Who is this for
This workflow is perfect for teams and individuals who manage extensive data in Notion and need a quick, AI-powered way to interact with their databases. If you're looking to streamline your knowledge management, automate searches, and get faster insights from your Notion databases, this workflow is for you. It‚Äôs ideal for support teams, project managers, or anyone who needs to query specific data across multiple records or within individual pages of their Notion setup.
Check out the Notion template this Assistant is set up to use: https://www.notion.so/templates/knowledge-base-ai-assistant-with-n8n
How it works
The Notion Database Assistant uses an AI Agent built with Retrieval-Augmented Generation (RAG) to query this Knowledge Base style Notion database. The assistant can search across multiple properties like tags or question and retrieves content from inside individual Notion pages for additional context.
Key features include:
Querying the database with flexible filters.
Searching within individual Notion pages and extracting relevant blocks.
Providing a reference link to the exact Notion pages used to inform its responses, ensuring transparency and easy verification.
This assistant uses two HTTP request tools‚Äîone for querying the Notion database and another for pulling data from within specific pages. It streamlines knowledge retrieval, offering a conversational, AI-driven way to interact with large datasets.
Set up
Find basic set up instructions inside the workflow itself or watch a quickstart video üëá"
AI web researcher for sales,https://n8n.io/workflows/2324-ai-web-researcher-for-sales/,"Who is this for?
This workflow is for all sales reps and lead generation manager who need to prepare their prospecting activities, and find relevant information to personalize their outreach.
Use Case
This workflow allows you to do account research with the web using AI.
It has the potential to replace manual work done by sales rep when preparing their prospecting activities by searching complex information available online.
What this workflow does
The advanced AI module has 2 capabilities:
Research Google using SerpAPI
Visit and get website content using a sub-workflow
From an unstructured input like a domain or a company name.
It will return the following properties:
domain
company Linkedin Url
cheapest plan
has free trial
has entreprise plan
has API
market (B2B or B2C)
The strength of n8n here is that you can adapt this workflow to research whatever information you need.
You just have to precise it in the prompt and to precise the output format in the ""Strutured Output Parser"" module.
Detailed instructions + video guide can be found by following this link."
Autonomous AI crawler,https://n8n.io/workflows/2315-autonomous-ai-crawler/,"This workflow with AI agent is designed to navigate through the page to retrieve specific type of information (in this example: social media profile links).
The agent is equipped with 2 tools:
text tool: to retrieve all the text from the page,
URLs tool: to extract all possible links from the page.
üí° You can edit prompt and JSON schema connected to the agent in order to return other data then social media profile links.
üëâ This workflow uses Supabase as storage (input/output). Feel free to change it to any other database of your choice.
üé¨ See this workflow in action in my YouTube video.
How it works?
The workflow uses the input URL (website) as a starting point to retrieve the data (e.g. example.com). Using the ""URLs tool"", the agent is able to retrieve all links from the page and navigate to them.
For example, if you want to retrieve contact information, agent will try to find a subpage that might contain this information (e.g. example.com/contact) and extract the information using the text tool.
Set up steps
Connect database with input data (website addresses) or pin sample data to trigger node.
Configure the crawling agent to retrieve the desired data (e.g. modify prompt and/or parsing schema).
Set credentials for OpenAI.
Optionally: split agent tools to separate workflows.
If you like this workflow, please subscribe to my YouTube channel and/or my newsletter."
Recipe Recommendation Engine with Bright Data MCP & OpenAI 4o mini,https://n8n.io/workflows/4591-recipe-recommendation-engine-with-bright-data-mcp-and-openai-4o-mini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
Recipe Recommendation Engine with Bright Data MCP & OpenAI is a powerful automated workflow combines Bright Data's MCP for scraping trending or regional recipe data with OpenAI 4o mini to generate personalized recipe recommendations.
This automated workflow is designed for:
Food Bloggers & Culinary Creators : Who want to automate the extraction and curation of recipes from across the web to generate content, compile cookbooks, or publish newsletters.
Nutritionists & Health Coaches : Who need structured recipe data to analyze ingredients, calories, and nutrition for personalized meal planning or dietary tracking.
AI/ML Engineers & Data Scientists : Building models that classify cuisines, predict recipes from ingredients, or generate dynamic meal suggestions using clean, structured datasets.
Grocery & Meal Kit Platforms : Who aim to extract recipes to power recommendation engines, ingredient lists, or personalized meal plans.
Recipe Aggregator Startups : Looking to scale recipe data collection, filtering, and standardization across diverse cooking websites with minimal human intervention.
Developers Integrating Cooking Features : Into apps or digital assistants that offer recipe recommendations, step-by-step cooking instructions, or nutritional insights.
What problem is this workflow solving?
This workflow solves:
Automated recipe data extraction from any public URL
AI-driven structured data extraction
Scalable looped crawling and processing
Real-time notifications and data persistence
What this workflow does
1. Set Recipe Extract URL
Configure the recipe website URL in the input node
Set your Bright Data zone name and authentication
2. Paginated Data Extract
Triggers a paginated extraction across multiple pages (recipe listing, index, or search pages)
Returns a list of recipe links for processing
3. Loop Over Items
Loops through the array of recipe links
Each link is passed individually to the scraping engine
4. Bright Data MCP Client (Per Recipe)
Scrapes each individual recipe page using scrape_as_html
Smartly bypasses common anti-bot protections via Bright Data Web Unlocker
5. Structured Recipe Data Extract (via OpenAI GPT-4o mini)
Converts raw HTML to clean text using an LLM preprocessing node
Uses OpenAI GPT-4o mini to extract structured data
6. Webhook Notification
Pushes the structured recipe data to your configured webhook endpoint
Format: JSON payload, ideal for Slack, internal APIs, or dashboards
7. Save Response to Disk
Saves the structured recipe JSON information to local file system
Pre-conditions
You need to have a Bright Data account and do the necessary setup as mentioned in the ""Setup"" section below.
You need to have an OpenAI Account.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
In n8n, configure the OpenAi account credentials.
Make sure to set the fields as part of Set the Recipe Extract URL. Remember to set the webhook_url to send a webhook notification of recipe response.
Set the desired local path in the Write the structured content to disk node to save the recipe response.
How to customize this workflow to your needs
You can tailor the Recipe Recommendation Engine workflow to better fit your specific use case by modifying the following key components:
1. Input Fields Node
Update the Recipe URL to target specific cuisine sites or recipe types (e.g., vegan, keto, regional dishes).
2. LLM Configuration
Swap out the OpenAI GPT-4o mini model with another provider (like Google Gemini) if you prefer.
Modify the structured data prompt to extract custom fields that you wish.
3. Webhook Notification
Configure the Webhook Notification node to point to your preferred integration (e.g., Slack, Discord, internal APIs).
4. Storage Destination
Change the Save to Disk node to store the structured recipe data in:
A cloud bucket (S3, GCS, Azure Blob etc.)
A database (MongoDB, PostgreSQL, Firestore)
Google Sheets or Airtable for spreadsheet-style access."
Extract Structured Data from Brave Search with Bright Data MCP & Google Gemini,https://n8n.io/workflows/4497-extract-structured-data-from-brave-search-with-bright-data-mcp-and-google-gemini/,"Notice
Community nodes can only be installed on self-hosted instances of n8n.
Who this is for
The Brave Search Structured Data Extractor workflow is designed for professionals and teams that need high-quality, structured insights from Brave search results in real time. Whether you're performing market research, tracking competitors, training AI models, or powering content engines, this workflow offers a robust and automated solution.
This workflow is tailored for:
Market Researchers - Who analyze trends across multimedia channels
AI Developers - Who require clean, structured datasets for model fine-tuning
SEO & Content - Analysts looking to monitor visibility across news, images, and videos
Media Researchers - Curating timely and relevant information across formats
Automation Engineers - Integrating search insights into downstream workflows
What problem is this workflow solving?
Traditional web scraping and search result parsing is fragmented, inconsistent, and prone to errors, especially when dealing with multimedia (images, videos, news) data from search engines. This workflow provides:
Centralized Brave search data extraction across all content types. Switches the search execution based upon the type of search that is being set. ex: news, images, videos, all
Automated structured data transformation using Google Gemini
Unified output persistence and notification across disk, webhook, and Google Sheets
What this workflow does
Input Configuration
Define your Brave search query
Set the search type: videos, images, news, or all
Configure your Bright Data MCP zone
Bright Data MCP Search Execution
Initiates a Brave search via Bright Data MCP using the correct URL pattern for each search type
Returns raw HTML of search results
Google Gemini LLM
Structured Data Extraction
Transforms raw results into structured data (e.g., title, URL, source, snippet)
Output Handling
Save to disk (e.g., JSON or CSV file)
Send Webhook notification with structured data (e.g., Slack, internal dashboards)
Store in Google Sheets for team-wide access or dashboarding
Pre-conditions
Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol
You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below.
You need to have the Google Gemini API Key. Visit Google AI Studio
You need to install the Bright Data MCP Server @brightdata/mcp
You need to install the n8n-nodes-mcp
Setup
Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp
Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine.
Sign up at Bright Data.
Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Google Gemini(PaLM) Api account with the Google Gemini API key (or access through Vertex AI or proxy).
In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below.
Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=<your-token>
How to customize this workflow to your needs
Enhance Output Analysis
Add additional LLM prompts for topic classification, sentiment scoring, or trend forecasting.
Output Format Options
Choose to output CSV, Markdown, or HTML reports based on your integration target.
Schedule Automation
Trigger the workflow on a schedule (daily/weekly) to keep monitoring topical content."
Sync Google Drive files to an InfraNodus Knowledge Graph,https://n8n.io/workflows/4495-sync-google-drive-files-to-an-infranodus-knowledge-graph/,"This template can be used to sync the files in your Google drive to a new or existing InfraNodus knowledge graph.
The InfraNodus graph will then reveal the main topics and ideas in your collection of documents and show the content gaps in them. You can also use the built-in AI to converse with the documents.
You can also access the InfraNodus Graphs via its GraphRAG API to re-use them in your other n8n workflows for high-quality content retrieval and knowledge base optimization.
The template showcases the use of multiple n8n nodes and processes:
Syncing documents from a Google Drive folder / extracting them
text extraction from files
optional: high-quality PDF conversion using ConvertAPI
InfraNodus knowledge graph generation
Note: If you want to upload files from your Google drive to an InfraNodus graph, check out our other workflow
How it works
Here's a description of this workflow step by step:
Wait for new file(s) to appear in the Google drive folder
Reiterate through each file
Retrieve the new file from the Google drive
For each file found: reiterate the workflow and
Identify the type of the file (TXT, PDF, Markdown)
For TXT and Markdown files extract the text data
For PDF files use a special PDF to Text convertor to extract the text data. (Optional: using ConvertAPI for better quality PDF conversion)
Forward everything to the InfraNodus graphAndStatements API endpoint with the name of the new graph, the text field with the text data, the text settings, and doNotSave=false to create a new graph
Reiterate through another file.
How to use
You need an InfraNodus GraphRAG API account and key to use this workflow.
Create an InfraNodus account
Get the API key at https://infranodus.com/api-access and create a Bearer authorization key for the InfraNodus HTTP nodes.
Use that API key to set up authorization for the InfraNodus tool in the workflow.
If you want to upload the files to an existing graph, you should copy its name from InfraNodus. Otherwise you can specify any name you want.
Requirements
An InfraNodus account and API key
A Google Drive account and authorization (you will need to set it up via Google Cloud using the n8n instructions provided in the Google Drive node).
Customizing this workflow
You can use Dropbox instead of Google Drive.
You can also modify this workflow slightly to make it Upload the files from a Google Drive when the new files appear in it.
Check out the complete guide at https://support.noduslabs.com/hc/en-us/articles/20267019838108-Upload-Sync-Your-Google-Drive-Folder-with-InfraNodus-using-n8n"
Evaluations Metric: Answer Similarity,https://n8n.io/workflows/4423-evaluations-metric-answer-similarity/,"This n8n template demonstrates how to calculate the evaluation metric ""Similarity"" which in this scenario, measures the consistency of the agent.
The scoring approach is adapted from the open-source evaluations project RAGAS and you can see the source here https://github.com/explodinggradients/ragas/blob/main/ragas/src/ragas/metrics/_answer_similarity.py
How it works
This evaluation works best where questions are close-ended or about facts where the answer can have little to no deviation.
For our scoring, we generate embeddings for both the AI's response and ground truth and calculate the cosine similarity between them.
A high score indicates LLM consistency with expected results whereas a low score could signal model hallucination.
Requirements
n8n version 1.94+
Check out this Google Sheet for a sample data https://docs.google.com/spreadsheets/d/1YOnu2JJjlxd787AuYcg-wKbkjyjyZFgASYVV0jsij5Y/edit?usp=sharing"
üì∏ Automate Photo Background Removal with Photoroom API and Google Drive,https://n8n.io/workflows/4398-automate-photo-background-removal-with-photoroom-api-and-google-drive/,"Tags: Marketing, Image Processing, Automation
Context
Hey! I‚Äôm Samir, a Data Scientist from Paris and the founder of LogiGreen Consulting.
We use AI, automation, and data to support sustainable business practices for small, medium and large companies.
I implemented this workflow to support an event agency to automate image processing like background removal using Photoroom API.
Automate your photos processing with n8n!
This n8n workflow collects all images in a Google Drive folder shared with multiple photographers.
For each image, it calls the Photoroom API:
A processed image w/o a background is saved in a subfolder Remove Background
The original pictures are saved in the subfolder Original
This workflow, triggered every morning, will process the backlog of images.
üì¨ For business inquiries, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is useful for:
Digital Marketing teams that use images for content creation
Photographs or Event Organisers that collect large amounts of photos that need processing
What does it do?
This n8n workflow:
‚è∞ Triggers automatically every morning
üñºÔ∏è Collects the names and IDs of all images in the folder
üßπ HTTP POST request to Photoroom API to remove the background
üìÑ Stores the processed image and the original image in two separate sub-folders
What do I need to get started?
You‚Äôll need:
A Google Drive Account connected to your n8n instance with credentials
A Photoroom API key that you can get for free (trial) here: Photoroom API
Follow the Guide!
Follow the sticky notes inside the workflow or check out my step-by-step tutorial on how to configure and deploy it.

üé• Watch My Tutorial
This workflow was built using n8n version 1.93.0
Submitted: May 26, 2025"
"-Powered Knowledge Base with Google Docs, Discord & GPT-4o-mini",https://n8n.io/workflows/4388-powered-knowledge-base-with-google-docs-discord-and-gpt-4o-mini/,"üß† MCP Memory Core: AI-Powered Knowledge Base
Store, retrieve, and act on organizational memories using Google Docs + Discord. Part of the MCP ecosystem.
Price: $5 (Advanced toolchain with multi-platform integration)
üîç What It Does
üìö Save Memories: Log conversations, decisions, or code snippets to a Google Doc (timestamped).

üîç Retrieve Memories: Fetch past context for AI agents or workflows.

üì§ Auto-Send Updates: Push memories to Discord channels/DMs via GPT-4o-mini formatting.

ü§ñ MCP-Compatible: Works with your existing GitHub/Discord MCP workflows.
üöÄ Key Features
üåê Multi-Trigger: Use via:

    save_memory / retrieve_memory API routes.

    Discord mentions via MCP Server integration.

üîê Secure Storage: Memories live in your Google Docs (no third-party DB).

üß© Modular Design: Swap Google Docs for Notion/Airtable with minimal changes.

ü§ñ AI Cleanup: GPT-4o-mini reformats raw data into Discord-friendly messages.
‚öôÔ∏è Use Cases
Team Knowledge Base: Archive meeting notes accessible via Discord commands.

AI Context Window: Give bots long-term memory for better issue resolution.

Audit Trail: Track project decisions with timestamps.
üõ†Ô∏è Setup Guide
üìù Configure Google Docs:

    In ""Edit Fields"", set sheet_id to your Google Doc URL.

ü§ñ Discord Permissions:

    Update discord_server_id, discord_channel_id, and discord_user_id.

üîó Link to MCP:

    Use the MCP Server Trigger node to connect to existing workflows.

üß† Customize AI:

    Tweak the AI Agent prompt for different formatting styles.
Works great with the $1 GitHub MCP Server template!"
"Notion Status-Based Alert Messages (Slack, Telegram, WhatsApp, Discord, Email)",https://n8n.io/workflows/4386-notion-status-based-alert-messages-slack-telegram-whatsapp-discord-email/,"Notion Status-Based Alert Template
Who is this for?
Teams that live in Notion and want an instant ping to the right person when a task changes state.
Perfect for content creators, project managers, or any small team that tracks work in a Notion database and prefers Slack / Telegram / Discord / e-mail notifications over manually checking a board.
What problem does this solve?
Polling Notion or checking a kanban board is slow and error-prone.
This workflow watches a Notion database and routes an alert to specific people based on the item‚Äôs Status. One central map decides who gets pinged for ‚ÄúOn Deck‚Äù, ‚ÄúIn Progress‚Äù, ‚ÄúReady for Review‚Äù, or ‚ÄúReady to Publish‚Äù.
How it works
Trigger ‚Äì choose either method
Polling (Notion Trigger) ‚Äì fires every minute.
Push (Webhook) ‚Äì register the production URL in a Notion automation and disable polling.
Set Notion Page Info ‚Äì copies Title, Status, URL, etc. into top-level fields.
Switch (Status router) ‚Äì routes the item down a branch that matches its Status.
Set-Mention nodes ‚Äì one per Status. Each node sets a single field mention (e.g. &lt;@U123456&gt;).
Add or edit these nodes to map new statuses or recipients.
Build Message ‚Äì assembles a rich text block:
Task title
Status:
<@UserIDs>
<Notion URL|Open in Notion>
Send nodes ‚Äì Slack (active) + optional Telegram / WhatsApp / Discord / Email (disabled by default). All reuse the same {{$json.message}}.
Setup steps
Import this template into n8n.
Connect credentials
Notion API token
Slack OAuth (and any other channels you enable)
Edit the Status ‚Üí Mention map
Open each Set-Mention node and replace the placeholder with the real Slack ID / chat ID / phone / email.
Copy a node for every extra Status you use, wire it to a new Switch output, and update the value.
Set environment variables (recommended)
NOTION_DB_ID, SLACK_CHANNEL, EMAIL_FROM, etc.
Pick your trigger style
Keep polling enabled or disable it and enable the Webhook, then register the webhook URL in Notion.
Test ‚Äì change a task‚Äôs Status in Notion ‚Üí watch Slack for the ping.
Example output
Title: ‚ÄúDraft blog post ‚Äì AI productivity‚Äù
Status: Ready for Review
Slack message:
*Draft blog post ‚Äì AI productivity*
Status: Ready for Review
&lt;@U789012&gt;
&lt;https://www.notion.so/‚Ä¶|Open in Notion&gt;
Extending the flow
Wire additional channels after Build Message‚Äîthey all consume the same {{$json.message}}.
Add richer logic (e.g. due-date reminders) in the Set Notion Page Info node.
Verify Notion webhook signatures in a Function node if you rely on push triggers.
This template is the leanest possible setup: one table of statuses ‚Üí direct pings to the right people. Swap the IDs, flip on your favourite channels, and ship."
"Automated Website Change Monitoring with Bright Data, GPT-4.1 & Google Workspace",https://n8n.io/workflows/4382-automated-website-change-monitoring-with-bright-data-gpt-41-and-google-workspace/,"Note: This template is for self-hosted n8n instances only
You can use this workflow to fully automate website content monitoring and change detection on a weekly basis‚Äîeven when there‚Äôs no native node for scraping or structured comparison. It uses an AI-powered scraper, structured data extraction, and integrates Google Sheets, Drive, Docs, and email for seamless tracking and reporting.
Main Use Cases
Monitor and report changes to websites (e.g., pricing, content, headings, FAQs) over time
Automate web audits, compliance checks, or competitive benchmarking
Generate detailed change logs and share them automatically with stakeholders
How it works
The workflow operates as a scheduled process, organized into these stages:
1. Initialization & Configuration
Triggers weekly (or manually) and initializes key variables: Google Drive folder, spreadsheet IDs, notification emails, and test mode.
2. Input Retrieval
Reads the list of URLs to be monitored from a Google Sheet.
3. Web Scraping & Structuring
For each URL, an AI agent uses Bright Data's scrape_as_markdown tool to extract the full web page content.
The workflow then parses this content into a well-structured JSON, capturing elements like metadata, headings, pricing, navigation, calls to action, contacts, banners, and FAQs.
4. Saving Current Week‚Äôs Results
The structured JSON is saved to Google Drive as the current week‚Äôs snapshot for each monitored URL.
The Google Sheet is updated with file references for traceability.
5. Comparison with Previous Snapshot
If a prior week‚Äôs file exists, it is downloaded and parsed.
The workflow compares the current and previous JSON snapshots, detecting and categorizing all substantive content changes (e.g., new/updated plans, FAQ edits, contact info modifications).
Optionally, in test mode, mock changes are introduced for demo and validation purposes.
6. Change Report Generation & Delivery
A rich Markdown-formatted changelog is generated, summarizing the detected changes, and then converted to HTML.
The changelog is uploaded to Google Docs and linked back to the tracking sheet.
An HTML email with the full report and relevant links is sent to recipients.
Summary Flow:
Schedule/workflow trigger ‚Üí initialize variables
Read URL list from spreadsheet
For each URL:
Scrape & structure as JSON
Save to Drive, update tracking sheet
If previous week exists:
Download & parse previous
Compare, generate changelog
Convert to HTML, save to Docs, update Sheet
Email results
Benefits:
Fully automated website change tracking with end-to-end reporting
Adaptable and extensible for any set of monitored pages and content types
Easy integration with Google Workspace tools for collaboration and storage
Minimal manual intervention required after initial setup"
Bright Data-Powered Competitive Price Lookup and Report Generator,https://n8n.io/workflows/4348-bright-data-powered-competitive-price-lookup-and-report-generator/,"This n8n workflow automates the collection, enrichment, and analysis of e-commerce product listings using Bright Data and AI, then delivers an HTML email report with the most competitive offers.
üöÄ What It Does
Pulls product titles from a Google Sheet.
For each product, searches a Bright Data marketplace dataset (Google Shopping) for available listings.
Extracts relevant fields: price, title, seller name, and listing URL.
Sends this data to Google Gemini for AI-powered Markdown report generation.
Converts Markdown to HTML and styles the output for better email rendering.
Sends an email report for each product with the top 20 most affordable offers.
üõ†Ô∏è Step-by-Step Setup
Load product list from Google Sheets.
For each product title, run a Bright Data filter request (case-sensitive match).
Poll the snapshot status until it is ready.
Retrieve snapshot content and clean the results with a Code node.
Pass the results to Gemini (PaLM/Gemini Flash) for analysis and report generation in Markdown.
Convert Markdown into styled HTML using Markdown + Code nodes.
Send formatted email to a predefined recipient.
Return to the loop and repeat for the next product.
üß† How It Works
Loop Control: SplitInBatches handles product-by-product processing.
Snapshot Handling: Snapshot status is polled every 30s until success/failure.
AI Formatting: Gemini summarizes listings and formats content.
Error Handling: Failed snapshots produce a warning message and resume the loop.
üì® Final Output
Each email contains:
The product name
A clean HTML of up to 20 sellers with lowest prices
Links to listings
AI-generated pricing summary
üîê Credentials Used
Bright Data account
Google Gemini (PaLM/Gemini Flash)
Google Sheets (OAuth2)
SMTP Email (emailSend node)
‚ö†Ô∏è Important Notes
Item title search is case-sensitive. Typos or casing mismatches may result in no results.
Requires n8n-nodes-brightdata community node to be installed."
AAVE Portfolio Professional AI Agent | Telegram + Email + GPT-4o + Moralis,https://n8n.io/workflows/4267-aave-portfolio-professional-ai-agent-or-telegram-email-gpt-4o-moralis/,"A next-generation AI-powered DeFi health monitor that tracks wallet positions across Aave V3 using GPT-4o and LangChain. It delivers human-readable reports via Telegram and Gmail, triggered on schedule or manually. Built for professionals monitoring multiple DeFi wallets.
üß© System Components
Component Role
‚úÖ Scheduler Triggers the workflow periodically
‚úÖ Google Sheets Wallet Loader Loads all monitored wallet addresses
‚úÖ Set Variables Injects dynamic wallet + date
‚úÖ AAVE Portfolio AI Agent GPT-4o + LangChain AI that generates human-readable summaries
‚úÖ Moralis API Nodes (3) Collect Aave V3 supply/borrow/collateral data
‚úÖ OpenAI Chat Model (gpt-4o-mini) Interprets on-chain data and explains it
‚úÖ Telegram Delivery Sends summary to Telegram chat
‚úÖ Gmail Email Sender Sends full HTML report to email
‚úÖ HTML Formatter Beautifies AI output into email structure
‚öôÔ∏è How It Works
Scheduled or manual trigger
Pulls wallet addresses from Google Sheets
For each wallet:
Pulls Aave data from Moralis
GPT-4o AI generates report
Sends summary to Telegram
Sends full HTML report via Gmail
üõ† Installation Steps
1. Import the Workflow
Upload AAVE_Portfolio_Professional_AI_Agent.json to your n8n instance.
2. Connect These Credentials
Service Required Credential Type
Moralis HTTP Header Auth (X-API-Key)
OpenAI GPT-4o via OpenAI API Key
Telegram Telegram Bot API Token
Gmail Gmail OAuth2 Credential
3. Create Google Sheet
Column name must be: wallet_address
Add wallet addresses you want monitored
üì¨ Output Format
Telegram Message Example
üìä Aave DeFi Health Report  
Wallet: 0xABC...123  
Date: 2025-05-21

‚ñ™Ô∏è Pool: Aave Ethereum USDC  
‚Ä¢ Supply: $10,040  
‚Ä¢ Borrowed: $5,500  
‚Ä¢ Health Factor: 3.43  
‚Ä¢ Liquidation Risk: No  
Email Report
Full HTML + plain text versions
Auto-generated date + styled per wallet
Includes notes and threshold warnings
üß† Smart Features
GPT-4o generates clear human summaries
Monitors multiple wallets in one run
Flags liquidation risk dynamically
Logs daily performance snapshots
üí° Customization Ideas
Add Telegram slash command /aave &lt;wallet&gt;
Expand to monitor Compound, Lido, or Uniswap
Export to Notion, Slack, or Data Studio
üßæ Licensing & Attribution
¬© 2025 Treasurium Capital Limited Company
Architecture, prompts, and report formatting are intellectual property protected.
No unauthorized rebranding, redistribution, or resale permitted.
üîó For support or licensing inquiries: LinkedIn ‚Äì Don Jayamaha
üöÄ Track all your Aave DeFi positions using AI‚Äîdelivered via Telegram + Gmail.
Perfect for funds, traders, and DeFi power users.
üé• Watch the Live Demo:"
"Reddit API Hub: Manage Posts, Comments & Subreddits via Server-Sent Events",https://n8n.io/workflows/4266-reddit-api-hub-manage-posts-comments-and-subreddits-via-server-sent-events/,"üîÑ Reddit Content Operations via MCP Server
üßë‚Äçüíº Who is this for?
This workflow is built for content creators, marketers, Reddit automation enthusiasts, and AI agent developers who want structured, programmable access to Reddit content. If you're researching niche communities, tracking trends, or automating Reddit engagement ‚Äî this is for you.
üí° What problem is this workflow solving?
Reddit has valuable content scattered across subreddits, but manual analysis or engagement is inefficient. This workflow acts as a centralized API interface to:
Query and manage Reddit posts
Create, fetch, delete, and reply to comments
Analyze subreddit metadata and behavior
Enable AI agents to autonomously operate on Reddit data
It does this using an MCP (Model Context Protocol) Server over Server-Sent Events (SSE).
‚öôÔ∏è What this workflow does
This template sets up a custom MCP Server that listens for JSON-based operation commands sent via SSE. Based on the operation, it routes the request to one of the following branches:
üü• Post CRUD
Create a new Reddit post
Search posts across subreddits
Fetch posts by ID
Delete existing posts
üü© Comment CRUD
Create or reply to comments
Fetch multiple comments from posts
Delete specific comments
üü¶ Subreddit Read Operations
Get information about subreddits
List subreddit posts
Retrieve subreddit rules
üõ† Setup
Import this workflow into your self-hosted n8n instance.
Configure Reddit credentials (OAuth2).
Connect your input system to the MCP Server Trigger node via SSE.
Send operation payloads to the server like this:
{
  ""operation"": ""post_search"",
  ""params"": {
    ""query"": ""AI agents"",
    ""subreddit"": ""machinelearning""
  }
}
The workflow will route to the appropriate node based on operation type.

üß© Supported Operations
post_create

post_get_many

post_search

post_delete

post_get_by_id

comment_create

comment_reply

comment_get_many

comment_delete

subreddit_get_about

subreddit_get_many

subreddit_get_rules
üß† How to customize this workflow to your needs
Add new operations to the operation_switch node for additional API functionality.
Chain results into Notion, Slack, Airtable, or external APIs.
Integrate with OpenAI/GPT to summarize posts or filter content.
Add logic to score and sort content by engagement, sentiment, or keywords.
üü® Sticky Notes
Each operation group is color-coded (Posts, Comments, Subreddits).
Sticky Notes explain the purpose and dependencies of each section.
Easy to maintain and extend with clear logical separation.
‚ö†Ô∏è This template uses a custom MCP Server node and only works in self-hosted n8n.
üñº Workflow Preview"
Send AI-Generated Emails via Telegram Using GPT-4o-mini and Gmail,https://n8n.io/workflows/4265-send-ai-generated-emails-via-telegram-using-gpt-4o-mini-and-gmail/,"‚úâÔ∏è Telegram Email Agent with GPT + Gmail
Category: Messaging / AI Agent
Level: Beginner-Friendly
Tags: Telegram, Email Automation, AI Agent, Gmail, GPT Model
ü§ñ What This Workflow Does
This workflow turns your Telegram bot into a personal email assistant powered by AI.
With just a message on Telegram, users can:
Send an email via Gmail
Automatically generate the email content using OpenAI Models.
Get confirmation or responses directly in Telegram
It's like ChatGPT meets Gmail, inside your Telegram chat.
üîß How It Works
Telegram Trigger ‚Äì Listens for incoming messages from your bot.
AI Agent ‚Äì Processes the input using OpenAI Model and converts it into structured email content (To, Subject, Body).
Memory Node ‚Äì Stores short-term context per user (via chat ID), so the agent can hold simple conversations.
Gmail Node ‚Äì Sends the generated email using your Gmail account.
Telegram Node ‚Äì Replies to the user confirming the output or status.
üß† Why This is Useful
Ever wanted to send an email while on the go, without typing the whole thing out in Gmail?
This is a fast, intuitive, and AI-powered way to:
Dictate or draft emails from anywhere
Create an AI-powered virtual assistant via Telegram
Integrate n8n's Langchain Agent with real-world productivity use cases
ü™ú Setup Instructions
Connect your Telegram bot via BotFather and add the credentials in n8n.
Set up your OpenAI API key (GPT-4o-mini recommended).
Add your Gmail OAuth credentials.
Activate the workflow and start messaging your bot!"
Extract Structured LinkedIn Profile Data with Airtop & AI Parsing,https://n8n.io/workflows/4203-extract-structured-linkedin-profile-data-with-airtop-and-ai-parsing/,"Extracting LinkedIn Profile Information
Use Case
Manually copying data from LinkedIn profiles is time-consuming and error-prone. This automation helps you extract structured, detailed information from any public LinkedIn profile‚Äîenabling fast enrichment, hiring research, or lead scoring.
What This Automation Does
This automation extracts profile details from a LinkedIn URL using the following input parameters:
airtop_profile: The name of your Airtop Profile connected to LinkedIn.
linkedin_url: The URL of the LinkedIn profile you want to extract data from.
How It Works
Starts with a form trigger or via another workflow.
Assigns the LinkedIn URL and Airtop profile variables.
Opens the LinkedIn profile in a real browser session using Airtop.
Uses an AI prompt to extract structured information, including:
Name, headline, location
Current company and position
About section, experience, and education history
Skills, certifications, languages, connections, and recommendations
Returns structured JSON ready for further use or storage.
Setup Requirements
Airtop API Key ‚Äî free to generate.
An Airtop Profile connected to LinkedIn (requires one-time login).
Next Steps
Sync with CRM: Push extracted data into HubSpot, Salesforce, or Airtable for lead enrichment.
Combine with Search Automation: Use with a LinkedIn search scraper to process profiles in bulk.
Adapt to Other Platforms: Customize the prompt to extract structured data from GitHub, Twitter, or company sites.
Read more about the Extract Linkedin Profile Information automation."
Convert Markdown Content to Contentful Rich Text with AI Formatting,https://n8n.io/workflows/4078-convert-markdown-content-to-contentful-rich-text-with-ai-formatting/,"Workflow: Publish to Contentful with Rich Text Formatting
‚ö° About the Creators
This workflow was created by Varritech Technologies, an innovative agency that leverages AI to engineer, design, and deliver software development projects 500% faster than traditional agencies. Based in New York City, we specialize in custom software development, web applications, and digital transformation solutions. If you need assistance implementing this workflow or have questions about content management solutions, please reach out to our team.
üèóÔ∏è Architecture Overview
This workflow takes a JSON article payload, splits its markdown content into logical chunks, converts each chunk into Contentful Rich Text JSON via an AI agent, merges the resulting rich text nodes back into a single document, formats the entire entry according to Contentful's field schema, and finally publishes it to Contentful.
Trigger ‚Üí Executes when called by another workflow
Split by Headings ‚Üí Breaks markdown into ##-delimited chunks
Markdown ‚Üí Rich Text ‚Üí AI agent converts each chunk to Contentful Rich Text JSON
Combine Rich Text Objects ‚Üí Aggregates all chunk outputs into one document
Format Entry ‚Üí Wraps metadata and rich-text content into Contentful schema
Publish Entry ‚Üí HTTP POST to Contentful API
üì¶ Node-by-Node Breakdown
flowchart LR
  A[When Executed by Another Workflow] --&gt; B[Split by Headings]
  B --&gt; C[Markdown to Contentful format]
  C --&gt; D[Combine Rich Text Objects]
  D --&gt; E[Merge1]
  E --&gt; F[Format1]
  F --&gt; G[Create newly formatted Contentful Entry]
1. When Executed by Another Workflow
Type: Execute Workflow Trigger
Input Example:
title, slug, category.id, description, keywords, content, metaTitle, metaDescription, readingTime, difficulty
Purpose: Receives the JSON payload from the upstream workflow.
2. Split by Headings
Type: Code
Logic:
Splits input.content into an array of markdown chunks at each second-level heading (##).
Emits one item per chunk with index, slug, title, and contentChunk.
3. Markdown to Contentful format
Type: LangChain Agent (+ OpenAI Chat model)
System Prompt:
Defines rules for generating valid Contentful Rich Text JSON (must include nodeType, data:{}, content:[], etc.).
Provides examples for paragraphs, headings, lists, links, and images.
User Prompt:
Here is the markdown content to convert:
{{ $json.contentChunk }}
Purpose: Converts each markdown chunk into an array of rich-text nodes.
4. Combine Rich Text Objects
Type: Code
Logic:
Parses and merges all content arrays returned by the AI agent into one combined content array under a document root.
5. Merge1
Type: Merge
Purpose: Joins the original item (with metadata) and the combined rich-text document into a single data stream.
6. Format1
Type: Code
Logic:
Maps workflow data into the Contentful entry schema by setting each field (title, slug, category link, description, keywords, rich-text content, metaTitle, metaDescription, readingTime, difficulty) under the appropriate locale and structure required by Contentful.
7. Create newly formatted Contentful Entry
Type: HTTP Request
Method: POST
URL:
https://api.contentful.com/spaces
Headers:
Authorization: Bearer token for Contentful Management API
Content-Type: application/vnd.contentful.management.v1+json
X-Contentful-Version: entry version number
X-Contentful-Content-Type: content type ID
Body: The formatted fields object produced by the previous node
Purpose: Publishes the new entry with rich-text content to Contentful.
üîç Design Rationale & Best Practices
Chunked Conversion
Splitting by headings prevents AI context limits and keeps conversions modular.
Strict Rich Text Schema
Enforcing nodeType, data, and content structure avoids validation errors on Contentful.
Two-Phase Merge
Separating ""combine AI outputs"" and ""format entry"" keeps transformations clear and testable.
Idempotent Publish
Uses explicit versioning and content type headers to ensure correct entry creation."
Tesla 1hour & 1day Klines Tool (Candlestick & Volume AI Pattern Detector),https://n8n.io/workflows/4099-tesla-1hour-and-1day-klines-tool-candlestick-and-volume-ai-pattern-detector/,"üìâ Detect key candlestick reversal patterns and volume divergence on Tesla (TSLA) using GPT-4.1 and real-time OHLCV data.
This AI agent evaluates 1-hour and 1-day candles and is an essential part of the Tesla Financial Market Data Analyst Tool. It identifies signals like Doji, Engulfing, Hammer, and volume anomalies to support trade entry and exit logic.
‚ö†Ô∏è Not a standalone template ‚Äî must be triggered by the Tesla Financial Market Data Analyst Tool
üîê Requires:
Alpha Vantage Premium API Key
OpenAI GPT-4.1 access
üîç What This Agent Does
Calls Alpha Vantage to fetch:
üïê 1-hour OHLCV data
üìÖ 1-day OHLCV data
GPT-4.1 evaluates:
üìä Candlestick patterns like Doji, Engulfing, Shooting Star
üîÑ Volume divergence (price/volume inconsistency)
Returns a structured JSON output like:
{
  ""summary"": ""Bearish signs detected on 1-day chart. A shooting star formed on high volume while RSI is elevated. Volume divergence seen on 1h chart as price rises but volume weakens."",
  ""candlestickPatterns"": {
    ""1h"": ""None"",
    ""1d"": ""Shooting Star""
  },
  ""volumeDivergence"": {
    ""1h"": ""Bearish"",
    ""1d"": ""None""
  },
  ""ohlcv"": {
    ""1h"": {
      ""close"": 174.1,
      ""volume"": 1430000,
      ""high"": 175.0,
      ""low"": 173.8
    },
    ""1d"": {
      ""close"": 188.3,
      ""volume"": 21234000,
      ""high"": 189.9,
      ""low"": 183.7
    }
  }
}
üõ†Ô∏è Setup Instructions
Import the Workflow
Name it: Tesla_1hour_and_1day_Klines_Tool
Install Dependencies
‚úÖ Tesla Financial Market Data Analyst Tool (this is the trigger parent)
Add Required Credentials
Alpha Vantage Premium ‚Üí via HTTP Query Auth
OpenAI GPT-4.1 ‚Üí via OpenAI credentials
Verify Web Access
This tool fetches data live from Alpha Vantage:
/query?function=TIME_SERIES_INTRADAY&interval=60min
/query?function=TIME_SERIES_DAILY
Run via Execute Workflow Trigger
This tool will activate only when called by the Financial Analyst Agent. Inputs:
message (optional)
sessionId (used for memory continuity)
üß† Agent Architecture
Component Description
Candlestick Data Hour Fetches 60min TSLA candles via Alpha Vantage
Candlestick Data Day Fetches daily TSLA candles via Alpha Vantage
OpenAI Chat Model GPT-4.1 reasoning engine for pattern detection
Simple Memory Maintains short-term logic context
Tesla Klines Agent LangChain AI agent analyzing both candle and volume
üìå Sticky Notes Overview
üìò Workflow Purpose
üß† Short-Term Memory Notes
üîç 1h/1d Data Fetch Logic
üìâ Candlestick Pattern Types Detected
üìä Volume Divergence Definitions
ü§ñ GPT-4.1 Prompt Configuration
üîê Licensing & Support
¬© 2025 Treasurium Capital Limited Company
Logic, pattern reasoning, and prompt structure are proprietary IP.
üîó Don Jayamaha ‚Äì LinkedIn
üîó n8n Creator Profile
üöÄ Automate technical edge: detect TSLA candle reversals and volume anomalies with precision using GPT-4.1 and Alpha Vantage.
Required by the Tesla Financial Market Data Analyst Tool."
Tesla 15min Indicators Tool (Short-Term AI Technical Analysis),https://n8n.io/workflows/4096-tesla-15min-indicators-tool-short-term-ai-technical-analysis/,"‚è±Ô∏è Analyze Tesla (TSLA) short-term market structure and momentum using 6 technical indicators on the 15-minute timeframe.
This AI agent tool is part of the Tesla Quant Trading AI Agent system. It is designed to detect intraday shifts in volatility, trend strength, and potential reversal signals.
‚ö†Ô∏è Not standalone. This agent is triggered via Execute Workflow by the Tesla Financial Market Data Analyst Tool.
üîå Requires:
Tesla Quant Technical Indicators Webhooks Tool
Alpha Vantage Premium API Key
üìä What It Does
This workflow pulls the latest 20 data points for 6 key technical indicators from a webhook-powered source, then uses GPT-4.1 to interpret market momentum and structure:
Connected Indicators:
RSI (Relative Strength Index)
MACD (Moving Average Convergence Divergence)
BBANDS (Bollinger Bands)
SMA (Simple Moving Average)
EMA (Exponential Moving Average)
ADX (Average Directional Index)
The output is a structured JSON with:
Market summary
Timeframe (15m)
Indicator values
üìã Sample Output
{
  ""summary"": ""TSLA shows fading momentum. RSI dropped below 60, MACD is flattening, and BBANDS are tightening. Expect short-term consolidation."",
  ""timeframe"": ""15m"",
  ""indicators"": {
    ""RSI"": 58.3,
    ""MACD"": {
      ""macd"": -0.020,
      ""signal"": -0.018,
      ""histogram"": -0.002
    },
    ""BBANDS"": {
      ""upper"": 183.10,
      ""lower"": 176.70,
      ""middle"": 179.90,
      ""close"": 177.60
    },
    ""SMA"": 178.20,
    ""EMA"": 177.70,
    ""ADX"": 19.6
  }
}
üß† Agent Components
Module Role
Webhook Data Node Calls /15minData endpoint for Alpha Vantage indicators
LangChain Agent Parses indicator payloads and generates reasoning
OpenAI GPT-4.1 Powers the AI logic to interpret technical structure
Memory Module Maintains session consistency for multi-agent calls
üõ†Ô∏è Setup Instructions
Import Workflow into n8n
Name it: Tesla_15min_Indicators_Tool
Configure Webhook Source
Install and publish: Tesla_Quant_Technical_Indicators_Webhooks_Tool
Ensure /15minData is publicly reachable (or tunnel-enabled)
Add Credentials
Alpha Vantage API Key (HTTP Query Auth)
OpenAI GPT-4.1 (OpenAI Chat Model)
Link as Sub-Agent
This workflow is not triggered manually. It is executed using Execute Workflow by:
üëâ Tesla_Financial_Market_Data_Analyst_Tool
Pass in:
message (optional)
sessionId (for short-term memory linkage)
üìå Sticky Notes Summary
üü¢ Trigger Integration ‚Äì Receives sessionId and message from parent
üü° Webhook Fetcher ‚Äì Pulls Alpha Vantage data from /15minData
üß† GPT-4.1 Reasoning ‚Äì Produces structured JSON insight
üîµ Session Memory ‚Äì Maintains evaluation flow across tools
üìò Tool Description ‚Äì Explains indicator use and AI output format
üîí Licensing & Author
¬© 2025 Treasurium Capital Limited Company
All logic, formatting, and agent design are protected under copyright. No resale or public re-use without permission.
Created by: Don Jayamaha
Creator Profile: https://n8n.io/creators/don-the-gem-dealer/
üöÄ Build faster intraday Tesla trading models using clean 15-minute indicator insights‚Äîprocessed by AI.
Required by the Tesla Financial Market Data Analyst Tool."
Query-to-Action Automation with Bright Data MCP & OpenAI GPT,https://n8n.io/workflows/4077-query-to-action-automation-with-bright-data-mcp-and-openai-gpt/,"üìå AI Agent Template with Bright Data MCP Tool Integration
This template obtains all the possible tools from Bright Data MCP, process this through chatbot, then run any tool based on the user's query
‚ùì Problem It Solves
The problem that the MCP solves is the complexity and difficulty of traditional automation, where users need to have specific knowledge of APIs or interfaces to trigger backend processes. By allowing interaction through natural language, automatically classifying and routing queries, and managing context and memory effectively, MCP simplifies complex data operations, customer support, and workflow orchestration scenarios where inputs and responses change dynamically.
üß∞ Pre-requisites
Before deploying this template, ensure you have:
An active n8n instance (self-hosted or cloud).
A valid OpenAI API key (or any AI models)
Access to Bright Data MCP API with credentials.
Basic familiarity with n8n workflows and nodes.
‚öôÔ∏è Setup Instructions
**Install the MCP Community Node in N8N
In your N8N self-hosted instance, go to Settings ‚Üí Community Nodes.
Search and install n8n-nodes-mcp.
Configure Credentials:
Add your OpenAI API key or any AI mdeols to the relevant nodes. If you want other AI model, please replace all associated nodes of OpenAI in the workflow
Set up Bright Data MCP client credentials in the installed community node (STDIO)
Obtain your API in Bright Data and put it in Environment field in the credentials window. It should be written as API_Key=<your api key from Bright Data>
üîÑ Workflow Functionality (Summary)
User message triggers the workflow.
AI Classifier (OpenAI) interprets the intent and maps it to a tool from Bright Data MCP.
If no match is found, the user is notified.
If more information is needed, the AI requests it.
Memory preserves context for follow-up actions.
The tool is executed, and results are returned contextually to the user.
üß† Optional memory buffer and chat memory manager nodes keep conversations context-aware across multiple messages.
üß© Use Cases
Data Scraping Automation: Trigger scraping tasks via chat.
Lead Generation Bots: Use MCP tools to fetch, enrich, or validate data.
Customer Support Agents: Automatically classify and respond to queries with tool-backed answers.
Internal Workflow Agents: Let team members trigger backend jobs (e.g., reports, lookups) by chatting naturally.
üõ†Ô∏è Customization
Tool Matching Logic: Modify the AI classifier prompt and schema to suit different APIs or services.
Memory Size and Retention: Adjust memory buffer size and filtering to fit your app‚Äôs complexity.
Tool Execution: Extend the ""Execute the tool"" sub-workflow to handle additional actions, fallback strategies, or logging.
Frontend Integration: Connect this with various platforms (e.g., WhatsApp, Slack, web chatbots) using the webhook.
‚úÖ Summary
This template delivers a powerful no-code/low-code agent that turns chat into automation, combining AI intelligence with real-world tool execution. With minimal setup, you can build contextual, dynamic assistants that drive backend operations using natural language."
Financial News Digest with Google Gemini AI to Outlook Email,https://n8n.io/workflows/4074-financial-news-digest-with-google-gemini-ai-to-outlook-email/,"üß† Key Features
Looping source scraping: Collects content from news sites you have selected (it might not work for all of them however)
HTML extraction & cleaning: Parses, cleans, and filters messy website data to isolate only the most relevant content.
AI-powered synthesis: Uses Google Gemini (via LangChain agent) to summarize and structure financial news into a clear, bullet-pointed format.
Email-ready output: Generates styled HTML summaries with coral-colored headings, ideal for daily email reports.
Fully automated delivery: Sends out beautifully formatted updates via Outlook ‚Äî no manual work required.
üîß Technologies Used
n8n workflow orchestration
LangChain agents for prompt logic and formatting
Google Gemini API for advanced NLP
Custom JS logic to manage dynamic inputs and cleanup
Microsoft Outlook API for final distribution
Feel free to reach out if needed !"
Monitor Amazon Product Prices with Bright Data and Google Sheets,https://n8n.io/workflows/3878-monitor-amazon-product-prices-with-bright-data-and-google-sheets/,"Amazon Price Monitoring Workflow
This workflow enables you to monitor the prices of Amazon product listings directly from a Google Sheet, using data provided by Bright Data‚Äôs Amazon Scraper API. It automates the retrieval of price data for specified products and is ideal for market research, competitor analysis, or personal price tracking.
‚úÖ Requirements
Before using this template, ensure you have the following:
A Bright Data account and access to the Amazon Scraper API.
An active API key from Bright Data.
A Google Sheet set up with the required columns.
N8N account (self-host or cloud version)
‚∏ª
‚öôÔ∏è Setup
1. Create a Google Sheet with the following columns:
Product URL
ZIP Code (used for regional price variations)
ASIN (Amazon Standard Identification Number)
2. Extract ASIN Automatically using the following formula in the ASIN column:
=REGEXEXTRACT(A2, ""/(?:dp|gp/product|product)/([A-Z0-9]{10})"")
Replace A2 with the appropriate cell reference
3. Obtain an API Key:
Sign in to your Bright Data account.
Go to the API section to generate an API key.
Create a Bearer Authentication Credential using this key in your automation tool.
4. Configure the Workflow:
Use a node (e.g., ‚ÄúGoogle Sheets‚Äù) to read data from your sheet.
Use an HTTP Request node to send a query to Bright Data‚Äôs Amazon API with the ASIN and ZIP code.
Parse the returned JSON response to extract product price and other relevant data.
Optionally write the output (e.g., current price, timestamp) back into the sheet or another data store.
‚∏ª
Workflow Functionality
The workflow is triggered periodically (or manually) and reads product details from your Google Sheet.
For each row, it extracts the Product URL and ZIP code and sends a request to the Bright Data API.
The API returns product price information, which is then logged or updated back into the sheet using ASIN.
You can also map the product URL to the product URL, but ensure that the URL has no parameters. If the URL has appended parameters, refer to the input field from the Bright Data snapshot result.
‚∏ª
üí° Use Cases
E-commerce sellers monitoring competitors‚Äô prices.
Consumers tracking price drops on wishlist items.
Market researchers collecting pricing data across ZIP codes.
Affiliate marketers ensuring accurate product pricing on their platforms.
‚∏ª
üõ†Ô∏è Customization
Add columns for additional product data such as rating, seller, or stock availability.
Schedule the workflow to run hourly, daily, or weekly depending on your needs.
Implement email or Slack alerts for significant price changes.
Filter by product category or brand to narrow your tracking focus."
Homey Pro - Smarthouse integration with LLM,https://n8n.io/workflows/4058-homey-pro-smarthouse-integration-with-llm/,"This n8n workflow sets up a smart home assistant using OpenAI and Homey integration. It uses LangChain agent tools to allow natural language queries (in Norwegian) to trigger workflows for controlling lights, curtains, temperature, TVs, and other devices across different rooms (e.g., living room, bedroom, cinema). The system uses tool-based workflows connected to specific smart home actions and responds in Norwegian. It‚Äôs designed to be modular and easily extended with new devices or capabilities."
Download Media Files from Slack Messages,https://n8n.io/workflows/4039-download-media-files-from-slack-messages/,"Description:
This n8n workflow helps you capture Slack messages via a webhook and download attached media files (like images, documents, or videos) directly from those messages.
How it works:
Slack Trigger (Webhook) ‚Äì Listens for new messages in a Slack channel where the app is added.
HTTP Request ‚Äì Uses the file's private download URL to retrieve the media securely.
Use cases:
Download files shared by team members in a Slack channel.
Capture and process media from specific project or support channels.
Prepare media for later processing, archiving, or review.
Requirements:
Slack app with appropriate permissions (files:read, channels:history, etc.).
Slack webhook set up to listen to channel messages.
Authenticated HTTP request to handle private Slack file URLs.
This template is ideal for users who want full control over file handling triggered by real-time Slack messages."
Send Telegram Text and Audio Messages to Notion with DeepSeek Summaries & OpenAI Transcription,https://n8n.io/workflows/3987-send-telegram-text-and-audio-messages-to-notion-with-deepseek-summaries-and-openai-transcription/,"Streamline Your Communication and Task Management with the Telegram to Notion Automation
This powerful automation effortlessly connects your Telegram messages with your Notion workspace, transforming how you capture information and manage tasks.
What it does:
Automatic Capture: Instantly captures messages, voice notes, and audio files sent to your designated Telegram chat.
Intelligent Processing:
Text messages are automatically summarized into clear, plain-text descriptions using advanced AI (DeepSeek).
Voice notes and audio files are transcribed into text using OpenAI's powerful transcription service.
Organized in Notion:
Text message summaries are added as new entries to your ""Tasks Tracker"" database in Notion, with the original message as the title and the summary as the description. The current date is also included.
Transcriptions of voice notes and audio files are saved to your ""Transcribes"" database in Notion, making them easily searchable and reviewable.
Saves You Time: Eliminates the manual effort of copying and pasting information, transcribing audio, and creating new entries in Notion.
Boosts Productivity: Keeps all your important communications and tasks organized in one central location.
This automation is perfect for anyone who uses Telegram for work or personal notes and wants to seamlessly integrate that information into their Notion workflow for better organization and productivity."
"Automate Cal.com Meeting Attendee Management with Google Sheets, Beehiiv & Telegram",https://n8n.io/workflows/3962-automate-calcom-meeting-attendee-management-with-google-sheets-beehiiv-and-telegram/,"This n8n workflow template automates the process of managing meeting guests booked through Cal.com. It captures attendee information, logs it in a Google Sheet, and subscribes new guests to your Beehiiv newsletter, while also notifying you in Telegram.
How it Works
This workflow is designed to streamline your post-booking process. When a new meeting is booked via Cal.com, the workflow automatically triggers. It extracts the attendee details, adds a new row with the guest's information to a designated Google Sheet, and then adds the guest as a subscriber to your Beehiiv newsletter. Finally, it sends a notification to a specified Telegram channel, keeping you informed of new subscribers.
Features
Automated Data Entry: Automatically log meeting guest details into Google Sheets.
Newsletter Growth: Effortlessly add new meeting guests to your Beehiiv subscriber list.
Real-time Notifications: Get instant alerts in Telegram for new subscribers.
Seamless Integration: Connects Cal.com, Google Sheets, Beehiiv, and Telegram.
Set Up
To use this workflow, you will need the following:
Cal.com: Set up a Cal.com account and configure a webhook to trigger the workflow on new bookings. You will need the webhook URL provided by the first node in this workflow.
Google Cloud: A Google Cloud account with access to Google Sheets. You will need to specify the Sheet and the range where data should be added.
Beehiiv: A Beehiiv account. You will need your Beehiiv API key and the publication ID of your newsletter.
Telegram Account: A Telegram account and a channel where you want to receive notifications. You will need your Telegram Bot Token and the chat ID of the channel.
Configure each node in the workflow with your respective API keys, IDs, and sheet details as required. The ""Define your parameters"" box in the workflow provides guidance on finding your Telegram chat ID and Beehiiv API key/publication ID.
Additional Enhancements
This workflow can be extended and customized further:
Conditional Logic: Add filters to only subscribe guests based on certain criteria (e.g., specific meeting types).
Data Enrichment: Integrate with other services to enrich guest data before adding to Google Sheets or Beehiiv.
CRM Integration: Connect to a CRM to create or update contact records for new guests.
Custom Notifications: Customize the content and format of the Telegram notification.
Need Help?
If you need assistance setting up this workflow, encounter any issues, or would like to explore setting up similar automation workflows tailored to your company's specific needs, please contact us at 1 Node."
Jira MCP Server,https://n8n.io/workflows/3939-jira-mcp-server/,"What it does
This n8n workflow creates a powerful AI-powered Jira management system that allows you to use Claude or other AI assistants to create, update, and manage Jira tickets through natural language requests. The workflow exposes key Jira functions as AI tools, enabling you to interact with your Jira instance through conversational commands.
How it works
The workflow sets up an MCP (Model Control Protocol) server, allowing compatible AI assistants to use a suite of Jira management tools
The AI assistant can perform various Jira operations including:
Creating new tickets with customized fields
Adding comments to existing tickets
Retrieving available status transitions for tickets
Attaching files to tickets
Changing ticket statuses
Retrieving detailed information about tickets
Getting available projects and issue types
When you make a request to your AI assistant, it determines which Jira operation to perform and executes it through this workflow
Setup Steps
Prerequisites:
Active Jira Cloud account with admin access
n8n instance with the Langchain and MCP nodes installed
Claude Desktop App or another compatible AI assistant
Jira Credentials Setup:
Configure your Jira Cloud API credentials in n8n
Ensure your Jira account has sufficient permissions for all operations
Workflow Configuration:
Import this template into your n8n instance
Set up the MCP Trigger node with your desired path (currently ""test_mcp"")
Verify that all Jira tool nodes are correctly connected to the MCP Server node
Activate the workflow
Using Claude as an MCP Client:
Open your Claude Desktop App
Navigate to Settings > Developer Settings
Enable ""Connect to local MCP servers""
Add a new connection with the URL path to your n8n MCP server (e.g., ""http://localhost:5678/webhook/test_mcp"")
Start a new conversation and ask Claude to perform Jira tasks
Example Usage with Claude
Once you've set up the connection between Claude and your MCP server, you can use natural language to manage your Jira tickets. Here are some examples:
Creating a ticket:
""Claude, please create a new Jira ticket in the Web Development project with bug issue type. The summary should be 'Homepage loading slowly' and the description should be 'Users are experiencing delays of 5+ seconds when loading the homepage on mobile devices.'""
Adding a comment:
""Claude, please add a comment to Jira ticket WEB-123 saying 'This issue has been reproduced on multiple devices and browsers. Priority should be increased.'""
Checking status:
""Claude, can you get the details of ticket WEB-123 and tell me its current status?""
Changing status:
""Claude, please move ticket WEB-123 to 'In Progress' status.""
This workflow creates a seamless bridge between your AI assistant and Jira, making project management more efficient through natural language interactions."
Auto-Post Medium.com Articles to LinkedIn with Telegram Alerts,https://n8n.io/workflows/3931-auto-post-mediumcom-articles-to-linkedin-with-telegram-alerts/,"üß† Problem This Solves
Manually sharing Medium articles to LinkedIn daily can be repetitive and time-consuming. This automation:
Fetches the latest Medium articles based on a tag (e.g., android)
Posts them on LinkedIn twice daily
Uses Airtable to prevent duplicates
Sends a confirmation to Telegram once posted
Stay consistently active on LinkedIn without lifting a finger.
üë• Who This Template Is For
Developers who write or follow Medium content
Tech creators or founders looking to grow an audience
Community or page managers needing regular curated posts
Busy professionals who want hands-free LinkedIn engagement
‚öôÔ∏è Workflow Breakdown
This automation runs at 9:00 AM and 7:00 PM daily and performs these steps:
Fetch articles from MediumAPI.com by tag
Check Airtable to prevent reposting the same article
Post on LinkedIn if it‚Äôs new
Store the article ID in Airtable
Send a Telegram message after successful posting
üßæ Step-by-Step Setup Instructions
‚úÖ 1. Airtable Configuration
Create a base with:
Table Name: PostedArticles
Column: ArticleID (Single line text ‚Äì to track posted articles)
üîó 2. MediumAPI Setup
Go to https://mediumapi.com
Sign up and generate your API key from the dashboard
Use this API endpoint in an HTTP node:
GET https://mediumapi.com/api/tag/YOUR_TAG/latest
Headers:
Authorization: Bearer YOUR_API_KEY
Replace YOUR_TAG with a topic like android, ai, webdev, etc.
üí¨ 3. Telegram Bot Setup
Go to @BotFather and create a new bot
Save the bot token
Use @userinfobot to get your Telegram chat ID
Add a Telegram node in n8n with the token + chat ID
üîó 4. LinkedIn Setup
Create a LinkedIn Developer App
Connect it via OAuth2 in n8n
Choose to post on your profile or company page
üß± 5. n8n Workflow Structure
Node Type Description
Cron Triggers the flow twice a day
HTTP Request Fetches articles from MediumAPI.com
Airtable Search Checks if article ID already exists
IF Node Skips duplicates
LinkedIn Post Publishes to your LinkedIn profile/page
Airtable Create Stores posted article ID
Telegram Node Sends success notification
üõ†Ô∏è Customization Tips
Change the tag in the API URL to match your niche
Add hashtags or personal comments to the LinkedIn message
Schedule different posting times in the Cron node
Filter Medium posts based on length or title keywords (optional)"
Discover & Enrich Decision-Makers with Apollo and Human Verification,https://n8n.io/workflows/3830-discover-and-enrich-decision-makers-with-apollo-and-human-verification/,"üß© What This Workflow Does
This workflow automates the process of identifying and enriching decision-maker contacts from a list of companies. By integrating with Apollo's APIs and Google Sheets, it streamlines lead generation, ensures data accuracy through human verification, and maintains an organized leads database.
üìö Use Case
Ideal for sales and marketing teams aiming to:
Automate the discovery of key decision-makers (e.g., CEOs, CTOs).
Enrich contact information with LinkedIn profiles, emails, and phone numbers.
Maintain an up-to-date leads database with minimal manual intervention.
Receive weekly summaries of newly verified leads.
üß™ Setup
1. Google Sheets Preparation:
Use the following pre-configured Google Sheet: Company Decision Maker Discovery Sheet.
This spreadsheet includes the necessary tabs and columns: Companies, Contacts, and Contacts (Verified).
It also contains a custom onEdit Apps Script function that automatically updates the Status column to Pending whenever the Domain field is modified.
To review or modify the script, navigate to Extensions > Apps Script within the Google Sheet.
2. Credentials Setup:
Configure the following credentials in your n8n instance:
Google Sheets: To read from and write to the spreadsheet.
Slack: To send verification prompts and weekly reports.
Apollo: To access the Organization Search, Organization Enrichment, People Search, and Bulk People Enrichment APIs.
LLM Service (e.g., OpenAI): To generate company summaries and determine departments based on job titles.
3. Workflow Configuration:
Import the workflow into your n8n instance.
Update the nodes to reference the correct Google Sheet and Slack channel.
Ensure that the Apollo and LLM nodes have the appropriate API keys and configurations.
4. Testing the Workflow:
Add a new company entry in the Companies tab of the Google Sheet.
Verify that the workflow triggers automatically, processes the data, and updates the Contacts and Contacts (Verified) tabs accordingly.
Check Slack for any verification prompts and confirm that weekly reports are sent as scheduled."
"AI Speech Coach & Generator using Telegram, Open AI and Gemini",https://n8n.io/workflows/3889-ai-speech-coach-and-generator-using-telegram-open-ai-and-gemini/,"Description
This n8n workflow acts as your personal AI speechwriting coach, directly accessible through Telegram. It listens to your spoken or typed drafts, provides insightful feedback on clarity, engagement, structure, and content, and iteratively refines your message based on your updates. Once you're ready, it synthesizes a brand-new speech or talk incorporating all the improvements and your accumulated ideas. This tool streamlines the speechwriting process, offering on-demand AI assistance to help you craft impactful and well-structured presentations.
How it Works
Input via Telegram: You interact with the workflow by sending your speech drafts or talking points directly to a designated Telegram bot.
AI Feedback: The workflow processes your input using AI models (OpenAI and/or Google Gemini) to analyze various aspects of your speech and provides constructive feedback via Telegram.
Iterative Refinement: You can then send updated versions of your speech to the bot, receiving further feedback to guide your revisions.
Speech Synthesis: When you send the command to ""generate speech,"" the workflow compiles all your previous input and the AI's feedback to synthesize a new, improved speech or talk, which is then sent back to you via Telegram.
New Speech Cycle: By sending the command ""new speech,"" the workflow clears its memory, allowing you to start the process anew for a different topic.
Set Up Steps (Takes Approximatly 5 Minutes)
Step 1: Create a Telegram Bot and Obtain its ID
Open the Telegram application and search for ""BotFather"". Start a chat with BotFather by clicking ""Start"" or sending the /start command. Create a new bot by sending the command /newbot. Follow BotFather's instructions to choose a name and username for your bot. Once your bot is created, BotFather will provide you with an API token. Keep this token secure as it's required to connect your n8n workflow to your bot.
Step 2: Obtain an OpenAI API Key
Go to the OpenAI website (https://platform.openai.com/) and sign up for an account if you don't already have one.
Navigate to the API keys section (usually under your profile settings or a ""Developers"" tab). Click on ""Create new secret key"". Copy the generated API key and store it securely. You will need to provide this key to your n8n workflow to access OpenAI's language models.
Step 3: Obtain a Google Gemini LLM API Key
Go to the Google Cloud AI Platform or Google AI Studio website (the specific platform may vary depending on the current Google AI offerings; search for ""Google AI API""). Sign up or log in with your Google account. Follow the instructions to enable the Gemini API and create an API key. This might involve creating a project if you haven't already. Copy the generated API key and store it securely. You can then configure your n8n workflow to utilize Google Gemini's language models as well.
Customization Options
This n8n workflow offers significant flexibility, below are a few options:
Modify AI prompts to tailor feedback and generation for presentations, storytelling, interviews, sales pitches, academic talks, and creative writing.
Switch the interface from Telegram to Slack, WhatsApp, or even a web interface by replacing the relevant n8n nodes.
Integrate analysis for sentiment, keyword density, pacing (with voice input), and filler word detection by adjusting the workflow.
Connect to external data sources to provide context to the AI for more targeted feedback and generation.
This adaptability allows you to re use this workflow for a wide range of specific use cases and communication environments."
Summarize Microsoft 365 Outage Alerts with ChatGPT and Send to Slack,https://n8n.io/workflows/3353-summarize-microsoft-365-outage-alerts-with-chatgpt-and-send-to-slack/,"Built this for a dedicated Slack outage-notifications channel ‚Äî works well on both desktop and mobile.
This is for:
IT Administrators & small MSPs looking to streamline M365 alerts from one or multiple mailboxes into a single or specific Slack channels
IT Admins who prefer ChatOps over management-by-email
What does it do
Scans for M365 outage alerts emails (every 1 min)
Checks if it impacts a specific user region (if the alert calls it out, countries have to be manually set)
Summarizes the incident using OpenAI o4-mini (cheap model - or you can swap for local Ollama)
Sends a Slack Block to your outage channel with incident link (can be extended)
Deletes the original alert email after successful delivery
Credentials
Outlook:
Create an Outlook credential (OAuth2.0) to point to the mailbox (regular or shared) where M365 service alerts will be received
Slack:
Create a Slack bot credential with access to the slack channel you want updates posted to
OpenAI:
Create a OpenAI credential that has access to the GPT-4O-MINI model.
Recommend you use projects in OpenAI so that you may set a per-project-budget and not impact other projects.
Review this OpenAI documentation for more info on managing Projects in the API portal. Expect this to consume no more than 1-2 cents per month on average.
Setup
Download & import the workflow
Modify the first Outlook block (Check for 365 Service Alert) to use the Outlook credential
Modify the OpenAI block's system prompt to call out the countries your users reside in
ie. ""- Assume the organization has users primarily in the U.S. and Australia. If those regions are affected, state: ""Your users may have been affected."" Otherwise, add: ""No impact expected for your user base."""" ‚Üê swap U.S. & Australia for desired countries
Modify the Slack block (Post outage to Slack) to specify the channel updates will be posted to
Sample Slack Output
Workflow Diagram"
Client Feedback Collector & Analyzer (Form ‚Üí AI Summary ‚Üí Email + Social Draft),https://n8n.io/workflows/3910-client-feedback-collector-and-analyzer-form-ai-summary-email-social-draft/,"What this workflow does
This n8n workflow collects client feedback through a form (Tally, Typeform, or Google Forms) and uses AI to analyze it.
It automatically generates a summary of the positive points, highlights areas for improvement, and drafts a short social media post based on the feedback.
Ideal for:
Freelancers
Customer support teams
Online service providers
Coaches and educators
Setup steps
Connect your form tool to the Webhook node (POST method) and make sure it sends a feedback field.
Add your DeepSeek (or other GPT-compatible) API key to the AI request node.
Configure the email node with your SMTP credentials and desired recipient address.
Replace the Telegram node with Slack, Buffer, or another integration if you prefer.
(Optional) Customize the prompt in the function node for different tone/language.
üïê Estimated setup time: ~15 minutes
üí¨ Sticky notes are included and clearly positioned to guide you.
Technologies used
n8n Webhook node
n8n Function node
DeepSeek Chat or compatible AI API
Email node (SMTP)
Telegram node (or other integration)
Sticky Notes for setup guidance
Use cases
Analyze feedback from onboarding or satisfaction surveys
Create ready-to-publish social media content from real customer praise
Help support or marketing teams act on feedback immediately"
"LINE Messages with GPT: Save Notes, Namecard Data and Tasks",https://n8n.io/workflows/3413-line-messages-with-gpt-save-notes-namecard-data-and-tasks/,"This workflow template, ""Personal Assistant to Note Messages and Extract Namecard Information"" is designed to streamline the processing of incoming messages on the LINE messaging platform. It integrates with powerful tools like Microsoft Teams , Microsoft To Do , OneDrive , and OpenRouter.ai to handle tasks such as saving notes, extracting namecard information, and organizing images. Whether you‚Äôre managing personal productivity or automating workflows for teams, this template offers a versatile and customizable solution.
By leveraging this workflow, you can automate repetitive tasks, improve collaboration, and enhance efficiency in handling LINE messages.
Who Is This Template For?
This template is ideal for:
Professionals: Who want to save important messages, extract data from namecards, or organize images automatically.
Teams: Looking to integrate LINE messages into tools like Microsoft Teams and Microsoft To Do for better collaboration.
Developers: Seeking to build intelligent workflows that process text, images, and other inputs from LINE.
Business Owners: Who need to manage customer interactions, follow-ups, and task tracking efficiently.
What Problem Does This Workflow Solve?
Managing incoming messages on LINE can be time-consuming, especially when dealing with diverse input types like text, images, and namecards. This workflow solves that problem by:
Automatically identifying and routing different message types (text, images, namecards) to appropriate actions.
Extracting structured data from namecards and saving it for follow-up tasks.
Uploading images to OneDrive and saving text messages to Microsoft Teams or Microsoft To Do for easy access.
Sending real-time feedback to users via LINE to confirm that their messages have been processed.
What This Workflow Does
Receive Messages via LINE Webhook:
The workflow is triggered whenever a user sends a message (text, image, or other types) to the LINE bot.
Display Loading Animation:
A loading animation is displayed to reassure the user that their request is being processed.
Route Input Types:
The workflow uses a Switch node to determine the type of input:
Text Starting with ""T"": Adds the message as a task in Microsoft To Do.
Plain Text: Saves the message in Microsoft Teams under a designated channel (e.g., ""Notes"").
Images: Identifies whether the image is a namecard, handwritten note, or other content, then processes accordingly.
Unsupported formats trigger a polite response indicating the limitation.
Process Namecards:
**Images **
If the image is identified as a namecard, the workflow extracts structured data (e.g., name, email, phone number) using OpenRouter.ai and saves it to Microsoft To Do for follow-up tasks.
Save Images to OneDrive:
Images are uploaded to OneDrive, renamed based on their unique message ID, and linked in Microsoft Teams for reference.
Send Feedback via LINE:
The workflow replies to the user with confirmation messages, such as ""[ Task Created ]"" or ""[ Message Saved ].""
Setup Guide
Pre-Requisites
Access to the LINE Developers Console to configure your webhook and bot.
Accounts for Microsoft Teams , Microsoft To Do, and OneDrive with API access.
An OpenRouter.ai account with credentials to access models like GPT-4o.
Basic knowledge of APIs, webhooks, and JSON formatting.
Step-by-Step Setup
Configure the LINE Webhook:
Go to the LINE Developers Console and set up a webhook to receive incoming messages.
Copy the Webhook URL from the Line Webhook node and paste it into the LINE Console.
Remove any ""test"" configurations when moving to production.
Set Up Microsoft Integrations:
Connect your Microsoft Teams, Microsoft To Do, and OneDrive accounts to the respective nodes in the workflow.
Set Up OpenRouter.ai:
Create an account on OpenRouter.ai and obtain your API credentials.
Connect your credentials to the OpenRouter nodes in the workflow.
Test the Workflow:
Simulate sending text, images, and namecards to the LINE bot to verify that all actions are processed correctly.
How to Customize This Workflow to Your Needs
Add More Actions: Extend the workflow to handle additional input types or integrate with other tools.
Enhance Image Processing: Use advanced OCR tools to improve text extraction from complex images.
Customize Feedback Messages: Modify the reply format to include emojis, links, or other formatting options.
Expand Use Cases: Adapt the workflow for specific industries, such as sales or customer support, by tailoring the actions to relevant tasks.
Why Use This Template?
Versatile Automation: Handles multiple input types (text, images, namecards) with ease.
Seamless Integration: Connects LINE messages to popular productivity tools like Microsoft Teams and To Do.
Structured Data Extraction: Extracts and organizes data from namecards, saving time and effort.
Real-Time Feedback: Keeps users informed about the status of their requests with instant notifications."
"Auto-assign Support Tickets with JIRA, Supabase and AI",https://n8n.io/workflows/3395-auto-assign-support-tickets-with-jira-supabase-and-ai/,"This n8n template builds a simple automation to ensure no JIRA issues go unassigned for more than a week to prevent them falling through the cracks. It uses AI to perform searching tasks against a Supabase Vector Store.
This can be one way to help reduce the amount of manual work in managing the issue backlog for busy teams with little effort.
How it works
This template contains 2 separate flows which run continuously via schedule triggers.
The first populates our Supabase vector store with resolved issues within the last day. This helps keep our vector store up-to-date and relevant for the purpose of finding similar issues.
It does this by pulling the latest resolved issues from JIRA and populating the Supabase vectorstore with carefully chosen metadata. This will come in handy later.
The second flow watches for stale, unassigned issues for the purpose of aut-assigning to a relevant team member.
It does this by comparing the stale issue against our vector store of resolved issues with the goal of identifying which team member would have best context regarding the issue.
In a busy team, this may net a few team members as possible candidates to assign. Therefore, we can introduce additional logic to count each team member's assigned, in-progress issues. This is intended to not overload our busiest members.
The team member with the least assigned issues is pressumed to have the most capacity and therefore is assigned. A comennt is left in the issue to notify the team member that they've been auto-assigned due to age of issue.
How to use
Modify the project and interval parameters to match those of your use-case and team members.
Add additional criteria before assigning to a team member eg. department, as required.
Requirements
OpenAI for LLM
JIRA for Issue Management
Supabase for Vector Store
Customising this workflow
Not using JIRA or Supabase? The beauty of these AI templates are these components are entirely interchangeable with competing services. Try Linear and Qdrant instead!
Auto-assigning logic is simplified in this template. Expand criteria as required for your team and organisation. eg. Might be a good idea to pull in annual leave information from HR system to prevent assigning to someone who is on currently on holiday!"
Convert HTML & PDF Files to PNG Images with CustomJS PDF Toolkit,https://n8n.io/workflows/3870-convert-html-and-pdf-files-to-png-images-with-customjs-pdf-toolkit/,"This n8n workflow shows how to convert PDF files into PNG format with the PDF Toolkit from www.customjs.space.
@custom-js/n8n-nodes-pdf-toolkit
Notice
Community nodes can only be installed on self-hosted instances of n8n.
What this workflow does
Generate PDF file from the requested HTML.
Convert the PDF to PNG images.
Use a Code node to handle URLs that point to PDF files.
Convert the PDF to PNG format.
Requirements
Self-hosted n8n instance.
CustomJS API key for converting PDF to PNG.
HTML Data to convert PDF files.
Code node for handling URL that indicates PDF file.
Workflow Steps:
Manual Trigger:
Runs with user interaction.
HTML to PDF:
Request HTML Data.
Convert HTML to PDF.
Request PDF from Code.
Convert PDF to PNG:
Convert the generated PNG from PDF
Usage
Get API key from customJS
Sign up to customJS platform.
Navigate to your profile page
Press ""Show"" button to get API key
Set Credentials for CustomJS API on n8n
Copy and paste your API key generated from CustomJS here.
Design workflow
A Manual Trigger for starting workflow.
HTTP Request Nodes for downloading PDF files.
Code node for handling URL that indicates PDF file.
Convert PDF to PNG.
You can replace logic for triggering and returning results.
For example, you can trigger this workflow by calling a webhook and get a result as a response from webhook. Simply replace Manual Trigger and Write to Disk nodes."
Convert HTML to PDF & Extract Text from PDFs with CustomJS API,https://n8n.io/workflows/3871-convert-html-to-pdf-and-extract-text-from-pdfs-with-customjs-api/,"This n8n workflow illustrates how to convert PDF files into text with the PDF Toolkit from www.customjs.space.
@custom-js/n8n-nodes-pdf-toolkit
Notice
Community nodes can only be installed on self-hosted instances of n8n.
What this workflow does
Change the requested HTML to PDF..
Extract text from the PDF.
Use a Code node to handle URLs that point to PDF files.
Convert the PDF to text.
Requirements
Self-hosted n8n instance.
CustomJS API key for converting PDF to text.
HTML Data to convert PDF files.
Code node for handling URL that indicates PDF file.
Workflow Steps:
Manual Trigger:
Runs with user interaction.
HTML to PDF:
Request HTML Data
Convert HTML to PDF
Convert PDF to Text:
Convert the generated Text from PDF
Usage
Get API key from customJS
Sign up to customJS platform.
Navigate to your profile page
Press ""Show"" button to get API key
Set Credentials for CustomJS API on n8n
Copy and paste your API key generated from CustomJS here.
Design workflow
A Manual Trigger for starting workflow.
HTTP Request Nodes for downloading PDF files.
Code node for handling URL that indicates PDF file.
Convert PDF to Text.
You can replace logic for triggering and returning results.
For example, you can trigger this workflow by calling a webhook and get a result as a response from webhook. Simply replace Manual Trigger and Write to Disk nodes."
OpenAI ImageGen1 via HTTP Request (Edit Image),https://n8n.io/workflows/3858-openai-imagegen1-via-http-request-edit-image/,"Edit an existing image with OpenAI ImageGen1 via API Request
Transform your creative pipeline by letting n8n call OpenAI ImageGen1‚Äôs edit image endpoint, automatically replacing or augmenting parts of any image you supply and returning a brand-new version in seconds. Designers, marketers, and product teams can eliminate repetitive manual edits and test more variations, faster.
Who is this for?
Content creators who need quick, on-brand image tweaks
Marketers running A/B visual tests at scale
Developers exploring the new ImageGen1 API inside low-code automations
Use case / problem solved
Opening design software to mask, fill, or swap objects is slow and error-prone. This workflow feeds an input image plus a prompt to OpenAI ImageGen1, receives the edited output, and passes it on to any service you like‚Äîperfect for bulk-editing product shots, social visuals, or UI mocks.
What this workflow does
Read or receive the source image (Webhook ‚Üí Binary Data).
Call OpenAI ImageGen1 with an HTTP Request node, sending the image and edit prompt.
Parse the JSON response to capture the returned image URL.
Download & hand off the edited file (e.g., upload to S3, post to Slack, or store in Drive).
Setup
Add your OpenAI API key in the API KEY node.
Follow the notes on the workflow for more information.
(Optional) Point the final node to your preferred storage or chat tool.
üìù A sticky note in the workflow summarizes these steps and links to the OpenAI documentation.
How to customize this workflow
Trigger alternatives: Replace the Chat with Google Drive, Airtable, etc.
Chained edits: Loop the output back for successive prompts.
Conditional flows: Add an If node to branch actions by image size or category.
With renamed nodes, color-coded sticky notes, and a concise setup guide, you‚Äôll be editing images via OpenAI ImageGen1 in under five minutes‚Äîno code, maximum creativity."
Multi-Level WordPress Blog Generator with PerplexityAI Research & OpenAI Content,https://n8n.io/workflows/3852-multi-level-wordpress-blog-generator-with-perplexityai-research-and-openai-content/,"WordPress Auto-Blogging Pro v2 - with DEEPER RESEARCH - Advanced Content Automation
(This version is an upgrade from v1: WordPress Auto-Blogging Pro - with DEEP RESEARCH - Content Automation Machine v1)
Take your content automation to unprecedented depths! This v2 template enhances the acclaimed ""WordPress Auto-Blogging Pro - with DEEP RESEARCH"" by introducing Deeper Research, a multi-level research capability designed to generate exceptionally comprehensive, long-form content like detailed articles, reports, and even e-books.
If you need more than just standard blog posts and crave truly authoritative, data-driven content generated automatically, this workflow is built for you. It optionally goes beyond simple topic research by breaking down subjects into sub-topics, and then further into sub-sub-topics, conducting real-time online research (powered by the Research Tool Sub-Flow using PerplexityAI) at each stage. This ensures unparalleled depth and coverage, perfect for tackling complex subjects. Combined with enhanced automation features for batch processing, categorization, tagging, linking, and image generation (using OpenAI's latest image model gpt-image-1 accessed via API), this is the ultimate engine for serious content automation on WordPress.
It operates using a three-flow system: a Trigger Flow acts as a scheduler and dispatcher, a powerful Main Flow executes the complex content generation process, and a dedicated Research Tool Sub-Flow handles interactions with PerplexityAI.
Visit AI Automation Pro‚Äôs website for more powerful n8n templates.
What problem is this workflow solving? / Use cases
Standard content automation often struggles with producing truly in-depth, comprehensive content on complex or rapidly evolving topics. This workflow addresses that challenge by:
Automating Multi-Level Research: Optionally eliminates the manual effort required to deeply research topics, sub-topics, and sub-sub-topics, ensuring content is thorough and well-supported when needed.
Generating Long-Form Content: Enables the creation of detailed articles, comprehensive reports, or even initial e-book drafts automatically.
Scaling Content Production: Efficiently handles lists of topics from Google Sheets, processing them sequentially for reliable, automated content creation over time.
Enhancing SEO Automatically: Integrates best practices like sitemap-based internal linking, external source linking, automatic categorization, tag generation/management, and Table of Contents generation (with ‚ÄúDeeper Research‚Äù articles).
Streamlining Complex Tasks: Combines research (basic or deep), writing, image generation, SEO optimization, publishing, and comprehensive backup (Google Docs, Drive, Sheets) into a single, automated system.
Ideal use cases include creating cornerstone content, detailed guides, industry reports, niche site authority building, and automating the production of deeply researched articles where maximum depth is required.
Who is this for?
This advanced workflow is designed for demanding users who require robust, comprehensive content automation:
Content Marketers & Strategists: Needing to produce authoritative, long-form content at scale, with options for varying research depth.
SEO Professionals: Aiming to build topical authority with well-researched, structured, and highly optimized content featuring strong internal/external linking.
Niche Site Builders & Affiliates: Seeking to dominate specific topics with comprehensive guides and articles.
Agencies: Managing large-scale content production for clients requiring in-depth material.
Researchers & Analysts: Looking to automate the drafting of reports or summaries based on online data.
Power Users of v1: Ready to upgrade to the next level of research depth, flexibility, and automation features.
Unique Features
This v2 workflow introduces significant upgrades and retains the best features of v1:
Conditional Deeper Research: Choose per-topic whether to activate multi-level research (topic -&gt; sub-topic -&gt; sub-sub-topic) for maximum depth or use standard chapter-level research.
Modular Three-Flow Architecture: Robust system with a Trigger Flow for scheduling, a Main Flow for generation, and a dedicated Research Tool Sub-Flow.
Table of Contents Generation: Automatically creates a ToC for improved readability and SEO on long articles.
Latest AI Image Models: Optimized for high-quality images using OpenAI's latest model (gpt-image-1 via API), with easy node setup to switch to alternatives.
Automatic Categorization & Tag Management: AI assigns posts to the most relevant existing WordPress category and creates/assigns relevant tags.
Dual Trigger Options: Use Google Sheets for batch processing lists of topics or n8n's Native Form for single, ad-hoc topic submission.
Batch Topic Processing: Handles multiple topic inputs efficiently via the Trigger Flow loop, processing them sequentially with status tracking.
Advanced External & Internal Linking: Automatically inserts relevant internal links (from sitemap) and external source links (from research).
Comprehensive Backup System: Saves final content in multiple formats and locations: Markdown to Google Docs, Images to Google Drive, Final URLs and details to Google Sheets.
Centralized Control Interface: Google Sheets acts as the primary interface for bulk input, parameter setting (like Have Deeper Research), and status monitoring.
Direct WordPress Integration: Seamlessly publishes content, metadata, tags, categories, and images.
Rate-Limit Aware: Includes configurable Wait nodes to prevent API errors during intensive processing.
Robust Error Catching: Uses 'If' nodes to validate AI outputs and ensure smooth data flow.
How this workflow works
This template utilizes three interconnected n8n flows: a Trigger Flow, a Main Flow, and a Research Tool Sub-Flow, orchestrated primarily via Google Sheets.
1. Trigger Flow (Scheduler & Dispatcher):
Trigger: Activates via a Schedule (e.g., every 10 minutes) to check for new tasks OR via a direct n8n Form submission.
Select Topics: Reads rows from a specific Google Sheet (e.g., Deeper Research - Create Topics -&gt; Create Topic sheet) where the Status column is 'To Do'.
Loop & Dispatch: Processes each 'To Do' topic one by one. For each topic, it executes the Main Flow, passing the topic details and parameters.
Wait: Pauses for a configurable duration (default: 1 hour) after each topic's processing is initiated by the Sub-Workflow, helping manage API limits and pacing.
2. Main Flow (Content Generation Engine):
Trigger: Starts when called by the Trigger Flow.
Setup: Receives topic parameters (Topic, Audience, Style, Word Count, Have Deeper Research flag, etc.) from the Trigger Flow. Updates the topic's Status to 'In Progress' in the Google Sheet.
Internal Link Gathering: Fetches your website's sitemap XML, parses it, and extracts a set number (e.g., up to 50) of internal URLs for later use.
Initial Research & Planning: Executes the Research Tool Sub-Flow to perform initial online research on the main topic. An AI Agent (""Blog Planner"") then uses this research to outline the entire article: Title, Subtitle, Introduction, Slug, Meta Description, Tags, Chapter outlines, and prompts for chapter/featured images.
Conditional Content Creation (Based on Have Deeper Research flag):
If YES: Executes the Deeper Research path: Loops through planned chapters -&gt; Executes Research Tool Sub-Flow for Chapter Research -&gt; AI splits chapter into subchapters -&gt; Loops through subchapters -&gt; Executes Research Tool Sub-Flow for Subchapter Research -&gt; AI (""Subchapter Copywriter"") writes detailed content for each subchapter -&gt; Aggregates subchapters back into the main chapter content.
If NO: Executes the Standard Research path: Loops through planned chapters -&gt; Executes Research Tool Sub-Flow for Main Chapter Research -&gt; AI (""Chapter Copywriter"") writes content for each chapter.
Image Generation & Upload: Generates images (using OpenAI's gpt-image-1 or equivalent via HTTP Request) for each main chapter and a featured image. Resizes images and uploads them to both your WordPress Media Library and a specified Google Drive folder.
Tag Management: Checks if the AI-generated tags exist in WordPress. Creates new tags if necessary and retrieves the IDs for all relevant tags.
Article Assembly: Combines Introduction, finalized chapter content (from either research path), Conclusion, CTA, etc., into a complete article structured in Markdown format (using a Code node).
HTML Conversion: Converts the final Markdown content to HTML suitable for WordPress.
Backup - Google Docs: Saves the final Markdown version of the article as a new document in a specified Google Drive folder. Note: saving to Google Docs only works on ""My Drive"" and will NOT work on a Shared Drive due to Google Drive API restrictions.
Publish to WordPress: Creates a new post on your WordPress site with the Title, HTML content, Slug, assigned Category (auto-detected or default), Tag IDs, and sets the generated Featured Image. Also sets the Meta Description and Post Excerpt.
Backup - Google Sheets: Saves the final details (Published URL, Markdown, HTML, Tags, Meta info, Slug, Excerpt, etc.) to a dedicated sheet (e.g., Final Blogs) within your main Google Sheet workbook.
Final Status Update: Updates the topic's Status to 'Done' in the original Create Topic sheet.
3. Research Tool Sub-Flow (PerplexityAI Interaction):
Trigger: Starts when called by the Main Flow.
Setup: Receives parameters like the research query, model (sonar or sonar-deep-research), system message, and max tokens.
API Call: Makes a POST request to the PerplexityAI API with the provided parameters and authentication.
Response Handling: Extracts the content from the API response.
Output: Returns the research content back to the calling Main Flow.
Requirements
n8n Instance.
OpenAI API Key: Or equivalent credentials for a powerful text generation model (default to GPT-4o) and an image generation model (like gpt-image-1, compatible with OpenAI node or direct HTTP Request).
Perplexity API Key: Or equivalent for an online research tool API (used via HTTP Request node in the Research Tool Sub-Flow).
WordPress Website: REST API enabled, with an Application Password granting permissions to create/edit posts, upload media, manage categories, and manage tags. Needs an accessible post-sitemap.xml.
Google Account: For Google Sheets (trigger, control parameters, status tracking, backup), Google Drive (image/text backup), and Google Docs (Markdown backup).
Setup Step-by-Step
Import Workflows: Download the three .json files (Trigger Flow, Main Flow, Research Tool Sub-Flow) and import them into your n8n instance. Name them clearly (e.g., ""WP Deeper Research - Trigger Flow"", ""WP Deeper Research - Main Flow"", ""WP Deeper Research - Research Tool Sub-Flow"").
Configure Google Sheet:
Clone or create a Google Sheet workbook (e.g., Deeper Research - Create Topics).
Ensure two sheets within it: Create Topic and Final Blogs.
Set up required columns in Create Topic (e.g., Topic, Status, Have Deeper Research, Audience, Style, etc. - match these exactly to the names used in the workflow nodes).
Set up columns in Final Blogs for backup (e.g., Topic Name, Final URL, Markdown, HTML, etc. - match workflow nodes).
Update all Google Sheets nodes in the Trigger Flow and Main Flow to point to the correct Workbook Name, Sheet Name, and ensure column names match.
Link Flows:
In the Trigger Flow, locate the ""Execute Workflow"" node (e.g., ""Execute Workflow - Create A Topic""). Edit this node and select the imported Main Flow from the dropdown.
In the Main Flow, locate all ""Execute Workflow"" nodes that perform research. Edit these nodes and select the imported Research Tool Sub-Flow from the dropdown.
Configure Credentials: In n8n, add credentials for: OpenAI, PerplexityAI, WordPress (using Application Password), Google (ensure Sheets, Drive, Docs permissions).
Connect Workflow Nodes: Open all three workflows. Go through each node requiring authentication (OpenAI, Perplexity HTTP Request, WordPress, Google Sheets, Google Drive, Google Docs) and assign the correct credential. Pay close attention to HTTP Request nodes, ensuring API keys are correctly placed.
Configure Triggers: In the Trigger Flow, configure the Schedule Trigger interval (default 10 mins) or the n8n Form Trigger fields as needed. Ensure only one trigger is active unless you intend both.
Customize Inputs & Parameters: Review key nodes:
Trigger Flow: Adjust Wait time if needed.
Main Flow: Verify Website URL (for sitemap), AI prompts (Planner, Copywriters), default Category ID, internal link limit, image sizes.
Research Tool Sub-Flow: Review default model (sonar), system message, max tokens if needed.
Test Connections: Perform test runs on individual nodes within each workflow (e.g., read Google Sheet, call Perplexity via Research Tool Sub-Flow, connect to WordPress) to verify credentials and configurations.
Initial Test Run: Add one test topic to your Create Topic sheet (set Status to 'To Do', specify 'Yes' or 'No' for Have Deeper Research). Manually trigger the Trigger Flow once (or wait for the schedule). Verify the process completes and check outputs (WordPress post, Google Drive folder, Google Doc, Final Blogs sheet, Create Topic status update).
Activate Workflows: Toggle the ""Active"" switch ON for the Trigger Flow in your n8n instance. The Sub-Workflows should remain inactive as they are triggered by other flows.
Start Automating: Add content topics to the Create Topic sheet (set Status='To Do'), and the scheduled Trigger Flow will pick them up.
Tips for Pros
Control Research Depth: Use the Have Deeper Research column ('Yes'/'No') in your Google Sheet to control the level of research per topic, optimizing for cost and complexity where needed.
AI Model Selection: Experiment with different models in the AI/Image nodes. An alternative to OpenAI‚Äôs image model is Flux.1, which can yield superior images. For research, test Perplexity's sonar-deep-research model (by passing it as a parameter to the Research Tool Sub-Flow if needed) if the standard sonar isn't deep enough.
Wait Times: Adjust the Trigger Flow's ""Wait"" node duration based on your API plan limits, the typical runtime of the Main Flow, and desired posting frequency.
Prompt Engineering: Refine the prompts within the Main Flow's AI nodes (Blog Planner, Chapter/Subchapter Copywriters, Image Prompts) and the Research Tool Sub-Flow's system message to fine-tune the output style, tone, structure, and image relevance.
Sitemap Accessibility: Ensure your post-sitemap.xml is publicly accessible and correctly formatted for the parsing node to work reliably for internal linking.
Important Considerations
API Costs: Deep research (especially the sub-sub-topic level) and generating multiple high-quality images per post can consume significant API credits. Monitor your costs closely. Use the Have Deeper Research flag wisely.
API Rate Limits: Even with Wait nodes, high volume can hit API rate limits. Be aware of your plan limits (OpenAI, Perplexity) and adjust workflow timing (schedule interval, wait time) accordingly.
Sub-Workflow Runtime: The Main Flow can take a considerable amount of time to run, especially with Deeper Research enabled. Factor this into scheduling and expectations.
Tool Dependencies: The workflow relies heavily on the specific functionalities and APIs of OpenAI, Perplexity, Google Workspace, and WordPress. Changes in these platforms might require workflow adjustments.
Initial Setup Complexity: Due to the advanced features, three-flow structure, and multiple integrations, the initial setup requires careful configuration of credentials, node settings (especially Google Sheets names/columns), and workflow linking. Test thoroughly.
For inquiries about this template, please contact AI Automation Pro."
Google Autocomplete Keyword Scraper,https://n8n.io/workflows/3836-google-autocomplete-keyword-scraper/,"Who is this template for?
This workflow template is built for SEO specialists and digital marketers looking to uncover keyword opportunities effortlessly.
It uses Google's autocomplete magic to help you spot what's trending.
How it works
Just give it a keyword.
The workflow then queries Google and collects all autocomplete suggestions by appending every letter from A to Z to your keyword.
Output example with the keyword ""n8n"" :
You can sort these keywords and give them to an LLM to produce entity-enriched text.
Setup instructions
It works right out of the box. üõ†Ô∏è
However, you may want to tweak the output format to better fit your use case.
Exporting the Keywords
You can easily add a node to export the keywords in various ways:
via a webhook
by email
as a file (e.g., saved to Google Drive)
directly to a website
Adapting the Language
Autocomplete results depend on the selected language.
You can change the &hl=en parameter in the Google Autocomplete node.
Replace the ""en"" part with the language code of your choice.
Examples:
&hl=fr ‚Üí French
&hl=es ‚Üí Spanish
&hl=de ‚Üí German"
AI Chat Agent: Dumpling AI + GPT-4o to Auto-Save Local Business Data to Airtable,https://n8n.io/workflows/3826-ai-chat-agent-dumpling-ai-gpt-4o-to-auto-save-local-business-data-to-airtable/,"Who is this for?
This workflow is for digital marketers, small business owners, lead generation agencies, and VAs who need a scalable way to find and store local business leads using AI. It‚Äôs especially useful for teams that want to enrich leads with real-time news insights and save the structured data to Airtable.
What problem is this workflow solving?
Manually researching local businesses and staying up to date with relevant news is time-consuming and inefficient. This automation eliminates that burden by using Dumpling AI chat agents to generate leads and context, GPT-4o to summarize, and Airtable to store everything in one place.
What this workflow does
This AI workflow listens for a manual trigger in n8n and executes the following steps:
Extracts local business leads using a Local Business Agent from Dumpling AI.
Pulls current news related to the business type or location using a News Agent from Dumpling AI.
Uses GPT-4o to combine both responses into a human-readable summary.
Extracts structured lead data like name, category, and city.
Saves the summary and lead data into Airtable for easy follow-up.
Setup
1. Create AI Agents in Dumpling AI
Sign in at Dumpling AI
Create two separate agents:
Local Business Agent: Designed to respond with structured lists of businesses by location and category.
News Agent: Designed to fetch relevant recent news and summaries about a specific industry or region.
After setting up each agent, copy the Agent Key from Dumpling AI. These keys will be required in the headers of your HTTP Request nodes in n8n.
2. Manual Trigger
This workflow begins with a manual trigger inside n8n, Which is the When chat message is recieved.
This makes it easy to test and reuse, especially during setup.
3. Get Local Business Data from Dumpling AI
The first HTTP Request node sends a prompt like List 5 top real estate companies in Atlanta with full address and services.
Include your Local Business Agent Key in the x-agent-key header.
The response will return a structured list of business leads.
4. Get News Context from Dumpling AI
The second HTTP Request node sends a prompt such as Give me the latest news related to the real estate market in Atlanta.
Use your News Agent Key in the header.
This fetches a brief set of recent news summaries relevant to the businesses being researched.
5. Use GPT-4o to Merge and Summarize
The GPT node combines the list of businesses and news into one coherent summary.
You can modify the prompt to output in paragraph format, bullet points, or structured notes.
6. Save Lead to Airtable
The Airtable node sends all structured fields into your selected base and table.
Be sure to connect your Airtable account and confirm the columns match exactly.
How to customize this workflow
Replace the prompt inside the HTTP node to focus on different types of businesses or cities.
Expand the GPT output to include additional lead info like websites, phone numbers, or emails if the agent includes them.
Add a webhook trigger to allow this flow to be run via a chatbot, external app, or button.
Link to HubSpot or another CRM to sync the leads automatically.
Duplicate the process to run for multiple industries in parallel.
Final Notes
You must create and configure your Dumpling AI agents first before running this workflow.
The Agent Keys from Dumpling AI are required in both HTTP Request nodes.
This flow is modular and flexible, ready for deeper CRM integrations.
The manual trigger is great for testing, but you can add a Webhook node to automate it.
This workflow helps you launch an intelligent lead gen process that combines location-targeted business discovery, AI-generated insights, and structured CRM-friendly output, all powered by Dumpling AI and OpenAI."
Connect Retell Voice Agents to Custom Functions,https://n8n.io/workflows/3805-connect-retell-voice-agents-to-custom-functions/,"Overview
This workflow allows you to trigger custom logic in n8n directly from Retell's Voice Agent using Custom Functions.
It captures a POST webhook from Retell every time a Voice Agent reaches a Custom Function node.
You can plug in any logic‚Äîcall an external API, book a meeting, update a CRM, or even return a dynamic response back to the agent.
Who is it for
For builders using Retell who want to extend Voice Agent functionality with real-time custom workflows or AI-generated responses.
Prerequisites
Have a Retell AI Account
A Retell agent with a Custom Function node in its conversation flow (see template below)
Set your n8n webhook URL in the Custom Function configuration (see ""How to use it"" below)
(Optional) Familiarity with Retell's Custom Function docs
Start a conversation with the agent (text or voice)
Retell Agent Example
To get you started, we've prepared a Retell Agent ready to be imported, that includes the call to this template.
Import the agent to your Retell workspace (top-right button on your agent's page)
You will need to modify the function URL in order to call your own instance.
This template is a simple hotel agent that calls the custom function to confirm a booking, passing basic formatted data.
How it works
Retell sends a webhook to n8n whenever a Custom Function is triggered during a call (or test chat).
The webhook includes:
Full call context (transcript, call ID, etc.)
Parameters defined in the Retell function node
You can process this data and return a response string back to the Voice Agent in real-time.
How to use it
Copy the webhook URL (e.g. https://your-instance.app.n8n.cloud/webhook/hotel-retell-template)
Modify the Retell Custom Function webhook URL (see template description for screenshots)
Edit the function
Modify the URL
Modify the logic in the Set node or replace it with your own custom flow
Deploy and test: Retell will hit your n8n workflow during the conversation
Extension Ideas
Call a third-party API to fetch data (e.g. hotel availability, CRM records)
Use an LLM node to generate dynamic responses
Trigger a parallel automation (Slack message, calendar invite, etc.)
üëâ Reach out to us if you're interested in analyzing your Retell Agent conversations."
Compare Different LLM Responses Side-by-Side with Google Sheets,https://n8n.io/workflows/3711-compare-different-llm-responses-side-by-side-with-google-sheets/,"This workflow allows you to easily evaluate and compare the outputs of two language models (LLMs) before choosing one for production.
In the chat interface, both model outputs are shown side by side. Their responses are also logged into a Google Sheet, where they can be evaluated manually or automatically using a more advanced model.
Use Case
You're developing an AI agent, and since LLMs are non-deterministic, you want to determine which one performs best for your specific use case. This template is designed to help you compare them effectively.
How It Works
The user sends a message to the chat interface.
The input is duplicated and sent to two different LLMs.
Each model processes the same prompt independently, using its own memory context.
Their answers, along with the user input and previous context, are logged to Google Sheets.
You can review, compare, and evaluate the model outputs manually (or automate it later).
In the chat, both responses are also shown one after the other for direct comparison.
How To Use It
Copy this Google Sheets template (File > Make a Copy).
Set up your System Prompt and Tools in the AI Agent node to suit your use case.
Start chatting! Each message will trigger both models and log their responses to the spreadsheet.
Note: This version is set up for two models. If you want to compare more, you‚Äôll need to extend the workflow logic and update the sheet.
About Models
You can use OpenRouter or Vertex AI to test models across providers.
If you're using a node for a specific provider, like OpenAI, you can compare different models from that provider (e.g., gpt-4.1 vs gpt-4.1-mini).
Evaluation in Google Sheets
This is ideal for teams, allowing non-technical stakeholders (not just data scientists) to evaluate responses based on real-world needs.
Advanced users can automate this evaluation using a more capable model (like o3 from OpenAI), but note that this will increase token usage and cost.
Token Considerations
Since each input is processed by two different models, the workflow will consume more tokens overall.
Keep an eye on usage, especially if working with longer prompts or running multiple evaluations, as this can impact cost."
"Analyze Client Transcripts & Route Feedback with GPT-4o Mini, HubSpot, and Gmail",https://n8n.io/workflows/3706-analyze-client-transcripts-and-route-feedback-with-gpt-4o-mini-hubspot-and-gmail/,"Who is this for?
This workflow is designed for Customer Satisfaction Managers (CSM), sales professionals, and operations managers who need to automate the analysis of client transcripts, save summarized notes to HubSpot, and route relevant feedback to the appropriate departments via email.
What problem is this workflow solving? / Use Case
Manually processing client conversations, extracting key insights, and distributing them to the right teams is time-consuming and error-prone. This workflow automates:
Transcript analysis using AI (OpenAI) to identify relevant content.
HubSpot integration to log meeting notes against client records.
Email routing to ensure feedback reaches the correct departments (e.g., support, sales, product, admin).
What this workflow does
Input Transcript: Accepts a client conversation transcript (e.g., from emails, calls, or chats).
HubSpot Sync:
Searches for the client‚Äôs HubSpot ID using their email.
Uploads a summarized version of the conversation as meeting notes.
AI-Powered Routing:
Uses an OpenAI model to analyze the transcript and categorize content by department.
Triggers emails (via Gmail) to route feedback to the relevant teams.
Form Completion: Ends the workflow with optional user confirmation.
Setup
Prerequisites:
n8n instance (cloud or self-hosted).
HubSpot API credentials (for contact lookup and notes upload).
OpenAI API key (for transcript analysis).
Gmail account (for sending emails).
Configuration:
Replace placeholder nodes (e.g., HubSpot, OpenAI, Gmail) with your authenticated accounts.
Define email templates and recipient addresses for routing.
Adjust the OpenAI prompt to match your categorization criteria (e.g., ""support,"" ""billing"").
How to customize this workflow to your needs
Transcript Sources: Extend the workflow to pull transcripts from other sources (e.g., Zoom, Slack).
Departments: Modify the routing logic to include additional teams or conditions.
Notifications: Add Slack/MS Teams alerts for urgent feedback.
Error Handling: Introduce retries or fallback actions for failed HubSpot/Gmail steps."
Backlink Monitoring Automation with Google Sheets + DataForSEO,https://n8n.io/workflows/3685-backlink-monitoring-automation-with-google-sheets-dataforseo/,"What This Workflow Does
This n8n workflow reads backlinks from a Google Sheet, sends each one to the DataForSEO On-Page API, and checks:
Whether the backlink is still live on the target page
Whether it's dofollow or nofollow
Whether it's missing (i.e., lost)
The result is then written back to the same Google Sheet under a Status column.
Your result will look like this:
Step-by-Step Setup Instructions
Add your DataForSEO and Google Sheets credentials in n8n
Make sure your Google Sheet has these columns: Backlink URL, Landing page, and Status
Click the Test Workflow button to check a batch of backlinks
Workflow Breakdown
Trigger: Manual test start
Read Data: Pulls backlink URLs and target pages from Google Sheets
Format URLs: Extracts domain from URL
Send POST Request to DataForSEO: Triggers a crawl on the backlink URL
Wait 20 seconds: Allows crawl to finish
Fetch Link Results: Retrieves backlink data from DataForSEO
Validate Backlink: Checks if the backlink is live, and whether it‚Äôs dofollow
Update Google Sheets: Logs the status as Live, Lost, or Lost (Nofollow)"
Automated Property Lead Generation with BatchData and CRM Integration,https://n8n.io/workflows/3665-automated-property-lead-generation-with-batchdata-and-crm-integration/,"How It Works
This N8N workflow creates an automated system for discovering high-potential real estate investment opportunities. The workflow runs on a customizable schedule to scan the market for properties that match your specific criteria, then alerts your team about the most promising leads.
The process follows these steps:
Connects to BatchData API on a regular schedule to search for properties matching your parameters
Compares new results with previous scans to identify new listings and property changes
Applies intelligent filtering to focus on high-potential opportunities (high equity, absentee owners, etc.)
Retrieves comprehensive property details and owner information for qualified leads
Delivers formatted alerts through multiple channels (email and Slack/Teams)
Each email alert includes detailed property information, owner details, equity percentage, and a direct Google Maps link to view the property location. The workflow also posts concise notifications to your team's communication channels for quick updates.
Who It's For
This workflow is designed for:
Real Estate Investors: Find off-market properties with high equity and motivated sellers
Real Estate Agents: Identify potential listing opportunities before they hit the market
Property Acquisition Teams: Streamline the lead generation process with automated scanning
Real Estate Wholesalers: Discover properties with significant equity spreads for potential deals
REITs and Property Management Companies: Monitor market changes and expansion opportunities
The workflow is especially valuable for professionals who want to:
Save hours of manual market research time
Get early notifications about high-potential properties
Access comprehensive property and owner information in one place
Focus their efforts on the most promising opportunities
About BatchData
BatchData is a powerful property data platform for real estate professionals. Their API provides access to comprehensive property and owner information across the United States, including:
Property details (bedrooms, bathrooms, square footage, year built, etc.)
Valuation and equity estimates
Owner information (name, mailing address, contact info)
Transaction history and sales data
Foreclosure and distressed property status
Demographic and neighborhood data
The platform specializes in providing accurate, actionable property data that helps real estate professionals make informed decisions and identify opportunities efficiently. BatchData's extensive database covers millions of properties nationwide and is regularly updated to ensure data accuracy.
The API's flexible search capabilities allow you to filter properties based on numerous criteria, making it an ideal data source for automated lead generation workflows like this one."
"üí¨ Daily WhatsApp Group Summarizer ‚Äì GPT-4o, Google Sheets & Evolution API",https://n8n.io/workflows/3646-daily-whatsapp-group-summarizer-gpt-4o-google-sheets-and-evolution-api/,"Hey! I‚Äôm Amanda ‚ù§Ô∏è
I made this little workflow with care for people like you who are part of busy WhatsApp groups and want a simple way to keep track of everything.
It connects to Evolution API, collects all the group messages throughout the day, stores them in Google Sheets, and uses GPT-4o to generate a daily summary. The summary is saved as a document in Google Drive ‚Äî ready to read, share, or archive.
It‚Äôs perfect for teams, communities, classes, or any group that talks a lot but doesn‚Äôt want to miss important info.
What it does
Collects WhatsApp group messages using Evolution API
Saves the messages in Google Sheets (organized by date)
Creates a clean, structured summary using GPT-4o
Saves the summary in Google Drive as a doc
Can run daily at a set time (fully automated)
How to set it up
Connect your Evolution API and provide the group ID
Use this Google Sheets template
Connect Google Sheets and Google Drive in n8n
Add your OpenAI API key
(Optional) Adjust the AI prompt for a custom tone or structure
‚úÖ Works on both n8n Cloud and Self-hosted
üîê All credentials stay safe in n8n
Want to customize this for your group or business?
‚ù§Ô∏è Buy Workflows: https://iloveflows.gumroad.com
üí¨ Hire My Services: +5517991557874 (WhatsApp)
Tradu√ß√£o em Portugu√™s (pt-br):
Oi! Eu sou a Amanda üí¨
Se voc√™ participa de grupos movimentados no WhatsApp e quer transformar tudo isso em resumos di√°rios organizadinhos, esse fluxo foi feito com todo carinho pra voc√™!
Ele conecta com a API da Evolution, coleta as mensagens trocadas em grupos, armazena tudo no Google Sheets, e no fim do dia gera um resumo completo usando GPT-4o. Esse resumo √© salvo como documento no seu Google Drive ‚Äî pronto pra ser lido, compartilhado ou arquivado.
Ideal pra equipes, comunidades, projetos colaborativos ou at√© grupos de estudos üìö
O que o fluxo faz
Monitora e salva conversas do grupo no Google Sheets
Gera resumos di√°rios com IA (formato estruturado e pronto pra leitura)
Salva o resumo como documento no seu Google Drive
Funciona com qualquer grupo conectado √† sua conta Evolution
Pode ser agendado pra rodar automaticamente todo fim de dia
Como configurar
Conecte sua API Evolution e informe o ID do grupo
Use essa planilha modelo para armazenar as mensagens
Conecte sua conta do Google (Sheets + Drive)
Adicione sua chave da OpenAI
Personalize o prompt do resumo (opcional)
‚úÖ Compat√≠vel com n8n Cloud e Auto-hospedado
üîê Tudo seguro, simples e sem complica√ß√µes
Quer adaptar esse fluxo pro seu sistema?
‚ù§Ô∏è Buy Workflows: https://iloveflows.gumroad.com
üí¨ Hire My Services: +5517991557874 (WhatsApp)"
Build your own SQLite MCP server,https://n8n.io/workflows/3632-build-your-own-sqlite-mcp-server/,"This template is for Self-Hosted N8N Instances only.
This n8n demonstrates how to build a simple SQLite MCP server to perform local database operations as well as use it for Business Intelligence.
This MCP example is based off an official MCP reference implementation which can be found here -https://github.com/modelcontextprotocol/servers/tree/main/src/sqlite
How it works
A MCP server trigger is used and connected to 5 tools: 2 Code Node and 3 Custom Workflow.
The 2 Code Node tools use the SQLLite3 library and are simple read-only queries and as such, the Code Node tool can be simply used.
The 3 custom workflow tools are used for select, insert and update queries as these are operations which require a bit more discretion.
Whilst it may be easier to allow the agent to use raw SQL queries, we may find it a little safer to just allow for the parameters instead. The custom workflow tool allows us to define this restricted schema for tool input which we'll use to construct the SQL statement ourselves.
All 3 custom workflow tools trigger the same ""Execute workflow"" trigger in this very template which has a switch to route the operation to the correct handler.
Finally, we use our Code nodes to handle select, insert and update operations. The responses are then sent back to the the MCP client.
How to use
This SQLite MCP server allows any compatible MCP client to manage a SQLite database by supporting select, create and update operations. You will need to have a SQLite database available before you can use this server.
Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop
Try the following queries in your MCP client:
""Please create a table to store business insights and add the following...""
""what business insights do we have on current retail trends?""
""Who has contributed the most business insights in the past week?""
Requirements
SQLite for database.
MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download
Customising this workflow
If the scope of schemas or tables is too open, try restrict it so the MCP serves a specific purpose for business operations. eg. Confine the querying and editing to HR only tables before providing access to people in that department.
Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!"
Automate Hyper-Personalized Outreach at Scale With Bright Data and LLMs,https://n8n.io/workflows/3561-automate-hyper-personalized-outreach-at-scale-with-bright-data-and-llms/,"LinkedIn Enrichment & Ice Breaker Generator
For SDRs, growth marketers, and founders looking to scale personalized outreach.
This workflow enriches LinkedIn profile data using Bright Data and generates AI-powered ice breakers using Claude (Anthropic).
It automates research and messaging to help you connect smarter and faster ‚Äî without manual effort.
üß© How It Works
This workflow combines Google Sheets, [Brigt Data](Bright Data), and Claude (Anthropic) to fully automate your outreach research:
Trigger
Manually trigger the workflow or run it on a schedule (via Manual Trigger or Schedule Trigger).
Read Input Sheet
Fetches rows from a Google Sheet. Each row must contain at least a Linkedin_URL_Person and row_number.
Prepare Input
Formats each row for Bright Data‚Äôs API using Set and SplitInBatches nodes.
Enrich Profile (Bright Data API)
Sends LinkedIn URLs to Bright Data‚Äôs Dataset API via HTTP Request.
Waits for snapshot to be ready using polling logic with Wait, If, and Snapshot Progress nodes.
Once ready, retrieves the enriched profile data including:
Name
City
Current company
About section
Recent posts
Update Sheet with Profile Data
Writes the retrieved enrichment data into the corresponding row in Google Sheets (via row_number).
Generate Ice Breaker (Claude AI)
Sends enriched profile content to Claude (Anthropic) using a custom prompt.
Focuses on recent posts for crafting relevant, respectful, 1‚Äì4-line ice breakers.
Update Sheet with Ice Breaker
Writes the generated ice breaker to the Ice Breaker 1 column in the original row.
‚úÖ Requirements
To use this workflow, you must have the following:
Google Sheets
A Google account
A Google Sheet with at least one sheet/tab containing:
Column: Linkedin_URL_Person
Column: row_number (used for mapping input and output rows)
Bright Data
A Bright Data account with access to the Dataset API
An active dataset that accepts LinkedIn URLs
API key with Dataset API access
Anthropic Claude
An Anthropic API key (for Claude 3.5 Haiku or other Claude models)
n8n Environment
Access to HTTP Request, Set, Wait, SplitInBatches, If, and Google Sheets nodes
Access to Claude integration (via LangChain nodes: @n8n/n8n-nodes-langchain)
Credential manager properly configured with:
Google Sheets OAuth2 credentials
Bright Data API key
Anthropic API key
‚öôÔ∏è Setup Instructions
Step 1: Copy the Google Sheets Template
üìÑ Click here to make a copy
Fill the Linkedin_URL_Person column with LinkedIn profile URLs you want to enrich
Do not modify headers or add filters to the sheet
Leave other columns (name, city, about, posts, ice breaker) blank ‚Äî the workflow fills them
Step 2: Connect Your Accounts in n8n
Google Sheets: Create a credential under Google Sheets OAuth2 API
Bright Data: Add your API key as a credential under HTTP Request (Authorization header)
Anthropic: Create a credential for Anthropic API with your Claude key
Step 3: Import and Configure the Workflow
Import the workflow into your n8n instance.
In each Google Sheets node:
Select the copied Google Sheet
Select the correct tab (usually input or Sheet1)
In the HTTP Request node to Bright Data:
Paste your Bright Data dataset ID
In the Claude prompt node:
Optionally adjust the tone and length of the ice breaker prompt
Step 4: Run the Workflow
Test it using the Manual Trigger node
For daily automation, enable the Schedule Trigger and configure interval settings
Watch your Google Sheet populate with enriched data and tailored ice breakers
üß† Tips & Best Practices
Bright Data Delay: Snapshots may take time. The workflow polls the status until complete.
Retry Protection: If and Wait nodes avoid infinite loops by checking snapshot status.
Mapping via row_number: Critical to ensure data is updated in the right row.
Prompt Engineering: You can fine-tune Claude's behavior by editing the text prompt.
üßæ Output Example
Once complete, each row in your Google Sheet will contain:
Linkedin_URL_Person Name City Company Recent Post Ice Breaker
linkedin.com/... Jane Doe NYC ACME Corp ‚ÄúWhy AI should replace meetings‚Äù ""Loved your post about AI and meetings ‚Äî finally someone said it!""
üí¨ Support & Feedback
Questions? Want to tweak the prompt or expand the enrichment?
üìß Email: Yaron@nofluff.online
üì∫ YouTube: @YaronBeen
üîó LinkedIn: linkedin.com/in/yaronbeen"
Get notified when your competitors change their pricing with Airtop and Slack,https://n8n.io/workflows/3480-get-notified-when-your-competitors-change-their-pricing-with-airtop-and-slack/,"About the Automation
Staying on top of competitor pricing changes can be a full-time job. Manual price tracking is time-consuming and prone to errors, especially when dealing with complex pricing structures and multiple subscription tiers. Paid competitor price monitoring tools like Competera, Visualping and Fluxguard can be expensive. What if you could automate this process and get instant alerts when competitors adjust their pricing?
How to easily monitor competitor pricing
With this automation, you'll learn how to set up automated price monitoring system using Airtop's built-in node in n8n. By the end, your system will automatically track competitor pricing changes and notify you of any modifications.
What You'll Need
A free Airtop API Key
Google Sheets account with a copy of this sheet
URLs of competitors' pricing pages
Understanding the Process
This automation continuously monitors competitor pricing pages and compares them against your baseline data. The workflow:
Tracks all different pricing plans (monthly, yearly, etc.).
Monitors feature changes across different tiers.
Detects and logs pricing structure modifications.
Alerts you via Slack when changes are detected
Setting Up Your Automation
We've created a ready-to-use blueprint for seamless price monitoring. Here's how to get started:
Connect your Google Sheets
Set up your Airtop API connection
Define update frequency
Customization Options
Enhance the basic template with these popular modifications:
Add other notification channels (Email, Telegram, etc.).
Include feature comparison tracking.
Set up threshold-based alerts for significant price changes
Track historical pricing trends
Real-World Applications
Case Study 1: A B2B SaaS company can use this automation to track competitors' pricing changes. When they identify a market-wide pricing shift, they can adjust their strategy proactively within minutes.
Case Study 2: An online Ecommerce retailer automates monitoring of 100+ competitor products, maintaining optimal pricing positions and increasing profit margins.
Best Practices
To ensure accurate tracking:
Include detailed baseline data for each pricing tier
Specify both monthly and annual pricing clearly
List all features included in each plan
Update your baseline data whenever you verify changes
Include any promotional pricing or special offers
Document currency and regional variations if applicable
Example Structure in Google Sheets:
Competitor: Acme Tools
Basic Plan:
- Monthly: $29
- Annual: $290 ($24.17/mo)
- Features: 5 users, 10GB storage, basic support
Pro Plan:
- Monthly: $79
- Annual: $790 ($65.83/mo)
- Features: 20 users, 50GB storage, priority support
What's Next?
After setting up your price monitoring automation, consider the following:
Creating automated competitive analysis reports
Setting up market trend analysis
Implementing automatic pricing recommendations
Expanding monitoring to feature changes
Happy monitoring!"
"MCP AI Agent Google Calendar - Create, Update & Manage Events",https://n8n.io/workflows/3589-mcp-ai-agent-google-calendar-create-update-and-manage-events/,"Hi! I‚Äôm Amanda :) üíñ
I created this sweet little workflow with lots of love and care, just for you who wants to manage your Google Calendar in a smart and gentle way üíå
This AI-powered agent connects with MCP (Multi-Channel Protocol) and understands natural language like ‚Äúbook a meeting tomorrow at 3pm‚Äù, ‚Äúreschedule my call to Monday‚Äù, or ‚Äúwhat events do I have on Wednesday?‚Äù ‚Äî and it does everything quietly and beautifully in your calendar üß∏
üí° What this lovely agent does
üóìÔ∏è Creates new events in your Google Calendar (with or without guests)
‚úèÔ∏è Updates existing events with new times or details
üßπ Deletes events you no longer need
üîç Retrieves scheduled events by date
ü§ñ Works through GPT-4o or any AI via MCP Agent
‚öôÔ∏è How to set it up (gently and step-by-step)
Webhook is ready for MCP messages at POST /mcp/calendar
Connect your Google Calendar account using OAuth2 inside n8n
Link it to your favorite AI tool (like LangChain, Typebot, etc.) that can talk to the MCP agent
All details like title, time, date, and guests are parsed automatically from natural language üí´
‚ú® Requirements
Google Calendar connected to n8n
n8n instance (Cloud or Self-hosted ‚Äî both are supported!)
An AI interface that talks to the MCP agent (like LangChain or Typebot)
MCP Trigger API set up in your n8n environment
This agent is perfect for therapists, consultants, coaches, small teams, or anyone who wants to keep their calendar flowing naturally and peacefully with a little help from AI üíÜ‚Äç‚ôÄÔ∏èüß†
Want something customized just for you?
Chat with me üíªüíõ Chat via WhatsApp (+55 17 99155-7874)
.
.
Tradu√ß√£o para Portugu√™s:
üíñ Oi! Eu sou a Amanda :)
Esse fluxinho aqui foi feito com muito cuidado pra voc√™ que quer automatizar sua agenda do Google Calendar de forma inteligente, simples e com muito carinho üíå
Ele funciona como um agente que conversa com outro sistema de IA (via MCP) e consegue entender pedidos como ‚Äúagende uma consulta amanh√£ √†s 15h‚Äù, ‚Äúremarque a reuni√£o para segunda‚Äù, ou ‚Äúquais eventos tenho na quarta?‚Äù ‚Äî tudo isso feito direto no seu calend√°rio, sem voc√™ precisar abrir nada üß∏
üí° O que ele faz com amor
üóìÔ∏è Cria eventos no seu Google Calendar (com ou sem convidados)
‚úèÔ∏è Atualiza eventos j√° existentes com novos hor√°rios
üßπ Exclui eventos que voc√™ n√£o precisa mais
üîç Busca seus compromissos com base em datas espec√≠ficas
‚ù§Ô∏è Tudo isso com suporte ao modelo GPT-4o via agente MCP
‚öôÔ∏è Como configurar (bem facinho, prometo!)
Conecte o webhook: o endpoint do MCP j√° vem prontinho com o caminho mcp/calendar
Conecte sua conta do Google Calendar usando o OAuth2 no n8n
Adicione a integra√ß√£o do MCP Trigger com seu sistema de IA (LangChain, Typebot, etc.)
Todos os campos como t√≠tulo, data, hora e convidados s√£o extra√≠dos automaticamente via IA üí´
‚ú® Requisitos
Conta Google Calendar integrada ao n8n
Inst√¢ncia n8n (Cloud ou Self-hosted)
Integra√ß√£o com uma IA que converse com o MCP Agent (como LangChain)
Acesso √† API MCP ativado no n8n
Esse agente √© ideal pra psic√≥logos, consultores, times de atendimento, terapeutas ‚Äî ou qualquer pessoa fofa que quer deixar sua agenda fluindo sozinha, com a ajuda de um toque de intelig√™ncia üíÜ‚Äç‚ôÄÔ∏èüß†
Quer algo feito s√≥ pra voc√™?
Fala comigo com carinho üíªüíõ Falar no WhatsApp (+55 17 99155-7874)"
Automate Purchase Order Form Submissions from Outlook Excel Attachments with AI,https://n8n.io/workflows/3545-automate-purchase-order-form-submissions-from-outlook-excel-attachments-with-ai/,"This n8n template imports purchase order submissions from Outlook and converts attached purchase order forms in XLSX format into structured output.
Data entry jobs with user-submitted XLSX forms are time consuming, incredibly mundane but necessary tasks which in likelihood are inherited and critical to business operation.
While we could dream of system overhauls and modernisation, the fact is that change is hard. There is another way however - using n8n and AI! N8N offers an end-to-end solution to parse XLSX form attachments using LLM-powered OCR and send the extracted output to your ERP or otherwise.
How it works
An Outlook trigger is used to watch for incoming purchase order forms submitted via a shared inbox.
The email attachment for the submission is a form in xlsx format - like this one Purchase Order Example - which is imported into the workflow.
The 'Extract from File' node is used with the 'code' node to convert the xlsx file to markdown. This is so our LLM can understand it.
The Information Extractor node is used to read and extract the relevant purchase order details and line items from the form.
A simple validation step is used to check for common errors such as missing PO number or the amounts not matching up. A notification is automated to reply to the buyer if so.
Once validation passes, a confirmation is sent to the buyer and the purchase order structured output can be sent along to internal systems.
How to use
This template only works if you're expecting and receiving forms in XLSX format. These can be invoices, request forms as well as purchase order forms.
Update the Outlook nodes with your email or other emails as required.
What's next? I've omitted the last steps to send to an ERP or accounting system as this is dependent on your org.
Requirements
Outlook for Emails
Check out how to setup credentials here: https://docs.n8n.io/integrations/builtin/credentials/microsoft
OpenAI for LLM document understanding and extraction.
Customising the workflow
This template should work for other Excel files. Some will be more complicated than others so experiment with different parsers and extraction tools and strategies.
Customise the Information Extractor Schema to pull out the specific data you need. For example, capture any notes or comments given by the buyer."
Automated LinkedIn Job Hunter: Get Your Best Daily Job Matches by Email,https://n8n.io/workflows/3543-automated-linkedin-job-hunter-get-your-best-daily-job-matches-by-email/,"Overview
This n8n template automates the tedious process of searching for jobs on LinkedIn. By integrating with tools for web scraping and leveraging AI (Google Gemini) for intelligent matching, this workflow delivers a curated list of the top 5 most relevant job opportunities published within past 24h directly to your inbox daily, based on your unique resume and preferences.The cost is only 0.1 USD per day and there is no subscription needed.
Who is this for?
This template is ideal for:
Active job seekers wanting to save time and effort.
Professionals looking to discreetly monitor relevant new opportunities.
Individuals seeking a highly personalized job feed tailored to their resume and preferences.
Anyone overwhelmed by manual job searching on LinkedIn.
What is Included:
n8n Workflow Template: The complete workflow file (.json) ready to import into your n8n instance.
Video Guidance: A step-by-step video walkthrough showing you exactly how to set up and configure the workflow.
What problem is this workflow solving?
Finding the right job on LinkedIn can be overwhelming and time-consuming. Sifting through hundreds of listings, tailoring searches, and checking daily takes significant effort. This workflow solves the problem of manual, repetitive job searching by automating the discovery and filtering process, ensuring you see the most relevant opportunities without the daily grind and reducing the risk of missing out on your ideal role.
What this workflow does:
This workflow automates the following steps:
Scheduled Job Fetching: Runs automatically (default: daily at 8 AM) to find the latest jobs.
Resume Processing: Downloads your resume (PDF) from Google Drive and extracts the text content.
Targeted LinkedIn Scraping: Uses Apify to scrape recent job listings from LinkedIn based on your custom search URL.
AI-Powered Matching: Employs an AI agent (Google Gemini) to analyze scraped jobs against your resume text and specified preferences.
Top 5 Ranking & Selection: Identifies and ranks the opportunities, selecting the 5 best matches for you.
Personalized Email Reporting: Generates and sends a detailed HTML email containing the top 5 jobs, including company name, job title, industry, a personalized reason for the match, and a direct application link.
Setup:
Follow these steps to configure the workflow:
Core Connections:
Connect your Google Drive and Gmail accounts to n8n via the Credentials section.
Ensure your n8n environment has access/credentials configured for the AI model used by the AI Agent node (e.g., Google Gemini).
Apify Integration:
Sign up for an Apify account (apify.com) and obtain your API key.
Action: In the Input node, paste your Apify API Key into the Value field for the ApifyAPIKey assignment.
Resume Setup:
Upload your current resume in PDF format to your Google Drive.
Action: Find the File ID of the uploaded resume in Google Drive (part of the shareable link). Paste this File ID into the File ID parameter within the DownloadResume (Google Drive) node.
LinkedIn Search Definition:
Go to LinkedIn Jobs (www.linkedin.com/jobs/search/) using an incognito/private browser window to ensure you get a public URL.
Apply all your desired filters (keywords, location, date posted, job type, industry, etc.).
Copy the complete URL from your browser's address bar.
Action: In the ScrapeLinkedin (HTTP Request) node, navigate to the Body > JSON parameter. Replace the example URL within the urls array [ ""YOUR_LINKEDIN_SEARCH_URL_HERE"" ] with the URL you just copied. Make sure the URL is enclosed in double quotes.
Personalization Inputs:
Action: Go to the Input node:
In the Preference assignment, replace the example text in the Value field with your detailed job preferences (e.g., ""Seeking remote Data Scientist roles in SaaS companies with less than 1000 employees, strong preference for Python/ML focus"").
In the EmailAddressToReceiveJobRecommendations assignment, enter the email address where you want to receive the daily job list in the Value field.
Email Sender Configuration:
Action: In the Email the top job recommendations (Gmail) node, ensure the correct Gmail credential (the account you want to send emails from) is selected.
How to customize this workflow:
Run Schedule: Modify the settings in the Schedule Trigger node to change the time or frequency (e.g., twice daily, weekly).
Job Search Criteria: Update the LinkedIn search URL in the ScrapeLinkedin node whenever you want to target different roles, industries, or locations.
Matching Preferences: Refine the text in the Preference field within the Input node to guide the AI's matching process more accurately.
AI Behavior: Advanced users can adjust the system prompt within the AI Agent: Find Best-matched jobs node to change how the AI analyzes or presents information (ensure the output structure still matches the Structured Output Parser and email node expectations).
Number of Jobs Scraped: Change the count value (e.g., from 100) in the JSON Body of the ScrapeLinkedin node. Note that higher numbers may increase Apify costs/usage.
Number of Jobs Emailed: To change the number of recommendations (e.g., top 3 or top 10), you'll need to:
Modify the AI prompt in the AI Agent: Find Best-matched jobs node to request the desired number.
Adjust the Structured Output Parser node's example/schema if needed.
Update the HTML code in the Email the top job recommendations node to correctly loop through and display the new number of jobs.
Email Appearance: Edit the HTML within the Message field of the Email the top job recommendations node to customize the email's style, colours, or layout.
Category:
Job Search, Automation, AI, Productivity, Career Management"
"Create AI-Ready Vector Datasets for LLMs with Bright Data, Gemini & Pinecone",https://n8n.io/workflows/3542-create-ai-ready-vector-datasets-for-llms-with-bright-data-gemini-and-pinecone/,"Who this is for?
This workflow enables automated, scalable collection of high-quality, AI-ready data from websites using Bright Data‚Äôs Web Unlocker, with a focus on preparing that data for LLM training. Leveraging LLM Chains and AI agents, the system formats and extracts key information, then stores the structured embeddings in a Pinecone vector database.
This workflow is tailored for:
ML Engineers & Researchers building or fine-tuning domain-specific LLMs.
AI Startups needing clean, structured content for product training.
Data Teams preparing knowledge bases for enterprise-grade AI apps.
LLM-as-a-Service Providers sourcing dynamic web content across niches.
What problem is this workflow solving?
Training a large language model (LLM) requires vast amounts of clean, relevant, and structured data. Manual collection is slow, error-prone, and lacks scalability.
This workflow:
Automatically extracts web data from specified URLs.
Bypasses anti-bot measures using Bright Data‚Äôs Web Unlocker.
Formats, cleans, and transforms raw content using LLM agents.
Stores semantically searchable vectors in Pinecone.
Makes datasets AI-ready for fine-tuning, RAG, or domain-specific training.
What this workflow does
This workflow automates the process of collecting, cleaning, and vectorizing web content to create structured, high-quality datasets that are ready to be used for LLM (Large Language Model) training or retrieval-augmented generation (RAG).
Web Crawling with Bright Data Web Unlocker.
AI Information Extraction and Data Formatting.
AI Data Formatting to produce a JSON structured data.
Persistence in Pinecone Vector DB.
Handle Webhook notification of structured data.
Setup
Sign up at Bright Data.
Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions.
In n8n, configure the Header Auth account under Credentials (Generic Auth Type: Header Authentication).

The Value field should be set with the
Bearer XXXXXXXXXXXXXX. The XXXXXXXXXXXXXX should be replaced by the Web Unlocker Token.
A Google Gemini API key (or access through Vertex AI or proxy).
Update the LinkedIn URL by navigating to the Set LinkedIn URL node.
Update the Set Fields - URL and Webhook URL node with the URL for web data extraction and the Webhook notification URL.
How to customize this workflow to your needs
Set Your Target URLs. Target sites that are high-quality, domain-specific, and relevant to your LLM's purpose.
Adjust Bright Data Web Unlocker Settings. Geo-location, Headers / User-Agent strings, Retry rules and proxies.
Modify the Information Extraction Logic. Change prompts to extract specific attributes. Use structured templates or few-shot examples in prompts.
Swap the Embedding Model. Use OpenAI, Hugging Face or other your own hosted embedding model API.
Customize Pinecone Metadata Fields. Store extra fields in Pinecone for better filtering & semantic querying.
Add Data Validation or Deduplication. Skip duplicates or low-quality content."
üöÄ Instagram Reels Automation - Turn YouTube Videos into Viral Instagram Reels ‚ú®,https://n8n.io/workflows/3367-instagram-reels-automation-turn-youtube-videos-into-viral-instagram-reels/,"Automatically Create and Post Instagram Reels from Podcasts and Other Videos Using AI & Spikes Studio (Simplified)
Transform podcasts and other videos into engaging, fully optimized Instagram Reels with this all-in-one automation solution‚Äîdesigned for content creators who love repurposing dynamic clips.
Overview
This automation transforms YouTube videos into engaging, professional-grade Instagram Reels using AI-powered editing. Perfect for content creators, podcasters, marketers, and brands, the workflow extracts your most captivating moments, enhances them with dynamic captions, transitions, and effects, and auto-schedules them for Instagram‚Äîall without manual editing or uploading.
With just a YouTube link, this template automates the entire lifecycle of short-form content creation‚Äîfrom input to upload‚Äîso you can repurpose long-form videos into viral clips in minutes.
Key Features
Insights-Driven Highlight Extraction:
Automatically analyzes engagement metrics to select the most resonant moments from podcasts or other creators‚Äô videos, ensuring each clip connects with your audience.
AI-Powered Editing:
Utilizes professional-grade editing‚Äîadding captions, effects, transitions, and more‚Äîto produce visually stunning, high-impact reels.
Auto-Generated Metadata:
Effortlessly creates dynamic titles, descriptions, and hashtags to optimize your clips for immediate upload on Instagram.
Customizable Scheduling:
Allows you to set upload intervals that fit your content strategy, ensuring a consistent flow of engaging reels on your profile.
Seamless Instagram Integration:
Directly connects with Instagram‚Äôs API, streamlining the posting process so your profile stays active and engaging with minimal effort.
Customization Options
üé® Style & Branding: Modify the Spikes Studio API request to include your custom branding, text overlays, or templates.
üé• Clips Length: Previously adjust the length of each clip generated.
üïí Scheduling Delay: Adjust the time interval to control how often posts are made.
üîÅ Multi-Platform Support: Add additional nodes to also post to TikTok, Facebook, or LinkedIn.
Step-by-Step Setup Instructions
Prepare Prerequisites:
Ensure you have a Spikes Studio account (free at spikes.studio), a YouTube video URL, an Upload-Post account (free at Upload-Post), and a working n8n instance.
Configure APIs:
Spikes Studio: Obtain your API key and connect credentials.
Upload-Post: Obtain your API key and connect credentials and customize the request.
Assemble the Workflow:
Trigger: Trigger the workflow with a YouTube video URL and desired clip length.
Output: Auto-upload the formatted Reels to Instagram on a scheduled basis.
Designed specifically for content creators repurposing YouTube videos‚Äîand requiring only a Spikes Studio account (it‚Äôs Free) for full functionality‚Äîthis simplified automation template streamlines the conversion of long-form content into bite-sized, shareable clips. Elevate your content strategy, maximize audience engagement, and reclaim valuable time with a solution built to drive your channel‚Äôs growth."
AI-Driven WooCommerce Product Importer from Google Sheet with Yoast SEO meta,https://n8n.io/workflows/3510-ai-driven-woocommerce-product-importer-from-google-sheet-with-yoast-seo-meta/,"This workflow streamlines your WooCommerce product creation process by integrating directly with Google Sheets. Simply input product details into your spreadsheet, and the workflow takes care of the rest-automatically creating new products on your WooCommerce store with inventory management.
But it doesn‚Äôt stop there. A dedicated SEO expert chain analyzes each product‚Äôs content and generates optimized meta titles and meta descriptions for the plugin Yoast SEO, enhancing visibility and ranking potential on search engines.
Key Benefits:
üîÑ Automation: No more manual uploads‚Äîsave time and reduce errors by syncing Google Sheets directly with WooCommerce.
‚ö° Speed: Instantly publish multiple products with just one action.
üß† Built-in SEO Intelligence: Automatically generate SEO-friendly meta titles and descriptions tailored to each product.
üìà Improved Search Visibility: Boost your store's traffic with optimized product listings.
üß© Customizable: Easily adapt the workflow to your specific needs or integrate with other platforms.
How It Works
This workflow automates the creation of WooCommerce products and generates optimized SEO meta tags (title and description) using AI. Here‚Äôs the step-by-step process:
Data Retrieval: The workflow starts by fetching product details (title, category, description, price, etc.) from a Google Sheets document.
Product Creation: Each product is created in WooCommerce using the retrieved data, including categories, pricing, stock details, and images.
AI-Powered SEO Optimization: An AI model (Google Gemini via OpenRouter) analyzes the product details and generates SEO-optimized meta titles (‚â§60 chars) and meta descriptions (‚â§160 chars).
Meta Tag Assignment: The generated meta tags are saved back to the Google Sheets and applied to the WooCommerce product using Yoast SEO metadata.
Completion Tracking: The workflow marks completed entries in Google Sheets and sends a Telegram notification upon finishing all products.
Set Up Steps
Before running the workflow, ensure the following steps are completed:
Step 1: Install the Yoast SEO plugin on WordPress and add the provided PHP code to functions.php to enable meta tag API support.
Step 2: Enable the WooCommerce REST API in WordPress and configure the Telegram node with a valid CHAT_ID for notifications.
Step 3: Prepare a Google Sheet with product data (columns A-I in specific formats) and share its ID in the workflow. Ensure columns B, E, and F are in text format, and column I is numeric.
Once set up, the workflow can be triggered manually or scheduled to run automatically, streamlining product creation and SEO optimization.
Who is it useful for?
Ideal for eCommerce managers, digital marketers, or anyone managing large product catalogs-this workflow turns your spreadsheet into a powerful product launcher.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Transcribe YouTube Videos with AI Enhancement via Chat Interface,https://n8n.io/workflows/3381-transcribe-youtube-videos-with-ai-enhancement-via-chat-interface/,"About:
This workflow automates the transcription of YouTube videos by processing a video URL provided via a chat message. Designed for users who need quick access to video content in text form, this workflow ensures a seamless experience for transcribing videos on demand, regardless of the topic.
Who is this for?
This workflow is designed for individuals who need quick and accurate transcriptions of YouTube videos without watching them in full. It is particularly useful for:
Students who need text-based notes from educational videos.
Researchers looking to extract information from lectures or discussions.
Professionals who prefer reading over watching videos.
Casual users who want an efficient way to summarize video content.
What problem is this workflow solving?
Manually transcribing YouTube videos is time-consuming and prone to errors. Watching long videos just to extract key information is inefficient. This workflow automates transcription, allowing users to quickly convert video content into text. Use cases include:
Summarizing lectures or webinars.
Extracting insights from interviews and discussions.
Creating searchable text from video content.
Generating reference material without watching entire videos.
What This Workflow Does?
This workflow automates the transcription of YouTube videos by:
Accepting Input: User provide a YouTube video URL through a chat message.
Processing the Video: It utilizes an external transcription service to retrieve the full transcript of the YouTube video from the provided URL.
Enhancing Output: An AI model (OpenAI) refines the transcription for accuracy and readability.
Delivering Results: The final text transcript is returned to the user via the chat interface.
Setup:
Install n8n: Ensure you have n8n installed and running.
Import the Workflow: Copy the JSON workflow file into your n8n instance.
Configure API Keys:
Set up your Supadata (Supadata) API key for transcription.
Configure the OpenAI (OpenAI) API key for additional processing.
Run the Workflow: Provide a YouTube video URL and receive a transcription in response.
How to customize this workflow to your needs:
The workflow is flexible and can be tailored to suit specific requirements. Here are some customization ideas:
Language Support: Adjust the transcription language in both the HTTP Request and OpenAI nodes to support transcriptions in different languages (e.g., French, German).
Integrate with Other Services: Store transcriptions in a database, send them via email, or connect with a document management system.
Notification: Add a notification node (e.g., email or Slack) to alert you when the transcription is complete, especially for long videos.
Quality Check: Integrate an additional AI step to summarize or highlight key points in the transcript for quicker insights.
This workflow is designed to be scalable, efficient, and adaptable to various transcription needs.
Limitations
Video Length Limitation: Very long videos may not have a complete transcription due to constraints in processing capacity or service limitations.
Transcription Dependency: The accuracy of the transcription relies entirely on the presence of video captions or subtitles. If a video lacks these, no transcription will be generated.
Access Restrictions: Private or restricted YouTube videos may not be accessible for transcription due to permission limitations.
Processing Time: The time required to process a video can vary significantly, especially for longer videos, depending on the transcription service and server resources.
Regional Restrictions: Some YouTube videos may have geographic or regional access limitations, which could prevent the workflow from retrieving the content for transcription."
Convert Reddit threads into short vertical videos with AI,https://n8n.io/workflows/3407-convert-reddit-threads-into-short-vertical-videos-with-ai/,"Convert Reddit threads into short vertical videos with AI
Who is this for?
This workflow is ideal for:
Content creators and video editors automating short-form content production
Reddit storytellers converting text posts into engaging TikTok, YouTube Shorts, or Reels
Social media managers repurposing community discussions into visual narratives
What problem is this solving?
Manually converting Reddit posts into vertical video content is time-consuming:
You have to read, summarize, write a script
Generate TTS
Find stock footage
Edit everything in a timeline
This workflow automates the full pipeline. It converts any Reddit thread into a polished video with:
TTS narration
Subtitle overlays
B-roll from Pexels
Automatic rendering via Shotstack
What this workflow does
This workflow:
Extracts Reddit post and comments via Reddit API
Summarizes the thread into structured clips using OpenAI
Generates search queries for each clip for stock footage
Queries Pexels API for relevant vertical videos
Generates TTS audio for each clip using OpenAI Whisper
Creates subtitles matching the audio
Uploads footage/audio to Shotstack
Renders a full vertical video (720x1280) with synced TTS, subtitles, and b-roll
Returns a final video URL
Setup
Create accounts and API keys for:
Reddit Developer App
OpenAI
Pexels
Shotstack
Add credentials in n8n:
Reddit (HTTP Basic Auth)
OpenAI (API Key)
Shotstack (HTTP Header Auth)
Pexels (HTTP Header Auth)
Trigger via webhook or manual node. The input must include:
{
  ""voice"": ""nova"",
  ""ttsSpeed"": 1,
  ""videoLength"": 60,
  ""redditLink"": ""https://www.reddit.com/r/example/comments/example_id/example_title""
}
How to customize this workflow
Tweak OpenAI prompts to change tone or clip granularity
Change stock source by swapping Pexels for another API
Adjust TTS voices or languages by modifying the voice field
Modify video styling (fonts, colors, fit modes) in the timeline construction code node
Control duration by editing the character length formula in the Limit comments length node
Additional Notes
All stock videos are selected to match clip themes using generalized keywords to avoid API misses
Includes wait nodes to ensure Shotstack's async upload/render processes complete before proceeding
Annotated with sticky notes explaining major sections like TTS, Reddit input, and media timeline
Avoids community nodes to ensure cloud compatibility
Template Category
AI, Marketing, Building Blocks, Other (Content Creation)"
Automate LinkedIn Candidates Sourcing with Google X-ray Boolean Search,https://n8n.io/workflows/3259-automate-linkedin-candidates-sourcing-with-google-x-ray-boolean-search/,"Auto Source LinkedIn Candidates with GPT-4 Boolean Search & Google X-ray
How It Works:
User Input:
The user pastes a job description or ideal candidate specifications into the workflow.
Boolean Search String Generation:
OpenAI processes the input and generates a precise LinkedIn Boolean search string formatted as:
site:linkedin.com/in (""Job Title"" AND ""Skill1"" AND ""Skill2"")
This search string is optimized to find relevant LinkedIn profiles matching the provided criteria.
Google Sheet Creation:
A new Google Sheet is automatically created within a specified document to store extracted LinkedIn profile URLs.
Google Search Execution:
The workflow sends a search request to Google using an HTTP node with the generated Boolean string.
Iterative Search & Data Extraction:
The workflow retrieves the first 10 results from Google.
If the desired number of LinkedIn profiles has not been reached, the workflow loops, fetching the next set of 10 results until the if condition is met.
Data Storage:
The workflow extracts LinkedIn profile URLs from the search results and saves them to the newly created Google Sheet for further review.
Setup Steps:
1. API Key Configuration
Under ""Credentials"", add your OpenAI API key from your OpenAI account settings.
This key is used to generate the LinkedIn Boolean search string.
2. Adjust Search Parameters
Navigate to the ""If"" node and update the condition to define the desired number of LinkedIn profiles to extract.
The default is 50, but you can set it to any number based on your needs.
3. Establish Google Sheets Connection
Connect your Google Sheets account to the workflow.
Create a document to store the sourced LinkedIn profiles.
The workflow automatically creates a new sheet for each new search, so no manual setup is needed.
4. Authenticate Google Search
Google search requires authentication for better results.
Use the Cookie-Editor browser extension to export your header string and enable authenticated Google searches within the workflow.
5. Run the Workflow
Execute the workflow and monitor the Google Sheet for newly added LinkedIn profiles.
Benefits:
‚úÖ Automates profile sourcing, reducing manual search time.
‚úÖ Generates precise LinkedIn Boolean search strings tailored to job descriptions.
‚úÖ Extracts and saves LinkedIn profiles efficiently for recruitment efforts.
This solution leverages OpenAI and advanced search techniques to enhance your talent sourcing process, making it faster and more accurate! üöÄ"
"WooCommerce AI Post-Sales Chatbot with GPT-4o, RAG, Google Drive and Telegram",https://n8n.io/workflows/3329-woocommerce-ai-post-sales-chatbot-with-gpt-4o-rag-google-drive-and-telegram/,"This WooCommerce-integrated chatbot is designed to transform post-sales customer support by combining automation and artificial intelligence to deliver fast, secure, and personalized assistance.
The chatbot retrieves real-time order information, including shipping details and tracking numbers, after verifying the customer's identity through a strict email-based authentication system.
Beyond order management, the chatbot answers frequently asked questions about return policies, delivery times, and terms of service using RAG.
If a request is too complex, the system seamlessly escalates it to a human operator via Telegram, guaranteeing no customer query goes unresolved.
Key Features of the Chatbot
Order Tracking: Retrieves real-time tracking information for WooCommerce orders, including carrier URLs and pickup dates.
Order Details Retrieval: Provides customers with past/current order information after strict email verification.
Policy & FAQ Assistance: Answers questions about shipping, returns, and store policies using a vectorized knowledge base (ToS tool).
Identity Verification: Ensures privacy by requiring exact email-order matches before sharing sensitive data.
Human Escalation: Transfers complex issues to human agents via Telegram when the AI cannot resolve them.
Context-Aware Conversations: Maintains dialogue context using memory buffers for seamless interactions.
Who Benefits from This Chatbot?
E-commerce Stores: WooCommerce businesses needing 24/7 automated post-sales support.
Customer Support Teams: Reduces ticket volume by handling repetitive queries (tracking, policies).
SMBs: Small-to-medium businesses lacking resources for full-time support staff.
Customers: Shoppers who want instant, self-service access to order status and FAQs.
How It Works
Customer Interaction: The workflow starts when a customer sends a chat message, triggering the AI agent.
Identity Verification: The agent requests the order number and associated email, strictly verifying the match before proceeding.
Order & Tracking Retrieval: Using WooCommerce API tools (get_order, get_tracking), it fetches order details and tracking information.
Policy & Support: The ToS tool answers shipping and policy questions, while human_assistance escalates unresolved issues to a human agent via Telegram.
Memory & Context: A buffer memory retains conversation context for coherent interactions.
Set Up Steps
Configure Qdrant Vector Store: Replace QDRANTURL and COLLECTION in the nodes to set up document storage.
Add Telegram Chat ID: Insert your Telegram CHAT_ID in the human_assistance node for escalations.
Integrate WooCommerce Tracking Plugin: Install the YITH WooCommerce Order Tracking plugin and update the HTTP request URL in the tracking node.
Test & Activate: Verify the workflow by testing order queries and ensuring proper email verification.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
"Youtube RAG search with Frontend using Apify, Qdrant and AI",https://n8n.io/workflows/3288-youtube-rag-search-with-frontend-using-apify-qdrant-and-ai/,"Ever wanted to build your own RAG search over Youtube videos? Well, now you can! This n8n template shows how you can build a very capable Youtube search engine powered by Apify, Qdrant and your LLM of choice to quickly and efficiently browse over many videos for research.
I originally started to template to ask questions on the ""n8n @ scale office-hours"" livestream videos but then extended it to include the latest videos on the official channel.
Check out a demo here: https://jimleuk.app.n8n.cloud/webhook/n8n_videos
How it works
Stage 1 is to collect the Youtube video transcripts and push them into a vector database. For this, I've used Apify to scrape Youtube and Qdrant to store the embeddings.
Transcripts are broken down into smaller chunks and carefully tagged with metadata to assist in later search and filtering.
Stage 2 is to build a web frontend for the user to query the vectorised transcripts. I'm using a webhook to serve a simple web app and API to dynamically fetch the results.
When searching for a video, I've opted to use Qdrant's search groups API which in this use-case, performs better as it returns a wider range of videos results.
In the web frontend, when the user clicks on the results, the matching Youtube video plays in an embedded video player.
How to use
Once credentials are all set, first run steps 1 - 3 to populate your vector store.
Next, set the workflow to active to expose the web frontend. Visit the webhook URL in your browser to use it.
If only for personal use, you may want to remove the rate limiting mechanism in step 4.
Requirements
Apify for Youtube Channel and Video Scraping
Qdrant for Vector store
OpenAI for LLM and Embeddings
Customising the template
Not interested in official n8n videos? Swap to a different channel - this template will work on many as long as videos are not private or set to prevent embeds.
Technically any vector store should work but may not have the same grouping API. Use the simple vector store node and revert back to basic searching instead."
AI Research Agents to Automate PDF Analysis with Mistral‚Äôs Best-in-Class OCR,https://n8n.io/workflows/3223-ai-research-agents-to-automate-pdf-analysis-with-mistrals-best-in-class-ocr/,"Overview
Mistral OCR is a cutting-edge document understanding API that improves how businesses extract and process information from complex documents. With top scores in benchmarks for accuracy and comprehension capabilities, Mistral OCR handles multi-column text, charts, diagrams, and multiple languages.
This workflow uses Mistral's Document understanding OCR API to automatically turns dense PDFs (such as financial reports) into either deep research reports or concise newsletters
Key Features
Superior Document Understanding: Processes complex documents with high-fidelity rendering
Multi-Format Support: Handles PDFs containing text, images, charts, and diagrams
Multilingual Capabilities: Accurately processes documents in various languages
Seamless API Integration: Easy implementation through cloud-based API
Customizable Research Depth: Generate comprehensive 8-page reports or concise 1,750-word newsletters
How It Works
Document Upload: Submit your PDF through an n8n form interface.
Output Format Selection: Choose between comprehensive deep research (3,500 words) or Concise newsletter (1,750 words)
Custom Instructions: Tailor the analysis by adding specific focus areas (e.g., quantitative data, growth catalysts).
AI Processing: The document undergoes multi-stage AI analysis: OCR and text extraction using Mistral AI and Content structuring and summarization using GPT models
Agents:
Research Leader: Plans and conducts initial research, creating a table of contents.
Project Planner: Breaks down the table of contents into manageable sections.
Research Assistants: Multiple agents that conduct in-depth research on assigned sections.
Editor: Compiles and refines the final article, ensuring coherence and proper citations.
Setup
API Key Acquisition:
Obtain an API key from OpenRouter.ai
Get an API key from Mistral.ai
n8n Configuration:
In your n8n instance, navigate to the credentials section.
Create new credentials for OpenRouter and Mistral, inputting the respective API keys.
Form Configuration:
Customize the input form fields if needed (e.g., adding company-specific options).
Output Customization: Adjust the word count parameters in the Project Planner node to change output length."
Automate Video Creation with Luma AI Dream Machine and Airtable (Part 1),https://n8n.io/workflows/3200-automate-video-creation-with-luma-ai-dream-machine-and-airtable-part-1/,"Automate Video Creation with Luma AI Dream Machine and Airtable (Part 1)
Description
This workflow automates video creation using Luma AI Dream Machine and n8n. It generates dynamic videos based on custom prompts, random camera motion, and predefined settings, then stores the video and thumbnail URLs in Airtable for easy access and tracking. This automation makes it easy to create high-quality videos at scale with minimal effort.
üëâ Airtable Base Template
üé• Tutorial Video
Setup
1. Luma AI Setup
Create an account with Luma AI.
Generate an API key from Luma AI for authentication.
Ensure the API key has permission to create and manage video requests.
2. Airtable Setup
Create an Airtable base with the following fields:
Generation ID ‚Äì To match incoming webhook data.
Status ‚Äì Workflow status (e.g., ""Done"").
Video URL ‚Äì Stores the generated video URL.
Thumbnail URL ‚Äì Stores the thumbnail URL.
Prompt ‚Äì The video prompt used in the request.
Aspect Ratio ‚Äì Defines the video format (e.g., 9:16).
Duration ‚Äì Length of the video.
üëâ Use the Airtable template linked above to simplify setup.
3. n8n Setup
Install n8n (local or cloud).
Set up Luma AI and Airtable credentials in n8n.
Import the workflow and customize the settings based on your needs.
How It Works
1. Global Settings Configuration
The Set node defines key settings such as:
Prompt ‚Äì Example: ""A crocheted parrot in a crocheted pirate outfit swinging on a crocheted perch.""
Aspect Ratio ‚Äì Example: ""9:16""
Loop ‚Äì Example: ""true""
Duration ‚Äì Example: ""5 seconds""
Cluster ID ‚Äì Used to group related videos for easy tracking.
Callback URL - Used for the Webhook workflow in Part 2
2. Random Camera Motion
The Code node randomly selects a camera motion (e.g., Zoom In, Pan Left, Crane Up) to create dynamic and visually engaging videos.
3. API Request to Luma AI
The HTTP Request node sends a POST request to Luma AI‚Äôs API with the following parameters:
Prompt ‚Äì Uses the defined global settings.
Aspect Ratio ‚Äì Matches the target platform (e.g., TikTok or YouTube).
Duration ‚Äì Length of the video.
Loop ‚Äì Determines if the video should loop.
Callback URL ‚Äì Sends a POST response when the video is complete.
4. Capture API Response
Luma AI sends a POST response to the callback URL once video generation is complete.
The response includes:
Video URL ‚Äì Direct link to the video.
Thumbnail URL ‚Äì Link to the video thumbnail.
Generation ID ‚Äì Used to match the record in Airtable.
5. Store in Airtable
The Airtable node updates the record with the video and thumbnail URLs.
Generation ID is crucial for matching future webhook responses to the correct video record.
Why This Workflow is Useful
‚úÖ Automates high-quality video creation
‚úÖ Reduces manual effort by handling prompt generation and API calls
‚úÖ Random camera motion makes videos more dynamic
‚úÖ Ensures organized tracking with Airtable
‚úÖ Scalable ‚Äì Ideal for automating large-scale content creation
Next Steps
Part 2 ‚Äì Handling webhook responses and updating Airtable automatically.
Future Enhancements ‚Äì Adding more camera motions, multi-platform support, and automated video editing."
Build Your Own Counseling Chatbot on LINE to Support Mental Health Conversations,https://n8n.io/workflows/2975-build-your-own-counseling-chatbot-on-line-to-support-mental-health-conversations/,"Are you looking to create a counseling chatbot that provides emotional support and mental health guidance through the LINE messaging platform ? This guide will walk you through connecting LINE with powerful AI language models like GPT-4 to build a chatbot that supports users in navigating their emotions, offering 24/7 conversational therapy and accessible mental health resources .
By leveraging LINE's webhook integration and Azure OpenAI , this template allows you to design a chatbot that is both empathetic and efficient, ensuring users receive timely and professional responses. Whether you're a developer, counselor, or business owner, this guide will help you create a customizable counseling chatbot tailored to your audience's needs.
Who Is This Template For?
Developers who want to integrate AI-powered chatbots into the LINE platform for mental health applications.
Counselors & Therapists looking to expand their reach and provide automated emotional support to clients outside of traditional sessions.
Businesses & Organizations focused on improving mental health accessibility and offering innovative solutions to their users.
Educators & Nonprofits seeking tools to provide free or low-cost counseling services to underserved communities.
How this work?
Line Webhook to receive new message
Send loading animation in Line
Check if the input is text or not
Send the text as prompt in chat model (GPT 4o)
Reply the message to user (you'll need 'edit field' to format it before reply)
Pre-Requisites
You have access to the LINE Developers Console.
An Azure OpenAI account with necessary credentials.
Set-up
To receive messages from LINE, configure your webhook:
Set up a webhook in LINE Developer Console.
Copy the Webhook URL from the Line Chatbot node and paste it into the LINE Console.
Ensure to remove any 'test' part when moving to production.
The loading animation reassures users that the system is processing their request.
Authorize using header authorization
Message Handling
Use the Check Message Type IsText? node to verify if the incoming message is text.
If the message type is text, proceed with ChatGPT processing; otherwise, send a reply indicating non-text inputs are not supported.
AI Agent Configuration
Define the system message within the AI Agent node to guide the conversation based on desired interaction principles.
Connect the Azure OpenAI Chat Model to the AI Agent.
Formatting Responses
Ensure responses are properly formatted before sending them back to the user.
Reply Message
Use the ReplyMessage - Line node to send the formatted response.
Ensure proper header authorization using Bearer tokens."
üìÑ‚ú® Easy WordPress Content Creation from PDF Docs + Human in the Loop Gmail,https://n8n.io/workflows/3010-easy-wordpress-content-creation-from-pdf-docs-human-in-the-loop-gmail/,"üìÑ‚ú® Easy WordPress Content Creation from PDF Docs + Human in the Loop Gmail
This n8n workflow automates the process of transforming PDF documents into engaging, SEO-friendly WordPress blog posts. It incorporates AI-powered text analysis, automatic image generation, and a human review step to ensure quality before publishing.
üöÄ How It Works
üóÇÔ∏è PDF Upload & Text Extraction
Users upload a PDF document through a form trigger.
The workflow extracts text from the uploaded file, ensuring compatibility with supported formats.
ü§ñ AI-Powered Blog Post Generation
The extracted text is analyzed by an AI model (GPT-based) to create a structured blog post.
The AI generates:
A captivating SEO-friendly title.
Well-formatted HTML content, including an introduction, chapters with subheadings, and a conclusion.
üé® Image Creation & Integration
An image is generated using Pollinations.ai based on the blog post title.
The vibrant image is uploaded to WordPress and set as the featured image for the post.
üìù WordPress Draft Creation
A draft blog post is created on WordPress with the AI-generated title, content, and featured image.
‚úÖ Human-in-the-Loop Approval
The draft content is sent via Gmail to a reviewer for manual approval.
If approved, the post is published on WordPress. If not, an error message is sent for troubleshooting.
üì¢ Multi-Channel Notifications
Once published, notifications are sent via Gmail and Telegram to relevant stakeholders.
üîß Setup Steps
üîë Configure API Credentials
Set up API connections for:
OpenAI (for AI content generation).
WordPress (for post creation and media uploads).
Gmail (for sending approval emails).
Telegram (for notifications).
imgbb (for saving blog image).
‚öôÔ∏è Customize Workflow Parameters
Adjust the AI prompt to match your desired blog structure and tone.
Modify the image generation parameters to align with your branding needs.
üß™ Test & Deploy
Test the workflow with sample PDFs to ensure:
Accurate text extraction.
Proper formatting of generated content.
Seamless approval and publishing processes.
This workflow streamlines content creation while maintaining quality control through human oversight, making it an ideal solution for efficient blog management! üéâ"
AI-Powered Chatbot Workflow with MySQL Database Integration,https://n8n.io/workflows/2985-ai-powered-chatbot-workflow-with-mysql-database-integration/,"AI-Powered Chatbot Workflow with MySQL Integration
This guide shows you how to deploy a chatbot that lets you query your database using natural language. You will build a system that accepts chat messages, retains conversation history, constructs dynamic SQL queries, and returns responses generated by an AI model. By following these instructions, you will have a working solution that integrates n8n‚Äôs AI Agent capabilities with MySQL.
Prerequisites
Before you begin, ensure that you have the following:
An active n8n instance (self-hosted or cloud) running version 1.50.0 or later.
Valid MySQL credentials configured in n8n.
API credentials for the Groq Chat Model (or your preferred AI language model).
Basic familiarity with SQL and n8n node concepts such as chat triggers and memory buffers.
Access to the n8n Docs on AI Agents for further reference.
Workflow Setup
1. Chat Interface & Trigger
When Chat Message Received
This node listens for incoming chat messages via a webhook. When a message arrives, it triggers the workflow immediately.
2. Conversation Memory
Chat History
This memory buffer node stores the last 10 interactions. It supplies conversation context to the AI Agent, ensuring that responses consider previous messages.
3. AI Agent Core
AI Agent (Tools Agent)
The AI Agent node orchestrates the conversation by receiving the chat input and conversation history. It dynamically generates SQL queries based on your requests and coordinates calls to external tools (such as MySQL nodes).
4. Database Interactions
MySQL Node
This node executes the SQL query generated by the AI Agent. You reference the query using an expression (e.g., {{$node[""AI Agent""].json.sql_query}}), allowing the agent‚Äôs output to control data retrieval.
MySQL Schema Node
This node retrieves a list of base tables from your MySQL database (excluding system schemas). The agent uses this information to understand the available tables.
MySQL Definition Node
This node fetches detailed metadata (such as column names, data types, and relationships) for a specific table. The table and schema names are supplied dynamically by the AI Agent.
5. Language Model Processing
Groq Chat Model
This node connects to the Groq Chat API to generate text completions. It processes the combined input (chat message, context, and data fetched from MySQL) and produces the final response.
6. Guidance & Customization
Sticky Notes
These nodes provide guidance on:
Switching the chat model if you wish to use another provider (e.g., OpenAI or Anthropic).
Adjusting the maximum token count per interaction.
Customizing the SQL queries and the context window size.
They help you modify the workflow to suit your environment and requirements.
Workflow Connections
The Chat Trigger passes the incoming message to the AI Agent.
The Chat History node supplies conversation context to the AI Agent.
The AI Agent calls the MySQL nodes as external tools, generating and sending dynamic SQL queries.
The Groq Chat Model processes the consolidated input from the agent and outputs the natural language response delivered to the user.
Testing the Workflow
Send a chat message using the chat interface.
Observe how the AI Agent processes the input and generates a corresponding SQL query.
Verify that the MySQL nodes execute the query and return data.
Confirm that the Groq Chat Model produces a coherent natural language response.
Refer to the sticky notes for guidance if you need to fine-tune any node settings.
Next Steps and References
Customize Your AI Model
Replace the Groq Chat Model with another language model (such as the OpenAI Chat Model) by updating the node credentials and configuration.
Enhance Memory Settings
Adjust the Chat History node‚Äôs context window to retain more or fewer messages based on your needs.
Modify SQL Queries
Update the SQL queries in the MySQL nodes to match your specific database schema and desired data.
Further Reading
Consult the n8n Docs on AI Agents for additional details and examples to expand your workflow‚Äôs capabilities.
Set Up a Website Chatbot
Copy & Paste and replace the placeholders in the following code to embed the chatbot into your personal or company's website: View in CodePen ü°•
By following these steps, you will deploy a robust AI chatbot workflow that integrates with your MySQL database, allowing you to query data using natural language."
Automate WooCommerce SEO with Yoast & AI-Powered Meta Tag Generation for FREE,https://n8n.io/workflows/2963-automate-woocommerce-seo-with-yoast-and-ai-powered-meta-tag-generation-for-free/,"This workflow is designed to automate the generation and updating of SEO meta titles and descriptions for WooCommerce products using n8n. It leverages Google Sheets for data input, a FREE language model (Gemini 2.0 Flash Exp. via OpenRouter) for generating SEO-optimized meta tags, and WooCommerce for updating product details.
How It Works:
Trigger: The workflow can be triggered manually or on a schedule. The manual trigger allows for testing, while the schedule trigger can be set to run at regular intervals (e.g., every few minutes) to process new products.
Data Retrieval:
The workflow starts by retrieving product IDs from a Google Sheets document. It looks for products that do not yet have meta titles or descriptions.
Using the retrieved product ID, the workflow fetches the corresponding product details from WooCommerce, including the product name, description, short description, and categories.
Meta Tag Generation:
The product details are passed to a language model (Gemini 2.0 Flash Exp) via OpenRouter. The model generates SEO-optimized meta titles and descriptions based on the provided content.
The generated meta tags are structured and validated to ensure they meet SEO best practices, such as character limits and keyword inclusion.
Update WooCommerce:
The generated meta title and description are then updated in the WooCommerce product metadata using the Yoast SEO fields.
Update Google Sheets:
Finally, the workflow updates the Google Sheets document with the newly generated meta tags, along with the product URL, title, and the timestamp of the update.
Set Up Steps:
Google Sheets Setup:
Create a copy of the provided Google Sheets template and insert WooCommerce product IDs in column ""B"".
Ensure the Google Sheets document has columns for METATITLE, METADESCRIPTION, URL, TITLE POST, and DATA (timestamp).
n8n Workflow Configuration:
Google Sheets Node: Configure the ""Get product ID"" node to connect to your Google Sheets document. Use OAuth2 for authentication.
WooCommerce Node: Set up the WooCommerce nodes to connect to your WooCommerce store using the WooCommerce API credentials.
OpenRouter Node: Configure the ""Gemini 2.0 Flash Exp"" node with your OpenRouter API credentials to access the language model.
Structured Output Parser: Ensure the output parser is set to handle the structured data format for meta titles and descriptions.
Workflow Execution:
Trigger the workflow manually to test the process or set up a schedule trigger to automate the workflow at regular intervals.
Monitor the workflow execution to ensure that meta tags are generated and updated correctly in both WooCommerce and Google Sheets.
Validation:
After the workflow runs, verify that the meta titles and descriptions in WooCommerce are correctly updated and that the Google Sheets document reflects the changes.
This workflow streamlines the process of optimizing WooCommerce product pages for SEO, saving time and ensuring consistency in meta tag generation.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Author and Publish Blog Posts From Google Sheets,https://n8n.io/workflows/2873-author-and-publish-blog-posts-from-google-sheets/,"What it is:
An automation to plan‚Üídraft‚Üífinalize and publish your textual blog post ideas to your wordpress blog
Works in stages and hand back control to you in between those
You can use a Google Spreadsheet for planning topics and configuring LLM models and prompts
What it does:
plans‚Üídrafts‚Üífinalizes blog post topics you specify in a Google Spreadsheet using an LLM with prompts that also ar configured in that spreadsheet (even which model to use)
savs the results in the corresponding
columns of the ""Schedule"" sheet in the spreadsheet
hands control back to the user for inspecting or changing the results and for setting the next ""Action"" for th workflow
Finally publishes the blog post to your Wordpress instance
Limitations
Probably slightly over-engineered ;-)
No media generation yet
some LLM models don't work because of their output format
How it works:
The Workflow is triggered manually or scheduled every hour
It ingests a Google Spreadsheet to get
Config for prompts/context tc
Blog-Topics and their status and next action
Depending on each blog topics ""Status"" and ""Action"" it then either uses an LLM for th next action (""plan""‚Üí""draft""‚Üí""final"" actions) or publishes the written content to your Wordpress instance (""publish"" actions)
Set up steps:
Import the workflow
Make your own copy of the Google Spreadsheet
Update the credentials using your individual credentials for:
Google Spreadsheets
OpenRouter
Edit the ""Settings"" node and enter your individual values for
Your spreadsheet copy URL
Your wordpress blog URL
Your wordpress blog username
Your wordpress blog app password (a 4x4 alphanumeric sequence), that you probably have to create first, for which your wordpress user has to have 2-factor-authentication enabled.
In your own copy of the spreadsheet:
individualize the ""Config"" sheet's ""Value"" column for the prompts/context/etc
Populate the ""Schedule"" sheet with at least one line in which you specify
a ""Topic""
a ""Schedulded"" date (YYYY-MM-DD HH:mm:ss)
a ""Status"" of ""idea""
an ""Action"" of ""plan"" (to kick off that action)"
üî•üìàü§ñ AI Agent for n8n Creators Leaderboard - Find Popular Workflows,https://n8n.io/workflows/2940-ai-agent-for-n8n-creators-leaderboard-find-popular-workflows/,"n8n Creators Leaderboard Workflow
Why Use This Workflow?
The n8n Creators Leaderboard Workflow is a powerful tool for analyzing and presenting detailed statistics about workflow creators and their contributions within the n8n community. It provides users with actionable insights into popular workflows, community trends, and top contributors, all while automating the process of data retrieval and report generation.
Benefits
Discover Popular Workflows: Identify workflows with the most unique visitors and inserters (weekly and monthly).
Understand Community Trends: Gain insights into what workflows are resonating with the community.
Recognize Top Contributors: Highlight impactful creators to foster collaboration and inspiration.
Save Time with Automation: Automates data fetching, processing, and reporting for efficiency.
Use Cases
For Workflow Creators: Track performance metrics of your workflows to optimize them for better engagement.
For Community Managers: Identify trends and recognize top contributors to improve community resources.
For New Users: Explore popular workflows as inspiration for building your own automations.
How It Works
This workflow aggregates data from GitHub repositories containing statistics about workflow creators and their templates. It processes this data, filters it based on user input, and generates a detailed Markdown report using an AI agent.
Key Features
Data Aggregation: Fetches creator and workflow statistics from GitHub JSON files.
Custom Filtering: Focuses on specific creators based on a username provided via chat.
AI-Powered Reports: Generates comprehensive Markdown reports with summaries, tables, and insights.
Output Flexibility: Saves reports locally with timestamps for easy access.
Data Retrieval & Processing
Creators Data: Retrieved via an HTTP Request node from a JSON file containing aggregated statistics about creators.
Workflows Data: Pulled from another JSON file with workflow metrics like visitor counts and inserter statistics.
Data Merging: Combines creator and workflow data by matching usernames to provide enriched statistics.
Report Generation
The AI agent generates a Markdown report that includes:
A summary of the creator‚Äôs contributions.
A table of workflows with key metrics (e.g., unique visitors, inserters).
Insights into trends or community feedback.
The report is saved locally as a file with a timestamp for tracking purposes.
Quick Start Guide
Prerequisites
Ensure your n8n instance is running.
Verify that the GitHub base URL and file variables are correctly set in the Global Variables node.
Confirm that your OpenAI credentials are configured for the AI Agent node.
How to Start
Activate the Workflow: Make sure the workflow is active in your n8n environment.
Trigger via Chat: Use the Chat Trigger node to initiate the workflow by sending a message like:
show me stats for username [desired_username]
Replace [desired_username] with the username you want to analyze.
Processing & Report Generation: The workflow fetches data, processes it, and generates a Markdown report.
View Output: The final report is saved locally as a file (with a timestamp), which you can review to explore leaderboard insights."
Smart Email Assistant: Automate Customer Support with AI & Supabase,https://n8n.io/workflows/2929-smart-email-assistant-automate-customer-support-with-ai-and-supabase/,"Intelligent Email Support System with Vector Database
Overview
This n8n workflow automates email support using AI and vector database technology to provide smart, context-aware responses. It seamlessly integrates email automation and document management, ensuring efficient customer support.
üìå System Components
‚úâÔ∏è Email Support System
Email Monitoring & Classification
Gmail trigger node monitoring inbox
AI-powered email classification
Intelligent routing (support vs non-support inquiries)
AI Response Generation
LangChain agent for response automation
OpenAI integration for NLP-driven replies
Vector-based knowledge retrieval
Automated draft creation in Gmail
Vector Database System
Supabase vector store for document management
OpenAI embeddings for vector conversion
Fast and efficient similarity search
üìÇ Document Management System
Google Drive Integration
Monitors specific folders for new/updated files
Automatic document processing
Supports various file formats
Document Processing Pipeline
Auto file download & text extraction
Smart text chunking for better indexing
Embedding generation via OpenAI
Storage in Supabase vector database
üîÑ Workflow Processes
üìß Email Support Flow
Monitor Gmail inbox for new emails
AI classification of incoming messages
Route support emails to AI response generator
Perform vector similarity search for knowledge retrieval
Generate personalized AI-driven response
Create email drafts in Gmail
üìÅ Document Management Flow
Monitor Google Drive for new/updated files
Auto-download and process documents
Clean up outdated vector entries for updated files
Extract and split document text efficiently
Generate OpenAI embeddings
Store processed data in Supabase vector DB
‚öôÔ∏è Setup Instructions
1Ô∏è‚É£ Prerequisites
Supabase account & project
OpenAI API key
Gmail account with OAuth2 setup
Google Drive API access
n8n installation
2Ô∏è‚É£ Supabase Database Setup
-- Create the vector extension
create extension if not exists vector;

-- Create the documents table
create table documents (
  id bigserial primary key,
  content text,
  metadata jsonb,
  embedding vector(1536)
);

-- Create an index for similarity search
create index on documents using ivfflat (embedding vector_cosine_ops)
  with (lists = 100);
3Ô∏è‚É£ Google Drive Setup
Create & configure two monitored folders:
RAG folder for new documents
documents
Assign correct folder permissions
Add folder IDs to the workflow
4Ô∏è‚É£ Document Processing Configuration
Set up triggers for file creation and file updates
Configure text extraction:
Define chunk size & overlap settings
Set document metadata processing
üîç Maintenance & Optimization
üìå Regular Tasks
Monitor system performance
Update the knowledge base regularly
Review AI response quality
Optimize vector search parameters
Clean up outdated document embeddings
‚úÖ Best Practices
Document Organization
Maintain structured folders & naming conventions
Keep knowledge base content updated
System Optimization
Track AI classification accuracy
Tune response times & chunk sizes
Perform regular database maintenance
üõ†Ô∏è Troubleshooting
Email Issues
Verify Gmail API credentials
Check AI service uptime
Monitor classification performance
Document Processing Issues
Ensure correct file permissions
Validate extraction & embedding processes
Debug vector database insertions"
HR Job Posting and Evaluation with AI,https://n8n.io/workflows/2773-hr-job-posting-and-evaluation-with-ai/,"Workflow Documentation: HR Job Posting and Evaluation with AI
Detailed Description
The HR Job Posting and Evaluation with AI workflow is designed to streamline and enhance recruitment for technical roles, such as Automation Specialists. By automating key stages in the hiring process, this workflow ensures a seamless experience for both candidates and HR teams. From collecting applications to evaluating candidates using AI and scheduling interviews, this workflow provides an end-to-end solution for recruitment challenges.
Who is this for?
This workflow is ideal for:
HR Professionals: Managing multiple job postings and candidates efficiently.
Recruitment Teams: Handling large volumes of applications for technical positions.
Hiring Managers: Ensuring structured and objective candidate evaluations.
What problem does this workflow solve?
Time-Consuming Processes: Automates repetitive tasks like data entry, CV management, and scheduling.
Fair Candidate Evaluation: Leverages AI to provide objective insights based on resumes and job descriptions.
Streamlined Communication: Ensures timely and personalized candidate interactions, improving their experience.
What this workflow does
This workflow automates the following steps:
Form Submission: Collects candidate information via a structured application form.
Data Storage: Stores applicant details in Airtable for centralized tracking.
CV Management: Automatically uploads resumes to Google Drive for easy access and organization.
AI-Powered Candidate Evaluation: Scores candidates based on their resumes and job descriptions using OpenAI, providing actionable insights.
Interview Scheduling: Automates scheduling based on candidate and interviewer availability.
Communication: Sends customized emails to candidates for interview invitations and feedback.
Setup
Prerequisites
To use this workflow, you‚Äôll need:
n8n Account: To create and run the workflow.
Airtable Account: For managing applicant data.
Google Drive Account: For storing candidate CVs.
OpenAI API Key: For AI-powered candidate scoring.
SMTP Email Account: For sending candidate communications.
Setup Process
Airtable Configuration:
Create a base in Airtable with tables for Applicants and Job Positions.
Google Drive Setup:
Create a folder for CV storage and ensure you have write permissions.
Integrate Airtable in n8n:
Use the Airtable API key to connect Airtable to n8n.
Integrate Google Drive in n8n:
Authorize Google Drive to enable CV storage automation.
OpenAI Integration:
Add your OpenAI API key to n8n for candidate scoring.
Email Configuration:
Set up your SMTP email account in n8n for sending notifications and invitations.
How to customize this workflow
Tailor the workflow to fit your unique recruitment needs:
Edit Job Descriptions:
Adjust the form parameters to match the specific role and qualifications.
Refine AI Evaluation Criteria:
Modify OpenAI prompts to reflect the skills and competencies for the desired position.
Personalize Email Templates:
Update email content to match your organization‚Äôs tone and branding.
Add New Features:
Incorporate additional steps like feedback collection or integration with other HR tools.
Conclusion
The HR Job Posting and Evaluation with AI workflow simplifies and automates the recruitment process, enabling HR teams to focus on engaging with candidates rather than handling administrative tasks. With its powerful integrations and customization options, this workflow helps organizations hire efficiently while improving the candidate experience."
AI Fitness Coach Strava Data Analysis and Personalized Training Insights,https://n8n.io/workflows/2790-ai-fitness-coach-strava-data-analysis-and-personalized-training-insights/,"Detailed Title
""Triathlon Coach AI Workflow: Strava Data Analysis and Personalized Training Insights using n8n""
Description
This n8n workflow enables you to build an AI-driven virtual triathlon coach that seamlessly integrates with Strava to analyze activity data and provide athletes with actionable training insights. The workflow processes data from activities like swimming, cycling, and running, delivers personalized feedback, and sends motivational and performance improvement advice via email or WhatsApp.
Workflow Details
Trigger: Strava Activity Updates
Node: Strava Trigger
Purpose: Captures updates from Strava whenever an activity is recorded or modified. The data includes metrics like distance, pace, elevation, heart rate, and more.
Integration: Uses Strava API for real-time synchronization.
Step 1: Data Preprocessing
Node: Code
Purpose: Combines and flattens the raw Strava activity data into a structured format for easier processing in subsequent nodes.
Logic: A recursive function flattens JSON input to create a clean and readable structure.
Step 2: AI Analysis with Google Gemini
Node: Google Gemini Chat Model
Purpose: Leverages Google Gemini's advanced language model to analyze the activity data.
Functionality:
Identifies key performance metrics.
Provides feedback and insights specific to the type of activity (e.g., running, swimming, or cycling).
Offers tailored recommendations and motivational advice.
Step 3: Generate Structured Output
Node: Structure Output
Purpose: Processes the AI-generated response to create a structured format, such as headings, paragraphs, and bullet lists.
Output: Formats the response for clear communication.
Step 4: Convert to HTML
Node: Convert to HTML
Purpose: Converts the structured output into an HTML format suitable for email or other presentation methods.
Output: Ensures the response is visually appealing and easy to understand.
Step 5: Send Email with Training Insights
Node: Send Email
Purpose: Sends a detailed email to the athlete with performance insights, training recommendations, and motivational messages.
Integration: Utilizes Gmail or SMTP for secure and efficient email delivery.
Optional Step: WhatsApp Notifications
Node: WhatsApp Business Cloud
Purpose: Sends a summary of the activity analysis and key recommendations via WhatsApp for instant access.
Integration: Connects to WhatsApp Business Cloud for automated messaging.
Additional Notes
Customization:
You can modify the AI prompt to adapt the recommendations to the athlete's specific goals or fitness levels.
The workflow is flexible and can accommodate additional nodes for more advanced analysis or output formats.
Scalability:
Ideal for individual athletes or coaches managing multiple athletes.
Can be expanded to include additional metrics or insights based on user preferences.
Performance Metrics Handled:
Swimming: SWOLF, stroke count, pace.
Cycling: Cadence, power zones, elevation.
Running: Pacing, stride length, heart rate zones.
Implementation Steps
Set Up Strava API Key:
Log in to Strava Developers to generate your API key.
Integrate the API key into the Strava Trigger node.
Configure Google Gemini Integration:
Use your Google Gemini (PaLM) API credentials in the Google Gemini Chat Model node.
Customize Email and WhatsApp Messaging:
Update the Send Email and WhatsApp Business Cloud nodes with the recipient‚Äôs details.
Automate Execution:
Deploy the workflow and use n8n's scheduling features or cron jobs for periodic execution.
GET n8n Now
N8N COURSE
n8n Book
Developer Notes
Author: Amjid Ali
improvements.
Resources:
See in Action: Syncbricks Youtube
PayPal: Support the Developer
Courses : SyncBricks LMS
By using this workflow, triathletes and coaches can elevate training to the next level with AI-powered insights and actionable recommendations."
Summarize YouTube Videos from Transcript,https://n8n.io/workflows/2736-summarize-youtube-videos-from-transcript/,"Who is this template for?
This workflow template is designed for content creators, researchers, educators, and professionals who need quick, accurate summaries of YouTube videos. It‚Äôs ideal for those looking to save time, extract key insights, or repurpose video content into concise formats for reports, studies, or social media.
What does it do?
The workflow automates the process of summarizing YouTube videos by extracting the transcript, analyzing the content, and generating a concise summary. It leverages AI tools to ensure accuracy and relevance, making it easier to digest lengthy videos in seconds.
Why is it useful?
This template saves hours of manual effort by automating video summarization, enabling users to focus on analyzing or sharing insights rather than watching entire videos. It‚Äôs particularly useful for staying updated with trends, conducting research, or creating content efficiently.
How does it work?
The workflow integrates with YouTube‚Äôs Transcript API powered by Apify Actor to fetch video transcripts, process the text using AI-powered summarization tools, and deliver a clear, concise summary.
Setup Instructions
You need an Apify account and an API key to connect with the Actor. Follow the steps below:
Create a Free Account.
Choose the appropriate Actor from the Apify search.
Under the Integration tab, click on ‚ÄúUse API endpoints.‚Äù
Select the API that best suits your needs."
AI Agent to chat with Supabase/PostgreSQL DB,https://n8n.io/workflows/2612-ai-agent-to-chat-with-supabasepostgresql-db/,"Video Guide
I prepared a detailed guide that showed the whole process of building a resume analyzer.
Who is this for?
This workflow is ideal for developers, data analysts, and business owners who want to enable conversational interactions with their database. It‚Äôs particularly useful for cases where users need to extract, analyze, or aggregate data without writing SQL queries manually.
What problem does this workflow solve?
Accessing and analyzing database data often requires SQL expertise or dedicated reports, which can be time-consuming. This workflow empowers users to interact with a database conversationally through an AI-powered agent. It dynamically generates SQL queries based on user requests, streamlining data retrieval and analysis.
What this workflow does
This workflow integrates OpenAI with a Supabase database, enabling users to interact with their data via an AI agent. The agent can:
Retrieve records from the database.
Extract and analyze JSON data stored in tables.
Provide summaries, aggregations, or specific data points based on user queries.
Dynamic SQL Querying: The agent uses user prompts to create and execute SQL queries on the database.
Understand JSON Structure: The workflow identifies JSON schema from sample records, enabling the agent to parse and analyze JSON fields effectively.
Database Schema Exploration: It provides the agent with tools to retrieve table structures, column details, and relationships for precise query generation.
Setup
Preparation
Create Accounts:
N8N: For workflow automation.
Supabase: For database hosting and management.
OpenAI: For building the conversational AI agent.
Configure Database Connection:
Set up a PostgreSQL database in Supabase.
Use appropriate credentials (username, password, host, and database name) in your workflow.
N8N Workflow
AI agent with tools:
Code Tool:
Execute SQL queries based on user input.
Database Schema Tool:
Retrieve a list of all tables in the database.
Use a predefined SQL query to fetch table definitions, including column names, types, and references.
Table Definition:
Retrieve a list of columns with types for one table."
Generate SEO Seed Keywords Using AI,https://n8n.io/workflows/2473-generate-seo-seed-keywords-using-ai/,"What this workflow does:
This flow uses an AI node to generate Seed Keywords to focus SEO efforts on based on your ideal customer profile. You can use these keywords to form part of your SEO strategy.
Outputs:
List of 20 Seed Keywords
Setup
Fill the Set Ideal Customer Profile (ICP)
Connect with your credentials
Replace the Connect to your own database with your own database
Pre-requisites / Dependencies
You know your ideal customer profile (ICP)
An AI API account (either OpenAI or Anthropic recommended)
More templates and n8n workflows >>> @simonscrapes"
Automate Customer Support Issue Resolution using AI Text Classifier,https://n8n.io/workflows/2468-automate-customer-support-issue-resolution-using-ai-text-classifier/,"This n8n template is designed to assist and improve customer support team member capacity by automating the resolution of long-lived and forgotten JIRA issues.
How it works
Schedule Trigger runs daily to check for long-lived unresolved issues and imports them into the workflow.
Each Issue is handled as a separate subworkflow by using an execute workflow node. This allows parallel processing.
A report is generated from the issue using its comment history allowing the issue to be classified by AI - determining the state and progress of the issue.
If determined to be resolved, sentiment analysis is performed to track customer satisfaction. If negative, a slack message is sent to escalate, otherwise the issue is closed automatically.
If no response has been initiated, an AI agent will attempt to search and resolve the issue itself using similar resolved issues or from the notion database. If a solution is found, it is posted to the issue and closed.
If the issue is blocked and waiting for responses, then a reminder message is added.
How to use
This template searches for JIRA issues which are older than 7 days which are not in the ""Done"" status. Ensure there are some issues that meet this criteria otherwise adjust the search query to suit.
Works best if you frequently have long-lived issues that need resolving.
Ensure the notion tool is configured as to not read documents you didn't intend it to ie. private and/or internal documentation.
Requirements
JIRA for issues management
OpenAI for LLM
Slack for notifications
Customising this workflow
Why not try classifying issues as they are created? One use-case may be for quality control such as ensuring reporting criteria is adhered to, summarising and rephrasing issue for easier reading or adjusting priority."
Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI,https://n8n.io/workflows/2440-building-rag-chatbot-for-movie-recommendations-with-qdrant-and-open-ai/,"Create a recommendation tool without hallucinations based on RAG with the Qdrant Vector database. This example is based on movie recommendations on the IMDB-top1000 dataset. You can provide your wishes and your ""big no's"" to the chatbot, for example: ""A movie about wizards but not Harry Potter"", and get top-3 recommendations.
How it works
a video with the full design process
Upload IMDB-1000 dataset to Qdrant Vector Store, embedding movie descriptions with OpenAI;
Set up an AI agent with a chat. This agent will call a workflow tool to get movie recommendations based on a request written in the chat;
Create a workflow which calls Qdrant's Recommendation API to retrieve top-3 recommendations of movies based on your positive and negative examples.
Set Up Steps
You'll need to create a free tier Qdrant Cluster (Qdrant can also be used locally; it's open-sourced) and set up API credentials
You'll OpenAI credentials
You'll need GitHub credentials & to upload the IMDB Kaggle dataset to your GitHub."
"AI Voice Chat using Webhook, Memory Manager, OpenAI, Google Gemini & ElevenLabs",https://n8n.io/workflows/2405-ai-voice-chat-using-webhook-memory-manager-openai-google-gemini-and-elevenlabs/,"Who is this for?
This workflow is designed for businesses or developers looking to integrate voice-based chat applications with dynamic responses and conversational memory.
What problem does this solve?
It automates AI-powered voice conversations, maintaining context between sessions and converting speech-to-text and text-to-speech.
What this workflow does:
The workflow receives audio input, transcribes it using OpenAI, and processes the conversation using Google Gemini Chat Model (you can use OpenAI Chat Model). Responses are converted back to speech using ElevenLabs.
Prerequisites:
You'll need API keys for:
OpenAI (you can obtain it from OpenAI website)
ElevenLabs (you can obtain it from their website)
Google Gemini (You can obtain it from Google AI Studio)
Setup:
Configure you API keys
Ensure that the value (voice_message) in the ""Path"" parameter in the Webhook node is used as the name of the parameter that will contain the voice message you are sending via the HTTP Post request."
Chat with PDF docs using AI (quoting sources),https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/,"This workflow allows you to ask questions about a PDF document. The answers are provided by an AI model of your choice, and the answer includes a citation pointing to the information it used.
You can use n8n‚Äôs built-in chat interface to ask the questions, or you could customise this workflow to use another one (e.g. Slack, Teams, etc.)
Example
The workflow is set up with the Bitcoin whitepaper. So you could ask things like:
Question: ‚ÄúWhich email provider does the creator of Bitcoin use?‚Äú
Answer: ‚ÄúGMX [Bitcoin whitepaper.pdf, lines 1-35]‚Äù
Requirements
A Pinecone account (they have a free tier at the time of writing that is easily enough for this workflow)
Access to a large language model (e.g. an OpenAI account)
Customizing this workflow
The workflow only reads in one document, but you could customise it to read in all the documents in a folder (or more).
The workflow is set up to use GPT 3.5, but you could swap that out for any other model (including self-hosted ones)."
ü¶ã Bluesky New Follower Auto DM,https://n8n.io/workflows/4713-bluesky-new-follower-auto-dm/,"ü¶ã Bluesky New Follower Auto DM
Effortlessly send welcome direct messages to new followers on Bluesky ‚Äî no storage, no dependencies, just real-time engagement.
üßë‚Äçüíº Who is this for?
This template is ideal for:
Creators, brands, and community builders on Bluesky looking to engage their followers automatically
n8n beginners who want an easy-to-configure automation with no external dependencies
Anyone seeking a lightweight, stateless solution for follower messaging ‚Äî without needing to manage databases or history
üß© What problem is this workflow solving? / Use case
Automatically welcoming new followers is a powerful way to start building relationships, increase engagement, and show your followers that you‚Äôre active and approachable.
Use it to:
Start conversations and build relationships immediately
Build a loyal community by making people feel seen
Boost follower retention
Automate engagement while keeping your workflow simple
‚öôÔ∏è What this workflow does
Runs every 4 hours (configurable)
Authenticates your Bluesky account via secure app password
Fetches the last 100 Bluesky notifications
Filters for new ""follow"" events
Starts a chat if there is no existing conversation with the new follower
Sends a customized welcome message automatically
All done without having to save data.
üõ†Ô∏è Setup
There is only one node that needs to be modified. In the Setup node:
Enter your Bluesky handle (e.g., yourname.bsky.social)
Create an App Password
‚Üí Generate it here
Customize your welcome message in the Setup node
Then save and activate the workflow
You‚Äôre now ready to automatically greet new followers as they arrive.
üß™ How to customize this workflow to your needs
Update the schedule
Modify the Cron node to change how often the workflow runs (default: every 4 hours)
Personalize the message
Update the welcomeMessage field in the Setup node to reflect your tone, brand, or goals
Add tracking or CRM integration
If you'd like to log interactions or sync with another system, you can extend the workflow with additional nodes."
"Sprint Cycle Announcements with Form Input, GPT-4 and Slack",https://n8n.io/workflows/4510-sprint-cycle-announcements-with-form-input-gpt-4-and-slack/,"How it works
Triggers on submitting an n8n form
Uses the form details to prepare a message
Sends the message to Slack
Set up Steps
Add in your team name
Add in message tone
Set up Open AI
Set up Slack"
"Analyze Crunchbase Startups by Keyword with Bright Data, Gemini AI & Google Sheets",https://n8n.io/workflows/4565-analyze-crunchbase-startups-by-keyword-with-bright-data-gemini-ai-and-google-sheets/,"This n8n workflow automates the discovery, enrichment, and comparative analysis of startups from the Crunchbase dataset via Bright Data, enhanced with AI, and exports structured results to Google Sheets.
üöÄ What It Does
Receives a keyword from the user that describes the area of interest ‚Äî such as an industry, sector, technology, or trend (e.g., ""AI in healthcare"", ""carbon capture"", ""edtech"").
This keyword is used to filter relevant startups from the Crunchbase dataset via Bright Data.
Fetches data from Bright Data's Crunchbase snapshot API.
Extracts and cleans key fields from the JSON response.
Sorts startups by most recent founding date.
Selects the top 10 most recent companies.
Sends these 10 companies to Google Gemini AI for comparative analysis.
Embeds the AI-generated summary into the final export.
Appends results to a Google Sheet for tracking and reporting.
üõ†Ô∏è Step-by-Step Setup
Get user keyword input from a form.
Use 3 Bright Data requests:
Start snapshot.
Poll snapshot status until ready.
Fetch snapshot data in JSON format.
Use a Python Code node to:
Parse and sort companies by founded_date.
Clean and standardize data fields.
Pass the top 10 companies into Gemini AI for comparative insight.
Merge the AI output back with company data.
Send everything to Google Sheets.
üß† How It Works
Snapshot Control: Polls every few seconds until the Bright Data snapshot is complete.
Code Cleanup: Ensures consistent structure and formatting across all records.
Comparative AI Analysis: Gemini compares all 10 companies at once and returns a unified analysis.
Merging Output: AI analysis is merged into the first company‚Äôs record (to avoid duplication), while all 10 are exported.
üì§ Google Sheet Output
Each row includes:
name,
founded,
about,
num_employees,
type,
ipo_status,
full_description,
social_media_links,
address,
website,
funding_total,
num_investors,
lead_investors,
founders,
products_and_services,
monthly_visits,
crunchbase_link,
ai_analysis.
AI comparative analysis summary (only once per batch ‚Äì attached to the first company).
All fields from above customizible through the python code (you can add additional ones from Bright Data output).
üîê Required Credentials
Bright Data ‚Äì Replace YOUR_API_KEY in 3 HTTP Request nodes.
Google Gemini API ‚Äì For AI analysis.
Google Sheets OAuth2 ‚Äì For spreadsheet export.
‚ö†Ô∏è Notes
AI output is shared once per batch of 10 companies, attached to the first company entry. You can configure the limit of batch size in the first ""Code"" node."
Automate Face Swapping for GIFs with Fal.run AI and Google Services,https://n8n.io/workflows/4450-automate-face-swapping-for-gifs-with-falrun-ai-and-google-services/,"This workflow allows you to automatically swap faces in animated GIFs using AI, without writing a single line of code.
By simply inserting the URL of a face image and a GIF into a Google Sheet, the automation takes care of everything: it sends the data to AI platform, monitors the processing status, retrieves the final face-swapped GIF, uploads it to Google Drive, and updates the Google Sheet with the result.
This solution is perfect for content creators, marketers, or developers looking to integrate AI-powered GIF editing into their workflows in a fast and scalable way.
Whether used manually or on a scheduled basis, the workflow turns a tedious creative task into a fully automated pipeline.
This workflow automates GIF face-swapping by integrating Google Sheets for input/output and Fal.run for AI processing, ensuring seamless execution via scheduled or manual triggers.
Example
Face image:
Gif Image:
Result:
How It Works
Trigger:
The workflow can be triggered manually (""When clicking ‚ÄòTest workflow‚Äô"") or automatically via a scheduled trigger (""Schedule Trigger"") set to run at intervals (e.g., every 5 minutes).
Data Retrieval:
The ""Google Sheets"" node fetches data from a predefined Google Sheet, which includes two columns:
FACE IMAGE: URL of the face image to swap.
GIF IMAGE: URL of the target GIF.
API Request:
The ""Set data"" node formats the retrieved URLs into variables (face_image and gif_image).
The ""Create Image"" node sends a POST request to the Fal.run API (easel-gifswap endpoint) with these URLs to initiate the face-swapping process. The API returns a request_id.
Status Check:
The ""Wait 60 sec."" node pauses execution for 60 seconds to allow processing time.
The ""Get status"" node queries the Fal.run API using the request_id to check if the task is COMPLETED.
If completed, the ""Get Url image"" node retrieves the final GIF URL.
Output Handling:
The ""Upload Image"" node saves the resulting GIF to Google Drive.
The ""Update result"" node writes the output GIF URL back to the Google Sheet under the RESULT column.
Set Up Steps
Prepare Google Sheet:
Create a Google Sheet with columns: FACE IMAGE, GIF IMAGE, and RESULT.
Populate the first two columns with image URLs. Leave RESULT empty for the workflow to fill.
Configure API Key:
Sign up to obtain an API key.
In the ""Create Image"" node, set HTTP header authentication:
Name: Authorization
Value: Key YOURAPIKEY
Schedule Execution:
Link the ""Schedule Trigger"" node to run periodically (e.g., every 5 minutes) or trigger manually for testing.
Test and Deploy:
Run the workflow to verify face-swapping functionality.
Monitor the Google Sheet for the RESULT column updates with the processed GIF URL.
Need help customizing?
Contact me for consulting and support or add me on Linkedin."
Analyze BeyondPresence Video Calls with GPT-4o-mini and Google Sheets,https://n8n.io/workflows/4436-analyze-beyondpresence-video-calls-with-gpt-4o-mini-and-google-sheets/,"Transform your BeyondPresence video agent conversations into comprehensive insights by automatically analyzing each call with AI and organizing 35+ data points in Google Sheets. This template helps customer success, support, and training teams save 30+ minutes per call on documentation while ensuring no critical action items or insights are missed.
How it works
Webhook receives completed call data from BeyondPresence including full transcript
Data validation ensures quality and adds enriched metadata (duration, time calculations)
AI analysis (GPT-4) extracts action items, sentiment, decisions, and recommendations
Parse response handles the AI output and structures it for sheets
Auto-append to Google Sheets with 35+ insights per call organized beautifully
Set up steps
Copy our Google Sheets template - One click! Get pre-formatted sheet: BeyondPresence Call Analytics Template
Connect accounts - Add OpenAI API key and Google Sheets OAuth2
Configure webhook - Copy URL from n8n to BeyondPresence Settings ‚Üí Webhooks
Customize AI prompt (optional) - Adjust analysis focus for your use case
Test with a call - Make a test call and watch insights appear!
Setup time: 5-10 minutes
Requirements: BeyondPresence account, OpenAI API key, Google account"
Auto-Post LinkedIn Updates from Spreadsheet Topics using GPT-4o,https://n8n.io/workflows/4344-auto-post-linkedin-updates-from-spreadsheet-topics-using-gpt-4o/,"How it works:
This workflow automates the entire LinkedIn content distribution process ‚Äî from AI-powered post creation to auto-posting on both personal LinkedIn profiles and LinkedIn groups, using GPT-4o and Google Sheets as the content source and control panel.
Auto-generates professional LinkedIn posts from spreadsheet topics using GPT-4o.
Posts to your LinkedIn profile and multiple groups.
Updates status to avoid duplicate posting.
Fully customizable and reusable with your spreadsheet.
Set up Steps
Create and Upload the Spreadsheet
Name it: Linkedin Post
Sheet1 (for post topics): Columns: ID | Linkedin Post Title | Status
Add post titles under Linkedin Post Title
Set Status to Pending
Create new sheet name as ""Groups"" (for group distribution): Column: GroupIds
Add LinkedIn Group IDs, one per row
Connect Google Sheets Nodes
Connect your Google account to these nodes:
Linkedin Post topic (Reads post topics)
Get group id (Reads LinkedIn groups)
Update Status (Writes back the status after posting)
Configure GPT-4o (OpenAI)
Add your OpenAI API key in the Linkedin Post creator node
This node will generate high-quality content from your topic titles
Connect LinkedIn Account
Add your LinkedIn credentials in the Linkedin user detail node
Ensure appropriate permissions to post on profile and groups
Activate the Workflow : Once live, the workflow will:
Monitor the Google Sheet for Pending posts.
Generate content via GPT-4o.
Post to:
Your LinkedIn Profile
Each LinkedIn Group listed in the Groups sheet
Update the post Status to Posted
Customization Tips
Want to personalize this template?
Change AI tone or style in the OpenAI node prompt
Add a scheduler node if you'd like to post at fixed intervals
Use a Slack or Telegram approval step before posting
Integrate analytics tools to track post performance
Suggested Sticky Notes for Workflow
Node or Section Sticky Note Content
Linkedin Post topic Reads the topic titles and statuses from Sheet1
OpenAI (GPT-4o) Generates content using topic title ‚Äî you can modify the tone/prompt here
Linkedin user detail Your personal LinkedIn credentials ‚Äî required to post
Group loop Iterates through LinkedIn Group IDs and posts the content
Update Status Updates spreadsheet so the topic isn't re-posted"
Extract Marketing Testimonials from Feedback with Gemini AI and Google Sheets,https://n8n.io/workflows/4378-extract-marketing-testimonials-from-feedback-with-gemini-ai-and-google-sheets/,"Transform raw customer feedback into powerful testimonial quotes automatically. This intelligent n8n workflow monitors feedback forms, uses AI to identify and extract the most emotionally engaging testimonial content, and organizes everything into a searchable database for your marketing campaigns.
üîÑ How It Works
This streamlined 4-step automation turns feedback into marketing assets:
Step 1: Continuous Feedback Monitoring
The workflow monitors your Google Sheets (connected to feedback forms) every minute, instantly detecting new customer submissions and triggering the extraction process.
Step 2: Intelligent Quote Extraction
Google Gemini AI analyzes each feedback submission using specialized prompts designed to:
Identify emotionally engaging phrases and statements
Extract short, impactful testimonial quotes from longer feedback
Filter out neutral, irrelevant, or negative content
Focus on marketing-ready, quotable customer experiences
Preserve the authentic voice and emotion of the original feedback
Step 3: Automated Database Population
Extracted testimonials are automatically written back to your Google Sheets in a dedicated ""Testimony"" column, creating an organized, searchable database of customer quotes ready for marketing use.
Step 4: Instant Team Notification
Email alerts are sent immediately to your marketing team with each new extracted testimonial, ensuring no valuable social proof goes unnoticed or unused.
‚öôÔ∏è Setup Steps
Prerequisites
Google Workspace account for Forms, Sheets, and Gmail
Google Gemini API access for intelligent quote extraction
n8n instance (cloud or self-hosted)
Basic understanding of Google Forms and customer feedback collection
Required Google Forms Structure
Create a customer feedback form with these essential fields:
üìù Required Form Fields:
- Name (Short answer text)
- Email Address (Email field with validation)
- Feedback (Paragraph text - this is where testimonials are extracted from)
- Testimony (Leave blank - will be auto-populated by AI)
Form Design Best Practices:
Use open-ended questions to encourage detailed responses
Ask specific questions about customer experience and outcomes
Include questions about before/after results for powerful testimonials
Make the feedback field prominent and easy to complete
Configuration Steps
1. Credential Setup
Google Sheets OAuth2: Monitor feedback responses and update testimonial database
Google Gemini API Key: Extract intelligent, emotionally engaging quotes from feedback
Gmail OAuth2: Send automated notifications to marketing team
Google Forms Integration: Ensure seamless data flow from feedback forms
2. Google Sheets Configuration
Verify your feedback response sheet contains proper column structure:
| Timestamp | Name | Email | Feedback | Testimony |
3. AI Extraction Optimization
The default prompt extracts impactful testimonials, but can be customized for:
Industry-Specific Language: Healthcare, technology, finance, retail terminology
Quote Length Preferences: Short punchy quotes vs longer detailed testimonials
Emotional Tone Targeting: Excitement, relief, satisfaction, transformation
Content Focus: Results-oriented, process-focused, or relationship-based testimonials
4. Notification Customization
Email alerts can be configured for:
Multiple Recipients: Marketing team, sales team, customer success
Custom Subject Lines: Include customer name, product type, or urgency indicators
Rich Content: Include full feedback alongside extracted testimonial
Categorization: Different alerts for different product lines or service types
5. Quality Control Implementation
Extraction Confidence: Set minimum quality thresholds for extracted quotes
Manual Review Process: Flag testimonials for human review before publication
Approval Workflows: Add approval steps for high-value or sensitive testimonials
Version Control: Track original feedback alongside extracted quotes
üöÄ Use Cases
E-commerce & Retail
Product Reviews: Extract compelling quotes from detailed product feedback
Customer Success Stories: Identify transformation narratives from user experiences
Social Proof Collection: Build testimonial libraries for product pages and ads
Review Mining: Turn long reviews into short, shareable testimonial quotes
SaaS & Technology Companies
User Experience Feedback: Extract quotes about software usability and impact
ROI Testimonials: Identify statements about business results and efficiency gains
Feature Feedback: Capture specific praise for product capabilities and benefits
Customer Success Metrics: Extract quantifiable results and outcome statements
Professional Services
Client Success Stories: Transform project feedback into powerful case study quotes
Service Quality Testimonials: Extract praise for expertise, communication, and results
Consulting Impact: Identify statements about business transformation and growth
Relationship Testimonials: Capture quotes about trust, partnership, and collaboration
Healthcare & Wellness
Patient Experience: Extract quotes about care quality and health outcomes
Treatment Success: Identify statements about symptom improvement and recovery
Provider Relationships: Capture testimonials about bedside manner and communication
Wellness Journey: Extract quotes about lifestyle changes and health transformations
Education & Training
Student Success Stories: Extract quotes about learning outcomes and career impact
Course Effectiveness: Identify statements about skill development and knowledge gains
Instructor Praise: Capture testimonials about teaching quality and support
Career Transformation: Extract quotes about professional growth and opportunities
üîß Advanced Customization Options
Multi-Category Extraction
Enhance extraction with specialized processing:
- Product-Specific: Extract testimonials for different product lines separately
- Service-Based: Customize extraction for various service offerings  
- Demographic-Focused: Tailor extraction for different customer segments
- Journey-Stage: Extract testimonials for awareness, consideration, and retention phases
Quality Enhancement Features
Implement advanced quality control:
Sentiment Scoring: Rate extracted testimonials for emotional impact
Authenticity Verification: Cross-reference testimonials with customer records
Duplicate Detection: Prevent similar testimonials from the same customer
Content Enrichment: Add context and customer details to extracted quotes
Marketing Integration Extensions
Connect to marketing and sales tools:
Social Media Publishing: Auto-post testimonials to Facebook, LinkedIn, Twitter
Website Integration: Push testimonials to website testimonial sections
Email Marketing: Include fresh testimonials in newsletter campaigns
Sales Enablement: Provide sales team with relevant testimonials for prospects
Analytics and Reporting
Generate insights from testimonial data:
Testimonial Performance: Track which quotes generate most engagement
Customer Satisfaction Trends: Analyze testimonial sentiment over time
Product/Service Insights: Identify most praised features and benefits
Competitive Advantages: Extract testimonials highlighting differentiators
üìä Extraction Examples
Before (Raw Feedback):
""I was really struggling with managing my team's projects and keeping track of all the deadlines. Everything was scattered across different tools and I was spending way too much time just trying to figure out what everyone was working on. Since we started using your project management software about 6 months ago, it's been a complete game changer. Now I can see everything at a glance, our team communication has improved dramatically, and we're actually finishing projects ahead of schedule. The reporting features are amazing too - I can finally show my boss concrete data about our team's productivity. I honestly don't know how we managed without it. The customer support team has been fantastic as well, always quick to help when we had questions during setup.""
After (AI Extracted Testimonial):
""Complete game changer - now I can see everything at a glance, our team communication has improved dramatically, and we're actually finishing projects ahead of schedule.""
Healthcare Example:
Before (Raw Feedback):
""I had been dealing with chronic back pain for over 3 years and had tried everything - physical therapy, medication, different doctors. Nothing seemed to help long-term. When I found Dr. Martinez, I was honestly pretty skeptical because I'd been disappointed so many times before. But after our first consultation, I felt hopeful for the first time in years. She really listened to me and explained everything clearly. The treatment plan she developed was comprehensive but manageable. Within just 2 months, I was experiencing significant pain reduction, and now after 6 months, I'm practically pain-free. I can play with my kids again, sleep through the night, and even started hiking on weekends. Dr. Martinez didn't just treat my symptoms - she helped me get my life back.""
After (AI Extracted Testimonial):
""Within just 2 months, I was experiencing significant pain reduction, and now I'm practically pain-free. Dr. Martinez didn't just treat my symptoms - she helped me get my life back.""
üõ†Ô∏è Troubleshooting & Best Practices
Common Issues & Solutions
Low-Quality Extractions
Improve Feedback Questions: Ask more specific, outcome-focused questions
Refine AI Prompts: Adjust extraction criteria for better quote selection
Set Minimum Length: Ensure feedback has sufficient content for meaningful extraction
Quality Scoring: Implement rating system for extracted testimonials
Insufficient Feedback Volume
Multiple Feedback Channels: Collect testimonials through various touchpoints
Incentivized Feedback: Offer small rewards for detailed feedback submissions
Follow-up Automation: Send feedback requests to satisfied customers
Timing Optimization: Request feedback at optimal moments in customer journey
Privacy and Consent Issues
Permission Management: Ensure customers consent to testimonial use
Attribution Control: Allow customers to specify how they want to be credited
Approval Workflows: Implement customer approval before publishing testimonials
Data Protection: Maintain compliance with privacy regulations
Optimization Strategies
Extraction Quality Enhancement
Prompt Engineering: Continuously refine AI prompts based on output quality
A/B Test Extractions: Test different extraction approaches for effectiveness
Human Review Integration: Combine AI extraction with human editorial oversight
Context Preservation: Maintain customer context alongside extracted quotes
Marketing Integration
Campaign Alignment: Extract testimonials that support specific marketing campaigns
Audience Segmentation: Categorize testimonials for different target audiences
Channel Optimization: Format testimonials for specific marketing channels
Performance Tracking: Monitor which testimonials drive best marketing results
Process Automation
Multi-Stage Processing: Implement multiple extraction and refinement steps
Quality Gates: Add checkpoints for testimonial quality and relevance
Workflow Branching: Route different types of feedback to appropriate processes
Error Handling: Implement fallbacks for failed extractions or poor-quality feedback
üìà Success Metrics
Extraction Efficiency
Processing Speed: Reduce time from feedback submission to usable testimonial
Success Rate: Percentage of feedback submissions yielding quality testimonials
Quote Quality: Average rating of extracted testimonials by marketing team
Volume Increase: Growth in testimonial collection and database size
Marketing Impact
Testimonial Usage: Frequency of extracted testimonials in marketing campaigns
Conversion Rates: Impact of AI-extracted testimonials on sales metrics
Social Proof Effectiveness: Engagement rates on testimonial-based content
Customer Acquisition: Attribution of new customers to testimonial-driven campaigns
üìû Questions & Support
Need help implementing your AI Testimonial Extractor Agent?
üìß Specialized Technical Support
Email: Yaron@nofluff.online
Response Time: Within 24 hours on business days
Expertise: AI testimonial extraction, feedback form optimization, marketing automation
üé• Comprehensive Learning Library
YouTube Channel: https://www.youtube.com/@YaronBeen/videos
Complete setup guides for feedback form design and AI extraction
Advanced prompt engineering techniques for testimonial quality
Integration tutorials for marketing platforms and social media
Best practices for customer feedback collection and testimonial usage
Troubleshooting common extraction and quality issues
ü§ù Professional Marketing Community
LinkedIn: https://www.linkedin.com/in/yaronbeen/
Connect for ongoing testimonial marketing automation support
Share your customer success story automation achievements
Access exclusive templates for feedback forms and testimonial campaigns
Join discussions about social proof marketing and customer experience automation
üí¨ Support Request Guidelines
Include in your support message:
Your industry and typical customer feedback patterns
Current testimonial collection process and challenges
Specific marketing channels where testimonials will be used
Volume expectations and quality requirements
Integration needs with existing marketing tools
Ready to turn every customer feedback into marketing gold? Deploy this AI Testimonial Extractor Agent and build a powerful testimonial database that drives sales and builds trust with prospects automatically!"
Extract Product Info from Webpage Screenshots using Dumpling AI and GPT-4o,https://n8n.io/workflows/4329-extract-product-info-from-webpage-screenshots-using-dumpling-ai-and-gpt-4o/,"Who is this for?
This workflow is perfect for eCommerce teams, market researchers, and product analysts who want to track or extract product information from websites that restrict scraping tools. It‚Äôs also useful for virtual assistants handling product comparison tasks.
What problem is this workflow solving?
Many eCommerce and retail sites use dynamic content or anti-bot protections that make traditional scraping methods unreliable. This workflow bypasses those issues by taking a screenshot of the full page, using OCR to extract visible text, and summarizing product information with GPT-4o‚Äîall fully automated.
What this workflow does
This workflow monitors a Google Sheet for new URLs. Once a new link is added, it performs the following steps:
Trigger on New URL in Sheet ‚Äì Watches for new rows added to a Google Sheet.
Screenshot URL via Dumpling AI ‚Äì Sends the URL to Dumpling AI‚Äôs screenshot endpoint to capture a full-page image of the product webpage.
Save Screenshot to Drive Folder ‚Äì Uploads the screenshot to a specific Google Drive folder for reference or logging.
Extract Text from Screenshot with Dumpling AI ‚Äì Uses Dumpling AI‚Äôs image-to-text endpoint to pull all visible content from the screenshot.
Extract Product Info from Screenshot Text with GPT-4o ‚Äì Sends the extracted raw text to GPT-4o, prompting it to identify structured product information such as product name, price, ratings, deals, and purchase options.
Split Each Product Entry ‚Äì Splits the GPT response (an array of product objects) so each product becomes an individual item for saving.
Save Products info to Google Sheet ‚Äì Appends each product‚Äôs structured details to a separate sheet in the same spreadsheet.
Setup
Google Sheet
Create a Google Sheet with at least two sheets:
Sheet1 should contain a header row with a column labeled URL.
Sheet2 should contain headers: Product Name, price, purchased, ratings, deal, buyingOptions.
Connect your Google account in both the trigger and final write-back node.
Dumpling AI
Sign up at Dumpling AI
Create an API key and use it for both HTTP modules:
Screenshot URL via Dumpling AI
Extract Text from Screenshot with Dumpling AI
The screenshot endpoint used is https://app.dumplingai.com/api/v1/screenshot.
Google Drive
Create a folder for storing screenshots.
In the Save Screenshot to Drive Folder node, select the correct folder or provide the folder ID.
Make sure permissions allow uploading from n8n.
OpenAI
Provide an API key for GPT-4o in the Extract Product Info from Screenshot Text with GPT-4o node.
The prompt is structured to return structured product listings in JSON format.
Split & Save
Split Each Product Entry takes the array of product objects from GPT and makes each one a separate execution.
Save Products info to Google Sheet writes structured fields into Sheet2 under:
Product Name, price, purchased, ratings, deal, buyingOptions.
How to customize this workflow
Adjust the GPT prompt to return different product fields (e.g., shipping info, product categories).
Use a filter node to limit which types of products get written to the final sheet.
Add sentiment analysis to analyze review content if available.
Replace Google Drive with Dropbox or another file storage app.
Notes
Make sure you monitor your API usage on both Dumpling AI and OpenAI to avoid rate limits.
This setup is great for snapshot-based extraction where scraping is blocked or unreliable."
Airtable Base Backups to S3,https://n8n.io/workflows/4302-airtable-base-backups-to-s3/,"This workflow exports every table in a base as its own CSV, saves the files in a time-stamped folder in Amazon S3, pings you on Slack, and optionally prunes older copies. You get an automated weekly backup that is easy to inspect or re-import as needed. You can easily swap the S3 node for the storage provider of your choice.
++How it works++
Weekly Backup
Schedule trigger fires weekly
Sets and formats the week ex. [2025-W12]
Create a folder in S3 bucket with the week
Loops through all tables in Airtable base creating CSVs and uploading to the new path
Slack message is sent on completion
Monthly Prune
Schedule trigger fires weekly
Sets a cut-off date 4 weeks in the past
Lists folders in S3
Deletes all folders > 4 weeks old
++Setup Steps++
Clone workflow
Swap credentials for Airtable, AWS, and Slack
Ensure AWS credential has appropriate IAM policy to manage bucket & objects
Set workflow to ""Active"""
Evaluation metric example: Categorization,https://n8n.io/workflows/4269-evaluation-metric-example-categorization/,"AI evaluation in n8n
This is a template for n8n's evaluation feature.
Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow.
By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't.
How it works
This template shows how to calculate a workflow evaluation metric: whether a category matches the expected one.
The workflow takes support tickets and generates a category and priority, which is then compared with the correct answers in the dataset.
We use an evaluation trigger to read in our dataset
It is wired up in parallel with the regular trigger so that the workflow can be started from either one. More info
Once the category is generated by the agent, we check whether it matches the expected one in the dataset
Finally we pass this information back to n8n as a metric"
Evaluation metric example: Check if tool was called,https://n8n.io/workflows/4268-evaluation-metric-example-check-if-tool-was-called/,"AI evaluation in n8n
This is a template for n8n's evaluation feature.
Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow.
By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't.
How it works
This template shows how to calculate a workflow evaluation metric: whether a specific tool was called by an agent.
We use an evaluation trigger to read in our dataset
It is wired up in parallel with the regular trigger so that the workflow can be started from either one. More info
We make sure that the agent outputs the list of tools that it used
We then check whether the expected tool (from the dataset) is in that list
Finally we pass this information back to n8n as a metric"
Generate Audio from Text Scripts using Self-Hosted Bark Model and Google Drive,https://n8n.io/workflows/4241-generate-audio-from-text-scripts-using-self-hosted-bark-model-and-google-drive/,"Audio Generator ‚Äì Documentation
üéØ Purpose:
Generate audio files from text scripts stored in Google Drive.
üîÅ Flow:
Receive repo IDs.
Fetch text scripts.
Generate .wav files using local Bark model.
Upload back to Drive.
üì¶ Dependencies:
Python script: /scripts/generate_voice.py
Bark (voice generation system)
n8n instance with access to local shell
Google Drive OAuth2 credentials
‚úèÔ∏è Notes:
Script filenames must end with .txt
Only works with plain text
No external API used = 100% free
üì¶ /scripts/generate_voice.py:
import sys
import torch
import numpy
import re
from bark import SAMPLE_RATE, generate_audio, preload_models
from scipy.io.wavfile import write as write_wav

# Patch to allow numpy._core.multiarray.scalar during loading
torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])

# Monkey patch torch.load to force weights_only=False
_original_torch_load = torch.load
def patched_torch_load(f, *args, **kwargs):
    if 'weights_only' not in kwargs:
        kwargs['weights_only'] = False
    return _original_torch_load(f, *args, **kwargs)
torch.load = patched_torch_load

# Preload Bark models
preload_models()

def split_text(text, max_len=300):
    # Split on punctuation to avoid mid-sentence cuts
    sentences = re.split(r'(?&lt;=[.?!])\s+', text)
    chunks = []
    current = """"
    for sentence in sentences:
        if len(current) + len(sentence) &lt; max_len:
            current += sentence + "" ""
        else:
            chunks.append(current.strip())
            current = sentence + "" ""
    if current:
        chunks.append(current.strip())
    return chunks

# Input text file and output path
input_text_path = sys.argv[1]
output_wav_path = sys.argv[2]

with open(input_text_path, 'r', encoding='utf-8') as f:
    full_text = f.read()

voice_preset = ""v2/en_speaker_7""

chunks = split_text(full_text)

# Generate and concatenate audio chunks
audio_arrays = []
for chunk in chunks:
    print(f""Generating audio for chunk: {chunk[:50]}..."")
    audio = generate_audio(chunk, history_prompt=voice_preset)
    audio_arrays.append(audio)

# Merge all audio chunks
final_audio = numpy.concatenate(audio_arrays)

# Write final .wav file
write_wav(output_wav_path, SAMPLE_RATE, final_audio)

print(f""Full audio generated at: {output_wav_path}"")"
"Personalized Taiwan Indie Music Recommendations with AI, Star Sign & Weather via Spotify",https://n8n.io/workflows/4210-personalized-taiwan-indie-music-recommendations-with-ai-star-sign-and-weather-via-spotify/,"This n8n workflow recommends Taiwan indie music based on a user's city, mood, birthday, today's weather, and star sign. Here's a concise overview:
Trigger: Starts manually with the ""When clicking ‚ÄòTest workflow‚Äô"" node.
Input Setup: The ""infomation"" node sets user inputs (e.g., city: Taipei, mood: Happy, birthday: 1996/11/21).
Song Recommendation: The ""get song recommendation"" node uses OpenAI's GPT-4o-mini to:
Fetch today's weather for the specified city.
Determine the user's zodiac sign from their birthday.
Check the zodiac sign's daily fortune.
Recommend a Taiwan indie song considering weather and fortune.
Explain the song choice and highlight its features.
Return results in JSON format.
Data Extraction: The ""Information Extractor"" node parses the JSON output, extracting fields like date, city, weather, zodiac sign, fortune, song, artist, and additional info.
Spotify Search: The ""Spotify"" node searches for the recommended song using the artist and song name, retrieving a Spotify URL.
Final Output: The ""Final Output"" node compiles all data, including the Spotify link, into a structured format.
Additional Note: A ""Sticky Note"" provides context about the workflow's purpose and credits the creator, n8nguide.
This workflow integrates AI, weather data, astrology, and Spotify to deliver personalized Taiwan indie music recommendations."
Real-time Gaming Strategy Coach with Telegram & GPT-4o Vision,https://n8n.io/workflows/4155-real-time-gaming-strategy-coach-with-telegram-and-gpt-4o-vision/,"Play smarter, not harder.
This AI-powered Telegram bot acts as your real-time gaming coach, giving you instant move recommendations for turn-based strategy games like poker, dominoes, mahjong, and more‚Äîbased on screenshots, voice notes, or text prompts.
Powered by GPT-4o (Vision + Language), the agent helps casual gamers improve decision-making, strategy, and pattern recognition in friendly games.
üß† Whether you're at the table or playing online, this agent reads the board and suggests your best next move.
üîß Key Features
üß† GPT-4o Vision: Analyzes images (e.g. poker hands, domino boards)
üéôÔ∏è Voice-to-Text Coaching: Transcribe audio and analyze intent
‚úçÔ∏è Smart Text Input: Ask for help directly in chat
üì© Telegram-Powered: Easy to use via a simple bot
üõ† Setup Instructions
1. Import the Workflow
Upload the JSON file into your n8n instance.
2. Create a Telegram Bot
Use @BotFather and connect your token under Telegram credentials.
3. Configure OpenAI API
Add your GPT-4o credentials
Enable both chat model and image vision model access
4. Secure Access
Replace the Telegram ID in the User Authentication node to limit access to your account only.
5. Test It
Send a screenshot or voice message to your Telegram bot
Example: ‚ÄúTexas Hold‚Äôem ‚Äì my turn ‚Äì suggest move‚Äù + upload an image
üí° What Can You Ask?
‚ÄúPoker ‚Äì flop: A‚ô†Ô∏è Q‚ô†Ô∏è 10‚ô†Ô∏è ‚Äì I have K‚ô†Ô∏è J‚ô†Ô∏è ‚Äì what now?‚Äù
‚ÄúDominoes ‚Äì screenshot attached ‚Äì should I play this tile?‚Äù
‚ÄúMahjong ‚Äì what‚Äôs the best discard move right now?‚Äù
üß† System Behavior (Prompt Logic)
GPT-4o is guided by a focused system prompt that makes it act as a gaming strategist, not a rulebook. It returns results like:
üß† Best Move Suggestion: [e.g. Raise with K‚ô†Ô∏è J‚ô†Ô∏è]  
üéØ Why This Move Works: [based on probability & game state]  
üìà Confidence Level: High / Medium / Low
If input lacks clarity, the agent will ask for better context or a new screenshot.
üéÆ Supported Input Modes
üñºÔ∏è Image-based: e.g. a poker or domino board screenshot
üé§ Voice prompts: e.g. ‚ÄúMy turn in blackjack, I have 16‚Ä¶‚Äù
üí¨ Plain text: typed strategy questions
üìå Sticky Notes (Included)
Telegram Trigger + Access Control
Image & Voice Input Routes
GPT-4o Vision Integration
Short-Term Memory
LangChain Agent
Structured Output Format
Telegram Response Node
Disclaimer + Licensing Note
‚ö†Ô∏è Disclaimer & Licensing
This tool is provided by Treasurium Capital under an educational, entertainment-focused license.
Do not use for gambling, cheating, or violating game platform terms of use.
Treasurium Capital and its creators are not liable for any game outcomes.
üë§ Author & Support
Built by Don Jayamaha
View more AI agents: https://n8n.io/creators/don-the-gem-dealer/
üéØ Level up your gameplay with AI‚Äîone move at a time.
üé• Watch the Live Demo:"
Route User Requests to Specialized Agents with GPT-4o Mini,https://n8n.io/workflows/4150-route-user-requests-to-specialized-agents-with-gpt-4o-mini/,"This n8n workflow template is designed to route user input to specialized agents (like a Reminder Agent, Email Agent, etc.) using a structured output from a language model. Here's a complete description of what it does and how each part works:
üîÅ Workflow Purpose:
This template receives a user's request via Webhook, processes it using an LLM, extracts structured data like the agent name and user query, and routes the input to the appropriate sub-workflow (agent) based on the specified agent type.
üß© Workflow Breakdown:
1. Webhook (Trigger)
Node: Webhook
Purpose: Accepts a POST request from any frontend or API source. It contains the raw user input.
2. GPT Model (LLM Inference)
Node: GPT 4o Mini
Purpose: Interprets the user input and determines:
Which agent should handle it (e.g., ""Reminder Agent"", ""Email Agent"", etc.)
The actual user request (in structured format)
3. Auto-Fixing Output Parser
Node: Auto-fixing Output Parser
Purpose: Ensures that the output from the LLM matches the expected structure. If there's a mismatch, it automatically corrects it using a re-prompt.
4. Structured Output Parser
Node: Structured Output Parser
Purpose: Converts the language model's response into a strict JSON structure with keys like:
""Agent Name""
""user input""
""sessionID""
5. Agent Router
Node: Switch (""Agent Route"")
Purpose: Based on ""Agent Name"", it routes the input to one of the following sub-workflows:
üìÖ Reminder Agent
üìß Email Agent
üßæ Document Agent
ü§ù Meeting Agent
6. Sub-Workflow Call (Execute Workflow)
Each agent is implemented as a separate n8n workflow:
The input is forwarded to the selected agent.
For example, if ""Agent Name"" is ""Reminder Agent"", the workflow ""Reminder Agent"" is called with ""user input"".
7. Webhook Response
After the sub-agent workflow finishes, a Respond to Webhook node sends the output back as an HTTP response.
‚úÖ Key Features:
Fully modular and extensible
LLM-driven routing using OpenRouter GPT-4o
Auto-corrects structured output errors
Clean separation of concerns (agent logic is decoupled in sub-workflows)
Easily add more agents by updating the switch logic
üì¶ Use Case Examples:
User says: ‚ÄúRemind me to call my mom tomorrow.‚Äù
‚Üí Routed to Reminder Agent
User says: ‚ÄúSend an email to the HR team.‚Äù
‚Üí Routed to Email Agent
User says: ‚ÄúSchedule a meeting with John next week.‚Äù
‚Üí Routed to Meeting Agent"
Bulk Delete All YouTube Playlists From Your Channel,https://n8n.io/workflows/4156-bulk-delete-all-youtube-playlists-from-your-channel/,"üßë‚Äçüíº Who is this for?
This workflow is for any YouTube user who wants to bulk delete all playlists from their own channel ‚Äî whether to start fresh, clean up old content, or prepare the account for a new purpose.
It‚Äôs useful for:
Creators reorganizing their channel
People transferring content to another account
Anyone who wants to avoid deleting playlists manually one by one
üß† What problem is this workflow solving?
YouTube does not offer a built-in way to delete multiple playlists at once. If you have dozens or hundreds of playlists, removing them manually is extremely time-consuming.
This workflow automates the entire deletion process in seconds, saving you hours of repetitive effort.
‚öôÔ∏è What this workflow does
Connects to your YouTube account
Fetches all playlists you‚Äôve created (excluding system playlists)
Deletes them one by one automatically
‚ö†Ô∏è This action is irreversible. Once a playlist is deleted, it cannot be recovered. Use with caution.
üõ†Ô∏è Setup
üîê Create a YouTube OAuth2 credential in n8n for your channel.
üß≠ Assign the credential to both YouTube nodes.
‚úÖ Click ‚ÄúTest workflow‚Äù to execute.
üü® By default, this workflow deletes everything. If you want to be more selective, see the customization tips below.
üß© How to customize this workflow to your needs
‚úÖ Add a confirmation flag
Insert a Set node with a custom field like confirm_delete = true, and follow it with an IF node to prevent accidental execution.
‚úÇÔ∏è Delete only some playlists
Add a Filter node after fetching playlists ‚Äî you can match by title, ID, or keyword (e.g. only delete playlists containing ‚Äúold‚Äù).
üõë Add a pause before deletion
Insert a Wait or NoOp node to give you a moment to cancel before it runs.
üîÅ Adapt to scheduled cleanups
Use a Cron trigger if you want to periodically clear temporary playlists."
Tesla 1day Indicators Tool (Macro-Level Technical AI),https://n8n.io/workflows/4098-tesla-1day-indicators-tool-macro-level-technical-ai/,"üìÖ Analyze Tesla‚Äôs daily trading structure with AI using 6 Alpha Vantage indicators.
This tool evaluates long-term trend health, volatility patterns, and potential reversal signals at the 1-day timeframe. Designed for use within the Tesla Financial Market Data Analyst Tool, this agent helps swing and position traders anchor macro sentiment.
‚ö†Ô∏è Not standalone. Must be executed via Execute Workflow
üîå Requires:
Tesla Quant Technical Indicators Webhooks Tool
Alpha Vantage Premium API Key
OpenAI GPT-4.1 credentials
üîç What It Does
This tool queries a secured webhook (/1dayData) to retrieve real-time, trimmed JSON data for:
RSI (Relative Strength Index)
BBANDS (Bollinger Bands)
SMA (Simple Moving Average)
EMA (Exponential Moving Average)
ADX (Average Directional Index)
MACD (Moving Average Convergence Divergence)
These values are then passed to a LangChain AI Agent powered by GPT-4.1, which returns:
A 2‚Äì3 sentence market condition summary
Structured indicator values
Timeframe tag (""1d"")
üìã Sample Output
{
  ""summary"": ""TSLA shows consolidation on the daily chart. RSI is neutral, BBANDS are contracting, and MACD is flattening."",
  ""timeframe"": ""1d"",
  ""indicators"": {
    ""RSI"": 51.3,
    ""BBANDS"": {
      ""upper"": 192.80,
      ""lower"": 168.20,
      ""middle"": 180.50,
      ""close"": 179.90
    },
    ""SMA"": 181.10,
    ""EMA"": 179.75,
    ""ADX"": 15.8,
    ""MACD"": {
      ""macd"": -0.25,
      ""signal"": -0.20,
      ""histogram"": -0.05
    }
  }
}
üß† Agent Components
Component Description
1day Data (HTTP Node) Pulls latest data from secured /1dayData webhook
OpenAI Chat Model GPT-4.1 powers the analysis logic
Tesla 1day Indicators Agent LangChain agent performing interpretation
Simple Memory Short-term session continuity
üõ†Ô∏è Setup Instructions
Import Workflow into n8n
Name: Tesla_1day_Indicators_Tool
Add Required Credentials
Alpha Vantage Premium (via HTTP Query Auth)
OpenAI GPT-4.1 (Chat Model)
Install Webhook Fetcher
Required: Tesla Quant Technical Indicators Webhooks Tool
Endpoint /1dayData must be active
Execution Context
This tool is only triggered via:
üëâ Tesla Financial Market Data Analyst Tool
Inputs expected:
message: optional context
sessionId: session memory linkage
üìå Sticky Notes Overview
üìò Tesla 1-Day Indicators Tool ‚Äì Purpose and integration
üì° Webhook Fetcher ‚Äì Pulls daily Alpha Vantage data via HTTPS
üß† GPT-4.1 Model ‚Äì Reasoning for trend classification
üîó Sub-Agent Trigger ‚Äì Used only by Financial Market Analyst
üß† Memory Buffer ‚Äì Ensures consistent session logic
üîí Licensing & Support
¬© 2025 Treasurium Capital Limited Company
This workflow‚Äîincluding prompts, logic, and formatting‚Äîis protected IP.
üîó Don Jayamaha ‚Äì LinkedIn
üîó Creator Profile
üöÄ Evaluate long-term Tesla price behavior with AI-enhanced technical analysis‚Äîcritical for swing trading strategy.
Required by the Tesla Financial Market Data Analyst Tool."
Tesla Quant Technical Indicators Webhooks Tool,https://n8n.io/workflows/4095-tesla-quant-technical-indicators-webhooks-tool/,"üì° This workflow serves as the central Alpha Vantage API fetcher for Tesla trading indicators, delivering cleaned 20-point JSON outputs for three timeframes: 15min, 1hour, and 1day.
It is required by the following agents:
Tesla 15min, 1h, 1d Indicators Tools
Tesla Financial Market Data Analyst Tool
‚úÖ Requires an Alpha Vantage Premium API Key
üöÄ Used as a sub-agent via webhook endpoints triggered by other workflows
üìà What It Does
For each timeframe (15min, 1h, 1d), this tool:
Triggers 6 technical indicators via Alpha Vantage:
RSI
MACD
BBANDS
SMA
EMA
ADX
Trims the raw response to the latest 20 data points
Reformats into a clean JSON structure:
{
  ""indicator"": ""MACD"",
  ""timeframe"": ""1hour"",
  ""data"": {
    ""timestamp"": ""..."",
    ""macd"": 0.32,
    ""signal"": 0.29
  }
}
Returns results via Webhook Respond for the calling agent
üìÇ Required Credentials
üîë Alpha Vantage Premium API Key
Set up under Credentials &gt; HTTP Query Auth
Name: Alpha Vantage Premium
Query Param: apikey
Get yours here: https://www.alphavantage.co/premium/
üõ†Ô∏è Setup Steps
Import Workflow into n8n
Name it: Tesla_Quant_Technical_Indicators_Webhooks_Tool
Add HTTP Query Auth Credential
Name: Alpha Vantage Premium
Param key: apikey
Value: your Alpha Vantage key
Publish and Use the Webhooks
This workflow exposes 3 endpoints:
/15minData ‚Üí used by 15m Indicator Tool
/1hourData ‚Üí used by 1h Indicator Tool
/1dayData ‚Üí used by 1d Indicator Tool
Connect via Execute Workflow or HTTP Request
Ensure caller sends webhook trigger correctly to the path
üß± Architecture Summary
Each timeframe section includes:
Component Details
üì° Webhook Trigger Entry node (/15minData, /1hourData, etc.)
üîÑ API Calls 6 nodes fetching indicators via Alpha Vantage
üßπ Formatters JS Code nodes to clean and trim responses
üß© Merge Node Consolidates cleaned JSONs
üöÄ Webhook Respond Returns structured data to calling workflow
üßæ Sticky Notes Overview
‚úÖ Webhook Entry: Instructions per timeframe
‚úÖ API Call Summary: Alpha Vantage endpoint for each indicator
‚úÖ Format Nodes: Explain JSON parsing and cleaning
‚úÖ Merge Logic: Final output format
‚úÖ Webhook Response: What gets returned to caller
All stickies follow n8n standard color-coding:
Blue = Webhook flow
Yellow = API request group
Purple = Formatters
Green = Merge step
Gray = Workflow overview and usage
üîê Licensing & Support
¬© 2025 Treasurium Capital Limited Company
This agent is part of the Tesla Quant AI Trading System and protected under U.S. copyright.
For support:
üîó Don Jayamaha ‚Äì LinkedIn
üîó n8n Creator Profile
üöÄ Use this API tool to feed Tesla technical indicators into any AI or trading agent across 15m, 1h, and 1d timeframes.
Required for all Tesla Quant Agent indicator tools."
"AI Chatbot Call Center: Taxi Booking Support (Production-Ready, Part 7)",https://n8n.io/workflows/4051-ai-chatbot-call-center-taxi-booking-support-production-ready-part-7/,"Workflow Name: ü´∂ Taxi Booking Support
Template was created in n8n v1.90.2
Skill Level: Mid
Categories: n8n, Chatbot
Stacks
Schedule Trigger node
Postgres node
AI Agent node
Google Calendar node
Execute Sub-workflow
If node, Switch node, Code node, Edit Fields (Set)
Prerequisite
Sub-workflow: Demo Call Back (or your own node)
Production Features
Scaling Design for n8n Queue mode in production environment
Customize Expired Booking Actions example
Multi-Language Design
What this workflow does?
This is a n8n Taxi Booking Support, the background node to process the job at scheduled. It is scheduled to check the database for outstanding booking and handle the after sales process. In this particular case, it will check for OPEN booking over 10 minutes, then update the booking status from OPEN to CANCELLED, delete the Calendar event and send a reply to the user.
How it works
The Schedule Trigger node is scheduled to run every 5 minutes.
It will check the database for OPEN or HOLD booking.
For OPEN booking
update the booking status to CANCELLED
delete the Calendar event
send a reply to the user
Optional: The AI Agent is used to create the reply message to the user in Multi-language based on the language set in the booking.
Set up instructions
Pull and Set up the required SQL from our Github repository.
Create you Postgres credentials, refer to n8n integration documentation for more information.
Select your Credentials in Open Hold Booking and Set Cancel Booking.
Create your Google Calendar credentials, refer to n8n integration documentation for more information.
Create a Google Calendar, e.g. DEMO
Select your Credentials in Delete Event, and select the above Calendar
Remember to activate this workflow for schedule to run.
How to adjust it to your needs
There should be more status for the booking.
The current action only check for OPEN and HOLD booking, you can do more based on your needs.
You can replace the sub-workflow trigger Call Back to another flow as needs."
Automatic Jest Test Generation for GitHub PRs with Dual AI Review,https://n8n.io/workflows/4013-automatic-jest-test-generation-for-github-prs-with-dual-ai-review/,"Workflow: Automatic Unit Test Creator from GitHub
üèóÔ∏è Architecture Overview
This workflow listens for GitHub pull-request events, analyzes changed React/TypeScript files, auto-generates Jest tests via AI, has them reviewed by a second AI pass, and posts suggestions back as PR comments:
GitHub Webhook ‚Üí PR opened or updated
Fetch & Diff ‚Üí Retrieve raw diff of changed files
Filter & Split ‚Üí Isolate .tsx files & their diffs
Fetch File Contents ‚Üí Provide full context for tests
Test Maker Agent ‚Üí Generate Jest tests for diff hunks
Code Reviewer Agent ‚Üí Refine tests for style & edge-cases
Post PR Comment ‚Üí Sends suggested tests back to GitHub
üì¶ Node-by-Node Breakdown
flowchart LR
  A[Webhook: /github/pr-events] --&gt; B[GitHub: Get PR]
  B --&gt; C[Function: Parse diff_url + owner/repo]
  C --&gt; D[HTTP Request: GET diff_url]
  D --&gt; E[Function: Split on ""diff --git""]
  E --&gt; F[Filter: /\.tsx$/]
  F --&gt; G[GitHub: Get File Contents]
  G --&gt; H[Test Maker Agent]
  H --&gt; I[Code Reviewer Agent]
  I --&gt; J[Function: Build Comment Payload]
  J --&gt; K[HTTP Request: POST to PR Comments]
Webhook: GitHub PR Events
Type: HTTP Webhook (/webhook/github/pr-events)
Subscribed Events: pull_request.opened, pull_request.synchronize
GitHub: Get PR
Node: GitHub
Action: ""Get Pull Request""
Inputs: owner, repo, pull_number
Function: Parse diff_url + owner/repo
Extracts:
diff_url (e.g. ‚Ä¶/pulls/123.diff)
owner, repo, merge_commit_sha
HTTP Request: GET diff_url
Fetches unified-diff text for the PR.
Function: Split on ""diff --git""
Splits the diff into file-specific segments.
Filter: /.tsx$/
Keeps only segments where the file path ends with .tsx.
GitHub: Get File Contents
For each .tsx file, fetches the latest blob via GitHub API.
Test Maker Agent
Prompt:
""Generate Jest unit tests covering only the behaviors changed in these diff hunks.""
Output: Raw Jest test code.
Code Reviewer Agent
Reads file + generated tests
Prompt:
""Review and improve these tests for readability, edge-cases, and naming conventions.""
Output: Polished test suite.
Function: Build Comment Payload
Wraps tests in TypeScript fences:
// generated tests‚Ä¶
Constructs JSON:
{ ""body"": ""&lt;tests&gt;"" }
HTTP Request: POST to PR Comments
URL: https://api.github.com/repos/{owner}/{repo}/issues/{pull_number}/comments
Body: Contains the suggested tests.
üîç Design Rationale & Best Practices
Focused Diff Analysis
Targets only .tsx files to cover UI logic.
Two-Stage AI
Separate ""generate"" + ""review"" steps mimic TDD + code review.
Stateless Functions
Pure JS for parsing & transformation, easy to test.
Non-Blocking PR Comments
Asynchronous suggestions‚Äîdevelopers aren't blocked.
Scoped Permissions
GitHub token limited to reading PRs & posting comments."
AI Email Triage & Alert System with GPT-4 and Telegram Notifications,https://n8n.io/workflows/3968-ai-email-triage-and-alert-system-with-gpt-4-and-telegram-notifications/,"Fully Automated AI Email System for n8n: Triage, Summarize & Alert‚ÄîNever Check Your Inbox Again!
Fully Automated Email Intelligence: Triage, Summarize, Escalate & Alert‚ÄîNever Check Your Inbox Again
üöÄ Overview
Turn every minute you waste in your inbox into revenue‚Äëgenerating focus time.
This is more than a workflow‚Äîit‚Äôs a profit engine disguised as an email assistant.
‚öôÔ∏è Eliminate 90‚ÄØ% of inbox noise with AI triage and smart labeling.
üö® Instantly surface high‚Äëvalue leads & crises via Telegram push alerts‚Äîbefore they cost you.
üì∞ Digest newsletters in 60‚ÄØseconds with bite‚Äësize AI briefs that keep you ahead of the curve.
ü§ñ Hand off replies, snoozes, and archiving to a tireless autonomous agent that never slips.
Set‚Äëup takes 10‚ÄØminutes. The payoff starts within the first hour‚Äîhours reclaimed, opportunities captured, and the calm certainty your inbox is handled perfectly, 24/7.
üéØ Who's It For?
Startup Founders & CEOs ‚Äì Stop wasting hours on inbox cleanup. Automatically surface leads, triage support, and escalate ops issues while you focus on growth.
Agencies & Consultants ‚Äì Instantly prioritize high-value client messages, flag urgent ops threads, and never drop the ball on critical communication.
Busy Professionals & Teams ‚Äì Reclaim 5‚Äì10 hours weekly by letting AI handle noise, flag what matters, and summarize newsletters into actionable insights.
No-Code Builders & AI Engineers ‚Äì Plug in, customize, and ship production-ready inbox automation in minutes‚Äîno training required.
Remote Operators & Virtual Assistants ‚Äì Delegate inbox management confidently with a system that flags what‚Äôs important and handles the rest.
üìå High‚ÄëImpact Results
What You‚Äôre Dealing With ‚Üí What This System Solves Instantly
Inbox Clutter & Constant Noise ‚Üí 90% Noise Reduction
Trivial, low-value messages are automatically triaged and labeled‚Äîonly high-value threads stay in your spotlight.
Missing Critical Emails ‚Üí Real-Time Telegram Alerts
Get instantly notified the moment a client, lead, or ops-critical email lands. Zero lag. Zero drop.
Manual Reply Fatigue ‚Üí AI-Generated Drafts in Seconds
Fully contextual, brand-aligned replies ready at a click‚Äîcut response time from minutes to seconds.
Newsletter Overload ‚Üí Actionable 60-Second Summaries
Dense newsletters converted into digestible, decision-ready highlights. Save hours while staying sharp.
Information Chaos ‚Üí Fully Structured Inbox Intelligence
From raw email to labeled insights to escalated alerts‚Äîyour inbox becomes an automated decision support system.
This system doesn‚Äôt just ""help""‚Äîit replaces hours of work per week, prevents costly misses, and ensures you never drown in emails again.
‚öôÔ∏è Powerful Features
üì• AI Email Triage ‚Äì Classifies and sorts emails using GPT/Gemini, labeling by intent and urgency.
üì∞ Newsletter Summarizer ‚Äì Extracts concise, actionable insights from newsletters into structured summaries.
üö® Real-Time Priority Alerts ‚Äì Instantly escalates mission-critical emails via Telegram.
ü§ñ Autonomous Inbox Agent ‚Äì Proactively manages email actions: replies, drafts, archives, and thread labeling.
üß† Persistent Context Memory ‚Äì Remembers senders, conversation history, and adjusts responses accordingly.
üèÖ Why Choose This Solution?
Fully Self-Hosted & Secure ‚Äì Keep full control of your data and privacy. No external storage, no SaaS lock-in‚Äîjust your infrastructure, your rules.
Production-Grade Reliability ‚Äì Built by a Lead Software Engineer. Tested, trusted, and optimized for uptime by myself.
AI-Native Workflow Engine ‚Äì Smart, adaptive, and deeply embedded in your processes. Automates with context, learns with use, and gets smarter over time.
üí∞ Pricing
Limited Time Launch Price: $29 (One-time payment)
üéØ Lifetime access‚Äîpay once, save time forever.
‚è±Ô∏è Designed to reclaim 5‚Äì10 hours per week per inbox.
üö® One missed priority email could cost more‚Äîthis system prevents that.
‚ö° Launch offer pricing will increase SOON!‚Äîlock in early.
üì¶ What's Included?
‚úÖ Fully documented and annotated n8n workflow (ready-to-import JSON)
üìÇ Gmail labeling system (production-ready taxonomy)
üì≤ Telegram integration and alert configuration
üõ† Customizable AI prompt templates for Gemini/GPT
üß† Custom prompt templates for GPT & Gemini
‚öôÔ∏è Autonomous email categorization & filtering
üì∞ Newsletter summarizer with structured JSON output
ü§ñ Optional AI agent for full inbox management tasks
üöÄ Quick Start Setup
Import the full workflow into your self-hosted n8n environment.
Connect your Gmail, Telegram, and preferred LLMs (OpenAI or Gemini).
Deploy Gmail labels using the included taxonomy‚Äîensures proper triage and filtering.
Trigger test alerts from real email data to verify AI classification and Telegram escalation.
Go live‚Äîstart receiving summarized insights, escalations, and triaged inbox updates within minutes.
‚ùì FAQ
Q: What kind of ROI can I expect?
A: On average, users reclaim 5‚Äì10 hours per week per inbox by eliminating low-priority noise, surfacing urgent emails instantly, and automating replies and summaries. For a business leader or agency, that‚Äôs hundreds of dollars in time savings weekly‚Äîand zero dropped balls.
Q: Can it scale for my team?
A: Absolutely. It‚Äôs architected with modular nodes and reusable components so you can rapidly duplicate, extend, or adapt it across teams, departments, and workflows. Whether you‚Äôre a solo founder or managing 20+ inboxes, this system grows with you‚Äîwithout code rewrites.
Q: Is my data secure?
A: Absolutely!. This system runs entirely in your self-hosted n8n environment, so your data never leaves your control. External LLMs (like OpenAI or Gemini) are only used when needed and receive minimal, context-limited input. You control all API keys and endpoints. No data is stored externally unless you configure it.
Made with ‚ù§Ô∏è by Bluegrass Digital Advantage"
Control Discord Bot with Natural Language via MCP Server,https://n8n.io/workflows/3946-control-discord-bot-with-natural-language-via-mcp-server/,"What it is-
I wanted to create a simple, easy-to-use, MCP server for your Discord bot(s).
How to set up-
Literally all you do is select your bot auth (or crease a new Discord Bot auth if you havn't entered your key in n8n before) and that's IT!
How to use it-
You can now ask your bot to do things via any MCP client, including from within N8N workflows!
Note:
If you need an example, you can check out my simple quickstart Discord MCP Server that uses 4o to send messages to channels on your server and users who are members of the server the bot is in."
Control your discord server with natural language via GPT4o and MCP Client,https://n8n.io/workflows/3945-control-your-discord-server-with-natural-language-via-gpt4o-and-mcp-client/,"What it is-
Very simple connection to your Discord MCP Server and 4o.
How to set it up-
Just specify your MCP Server's url, select your OpenAI credential, and you're set!
How to use it-
You can now send a chat message to the production URL from anywhere and the actions will occur on discord! It really is that easy.
Note: If you don't yet have a Discord MCP server set up, there is a template called ""Discord MCP Server"" to get you a jumpstart!"
Track LLM Token Costs per Customer Using the Langchain Code Node,https://n8n.io/workflows/3440-track-llm-token-costs-per-customer-using-the-langchain-code-node/,"Note: This template only works for self-hosted n8n.
This n8n template demonstrates how to use the Langchain code node to track token usage and cost for every LLM call.
This is useful if your templates handle multiple clients or customers and you need a cheap and easy way to capture how much of your AI credits they are using.
How it works
In our mock AI service, we're offering a data conversion API to convert Resume PDFs into JSON documents.
A form trigger is used to allow for PDF upload and the file is parsed using the Extract from File node.
An Edit Fields node is used to capture additional variables to send to our log.
Next, we use the Information Extractor node to organise the Resume data into the given JSON schema.
The LLM subnode attached to the Information Extractor is a custom one we've built using the Langchain Code node.
With our custom LLM subnode, we're able to capture the usage metadata using lifecycle hooks.
We've also attached a Google Sheet tool to our LLM subnode, allowing us to send our usage metadata to a google sheet.
Finally, we demonstrate how you can aggregate from the google sheet to understand how much AI tokens/costs your clients are liable for.
Check out the example Client Usage Log - https://docs.google.com/spreadsheets/d/1AR5mrxz2S6PjAKVM0edNG-YVEc6zKL7aUxHxVcffnlw/edit?usp=sharing
How to use
SELF-HOSTED N8N ONLY - the Langchain Code node is only available in the self-hosted version of n8n. It is not available in n8n cloud.
The LLM subnode can only be attached to non-""AI agent"" nodes; Basic LLM node, Information Extractor, Question & Answer Chain, Sentiment Analysis, Summarization Chain and Text Classifier.
Requirements
Self-hosted version of n8n
OpenAI for LLM
Google Sheets to store usage metadata
Customising this template
Bring the custom LLM subnode into your own templates! In many cases, it can be a drop-in replacement for the regular OpenAI subnode.
Not using Google Sheets? Try other databases or a HTTP call to pipe into your CRM."
Automated Discord Spam Moderation with AI and Human-in-the-Loop,https://n8n.io/workflows/3351-automated-discord-spam-moderation-with-ai-and-human-in-the-loop/,"This n8n template demonstrates how you can automate community moderation using human-in-the-loop functionality for Discord.
The use-case is for detecting and dealing with spam messages in a predefined and consistent way. Human-in-the-loop allows for a balance between overly aggressive bots and time and effort from the moderation team.
How it works
A scheduled trigger is used to scan the most recent messages in a Discord Channel. Messages are tagged via the ""Remove Duplicates"" node so they don't get processed again in the future.
Messages are grouped by user to allow for minimising of number of notifications sent.
An AI text classifier node is then used to detect for spam in each user's message.
When detected, a notification is sent to a moderation channel using the Send-and-wait mode for Discord. This notification comes with an n8n form and dropdown list of predefined actions to take in dealing with the spam messages. Once sent the workflow waits until a response is received.
Once a moderator selects an action, the workflow continues and carries out a predefined moderation action.
How to use
Depending on how busy your community is and subject to spammers, you may need to increase the scheduled interval.
Add as many or few moderation actions as required.
Remember to activate the workflow to get it started.
Requirements
Discord channel for messages to moderate
OpenAI for text classification
Customising this template
It is possible to cover multiple channels. Add as many as your community needs.
Not using Discord. The template can also work in slack or other services which offer the same bot functionality."
Sync Youtube Video URLs with Google Sheets,https://n8n.io/workflows/3897-sync-youtube-video-urls-with-google-sheets/,"Sync Youtube Videos with Google Sheets
(Part 1 of Youtube comments sentiment analyze automation along with detailed dashboard)
This workflow is the first part of a multi-part automation system designed to perform large-scale YouTube comment sentiment analysis alongwith detailed dashboard. It solves the problem of manually tracking new videos across multiple YouTube channels by automatically fetching and organizing video URLs in a Google Sheet, setting the stage for deeper analysis in Part 2.
What It Does
Reads Channel IDs from Sheet3 of a connected Google Sheet.
Fetches the latest videos from each Channel ID using the YouTube Data API.
Extracts video URLs and metadata (like title and publish date).
Appends the video data to Sheet2 of the same Google Sheet ‚Äî this sheet is later used by Part 2 for further processing.
Part of a Multi-Step System
This is Part 1 of a 2-workflow system:
Part 1 (this workflow) populates a sheet with the latest videos from a list of channels.
Part 2 reads the video URLs from Sheet2, fetches comments for each video, analyzes their sentiment using OpenAI, and stores structured results in Sheet1.
üëâ Continue to Part 2 ‚Äì YouTube Comment Sentiment Analyzer with Google Sheets & OpenAI
‚úÖ Use Cases
Monitor and organize new videos from a list of YouTube channels
Automate content pipelines for social media teams and analysts
Build scalable datasets for comment and sentiment analysis
Perfect for creators, agencies, or data analysts managing multiple YouTube accounts
üîß Apps Used
Google Sheets ‚Äì To read and write channel/video data
YouTube ‚Äì To fetch video data from public channels
üí° Why Use This?
Manually checking YouTube channels for new content is time-consuming and error-prone. This automation ensures your data stays current and structured ‚Äî enabling consistent tracking and deeper analysis (especially when paired with Part 2). It brings speed, scale, and automation to your YouTube content operations.
How to Customize
1. Modify Trigger Settings
Change the Google Sheet (Sheet 3) channel ID entry to track other channels.
Use a time-based trigger to fetch new videos regularly, ensuring your data stays up to date.
2. Adjust Output Fields
Fetch additional details from YouTube, such as view count, description, or thumbnails.
Add custom columns in Sheet 2 for organizing videos by different criteria, such as:
""Published Date""
""Video Type""
""View Count""
""Video Description""
3. Extend with Integrations
Integrate with other workflows like YouTube Comment Sentiment Analysis (Part 2) for a deeper dive into content analysis.
Use filters to fetch videos by certain tags, keywords, or publish dates.
4. Adjust Sheet Structure
Modify the structure of Sheet 2 to categorize videos based on criteria like:
Channel
Video Status (e.g., ""Published,"" ""Scheduled"")
Video Type (e.g., ""Tutorial,"" ""Review"")
5. Schedule Regular Fetching
Set a schedule trigger to fetch videos at regular intervals (e.g., daily or weekly), ensuring new content is automatically added to your sheet.
6. Customize Google Sheet Layout
Change the layout of Sheet 2 to better fit your needs. For example, you can add additional columns for"
"PostgreSQL Conversational Agent with Claude & DeepSeek (Multi-KPI, Secure)",https://n8n.io/workflows/3892-postgresql-conversational-agent-with-claude-and-deepseek-multi-kpi-secure/,"üß† Conversational PostgreSQL Agent
Enable AI-driven conversations with your PostgreSQL database using a secure and visual-free agent powered by n8n‚Äôs Model Context Protocol (MCP). This template allows users to ask multiple KPIs in a single message, returning consolidated insights ‚Äî more efficient than the original Conversing with Data template.
üöÄ Why This Template
Unlike the Conversing with Data workflow, which handles one KPI per message, this version:
‚úÖ Supports multi-KPI questions
‚úÖ Returns structured, human-readable reports
‚úÖ Uses fewer AI calls, making it faster and cheaper
‚úÖ Avoids raw SQL execution for enhanced security
üí≤ Estimated cost per full multi-request run: ~$0.01
This template is optimized for efficiency. Each message can return 2‚Äì4 KPIs (You can change the MaxIteration of the Agent to make it more, it is currently set up at 30 iterations) using a single Claude 3.5 Haiku session and DeepSeek-based SQL generation ‚Äî balancing speed, reasoning, and affordability.
üí¨ Sample Use Case
User:
‚ÄúCan you show product performance, revenue trends, and top 5 customers?‚Äù
Agent:
Uses ListTables and GetTableSchema
Generates three SQL queries using get_query_and_data
Returns:
üìä Product Performance
High-Waist Jeans ‚Äî 10 units, $1,027 revenue
Denim Jacket ‚Äî 10 units, $783 revenue
üìà Sales Trends
Peak Month: January 2024 ‚Äî 32 units, $2,378
Average Monthly Units: 10‚Äì16
üßç Customer Insights
Bob Brown ‚Äî $1,520 spent
Diana Wilson ‚Äî $925 spent
All from one natural prompt.
üñºÔ∏è Real-World Interaction Screenshot
üß∞ What‚Äôs Inside
Node Purpose
MCP Server Trigger Receives user queries via /mcp/...
AI Agent + Memory Understands and plans multi-step queries
Think Tool Breaks down the user‚Äôs question into structured goals
get_query_and_data Generates SQL securely from natural language
ListTables, GetSchema AI tools to explore DB safely
Read/Insert/Update Tools Execute structured operations (never raw SQL)
checkdatabase Subflow Validates SQL, formats response as clean text
ü§ñ Model Selection Recommendations
This template uses two types of models, selected for cost-performance balance and role alignment:
1. Claude 3.5 Haiku (Anthropic) ‚Äì for the MCP Agent
The main conversational agent uses Claude 3.5 Haiku, ideal for MCP because it was built by Anthropic ‚Äî the creators of the MCP standard. It‚Äôs fast, affordable, and performs excellently in tool-calling and reasoning tasks.
2. DeepSeek ‚Äì for the SQL subworkflow
The subworkflow that turns natural language into SQL uses DeepSeek. It‚Äôs one of the most affordable and performant models available today for structured outputs like SQL, making it a perfect fit for utility logic.
‚úÖ This setup provides top-tier reasoning + low-cost execution.
üîê Security Benefits
No raw SQL accepted from the user or LLM
All queries are parameterized
Schema is dynamically retrieved
Final output is clean, safe, and human-readable
üß™ Try a Prompt
‚ÄúShow me the top 5 products by units sold and revenue, total monthly sales trend, and top 5 customers by spending.‚Äù
In one message, the agent will:
Generate and run multiple queries
Use the schema to validate logic
Return a single, comprehensive answer
üõ† How to Use
üì• Upload both workflow files into your n8n instance:
Build_your_own_PostgreSQL_MCP_server_No_visuals_.json
checkdatabase.json
üîê Set up PostgreSQL credentials (e.g. ‚ÄúPostgres account 3‚Äù)
üß† Confirm model setup:
Claude 3.5 Haiku for the main agent
DeepSeek for the subflow
üåê Use the /mcp/... URL from the MCP Server Trigger to connect your frontend or chatbot
üó£ Ask questions naturally ‚Äî the agent takes care of planning, querying, and formatting
üîÑ Customization Ideas
Swap Claude or DeepSeek for OpenAI, Mistral, Gemini, etc.
Export insights to Slack, Notion, or Google Sheets
Add Switch nodes to control access to specific tables
Integrate with any front-end app, internal dashboard, or bot
üì¶ What's Included
Build_your_own_PostgreSQL_MCP_server_No_visuals_.json ‚Äì MCP agent logic
checkdatabase.json ‚Äì SQL generation and formatting utility workflow
üìù These must be uploaded into your n8n workspace for the template to function.
üìä Comparison: Conversing with Data vs This Workflow
Feature Conversing with Data This Workflow
Handles multi-KPI questions ‚ùå No ‚úÖ Yes
Secure query execution ‚úÖ Yes ‚úÖ Yes
Structured response ‚ö†Ô∏è JSON / raw ‚úÖ Clean natural language
Cost-efficiency ‚ö†Ô∏è More calls ‚úÖ Optimized with fewer calls
Endpoint support ‚ùå Manual interaction ‚úÖ MCP-ready (/mcp/...)
üîó Prefer something more lightweight and cost-sensitive?
Try the original Conversing with Data template (single KPI + chart support):
Conversing with Data: Transforming Text into SQL Queries and Visual Curves
I used this version for over 3 months and only spent $0.80 total, making it a great entry point if you're just getting started or on a limited budget.
üìö More from the Same Creator
Looking for a different kind of AI reporting workflow?
Explore:
Customer Feedback Analysis with AI, QuickChart & HTML Report Generator
‚Üí Automatically analyze customer input and generate full reports with insights and charts.
Customer Feedback Analysis with AI, QuickChart & HTML Report Generator"
Send YouTube Video Summaries to Obsidian via Dropbox,https://n8n.io/workflows/3886-send-youtube-video-summaries-to-obsidian-via-dropbox/,"How it works
This workflow runs on a schedule you set (default is every 10 minutes). It fetches all videos from a specific YouTube playlist, retrieves details and transcripts for each video using RapidAPI, cleans the transcript text, and then uses OpenAI (GPT models) to generate detailed notes, YAML frontmatter, and internal links formatted for Obsidian. Finally, it assembles everything into a Markdown file, saves it to your designated Dropbox folder, and removes the processed video from the source YouTube playlist.
Set up steps
Getting started should take about 5 minutes. A Quick Start Guide (PDF) is included in the download with step-by-step instructions!
Here's a quick overview of the setup:
Connect Credentials: Add your n8n credentials for YouTube (OAuth2), OpenAI, and Dropbox (OAuth2).
Add API Keys:
RapidAPI: You'll need a key for the yt-api service. The template uses ={{ $env[""RAPIDAPI_API_KEY""] }}. You can either set up an environment variable in n8n named RAPIDAPI_API_KEY or simply delete that expression in the ""Get Video Details"" and ""Get Video Transcript"" nodes and paste your API key directly into the x-rapidapi-key field.
Configure Nodes:
Update the Playlist ID in the ""Get Playlist Videos"" node to your target YouTube playlist.
(Optional) Adjust the Save Path in the ""Save Note to Dropbox"" node if you want to save files to a different folder.
Activate: Turn on the workflow.
Bonus Trick: Symlink the Dropbox folder (using the Dropbox desktop app) to a folder in your Obsidian vault for beautifully organized YouTube summary notes!"
"Travel Planning Agent with Couchbase Vector Search, Gemini 2.0 Flash and OpenAI",https://n8n.io/workflows/3881-travel-planning-agent-with-couchbase-vector-search-gemini-20-flash-and-openai/,"Disclaimer: this workflow template uses the n8n-nodes-couchbase community package. Community nodes are unverified and usage of them comes with some risks. See here for instructions on installing n8n community nodes.
This template is intended for use by those interested in learning more about Agentic AI workflow development, as well as those interested in learning how to use the Couchbase Search Vector Store node for practical applications.
This workflow helps users decide on travel destinations based on descriptions of several points of interest loaded into Couchbase and retrieved using Vector Search.
How it Works
This template contains two workflows:
The Data Ingestion workflow uses the following nodes
Webhook node (to listen for HTTP requests)
OpenAI Embeddings node (to generate embeddings on document insertion)
Note: You‚Äôll need to configure OpenAI credentials for this node
Couchbase Vector node (configured for document insertion)
Default Data Loader and Recursive Character Text Splitter
The Chat Application workflow uses the following nodes
Chat Trigger node
AI Tools Agent node connect to:
Gemini (as the Chat Model, for generating responses)
Note: You will have to configure Gemini credentials for this node
Simple Memory (as the Memory, to maintain conversation context)
Couchbase Search Vector node (as the Tool, for search)
OpenAI Embeddings node (as the Embedding model for the Couchbase Search Vector node, to convert queries to vectors)
Note: You‚Äôll need to configure OpenAI credentials for this node
Set up
Setting up this workflow is easy and only takes around 10 minutes.
Prerequisites
A Couchbase Cluster running the Search Service, and corresponding database access credentials
Be sure the Couchbase cluster allows the incoming IP address for n8n
Create a Vector Search Index using this index definition
Create a bucket (called travel-agent), scope (called vectors), and collection (called points-of-interest) in your Cluster
OpenAI API Key
Gemini API Key
Steps
Configure all necessary credentials (Couchbase, OpenAI, and Gemini)
Select your bucket, scope, and collection for each of the Couchbase vector nodes
Ingest data, either using the cURL statements found on the sticky note within the workflow, or using this shell script to ingest 6 points of interest
Open the chat and test out your travel agent!
Customization and Next Steps
This workflow template can be made more robust by enhancing the data model to include more information about each point of interest. For example, the addition of price ranges, ideal seasons to visit, activity types, and accomodation options can help inform the LLM further about each destination, and in turn allow it to provide a more tailored response and be more helpful for travel planning.
Alternatively, the data model could be entirely re-configured to suit a wide variety of other use cases. This template can serve as a building block for all sorts of AI Agent applications using RAG and is not limited to only travel recommendations."
üåç AI Interpreter and Translator for WhatsApp ‚Äì Translate Voice & Text,https://n8n.io/workflows/3875-ai-interpreter-and-translator-for-whatsapp-translate-voice-and-text/,"üåç AI WhatsApp Translator ‚Äì Multilingual Voice & Text Translator for n8n
Hi! I'm Bruno ‚Äî I‚Äôve been building AI-powered workflows for n8n and Make for 2+ years, focused on smart automation and real conversational agents.
This powerful workflow allows you to receive WhatsApp messages (text or audio), automatically detect the sender‚Äôs language using their phone prefix, translate the content using GPT-4o via LangChain, and reply instantly via WhatsApp ‚Äî all with native tone and cultural fluency.
It‚Äôs ideal for international support, global communities, multilingual bots, and agencies managing clients from different countries.
üîß How it works
Message (text or voice) is received via webhook (WhatsApp API or Evolution API)
Sender's prefix is mapped to a country/language
Audio messages are transcribed using OpenAI Whisper
The text is translated using LangChain agent with GPT-4o
The reply is structured in JSON and sent back via WhatsApp
üß∞ Nodes & Tech Used
HTTP Request (Webhook & API calls)
OpenAI Whisper (Audio transcription)
LangChain + GPT-4o (LLM for translation)
If, Set, Merge (Routing and logic)
Custom JSON response for WhatsApp formats
üìå What you‚Äôll need
OpenAI API key
WhatsApp API or Evolution API access
n8n (Cloud or self-hosted)
(Optional) Typebot, Airtable, CRM for integration
üõí Want to use it?
‚ù§Ô∏è Buy workflows: https://iloveflows.com
‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda"
Store Chat Data in Supabase PostgreSQL for WhatsApp/Slack Chatbot,https://n8n.io/workflows/3867-store-chat-data-in-supabase-postgresql-for-whatsappslack-chatbot/,"n8n Template: Store Chat Data in Supabase PostgreSQL for WhatsApp/Slack Integration
This n8n template captures chat data (like user ID, name, or address) and saves it to a Supabase PostgreSQL database. It‚Äôs built for testing now but designed to work with WhatsApp, Slack, or similar platforms later, where chat inputs aren‚Äôt predefined.
Guide with images can be found on: https://github.com/JimPresting/Supabase-n8n-Self-Hosted-Integration/
Step 1: Configure Firewall Rules in Your VPC Network
To let your n8n instance talk to Supabase, add a firewall rule in your VPC network settings (e.g., Google Cloud, AWS, etc.).
Go to VPC Network settings.
Add a new firewall rule:
Name: allow-postgres-outbound
Direction: Egress (outbound traffic)
Destination Filter: IPv4 ranges
Destination IPv4 Ranges: 0.0.0.0/0 (allows all; restrict to Supabase IPs for security)
Source Filter:
Pick IPv4 ranges and add the n8n VM‚Äôs IP range, or
Pick None if any VM can connect
Protocols and Ports:
Protocol: TCP
Port: 5432 (default PostgreSQL port)
Save the rule.
Step 2: Get the Supabase Connection String
Log into your Supabase Dashboard.
Go to your project, click the Connect button in the header.
Copy the PostgreSQL connection string:
postgresql://postgres.fheraruzdahjd:[YOUR-PASSWORD]@aws-0-eu-central-1.pooler.supabase.com:6543/postgres
Replace [YOUR-PASSWORD] with your Supabase account password (no brackets) and replace the string before that with your actual unique identifier.
Note the port (6543 or 5432)‚Äîuse what‚Äôs in the string.
Step 3: Set Up the n8n Workflow
This workflow takes chat data, maps it to variables, and stores it in Supabase. It‚Äôs built to handle messy chat inputs from platforms like WhatsApp or Slack in production.
Workflow Steps
Trigger Node: ""When clicking 'Test workflow'"" (manual trigger).
For now, it‚Äôs manual. In production, this will be a WhatsApp or Slack message trigger, which won‚Äôt have a fixed input format.
Set Node: ""Set sample input variables (manual)"".
This node sets variables like id, name, address to mimic chat data.
Why? Chat platforms send unstructured data (e.g., a message with a user‚Äôs name or address). We map it to variables so we can store it properly. The id will be something unique like a phone number, account ID, or account number.
Sample Agent Node: Uses a model (e.g., GeminiFlash2.0 but doesn't matter).
This is a placeholder to process data (e.g., clean or validate it) before saving. You can skip or customize it.
Supabase PostgreSQL Node: ""Supabase PostgreSQL Database"".
Connects to Supabase using the connection string from Step 2.
Saves the variables (id, name, address) to a table.
Why store extra fields? The id (like a phone number or account ID) is the key. Extra fields like name or address let us keep all user info in one place for later use (e.g., analytics or replies).
Output Node: ""Update additional values e.g., name, address"".
Confirms the data is saved. In production, this could send a reply to the chat platform.
Why This Design?
Handles Unstructured Chat Data: WhatsApp or Slack messages don‚Äôt have a fixed format. The ""Set"" node lets us map any incoming data (e.g., id, name) to our database fields.
Scales for Production: Using id as a key (phone number, account ID, etc.) with extra fields like name makes this workflow flexible for many use cases, like user profiles or support logs.
Future-Ready: It‚Äôs built to swap the manual trigger for a real chat platform trigger without breaking.
Step 4: Configure the Supabase PostgreSQL Node
In the n8n workflow, set up the Supabase PostgreSQL node:
Host: aws-0-eu-central-1.pooler.supabase.com (from the connection string)
Port: 6543 (or what‚Äôs in the connection string)
Database: postgres
User: postgres.fhspudlibstmpgwqmumo (from the connection string)
Password: Your Supabase password
SSL: Enable (Supabase usually requires it)
Set the node to Insert or Update:
Map id to a unique column in your Supabase table (e.g., phone number, account ID).
Map fields like name, address to their columns.
Test the workflow to confirm data saves correctly.
Security Tips
Limit Firewall Rules: Don‚Äôt use 0.0.0.0/0. Find Supabase‚Äôs IP ranges in their docs and use those.
Hide Passwords: Store your Supabase password in n8n‚Äôs environment variables.
Use SSL: Enable SSL in the n8n node for secure data transfer."
"Automate Research-Based Newsletters with Perplexity, GPT-4, and Image Generation",https://n8n.io/workflows/3862-automate-research-based-newsletters-with-perplexity-gpt-4-and-image-generation/,"This n8n workflow automates the process of creating and sending high-quality newsletters with images generated by a GPT image generator. It is triggered on a schedule.
What is included?
1 n8n workflow (json)
1 Setup tutorial doc
1 video guidance on how to connect n8n with all Google products
1 video guidance on how to connect n8n with Google Cloud Storage
Who is this for?
This template is ideal for:
Content creators looking to automate their newsletter process.
Marketers seeking to efficiently produce engaging newsletters with AI-generated content and imagery.
Anyone wanting to create and send personalized newsletters on a schedule.
What problem is this workflow solving?
Manually creating and sending newsletters can be time-consuming, especially when including relevant and engaging content and imagery. This workflow automates the content generation (using AI and web search), image creation, and sending process, freeing up time and ensuring consistent communication with clients.
What this workflow does:
This workflow automates the following steps:
Scheduled Trigger: The workflow is triggered automatically on a predefined schedule.
Get Newsletter Topic: Retrieves the first unpublished newsletter topic from a Google Sheet titled ""Newsletter Topics"". It then updates the status of this topic and records the publication date to avoid duplication in future runs.
AI Newsletter & Image Generator: This is the core content creation step. It uses an AI agent (configured with OpenAI and Perplexity APIs) to generate an engaging newsletter and a detailed, ultra-realistic image prompt based on the selected topic and real-time web research.
GPT Image Generation: Uses the OpenAI API to generate an image based on the detailed image prompt created in the previous step.
Get Client List: Retrieves client names and email addresses from a Google Sheet titled ""Clients"".
Send Newsletter: Sends the personalized newsletter via Gmail to each client on the list, including the AI-generated title, content, and the uploaded image. The email body HTML is customizable.
Setup:
Configure the workflow's run frequency within the ""Schedule Trigger"" node.
Create a Google Sheet named ""Newsletter Topics"" and ""Clients"" in your Google Drive.
Enter your OpenAI and Perplexity API key in the 'Input' node.
Connect your Gmail account to the ""SendNewsletter"" node.
How to customize this workflow:
Adjust the prompt for your newsletter style in the ""AI Agent"" node.
Adjust the Email Body HTML for your newsletter style in the ""SendNewsletter"" node.
Optionally, enable the ""Tavily WebSearch"" node and configure it if you prefer to use Tavily instead of Perplexity for web search.
Category:
Marketing, Content Creation, Automation"
Analyze Competitor Facebook Ads with AI (GPT-4 & Gemini) & Email Reports,https://n8n.io/workflows/3839-analyze-competitor-facebook-ads-with-ai-gpt-4-and-gemini-and-email-reports/,"Facebook Ads Competitor Creative Analysis & Automated Email Report
üìù What this workflow does
This workflow automates the process of competitor creative analysis for Facebook Pages. When a user submits their email and a target Facebook Page URL via a web form, the workflow:
Scrapes the latest image and video Facebook ads from the submitted Page using Apify.
Deduplicates and selects the top 5 image and top 5 video ads for further analysis.
Applies advanced AI analysis:
Uses OpenAI Vision (GPT-4) to analyze image ads.
Uses Google Gemini to analyze video ads.
Aggregates all ad insights and generates a structured, visually engaging creative analytics report.
Sends the report directly to the user‚Äôs email with ad previews and detailed creative evaluation.
üë• Who is this for?
Marketers and agencies conducting competitor research on Facebook.
Brand managers who want quick, actionable insights into rival ad creative.
Growth hackers and performance advertisers analyzing top-performing creative trends.
Anyone in need of an automated, AI-based Facebook ad evaluation and reporting tool using n8n.
üéØ What problem does this workflow solve?
Manual ad analysis is time-consuming: Scraping, filtering, and reviewing Facebook competitor ads by hand is labor-intensive.
Creative insight requires expertise: Understanding the message, visuals, and targeting of competitor ads needs marketing/creative skill.
Lack of automated reporting: Easily and visually summarize findings for yourself or stakeholders‚Äîwithout manual collation or formatting.
This workflow provides a turnkey, end-to-end solution for competitor creative analysis, combining real ad data with unbiased, AI-driven creative insights in a shareable email format.
‚öôÔ∏è Setup instructions (About 10-15 minutes)
Before using this AI marketing automation tool, you'll need:
Add api key to environment variable
Add required credentials
üîß How to Customize This Workflow
Adapt Image/Video Analysis Goals: Modify the prompts or parameters sent to OpenAI Vision and Gemini to suit your specific analysis objectives‚Äîe.g., evaluate branding consistency, detect call-to-action effectiveness, or extract emotional tone.
Revise Email Report Design: Customize the formatting, insights, and visuals in the delivered report to match your organization‚Äôs standards or stakeholder preferences.
Expand Input Fields: Add more fields to the user-facing form, such as a competitor list, campaign dates, or target audience details, for more granular analysis."
Auto-Generate MVP Startup Ideas from Reddit with AI & Excel Storage,https://n8n.io/workflows/3824-auto-generate-mvp-startup-ideas-from-reddit-with-ai-and-excel-storage/,"üß† Reddit MVP Generator ‚Äì Auto-Generate Startup Ideas from Real User Pain Points
The Reddit MVP Generator is a fully automated business idea mining system built in n8n. It scans trending posts and user comments on Reddit to identify real-world pain points and uses an AI-powered chain to generate detailed, structured MVP business ideas‚Äîincluding industry, business type, revenue potential, and startup cost estimates.
üöÄ Key Features
Live Reddit Scraper: Select from a curated list of entrepreneur and business subreddits (e.g., r/smallbusiness, r/entrepreneur, r/marketing) to fetch top trending posts.
Comment Analysis: Pulls Reddit comments and flattens threads to extract community pain points.
AI Business Generator: Uses OpenRouter-compatible LLMs (like GPT-4o-mini or Gemini Flash) to analyze post content and comments. Outputs are structured JSON objects with MVP ideas.
Zero Duplicate Analysis: Automatically deduplicates previously analyzed Reddit posts using a dynamic blocklist.
Excel Integration: Saves each MVP idea directly into your Microsoft Excel 365 sheet, organized by MVP, Industry, Pain Point, Startup Costs, and Revenue potential.
Easy Subreddit Selection: User-friendly form trigger lets you choose your target subreddit without editing the workflow.
Offline-Friendly: All results are stored in Excel for later review or distribution‚Äîideal for content creators, market researchers, or startup founders.
üõ†Ô∏è Requirements
Free n8n account (self-hosted or cloud)
Reddit developer credentials
OpenRouter API key (supports Qwen, GPT-4o-mini, Gemini)
Microsoft Excel 365 with Azure OAuth2 app setup
üì¶ What You Get
Complete n8n workflow file
Setup notes inside the workflow (via Sticky Notes)
Instructions for Excel table formatting and OAuth2 setup
Output schema designed for downstream automation or AI agents"
"YouTube to WhatsApp Sales Automation with WordPress, FluentCRM and Whinta",https://n8n.io/workflows/3808-youtube-to-whatsapp-sales-automation-with-wordpress-fluentcrm-and-whinta/,"üöÄ WhatsApp Automation Template
Designed & Developed by Infridet Solutions Private Limited
üîß Objective:
Automate your lead nurturing and sales process from YouTube/Instagram ‚Üí Landing Page ‚Üí CRM ‚Üí Email ‚Üí WhatsApp ‚Üí Sales ‚Üí Deal Closure using tools like:
üåê WordPress (Landing Page + Fluent Forms)
üßæ Google Sheets (Backup Log)
üì© FluentCRM (Lead Tagging + Email Sequences)
üí¨ Whinta.com (WhatsApp Messaging API)
‚öôÔ∏è N8N (Workflow Automation Engine)
üß© System Flow Overview:
Lead Source: YouTube or Instagram CTA
Landing Page: Built on WordPress with a story-driven design
Form Capture: Fluent Forms with dynamic input fields
Data Sync:
Backup to Google Sheets
Push lead to FluentCRM and tag as New Lead
Email Sequence:
Warm-up emails (1 to 5)
Introduce offer or service
WhatsApp Outreach:
Send personalized message via Whinta
Triggered 1 hour after form fill or last email
Sales Follow-Up:
Sales team handles replies manually
CRM tag updated to Customer upon closing
üìÅ Folder Structure (Optional Git/Zip File):
üì¶ WhatsApp-Automation-Infridet/
‚îÇ
‚îú‚îÄ‚îÄ whatsapp-automation-n8n.json      # N8N Flowchart Import File
‚îú‚îÄ‚îÄ email-templates.docx              # Warm-up Email Scripts
‚îú‚îÄ‚îÄ whinta-api-integration.pdf        # API Documentation
‚îú‚îÄ‚îÄ crm-tagging-notes.txt             # CRM Tag Setup Details
‚îî‚îÄ‚îÄ readme.md                         # This Instruction File
üõ†Ô∏è Required Integrations & Setup
‚úÖ Fluent Forms (WordPress)
Embed form with Name, Email, Phone
Enable webhook to N8N: /lead-capture
‚úÖ Google Sheets
Use n8n-nodes-base.googleSheets node
Capture name, email, phone, source, timestamp
‚úÖ FluentCRM
REST API enabled
Push contact and assign tag New Lead
Setup Email Automation via tag trigger
‚úÖ SMTP Email (Optional)
Use Gmail SMTP or Brevo
Trigger email on form submission
‚úÖ Whinta.com (WhatsApp API)
Send POST request
Payload includes phone, message, sender_id
Customize message with personalization
üí¨ Sample WhatsApp Message:
Hey {{name}}, Gyan here from Account Craft üëã  
I saw your form submission ‚Äì would you like help in starting your YouTube journey this week?  
Let me know. I'm just one text away. ‚úÖ  
üìß Sample Email (Warmup Day 1):
Subject: Welcome to Account Craft üöÄ
Body:
Hi {{name}},
I‚Äôm Gyan from Account Craft. Thanks for joining us!
Here‚Äôs what‚Äôs coming next: exclusive videos, personalized tips, and real support to get your YouTube channel earning.
Let‚Äôs go!
‚Äì Gyan
üîÅ CRM Tag Updates:
Action Tag Assigned
On form fill New Lead
After WhatsApp Engaged
After sale closed Customer
üìå Final Output:
Once completed, the system will:
Log all leads into a database
Automatically send emails and WhatsApp messages
Notify your sales team
Update lead status without manual entry
Automation Template Designed & Deployed by
Infridet Solutions Private Limited
Smart Integrations. Seamless Business.
üåê www.infridetsolutions.com | üìû +91-8853354829"
Automate Solar Lead Qualification & Follow-ups with Google Sheets and Gmail,https://n8n.io/workflows/3794-automate-solar-lead-qualification-and-follow-ups-with-google-sheets-and-gmail/,"Automate Solar Lead Qualification & Follow-ups with Google Sheets and Gmail
Note: This template is designed for self-hosted n8n instances. The workflow image above shows the complete automation flow.
This n8n workflow automates the entire solar lead qualification process - from capturing lead information through a webhook, storing data in Google Sheets, evaluating qualification criteria, and sending personalized email follow-ups based on qualification status.
Who is this for?
This workflow is designed for:
Solar installation companies
Solar sales teams
Renewable energy consultants
Lead generation specialists in the solar industry
What problem does this workflow solve?
Managing solar leads efficiently can be challenging. This workflow solves several key pain points:
Time-consuming manual lead qualification: Automatically evaluates leads against predefined criteria
Inconsistent follow-up: Ensures every lead receives a timely, personalized response
Document management: Securely stores and shares utility bill documents
Lead tracking inefficiency: Centralizes lead data in Google Sheets with qualification status
What this workflow does
This workflow creates a complete solar lead management system that:
Captures lead information through a webhook endpoint
Securely stores utility bill uploads in Google Drive
Records all lead data in Google Sheets
Automatically evaluates leads based on three qualification criteria:
Homeownership status
Credit score (must be 650+)
Absence of trees on roof
Updates qualification status in the Google Sheet
Sends personalized email follow-ups based on qualification status:
Qualified leads receive a congratulatory email with next steps
Disqualified leads receive helpful information about why they didn't qualify and suggestions for remediation
Setup
Prerequisites
Before setting up this workflow, you'll need:
A self-hosted n8n instance
Google account with access to:
Google Sheets
Google Drive
Gmail
A form on your website that can make POST requests to a webhook
Step 1: Google Sheets Setup
Create a new Google Sheet for storing leads
Add the following columns in the first row (exact naming is important):
Name
Address
Has Trees on Roof
credit score
phone
Zip code
Email
Homeowner
utility bill
Qualification status
Disqualification reason
Step 2: Google Drive Setup
Sign in to your Google Drive account
Create a folder named ""Solar Lead Utility Bills"" (or your preferred name)
Right-click on the folder and select ""Share""
Set permissions to ""Anyone with the link can view""
Note the folder ID from the URL for configuration (the long string after /folders/ in the URL)
Step 3: Configure Google Credentials in n8n
In your n8n instance, go to Settings ‚Üí Credentials
Add credentials for:
Google Sheets: Create new credentials, follow OAuth2 authentication
Google Drive: Create new credentials, follow OAuth2 authentication
Gmail: Create new credentials, follow OAuth2 authentication
Ensure all credentials have the necessary scopes:
Google Sheets: .../auth/spreadsheets
Google Drive: .../auth/drive
Gmail: .../auth/gmail.send
Step 4: Import and Configure the Workflow
In n8n, go to Workflows ‚Üí Import from File
Upload the workflow JSON file
Update all Google Sheets nodes with your Google Sheet document ID:
Open your Google Sheet
Copy the ID from the URL (long string between /d/ and /edit)
Update the document ID field in the Google Sheets nodes
In the ""[STEP 2] Upload Utility Bill"" node, set the folder destination to your created folder
Step 5: Configure the Webhook
Activate the ""[STEP 1] Receive Form Submission"" webhook node
Copy the generated webhook URL
Configure your website form to send data to this URL
Ensure your form submits the following fields with exact naming:
firstName
lastName
address
hasTreesOnRoof
creditScore
phone
zipCode
email
homeOwnership
utilityBill (file upload)
Step 6: Customize Email Templates
Open the ""[STEP 10A] Send Acceptance Email"" node
Customize the email subject and message to match your company's branding
Open the ""[STEP 10B] Send Rejection Email"" node
Customize the rejection email to reflect your company's voice
Step 7: Activate and Test
Click ""Save"" on the workflow
Toggle the ""Active"" switch to activate the workflow
Submit a test lead through your form
Check that:
The data appears in your Google Sheet
The qualification status is updated correctly
The appropriate email is sent
How to customize this workflow to your needs
Adjusting Qualification Criteria
You can modify the qualification logic in the ""[STEP 7] Check Qualification Criteria"" node:
Open the node and click the ""Edit Code"" button
Locate the criteria sections (homeowner, credit score, trees on roof)
Modify the conditions as needed:
// Example: Change credit score threshold
if (creditScoreRaw.includes(""600 - 649"") ||
    creditScoreRaw.includes(""650 - 689"") ||
    creditScoreRaw.includes(""690 - 719"") ||
    creditScoreRaw.includes(""720+"")) {
    creditQualified = true;
}
Add additional criteria if needed
Customizing Email Templates
Personalize your emails further:
Open the email nodes
Use variable references to include more customer data:
Dear {{ $json.Name }},

We noticed your utility bill shows an average of {{ $json.monthlyBill }} per month.
With solar, you could save approximately {{ $json.monthlySavings }}.
Adding Integration with CRM Systems
Extend this workflow by connecting it to your CRM:
Add a Hubspot/Salesforce/etc. node after the ""[STEP 8] Update Qualification Status"" node
Configure the node to create or update contacts in your CRM
Map the lead data fields to your CRM fields
Troubleshooting
Common Issues
Webhook not receiving data
Verify your form is correctly configured to send POST requests
Check CORS settings on your website
Ensure all required fields are being sent
Google Drive upload failing
Check Google Drive permissions
Verify your OAuth scopes include drive.file
Ensure your Drive has sufficient storage space
Email not sending
Verify Gmail credentials
Check if Gmail API is enabled in your Google Cloud Console
Look for Send Rate Exceeded errors in execution logs
Google Sheets Column Format
If you're having issues with data not appearing correctly:
Make sure the column names exactly match those in the code
Check that the Google Sheet permissions allow editing
Verify the sheet name is correctly referenced in the nodes
Getting Help
If you encounter issues with this template, you can:
Check the n8n documentation on webhooks
Review Google Sheets integration documentation
Post in the n8n community forum
This template was created by David Olusola. If you find it helpful, please consider giving it a star in the n8n template library!"
üé• Gemini AI Video Analysis,https://n8n.io/workflows/3775-gemini-ai-video-analysis/,"üìù Overview
This workflow leverages Google Gemini 2.0 Flash multimodal AI to automatically generate detailed descriptions of video content from any public URL. It streamlines video understanding, making it ideal for content cataloging, accessibility, and content moderation.
üí° Use Cases
‚ôø Accessibility: Automatically generate detailed video descriptions for visually impaired users.
üõ°Ô∏è Content Moderation: Detect inappropriate or off-brand material without manual watching.
üóÇÔ∏è Media Cataloging: Enrich your media library with automatically extracted metadata.
üìà Marketing & Branding: Gain fast insights into key elements, tone, and branding in video content.
‚öôÔ∏è Setup Instructions
üîë Get a Gemini API Key
Register at ai.google.dev and create an API key.
Before running the workflow, set your Gemini API key as an environment variable named GeminiKey for secure access within the workflow.
In the Set Input node, reference this environment variable instead of hardcoding the key.
üåê Configure Video URL
Replace the sample URL in the Set Input node with your desired public video URL.
Ensure the video is directly accessible (no login or special permissions required).
üìù Optional: Customize the Analysis
Edit the prompt in the Analyze video Gemini node to focus on the most relevant video details for your use case (e.g., branding, key actions, visual elements).
üîí Security Tip
Use n8n's credentials manager or environment variables (like GeminiKey) to store your API key securely.
Avoid hardcoding API keys directly in workflow nodes, especially in production environments.
üîÑ How It Works
üì• Download the video from the provided URL.
‚òÅÔ∏è Upload the video to Gemini‚Äôs server for processing.
‚è≥ Wait for Gemini to complete processing.
ü§ñ Analyze the video with Gemini AI using your customized prompt.
üìÑ Output a comprehensive description of the video as videoDescription.
‚ö° Technical Details
Uses HTTP Request nodes to interact with Gemini API endpoints.
Handles file download, upload, status checking, and result retrieval.
Customizable Gemini AI parameters for fine-tuned response.
Main output: videoDescription (detailed text describing video content).
üöÄ Quickstart
Set your Gemini API key as the GeminiKey environment variable and configure your video URL in the workflow.
Execute the workflow.
Retrieve your rich, AI-generated video description for downstream use such as automation, tagging, or reporting."
Update Hubspot engagement by parsing inbox mail with AI,https://n8n.io/workflows/3767-update-hubspot-engagement-by-parsing-inbox-mail-with-ai/,"Who is this for?
This workflow is designed for Customer Success Managers (CSM), sales, support, or marketing teams using HubSpot CRM who want to automate customer engagement tracking when new emails arrive. It‚Äôs ideal for businesses looking to streamline CRM updates without manual data entry.
Problem Solved / Use Case
Manually logging email interactions in HubSpot is time-consuming. This workflow automatically parses incoming emails, checks if the sender exists in HubSpot, and either:
Creates a new contact + logs the email as an engagement (if the sender is new).
Logs the email as an engagement for an existing contact.
What This Workflow Does
Triggers when a new email arrives in a connected IMAP inbox.
Parses the email using AI (OpenAI) to extract structured data.
Searches HubSpot for the sender‚Äôs email address.
Updates HubSpot:
Creates a contact (if missing) and logs the email as an engagement.
Or logs the engagement for an existing contact.
Setup
Configure Email Account: Replace the default IMAP node with your email provider
HubSpot Credentials: Add your HubSpot API key in the HubSpot nodes.
OpenAI Integration: Ensure your OpenAI API key is set for email parsing.
Customization Tips
Improve AI Prompt: Modify the OpenAI prompt to extract specific email data (e.g., customer intent).
Add Filters: Exclude auto-replies or spam by adding a filter node.
Extend Functionality: Use the parsed data to trigger follow-up tasks (e.g., Slack alerts, tickets).
Need Help? Contact thomas@pollup.net for workflow modifications or help.
Discover my other workflows here"
AI-Powered Gmail MCP Server,https://n8n.io/workflows/3623-ai-powered-gmail-mcp-server/,"ü§ñ AI-Powered Gmail MCP Server for n8n
Description
This n8n workflow template leverages an external AI Model Control Plane (MCP) Server to automate various Gmail tasks, such as composing emails, replying to threads, and handling follow-ups using dynamically generated content. It uses the native n8n Gmail nodes available from v1.88.0 onwards.
Who is this template for?
Ideal for developers, automation engineers, and power users using self-hosted n8n (v1.88.0+) who want to integrate artificial intelligence directly into their email workflows via a dedicated MCP Server for enhanced control and customization over AI interactions.
What problem does this workflow solve?
‚öôÔ∏è Reduces Manual Effort: Decreases the work involved in writing, sending, and following up on emails in Gmail.
‚úÖ Consistency and Quality: Ensures standardized, professional responses free from typos by leveraging controlled AI prompts.
üîÑ Complete Automation: Automates the entire email cycle: from the initial send, through waiting for a reply, to sending automated follow-ups based on AI logic.
Workflow Overview
This template provides a structured approach to integrating Gmail with an MCP Server:
üì° MCP Trigger (‚ÄúMCP_GMAIL‚Äù): An n8n Webhook node that receives HTTP calls from your MCP Server. It standardizes the inputs (like recipient, subject, AI prompt) for all subsequent Gmail nodes. (You will need to configure your MCP Server to call this webhook URL).
üì§ SEND_EMAIL (Gmail Node v2.1): Sends new messages. The email body (message field) is typically populated by content generated from an AI prompt processed by your MCP server and passed via the trigger.
üîÑ REPLY_EMAIL (Gmail Node v2.1): Automatically replies to existing conversations (threads). It uses AI-generated content (via MCP) to formulate the reply based on the thread context. Requires Message ID and/or Thread ID.
üì• GET_EMAIL (Gmail Node v2.1): Fetches data for a specific message (using Message ID) for analysis, processing, or archiving. Useful for retrieving context before replying.
‚è≥ SEND_AND_WAIT (Gmail Node v2.1): Sends an email and pauses the workflow execution until a reply is received in that specific conversation (thread). This is crucial for building automated follow-up sequences. It then outputs data from the reply message.
Note: All Gmail nodes in this template use the native n8n Gmail Tool, integrated since v1.88.0. No additional installation of community nodes is required. See the official n8n documentation for more details on node configuration.*
Prerequisites
Ensure you have the following before importing:
üöÄ A self-hosted n8n instance running version 1.88.0 or higher.
‚òÅÔ∏è A Google Cloud project with the Gmail API correctly enabled.
üîë Gmail OAuth2 credentials configured in your n8n instance. Navigate to Settings > Credentials > New > Google > Gmail (OAuth2 API) to set this up if you haven't already.
üß† Access to your MCP Server and an API Key (or other authentication method) required to interact with it via HTTP requests.
How to Import and Configure
Follow these steps to get the template running:
In your n8n interface, navigate to Templates ‚Üí Import from URL.
Paste the JSON link provided for this workflow template.
Configure the necessary credentials within n8n under Credentials:
Gmail OAuth2: Select the Google OAuth2 credential you previously configured that has access to the desired Gmail account.
MCP API Key: You'll likely need to configure credentials for interacting with your MCP Server. This might involve setting up a Header Auth credential in n8n with your MCP API Key, or configuring the HTTP Request node within the workflow directly, depending on your MCP's authentication scheme. Link this credential where needed (e.g., in the Trigger node if MCP calls n8n with auth, or in HTTP Request nodes if n8n calls MCP).
Activate the Workflow: Ensure the workflow toggle is set to ""Active"" in the top right corner.
Webhook URL: Copy the Webhook URL from the ""MCP_GMAIL"" Trigger node (Test or Production URL as needed) and configure your MCP Server to send requests to this URL.
Recommendation:
Rename the nodes with clear, descriptive names relevant to your specific use case (e.g., ‚ú® Generate Sales Email Body via MCP, üì• Fetch Customer Replies).
Utilize the workflow notes (sticky notes on the canvas) to document specific logic, prompt details, or configuration choices for future reference.
Customization & Technical Guidance
Tailor the workflow to your specific needs:
üîç Search Filters: In nodes that fetch emails (like GET_EMAIL or if you add a Gmail - Get Many node), refine the search using the Search field (standard Gmail search operators) or filter by Label Names to process specific emails (e.g., unread from a specific sender, emails with a certain label).
‚úçÔ∏è AI Fine-Tuning (Prompt Engineering): The core of the AI integration happens in the prompts sent to your MCP Server. Modify these prompts (often constructed within function nodes or directly in the trigger input expected from MCP) in the message or body fields passed to the send/reply nodes. Adjust prompts to control:
Tone & Style: Formal, informal, empathetic, technical, etc.
Content & Format: Request bullet points, summaries, specific data extraction.
Dynamic Variables: Inject data from previous n8n nodes (e.g., customer name, order details, previous email content) into the prompt for context-aware generation. Example: ""Reply to the following email thread [{{ $json.thread_content }}] addressing the customer {{ $json.customer_name }} about their query...""
üîó Post-Response Actions: Extend the workflow after key actions, especially the SEND_AND_WAIT node. Add nodes to:
Log results to a database (MySQL, PostgreSQL, Airtable).
Update CRM records (HubSpot, Salesforce).
Send notifications (Slack, Discord, Telegram).
Trigger other n8n workflows.
üõ°Ô∏è Error Handling: Implement robust error handling. Connect the red output pins (error output) of critical nodes (like Gmail nodes or HTTP Requests to MCP) to an Error Trigger node. From there, you can:
Log detailed error information to a monitoring tool or spreadsheet.
Send failure notifications.
Implement retry logic (using loops or specific retry settings on nodes).
Route to alternative paths or fallback workflows."
"üå≥ EU Green Legislation Tracker with GPT-4o, Google Sheets and Tasks",https://n8n.io/workflows/3644-eu-green-legislation-tracker-with-gpt-4o-google-sheets-and-tasks/,"Tags: EU Legislation, Sustainability, Automation, Web Scraping, OpenAI, Google Sheets, Policy Monitoring, Climate
Context
Hey! I‚Äôm Samir, a Supply Chain Engineer and Data Scientist from Paris, and the founder of LogiGreen Consulting.
We use AI, automation, and data to support sustainable business practices for small, medium and large companies.
This workflow is part of our broader initiative to monitor and act on sustainability legislation in Europe.
How do you know if new EU laws will impact your business's sustainability goals?
This n8n workflow automatically scrapes the EU Parliament‚Äôs legislative portal to find and flag procedures related to environmental sustainability.
üì¨ For business inquiries, feel free to connect with me on LinkedIn
Who is this template for?
This workflow is useful for:
Sustainability consultants monitoring legal frameworks
NGOs and researchers tracking environmental regulations
Companies aligning with CSRD or EU Green Deal objectives
Policy analysts looking for automation tools
What does it do?
This n8n workflow:
üåê Scrapes the EU Parliament legislative portal for yesterday‚Äôs entries
üß† Uses OpenAI to classify if each procedure is related to sustainability
üóÇÔ∏è Filters out irrelevant items
üìä Saves the results in a Google Sheet
‚úÖ Creates a Google Task for each relevant file to review the legislation
How it works
Trigger manually or on schedule
Scrape HTML blocks for scheduled debates
Parse each procedure to extract Title, Committee, Rapporteur, PDF link
Call GPT-4-turbo to check if the topic matches sustainability criteria
Filter responses based on ‚Äúyes‚Äù or ‚Äúno‚Äù
Store valid items into Google Sheets
Generate tasks in Google Tasks
The AI only flags procedures that directly impact the environment, circular economy, or pollution control.
What do I need to get started?
You‚Äôll need:
A Google Sheet connected to your n8n instance
An OpenAI account with GPT-4 access
A Google Task List
Follow the Guide!
Follow the sticky notes in the workflow or check my tutorial to configure each node and start using AI to monitor sustainability regulations in Europe.
üé• Watch My Tutorial
Notes
AI filters are strict ‚Äî you can customise the system prompt to match your needs
This is ideal for tracking legislative risk for climate regulations
This workflow was built using n8n version 1.85.4
Submitted: April 21, 2025"
